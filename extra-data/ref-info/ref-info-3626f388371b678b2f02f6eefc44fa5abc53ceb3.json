{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145414142"
                        ],
                        "name": "B. Chandrasekaran",
                        "slug": "B.-Chandrasekaran",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Chandrasekaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chandrasekaran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116860574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55ab64d91344cdde7d4959a181c7652245c19597",
            "isKey": false,
            "numCitedBy": 575,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "39-Dimensionality-and-sample-size-considerations-in-Jain-Chandrasekaran",
            "title": {
                "fragments": [],
                "text": "39 Dimensionality and sample size considerations in pattern recognition practice"
            },
            "venue": {
                "fragments": [],
                "text": "Classification, Pattern Recognition and Reduction of Dimensionality"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4453577"
                        ],
                        "name": "D. Ridder",
                        "slug": "D.-Ridder",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Ridder",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ridder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2743835"
                        ],
                        "name": "D. Tax",
                        "slug": "D.-Tax",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tax",
                            "middleNames": [
                                "M.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tax"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 17293953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c2cccc417b32112f3fd1a4b471bdb3fdf734db1",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Experiments-with-a-featureless-approach-to-pattern-Duin-Ridder",
            "title": {
                "fragments": [],
                "text": "Experiments with a featureless approach to pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087848"
                        ],
                        "name": "R. Dubes",
                        "slug": "R.-Dubes",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dubes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dubes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830341"
                        ],
                        "name": "Chaur-Chin Chen",
                        "slug": "Chaur-Chin-Chen",
                        "structuredName": {
                            "firstName": "Chaur-Chin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaur-Chin Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15415663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832b00e86a2cf638124128825210ab7789c7b7f6",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of a pattern recognition system requires careful attention to error estimation. The error rate is the most important descriptor of a classifier's performance. The commonly used estimates of error rate are based on the holdout method, the resubstitution method, and the leave-one-out method. All suffer either from large bias or large variance and their sample distributions are not known. Bootstrapping refers to a class of procedures that resample given data by computer. It permits determining the statistical properties of an estimator when very little is known about the underlying distribution and no additional samples are available. Since its publication in the last decade, the bootstrap technique has been successfully applied to many statistical estimations and inference problems. However, it has not been exploited in the design of pattern recognition systems. We report results on the application of several bootstrap techniques in estimating the error rate of 1-NN and quadratic classifiers. Our experiments show that, in most cases, the confidence interval of a bootstrap estimator of classification error is smaller than that of the leave-one-out estimator. The error of 1-NN, quadratic, and Fisher classifiers are estimated for several real data sets."
            },
            "slug": "Bootstrap-Techniques-for-Error-Estimation-Jain-Dubes",
            "title": {
                "fragments": [],
                "text": "Bootstrap Techniques for Error Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Results on the application of several bootstrap techniques in estimating the error rate of 1-NN and quadratic classifiers show that, in most cases, the confidence interval of a bootstrap estimator of classification error is smaller than that of the leave-one-out estimator."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30545896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "356125478f5d06b564b420755a4944254045bbbe",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology."
            },
            "slug": "Support-vector-learning-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This book provides a comprehensive analysis of what can be done using Support vector Machines, achieving record results in real-life pattern recognition problems, and proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which it is considered as the most natural and elegant way for generalization of classical Principal Component analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18208850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145d334f7d60bb5a4ed904365eeb035f068fa826",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews statistical, adaptive, and heuristic techniques used in laboratory investigations of pattern recognition problems. The discussion includes correlation methods, discriminant analysis, maximum likelihood decisions minimax techniques, perceptron-like algorithms, feature extraction, preprocessing, clustering and nonsupervised learning. Two-dimensional distributions are used to illustrate the properties of the various procedures. Several experimental projects, representative of prospective applications, are also described."
            },
            "slug": "State-of-the-art-in-pattern-recognition-Nagy",
            "title": {
                "fragments": [],
                "text": "State of the art in pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper reviews statistical, adaptive, and heuristic techniques used in laboratory investigations of pattern recognition problems and includes correlation methods, discriminant analysis, maximum likelihood decisions minimax techniques, perceptron-like algorithms, feature extraction, preprocessing, clustering and nonsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8337664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87d4d0f14181e19b4df27aa266eae69f4711ce77",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Classical feature extraction and data projection methods have been well studied in the pattern recognition and exploratory data analysis literature. We propose a number of networks and learning algorithms which provide new or alternative tools for feature extraction and data projection. These networks include a network (SAMANN) for J.W. Sammon's (1969) nonlinear projection, a linear discriminant analysis (LDA) network, a nonlinear discriminant analysis (NDA) network, and a network for nonlinear projection (NP-SOM) based on Kohonen's self-organizing map. A common attribute of these networks is that they all employ adaptive learning algorithms which makes them suitable in some environments where the distribution of patterns in feature space changes with respect to time. The availability of these networks also facilitates hardware implementation of well-known classical feature extraction and projection approaches. Moreover, the SAMANN network offers the generalization ability of projecting new data, which is not present in the original Sammon's projection algorithm; the NDA method and NP-SOM network provide new powerful approaches for visualizing high dimensional data. We evaluate five representative neural networks for feature extraction and data projection based on a visual judgement of the two-dimensional projection maps and three quantitative criteria on eight data sets with various properties."
            },
            "slug": "Artificial-neural-networks-for-feature-extraction-Mao-Jain",
            "title": {
                "fragments": [],
                "text": "Artificial neural networks for feature extraction and multivariate data projection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The SAMANN network offers the generalization ability of projecting new data, which is not present in the original Sammon's projection algorithm; the NDA method and NP-SOM network provide new powerful approaches for visualizing high dimensional data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6686370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >"
            },
            "slug": "Automatic-Pattern-Recognition:-A-Study-of-the-of-Devroye",
            "title": {
                "fragments": [],
                "text": "Automatic Pattern Recognition: A Study of the Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Vapnik-Chervonenkis method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597670"
                        ],
                        "name": "L. Kanal",
                        "slug": "L.-Kanal",
                        "structuredName": {
                            "firstName": "Laveen",
                            "lastName": "Kanal",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kanal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32356452,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "a9d03e50751ec8daad969db65adfde91add224e2",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper selectively surveys contributions to major topics in pattern recognition since 1968. Representative books and surveys pattern recognition published during this period are listed. Theoretical models for automatic pattern recognition are contrasted with practical,, design methodology. Research contributions to statistical and structural pattern recognition are selectively discussed, including contributions to error estimation and the experimental design of pattern classifiers. The survey concludes with a representative set of applications of pattern recognition technology."
            },
            "slug": "Patterns-in-pattern-recognition:-1968-1974-Kanal",
            "title": {
                "fragments": [],
                "text": "Patterns in pattern recognition: 1968-1974"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper selectively surveys contributions to major topics in pattern recognition since 1968, including contributions to error estimation and the experimental design of pattern classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129654"
                        ],
                        "name": "S. Raudys",
                        "slug": "S.-Raudys",
                        "structuredName": {
                            "firstName": "Sarunas",
                            "lastName": "Raudys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335089"
                        ],
                        "name": "Vitalijus Pikelis",
                        "slug": "Vitalijus-Pikelis",
                        "structuredName": {
                            "firstName": "Vitalijus",
                            "lastName": "Pikelis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vitalijus Pikelis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15314450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4f4ee04501dfb66d8fd9a2c208845e661a6008a",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares four classification algorithms-discriminant functions when classifying individuals into two multivariate populations. The discriminant functions (DF's) compared are derived according to the Bayes rule for normal populations and differ in assumptions on the covariance matrices' structure. Analytical formulas for the expected probability of misclassification EPN are derived and show that the classification error EPN depends on the structure of a classification algorithm, asymptotic probability of misclassification P\u00bf, and the ratio of learning sample size N to dimensionality p:N/p for all linear DF's discussed and N2/p for quadratic DF's. The tables for learning quantity H = EPN/P\u00bf depending on parameters P\u00bf, N, and p for four classifilcation algorithms analyzed are presented and may be used for estimating the necessary learning sample size, detennining the optimal number of features, and choosing the type of the classification algorithm in the case of a limited learning sample size."
            },
            "slug": "On-Dimensionality,-Sample-Size,-Classification-and-Raudys-Pikelis",
            "title": {
                "fragments": [],
                "text": "On Dimensionality, Sample Size, Classification Error, and Complexity of Classification Algorithm in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Four classification algorithms-discriminant functions when classifying individuals into two multivariate populations are compared and it is shown that the classification error EPN depends on the structure of a classification algorithm, asymptotic probability of misclassification P\u00bf, and the ratio of learning sample size N to dimensionality p:N/p."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31129825"
                        ],
                        "name": "A. Webb",
                        "slug": "A.-Webb",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Webb",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Webb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206421762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8582dd12391ea0d5fb6f2ae3ebf520f7f730dff8",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of multiclass pattern classification using adaptive layered networks is addressed. A special class of networks, i.e., feed-forward networks with a linear final layer, that perform generalized linear discriminant analysis is discussed, This class is sufficiently generic to encompass the behavior of arbitrary feed-forward nonlinear networks. Training the network consists of a least-square approach which combines a generalized inverse computation to solve for the final layer weights, together with a nonlinear optimization scheme to solve for parameters of the nonlinearities. A general analytic form for the feature extraction criterion is derived, and it is interpreted for specific forms of target coding and error weighting. An important aspect of the approach is to exhibit how a priori information regarding nonuniform class membership, uneven distribution between train and test sets, and misclassification costs may be exploited in a regularized manner in the training phase of networks. >"
            },
            "slug": "Optimized-Feature-Extraction-and-the-Bayes-Decision-Lowe-Webb",
            "title": {
                "fragments": [],
                "text": "Optimized Feature Extraction and the Bayes Decision in Feed-Forward Classifier Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An important aspect of the approach is to exhibit how a priori information regarding nonuniform class membership, uneven distribution between train and test sets, and misclassification costs may be exploited in a regularized manner in the training phase of networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145652961"
                        ],
                        "name": "L. Xu",
                        "slug": "L.-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9745969"
                        ],
                        "name": "A. Krzy\u017cak",
                        "slug": "A.-Krzy\u017cak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Krzy\u017cak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krzy\u017cak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206400534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc",
            "isKey": false,
            "numCitedBy": 2369,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Possible solutions to the problem of combining classifiers can be divided into three categories according to the levels of information available from the various classifiers. Four approaches based on different methodologies are proposed for solving this problem. One is suitable for combining individual classifiers such as Bayesian, k-nearest-neighbor, and various distance classifiers. The other three could be used for combining any kind of individual classifiers. On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers can be improved significantly. For example, on the US zipcode database, 98.9% recognition with 0.90% substitution and 0.2% rejection can be obtained, as well as high reliability with 95% recognition, 0% substitution, and 5% rejection. >"
            },
            "slug": "Methods-of-combining-multiple-classifiers-and-their-Xu-Krzy\u017cak",
            "title": {
                "fragments": [],
                "text": "Methods of combining multiple classifiers and their applications to handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers can be improved significantly."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1900499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4a422669ec9b6a60b05d2d2595314008a5fb419",
            "isKey": false,
            "numCitedBy": 1314,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "slug": "Comparing-support-vector-machines-with-Gaussian-to-Sch\u00f6lkopf-Sung",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system, and the SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795578"
                        ],
                        "name": "T. Ho",
                        "slug": "T.-Ho",
                        "structuredName": {
                            "firstName": "Tin",
                            "lastName": "Ho",
                            "middleNames": [
                                "Kam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8412354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d6d5cfa8e99dd53e50bf870e24e72b0be7f7aeb",
            "isKey": false,
            "numCitedBy": 1676,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "A multiple classifier system is a powerful solution to difficult pattern recognition problems involving large class sets and noisy input because it allows simultaneous use of arbitrary feature descriptors and classification procedures. Decisions by the classifiers can be represented as rankings of classifiers and different instances of a problem. The rankings can be combined by methods that either reduce or rerank a given set of classes. An intersection method and union method are proposed for class set reduction. Three methods based on the highest rank, the Borda count, and logistic regression are proposed for class set reranking. These methods have been tested in applications of degraded machine-printed characters and works from large lexicons, resulting in substantial improvement in overall correctness. >"
            },
            "slug": "Decision-Combination-in-Multiple-Classifier-Systems-Ho-Hull",
            "title": {
                "fragments": [],
                "text": "Decision Combination in Multiple Classifier Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes three methods based on the highest rank, the Borda count, and logistic regression for class set reranking that have been tested in applications of degraded machine-printed characters and works from large lexicons, resulting in substantial improvement in overall correctness."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14159881,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "20ce95262aa2781c2c3127ca77f18afece3c8f69",
            "isKey": false,
            "numCitedBy": 2627,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a systematic account of the subject area, concentrating on the most recent advances in the field. While the focus is on practical considerations, both theoretical and practical issues are explored. Among the advances covered are: regularized discriminant analysis and bootstrap-based assessment of the performance of a sample-based discriminant rule and extensions of discriminant analysis motivated by problems in statistical image analysis. Includes over 1,200 references in the bibliography."
            },
            "slug": "Discriminant-Analysis-and-Statistical-Pattern-McLachlan",
            "title": {
                "fragments": [],
                "text": "Discriminant Analysis and Statistical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 216
                            }
                        ],
                        "text": "As the number of feature subset evaluations may easily become prohibitive for large feature sizes, a number of suboptimal selection techniques have been proposed which essentially tradeoff the optimality of the selected subset for computational efficiency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The recognition system is operated in two modes: training (learning) and classification (testing) (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": true,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145822375"
                        ],
                        "name": "G. Toussaint",
                        "slug": "G.-Toussaint",
                        "structuredName": {
                            "firstName": "Godfried",
                            "lastName": "Toussaint",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Toussaint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4659657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "848bf8ed51d0dfbef7f7b06b7728d0aab2ff3dc5",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-use-of-context-in-pattern-recognition-Toussaint",
            "title": {
                "fragments": [],
                "text": "The use of context in pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34413349"
                        ],
                        "name": "Youngtae Park",
                        "slug": "Youngtae-Park",
                        "structuredName": {
                            "firstName": "Youngtae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngtae Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765522"
                        ],
                        "name": "J. Sklansky",
                        "slug": "J.-Sklansky",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Sklansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sklansky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5492789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6334aa47fb727069b10a5bd2ffece0394d3cbd5",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-design-of-linear-tree-classifiers-Park-Sklansky",
            "title": {
                "fragments": [],
                "text": "Automated design of linear tree classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2789496"
                        ],
                        "name": "Chulhee Lee",
                        "slug": "Chulhee-Lee",
                        "structuredName": {
                            "firstName": "Chulhee",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chulhee Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2045747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b56634bc0cad05e7adffd48aa0609616c3c10b2e",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach to feature extraction for classification based directly on the decision boundaries is proposed. It is shown how discriminantly redundant features and discriminantly informative features are related to decision boundaries. A procedure to extract discriminantly informative features based on a decision boundary is proposed. The proposed feature extraction algorithm has several desirable properties: (1) it predicts the minimum number of features necessary to achieve the same classification accuracy as in the original space for a given pattern recognition problem; and (2) it finds the necessary feature vectors. The proposed algorithm does not deteriorate under the circumstances of equal class means or equal class covariances as some previous algorithms do. Experiments show that the performance of the proposed algorithm compares favorably with those of previous algorithms. >"
            },
            "slug": "Feature-Extraction-Based-on-Decision-Boundaries-Lee-Landgrebe",
            "title": {
                "fragments": [],
                "text": "Feature Extraction Based on Decision Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed feature extraction algorithm has several desirable properties: it predicts the minimum number of features necessary to achieve the same classification accuracy as in the original space for a given pattern recognition problem; and it finds the necessary feature vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697083"
                        ],
                        "name": "R. Setiono",
                        "slug": "R.-Setiono",
                        "structuredName": {
                            "firstName": "Rudy",
                            "lastName": "Setiono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Setiono"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38746648"
                        ],
                        "name": "Huan Liu",
                        "slug": "Huan-Liu",
                        "structuredName": {
                            "firstName": "Huan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huan Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7612105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b27ad82f6c660f1881f725b33015e3ba9ff5853",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature selection is an integral part of most learning algorithms. Due to the existence of irrelevant and redundant attributes, by selecting only the relevant attributes of the data, higher predictive accuracy can be expected from a machine learning method. In this paper, we propose the use of a three-layer feedforward neural network to select those input attributes that are most useful for discriminating classes in a given set of input patterns. A network pruning algorithm is the foundation of the proposed algorithm. By adding a penalty term to the error function of the network, redundant network connections can be distinguished from those relevant ones by their small weights when the network training process has been completed. A simple criterion to remove an attribute based on the accuracy rate of the network is developed. The network is retrained after removal of an attribute, and the selection process is repeated until no attribute meets the criterion for removal. Our experimental results suggest that the proposed method works very well on a wide variety of classification problems."
            },
            "slug": "Neural-network-feature-selector-Setiono-Liu",
            "title": {
                "fragments": [],
                "text": "Neural-network feature selector"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes the use of a three-layer feedforward neural network to select those input attributes that are most useful for discriminating classes in a given set of input patterns."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6175119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ce8bc485df9ac987f18d99c7af1d95f9cbea6b2",
            "isKey": false,
            "numCitedBy": 952,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "The deterministic annealing approach to clustering and its extensions has demonstrated substantial performance improvement over standard supervised and unsupervised learning methods in a variety of important applications including compression, estimation, pattern recognition and classification, and statistical regression. The application-specific cost is minimized subject to a constraint on the randomness of the solution, which is gradually lowered. We emphasize the intuition gained from analogy to statistical physics. Alternatively the method is derived within rate-distortion theory, where the annealing process is equivalent to computation of Shannon's rate-distortion function, and the annealing temperature is inversely proportional to the slope of the curve. The basic algorithm is extended by incorporating structural constraints to allow optimization of numerous popular structures including vector quantizers, decision trees, multilayer perceptrons, radial basis functions, and mixtures of experts."
            },
            "slug": "Deterministic-annealing-for-clustering,-regression,-Rose",
            "title": {
                "fragments": [],
                "text": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The deterministic annealing approach to clustering and its extensions has demonstrated substantial performance improvement over standard supervised and unsupervised learning methods in a variety of important applications including compression, estimation, pattern recognition and classification, and statistical regression."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8568062"
                        ],
                        "name": "C. Chatterjee",
                        "slug": "C.-Chatterjee",
                        "structuredName": {
                            "firstName": "Chanchal",
                            "lastName": "Chatterjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chatterjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686063"
                        ],
                        "name": "V. Roychowdhury",
                        "slug": "V.-Roychowdhury",
                        "structuredName": {
                            "firstName": "Vwani",
                            "lastName": "Roychowdhury",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Roychowdhury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 45253822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acdfaf33b52b6e970ce89aeba2a0e23346482e3c",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe self-organizing learning algorithms and associated neural networks to extract features that are effective for preserving class separability. As a first step, an adaptive algorithm for the computation of Q(-1/2) (where Q is the correlation or covariance matrix of a random vector sequence) is described. Convergence of this algorithm with probability one is proven by using stochastic approximation theory, and a single-layer linear network architecture for this algorithm is described, which we call the Q(-1/2) network. Using this network, we describe feature extraction architectures for: 1) unimodal and multicluster Gaussian data in the multiclass case; 2) multivariate linear discriminant analysis (LDA) in the multiclass case; and 3) Bhattacharyya distance measure for the two-class case. The LDA and Bhattacharyya distance features are extracted by concatenating the Q (-1/2) network with a principal component analysis network, and the two-layer network is proven to converge with probability one. Every network discussed in the study considers a flow or sequence of inputs for training. Numerical studies on the performance of the networks for multiclass random data are presented."
            },
            "slug": "On-self-organizing-algorithms-and-networks-for-Chatterjee-Roychowdhury",
            "title": {
                "fragments": [],
                "text": "On self-organizing algorithms and networks for class-separability features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An adaptive algorithm for the computation of Q(-1/2) (where Q is the correlation or covariance matrix of a random vector sequence) is described, and convergence of this algorithm with probability one is proven by using stochastic approximation theory."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129654"
                        ],
                        "name": "S. Raudys",
                        "slug": "S.-Raudys",
                        "structuredName": {
                            "firstName": "Sarunas",
                            "lastName": "Raudys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudys"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10058367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0df5fc4bd230e5ed26f69168e7f4809bedd0673d",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolution-and-generalization-of-a-single-neurone:-Raudys",
            "title": {
                "fragments": [],
                "text": "Evolution and generalization of a single neurone: I. Single-layer perceptron as seven statistical classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058879081"
                        ],
                        "name": "J. Sch\u00fcrmann",
                        "slug": "J.-Sch\u00fcrmann",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Sch\u00fcrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sch\u00fcrmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35830716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbc8a7f43f18b69d9c82baff37d1b984e85c1c8f",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Decision Theory. Need for Approximations: Fundamental Approaches. Classification Based on Statistical Models Determined by First-and-Second Order Statistical Moments. Classification Based on Mean-Square Functional Approximations. Polynomial Regression. Multilayer Perceptron Regression. Radial Basis Functions. Measurements, Features, and Feature Section. Reject Criteria and Classifier Performance. Combining Classifiers. Conclusion. STATMOD Program: Description of ftp Package. References. Index."
            },
            "slug": "Pattern-classification-a-unified-view-of-and-neural-Sch\u00fcrmann",
            "title": {
                "fragments": [],
                "text": "Pattern classification - a unified view of statistical and neural approaches"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720816"
                        ],
                        "name": "P. Chou",
                        "slug": "P.-Chou",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Chou",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12866188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9576c0f4202ea4e6b30f9c13b397e3ccc5476250",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative algorithm that finds a locally optimal partition for an arbitrary loss function, in time linear in N for each iteration is presented. The algorithm is a K-means-like clustering algorithm that uses as its distance measure a generalization of Kullback's information divergence. Moreover, it is proven that the globally optimal partition must satisfy a nearest neighbour condition using divergence as the distance measure. These results generalize similar results of L. Breiman et al. (1984) to an arbitrary number of classes or regression variables and to an arbitrary number of bills. Experimental results on a text-to-speech example are provided and additional applications of the algorithm, including the design of variable combinations, surrogate splits, composite nodes, and decision graphs, are suggested. >"
            },
            "slug": "Optimal-Partitioning-for-Classification-and-Trees-Chou",
            "title": {
                "fragments": [],
                "text": "Optimal Partitioning for Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An iterative algorithm that finds a locally optimal partition for an arbitrary loss function, in time linear in N for each iteration, is presented and it is proven that the globally optimal partition must satisfy a nearest neighbour condition using divergence as the distance measure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938732"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19666061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3132c544e52ea158928a0f39b54ca335cba5517",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 183,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision researchers have been frustrated in their attempts to automatically derive depth information from conventional two-dimensional intensity images. Research on \"shape from texture\", \"shape from shading\", and \"shape from focus\" is still in a laboratory stage and had not seen much use in commercial machine vision systems. A range image or a depth map contains explicit information about the distance from the sensor to the object surfaces within the field of view in the scene. Information about \"surface geometry\" which is important for, say, three-dimensional object recognition is more easily extracted from \"2 1/2 D\" range images than from \"2D\" intensity images. As a result, both active sensors such as laser range finders and passive techniques such as multi-camera stereo vision are being increasingly utilized by vision researchers to solve a variety of problems. This book contains chapters written by distinguished computer vision researchers covering the following areas: Overview of 3D Vision Range Sensing Geometric Processing Object Recognition Navigation Inspection Multisensor Fusion A workshop report, written by the editors, also appears in the book. It summarizes the state of the art and proposes future research directions in range image sensing, processing, interpretation, and applications. The book also contains an extensive, up-to-date bibliography on the above topics. This book provides a unique perspective on the problem of three-dimensional sensing and processing; it is the only comprehensive collection of papers devoted to range images. Both academic researchers interested in research issues in 3D vision and industrial engineers in search of solutions to particular problems will find this a useful reference book."
            },
            "slug": "Analysis-and-Interpretation-of-Range-Images-Jain-Jain",
            "title": {
                "fragments": [],
                "text": "Analysis and Interpretation of Range Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book provides a unique perspective on the problem of three-dimensional sensing and processing; it is the only comprehensive collection of papers devoted to range images."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Perception Engineering"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9175683"
                        ],
                        "name": "Michael L. Baird",
                        "slug": "Michael-L.-Baird",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Baird",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael L. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Deformable template models\n[69] or rubber sheet deformations [9] can be used to match\npatterns when the deformation cannot be easily explained\nor modeled directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14939484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15db30ddcd46d8f3dfb611d6f437d14660807d2b",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "recognition. Classic applications are illustrated such as Shaw's picture description language (1969) applied to bubble chamber photographs, and Ledley's chromosome grammar ( 1965). The student with no particular interest in pattern recognition, who nevertheless desires to grasp a quick overview of what formal language theory is all about, could read this chapter. A minimum of preliminary definitions and notation must be waded through before learning about-types of grammars (regular, context-free, context-sensitive), equivalency of grammars, syntaxdirected translations, and deterministic, nondeterministic, and stochastic systems. Higher dimensional grammars are treated in Chapter Three, i.e., those allowing more complex primitive description and interconnection capabilities. Tree, web, plex, and shape grammars are each described with accompanying examples which make the material very easy to absorb. Chapter Four, on recognition and translation of syntactic structures, presents that integral part of formal language theory known as automata theory. The one-to-one correspondences between types of string grammars and automata are presented here, namely finite automation/regular grammar, push-down automaton/context-free grammar, linear-bounded automaton/ context-sensitive grammar, and Turing machine/unrestricted grammar. Quite properly, the authors restrict their attention to the finite and push-down automata used to accept or reject input strings and to automata for tree recognition, since these are the models thus far proved to be most useful for syntactic pattern recognition tasks. Chapter Five introduces stocastic grammars, languages, and recognizers. The material is well covered and brings the reader up to date (1978). As the authors mention, the impact of this theory on syntactic pattern processing has not been thoroughly realized, and references are cited for one interested in pursuing the topic further. The final chapter deals with grammatical inference and exemplifies the principalideas underlying the problem of obtaining a pattern grammar from a set of samples. Again, the material in this chapter is presented in a logical and easy to understand format. By this time the reader is beginning to understand the limitations of the linguistic aspect of syntactic pattern recognition (Reviewer's opinion), and will draw his or her attention to some of the material which came out of this work in the late sixties, which can be described as structural pattern recognition, and which is the topic of the book review which follows."
            },
            "slug": "Structural-Pattern-Recognition-Baird",
            "title": {
                "fragments": [],
                "text": "Structural Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors restrict their attention to the finite and push-down automata used to accept or reject input strings and to automata for tree recognition, since these are the models thus far proved to be most useful for syntactic pattern recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206447772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "455d9a4ff96561d543acbcb2aa81d6cd8fcd20df",
            "isKey": false,
            "numCitedBy": 2522,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "slug": "Trends-&-Controversies:-Support-Vector-Machines-Hearst",
            "title": {
                "fragments": [],
                "text": "Trends & Controversies: Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This issue's collection of essays should help familiarize readers with this interesting new racehorse in the Machine Learning stable, and give a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711099"
                        ],
                        "name": "Kagan Tumer",
                        "slug": "Kagan-Tumer",
                        "structuredName": {
                            "firstName": "Kagan",
                            "lastName": "Tumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kagan Tumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724702"
                        ],
                        "name": "Joydeep Ghosh",
                        "slug": "Joydeep-Ghosh",
                        "structuredName": {
                            "firstName": "Joydeep",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joydeep Ghosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1153716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d57e848b78a167a41206a44d3cb10e5de3e4f75",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-decision-boundaries-in-linearly-neural-Tumer-Ghosh",
            "title": {
                "fragments": [],
                "text": "Analysis of decision boundaries in linearly combined neural classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49554480"
                        ],
                        "name": "S. Mishra",
                        "slug": "S.-Mishra",
                        "structuredName": {
                            "firstName": "Subhada",
                            "lastName": "Mishra",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714283"
                        ],
                        "name": "Vijay V. Raghavan",
                        "slug": "Vijay-V.-Raghavan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Raghavan",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay V. Raghavan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61049857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20ad23b33710abf7c5cbd1669aabdb84c3f4fabb",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-empirical-study-of-the-performance-of-heuristic-Mishra-Raghavan",
            "title": {
                "fragments": [],
                "text": "An empirical study of the performance of heuristic methods for clustering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746315"
                        ],
                        "name": "H. Frigui",
                        "slug": "H.-Frigui",
                        "structuredName": {
                            "firstName": "Hichem",
                            "lastName": "Frigui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Frigui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769461"
                        ],
                        "name": "R. Krishnapuram",
                        "slug": "R.-Krishnapuram",
                        "structuredName": {
                            "firstName": "Raghu",
                            "lastName": "Krishnapuram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Krishnapuram"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 246
                            }
                        ],
                        "text": "The speed, reliability, and consistency with which a clustering algorithm can organize large amounts of data constitute overwhelming reasons to use it in applications such as data mining [88], information retrieval [17], [25], image segmentation [55], signal compression and coding [1], and machine learning [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2221217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e55b8f9f9bad54e0411e06e06841e8e3bd4f4604",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses three major issues associated with conventional partitional clustering, namely, sensitivity to initialization, difficulty in determining the number of clusters, and sensitivity to noise and outliers. The proposed robust competitive agglomeration (RCA) algorithm starts with a large number of clusters to reduce the sensitivity to initialization, and determines the actual number of clusters by a process of competitive agglomeration. Noise immunity is achieved by incorporating concepts from robust statistics into the algorithm. RCA assigns two different sets of weights for each data point: the first set of constrained weights represents degrees of sharing, and is used to create a competitive environment and to generate a fuzzy partition of the data set. The second set corresponds to robust weights, and is used to obtain robust estimates of the cluster prototypes. By choosing an appropriate distance measure in the objective function, RCA can be used to find an unknown number of clusters of various shapes in noisy data sets, as well as to fit an unknown number of parametric models simultaneously. Several examples, such as clustering/mixture decomposition, line/plane fitting, segmentation of range images, and estimation of motion parameters of multiple objects, are shown."
            },
            "slug": "A-Robust-Competitive-Clustering-Algorithm-With-in-Frigui-Krishnapuram",
            "title": {
                "fragments": [],
                "text": "A Robust Competitive Clustering Algorithm With Applications in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper addresses three major issues associated with conventional partitional clustering, namely, sensitivity to initialization, difficulty in determining the number of clusters, and sensitivity to noise and outliers with the proposed robust competitive agglomeration (RCA)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527920"
                        ],
                        "name": "Douglas E. Zongker",
                        "slug": "Douglas-E.-Zongker",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Zongker",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas E. Zongker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1764288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5b31aaefd943b8eb3334fda53311b06f0bdbf5",
            "isKey": false,
            "numCitedBy": 2259,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations."
            },
            "slug": "Feature-Selection:-Evaluation,-Application,-and-Jain-Zongker",
            "title": {
                "fragments": [],
                "text": "Feature Selection: Evaluation, Application, and Small Sample Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work studies the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models and shows that pooling features derived from different texture Models, followed by a feature selection results in a substantial improvement in the classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702517"
                        ],
                        "name": "I. Sethi",
                        "slug": "I.-Sethi",
                        "structuredName": {
                            "firstName": "Ishwar",
                            "lastName": "Sethi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3266532"
                        ],
                        "name": "G. P. R. Sarvarayudu",
                        "slug": "G.-P.-R.-Sarvarayudu",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Sarvarayudu",
                            "middleNames": [
                                "P.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. P. R. Sarvarayudu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18707355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66ee4bb5d704856f43d751c259a7b5de9c77b764",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonparametric algorithm is presented for the hierarchical partitioning of the feature space. The algorithm is based on the concept of average mutual information, and is suitable for multifeature multicategory pattern recognition problems. The algorithm generates an efficient partitioning tree for specified probability of error by maximizing the amount of average mutual information gain at each partitioning step. A confidence bound expression is presented for the resulting classifier. Three examples, including one of handprinted numeral recognition, are presented to demonstrate the effectiveness of the algorithm."
            },
            "slug": "Hierarchical-Classifier-Design-Using-Mutual-Sethi-Sarvarayudu",
            "title": {
                "fragments": [],
                "text": "Hierarchical Classifier Design Using Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A nonparametric algorithm is presented for the hierarchical partitioning of the feature space that generates an efficient partitioning tree for specified probability of error by maximizing the amount of average mutual information gain at each partitioning step."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Most NNs conceal the statistics from the user.\u00ba Despite these similarities, neural networks do offer several advantages such as, unified approaches for feature extraction and classification and flexible procedures for finding good, moderately nonlinear solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "The rigid template\nmatching mentioned above, while effective in some\napplication domains, has a number of disadvantages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1520136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "551bb4142794dd682acf9a1159063158895e8214",
            "isKey": false,
            "numCitedBy": 1319,
            "numCiting": 165,
            "paperAbstract": {
                "fragments": [],
                "text": "A common problem encountered in such disciplines as statistics, data analysis, signal processing, and neural network research, is nding a suitable representation of multivariate data. For computational and conceptual simplicity, such a representation is often sought as a linear transformation of the original data. Well-known linear transformation methods include, for example, principal component analysis, factor analysis, and projection pursuit. A recently developed linear transformation method is independent component analysis (ICA), in which the desired representation is the one that minimizes the statistical dependence of the components of the representation. Such a representation seems to capture the essential structure of the data in many applications. In this paper, we survey the existing theory and methods for ICA."
            },
            "slug": "Survey-on-Independent-Component-Analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Survey on Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper surveys the existing theory and methods for independent component analysis (ICA), in which the desired representation is the one that minimizes the statistical dependence of the components of the representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 2
                            }
                        ],
                        "text": "5 [129] are available in the public domain(4) and therefore, often"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[129], which is trained by an iterative selection of individual"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": false,
            "numCitedBy": 7028,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2225647"
                        ],
                        "name": "M. V. Breukelen",
                        "slug": "M.-V.-Breukelen",
                        "structuredName": {
                            "firstName": "Martijn",
                            "lastName": "Breukelen",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Breukelen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2743835"
                        ],
                        "name": "D. Tax",
                        "slug": "D.-Tax",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tax",
                            "middleNames": [
                                "M.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tax"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784973"
                        ],
                        "name": "J. D. Hartog",
                        "slug": "J.-D.-Hartog",
                        "structuredName": {
                            "firstName": "Jurgen",
                            "lastName": "Hartog",
                            "middleNames": [
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Hartog"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31479512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bac47a037ed187d3c69882b014e2b859a8525fb",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Classifiers can be combined to reduce classification errors. We did experiments on a data set consisting of different sets of features of handwritten digits. Different types of classifiers were trained on these feature sets. The performances of these classifiers and combination rules were tested. The best results were acquired with the mean, median and product combination rules. The product was best for combining linear classifiers, the median for $k$-NN classifiers. Training a classifier on all features did not result in less errors."
            },
            "slug": "Handwritten-digit-recognition-by-combined-Breukelen-Duin",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition by combined classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The product was best for combining linear classifiers, the median for $k$-NN classifiers and the mean, median and product combination rules with the best results were acquired."
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 162
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4042,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700484"
                        ],
                        "name": "S. Gelfand",
                        "slug": "S.-Gelfand",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Gelfand",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelfand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36729903"
                        ],
                        "name": "C. Ravishankar",
                        "slug": "C.-Ravishankar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Ravishankar",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ravishankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741483"
                        ],
                        "name": "E. Delp",
                        "slug": "E.-Delp",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Delp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Delp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "avoided by using a pruning stage [63], [106], [128]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43158592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da15bf953fd7bab529046c5ba3826e48288f1272",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient iterative method is proposed to grow and prune classification trees. This method divides the data sample into two subsets and iteratively grows a tree with one subset and prunes it with the other subset, successively interchanging the roles of the two subsets. The convergence and other properties of the algorithm are established. Theoretical and practical considerations suggest that the iterative tree growing and pruning algorithm should perform better and require less computation than other widely used tree growing and pruning algorithms. Numerical results on a waveform recognition problem are presented to support this view.<<ETX>>"
            },
            "slug": "An-iterative-growing-and-pruning-algorithm-for-tree-Gelfand-Ravishankar",
            "title": {
                "fragments": [],
                "text": "An iterative growing and pruning algorithm for classification tree design"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Numerical results on a waveform recognition problem are presented to support the theory and practical considerations suggest that the iterative tree growing and pruning algorithm should perform better and require less computation than other widely used tree grow and prune algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795578"
                        ],
                        "name": "T. Ho",
                        "slug": "T.-Ho",
                        "structuredName": {
                            "firstName": "Tin",
                            "lastName": "Ho",
                            "middleNames": [
                                "Kam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206420153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b41d0fa5fdaadd47fc882d3db04277d03fb21832",
            "isKey": false,
            "numCitedBy": 5432,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy."
            },
            "slug": "The-Random-Subspace-Method-for-Constructing-Forests-Ho",
            "title": {
                "fragments": [],
                "text": "The Random Subspace Method for Constructing Decision Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7635678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach."
            },
            "slug": "Semi-Supervised-Support-Vector-Machines-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general S3VM model is proposed that minimizes both the misclassification error and the function capacity based on all the available data that can be converted to a mixed-integer program and then solved exactly using integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706882"
                        ],
                        "name": "H. M. Abbas",
                        "slug": "H.-M.-Abbas",
                        "structuredName": {
                            "firstName": "Hazem",
                            "lastName": "Abbas",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M. Abbas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792810"
                        ],
                        "name": "M. Fahmy",
                        "slug": "M.-Fahmy",
                        "structuredName": {
                            "firstName": "Moustafa",
                            "lastName": "Fahmy",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fahmy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26256028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e21da352271ecc713a33bf3f162fb3d43b4b8856",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-for-maximum-likelihood-clustering-Abbas-Fahmy",
            "title": {
                "fragments": [],
                "text": "Neural networks for maximum likelihood clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129654"
                        ],
                        "name": "S. Raudys",
                        "slug": "S.-Raudys",
                        "structuredName": {
                            "firstName": "Sarunas",
                            "lastName": "Raudys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37044463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cf994cc7cec3c76f49f98aa6ded0824187e786d",
            "isKey": false,
            "numCitedBy": 1263,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The effects of sample size on feature selection and error estimation for several types of classifiers are discussed. The focus is on the two-class problem. Classifier design in the context of small design sample size is explored. The estimation of error rates under small test sample size is given. Sample size effects in feature selection are discussed. Recommendations for the choice of learning and test sample sizes are given. In addition to surveying prior work in this area, an emphasis is placed on giving practical advice to designers and users of statistical pattern recognition systems. >"
            },
            "slug": "Small-Sample-Size-Effects-in-Statistical-Pattern-Raudys-Jain",
            "title": {
                "fragments": [],
                "text": "Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The effects of sample size on feature selection and error estimation for several types of classifiers are discussed and an emphasis is placed on giving practical advice to designers and users of statistical pattern recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285789"
                        ],
                        "name": "C. Kulikowski",
                        "slug": "C.-Kulikowski",
                        "structuredName": {
                            "firstName": "Casimir",
                            "lastName": "Kulikowski",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kulikowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12484204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "664b701a39371c5356754dc72cea1349233c8506",
            "isKey": false,
            "numCitedBy": 1046,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1 Overview of Learning Systems 1.1 What is a Learning System? 1.2 Motivation for Building Learning Systems 1.3 Types of Practical Empirical Learning Systems 1.3.1 Common Theme: The Classification Model 1.3.2 Let the Data Speak 1.4 What's New in Learning Methods 1.4.1 The Impact of New Technology 1.5 Outline of the Book 1.6 Bibliographical and Historical Remarks 2 How to Estimate the True Performance of a Learning System 2.1 The Importance of Unbiased Error Rate Estimation 2.2. What is an Error? 2.2.1 Costs and Risks 2.3 Apparent Error Rate Estimates 2.4 Too Good to Be True: Overspecialization 2.5 True Error Rate Estimation 2.5.1 The Idealized Model for Unlimited Samples 2.5.2 Train-and Test Error Rate Estimation 2.5.3 Resampling Techniques 2.5.4 Finding the Right Complexity Fit 2.6 Getting the Most Out of the Data 2.7 Classifier Complexity and Feature Dimensionality 2.7.1 Expected Patterns of Classifier Behavior 2.8 What Can Go Wrong? 2.8.1 Poor Features, Data Errors, and Mislabeled Classes 2.8.2 Unrepresentative Samples 2.9 How Close to the Truth? 2.10 Common Mistakes in Performance Analysis 2.11 Bibliographical and Historical Remarks 3 Statistical Pattern Recognition 3.1 Introduction and Overview 3.2 A Few Sample Applications 3.3 Bayesian Classifiers 3.3.1 Direct Application of the Bayes Rule 3.4 Linear Discriminants 3.4.1 The Normality Assumption and Discriminant Functions 3.4.2 Logistic Regression 3.5 Nearest Neighbor Methods 3.6 Feature Selection 3.7 Error Rate Analysis 3.8 Bibliographical and Historical Remarks 4 Neural Nets 4.1 Introduction and Overview 4.2 Perceptrons 4.2.1 Least Mean Square Learning Systems 4.2.2 How Good Is a Linear Separation Network? 4.3 Multilayer Neural Networks 4.3.1 Back-Propagation 4.3.2 The Practical Application of Back-Propagation 4.4 Error Rate and Complexity Fit Estimation 4.5 Improving on Standard Back-Propagation 4.6 Bibliographical and Historical Remarks 5 Machine Learning: Easily Understood Decision Rules 5.1 Introduction and Overview 5.2 Decision Trees 5.2.1 Finding the Perfect Tree 5.2.2 The Incredible Shrinking Tree 5.2.3 Limitations of Tree Induction Methods 5.3 Rule Induction 5.3.1 Predictive Value Maximization 5.4 Bibliographical and Historical Remarks 6 Which Technique is Best? 6.1 What's Important in Choosing a Classifier? 6.1.1 Prediction Accuracy 6.1.2 Speed of Learning and Classification 6.1.3 Explanation and Insight 6.2 So, How Do I Choose a Learning System? 6.3 Variations on the Standard Problem 6.3.1 Missing Data 6.3.2 Incremental Learning 6.4 Future Prospects for Improved Learning Methods 6.5 Bibliographical and Historical Remarks 7 Expert Systems 7.1 Introduction and Overview 7.1.1 Why Build Expert Systems? New vs. Old Knowledge 7.2 Estimating Error Rates for Expert Systems 7.3 Complexity of Knowledge Bases 7.3.1 How Many Rules Are Too Many? 7.4 Knowledge Base Example 7.5 Empirical Analysis of Knowledge Bases 7.6 Future: Combined Learning and Expert Systems 7.7 Bibliographical and Historical Remarks References Author Index Subject Index"
            },
            "slug": "Computer-Systems-That-Learn:-Classification-and-and-Weiss-Kulikowski",
            "title": {
                "fragments": [],
                "text": "Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book discusses how to Estimate the True Performance of a Learning System, and the Importance of Unbiased Error Rate Estimation, and Machine Learning: Easily Understood Decision Rules."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758144"
                        ],
                        "name": "L. Perlovsky",
                        "slug": "L.-Perlovsky",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Perlovsky",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Perlovsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12170124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba18ee677d56325237b7b16700637ac26d310e3e",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines fundamental problems underlying difficulties encountered by pattern recognition algorithms, neural networks, and rule systems. These problems are manifested as combinatorial complexity of algorithms, of their computational or training requirements. The paper relates particular types of complexity problems to the roles of a priori knowledge and adaptive learning. Paradigms based on adaptive learning lead to the complexity of training procedures, while nonadaptive rule-based paradigms lead to complexity of rule systems. Model-based approaches to combining adaptivity with a priori knowledge lead to computational complexity. Arguments are presented for the Aristotelian logic being culpable for the difficulty of combining adaptivity and a priority. The potential role of the fuzzy logic in overcoming current difficulties is discussed. Current mathematical difficulties are related to philosophical debates of the past."
            },
            "slug": "Conundrum-of-Combinatorial-Complexity-Perlovsky",
            "title": {
                "fragments": [],
                "text": "Conundrum of Combinatorial Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper examines fundamental problems underlying difficulties encountered by pattern recognition algorithms, neural networks, and rule systems as combinatorial complexity of algorithms, of their computational or training requirements, and the potential role of the fuzzy logic in overcoming current difficulties."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055922791"
                        ],
                        "name": "B. Cheng",
                        "slug": "B.-Cheng",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742419"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 62179812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87e101b7b9ef8b85ab3377fc3fc3e2e6baf5ad58",
            "isKey": false,
            "numCitedBy": 1151,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsupervised learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics."
            },
            "slug": "Neural-Networks:-A-Review-from-a-Statistical-Cheng-Titterington",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Review from a Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit, and treats various topics in more depth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074275595"
                        ],
                        "name": "M. Hansen",
                        "slug": "M.-Hansen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hansen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144923779"
                        ],
                        "name": "Bin Yu",
                        "slug": "Bin-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14460386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6ac30380a592f96d1ec43e9cac1a560b3f4fbf4",
            "isKey": false,
            "numCitedBy": 739,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems."
            },
            "slug": "Model-Selection-and-the-Principle-of-Minimum-Length-Hansen-Yu",
            "title": {
                "fragments": [],
                "text": "Model Selection and the Principle of Minimum Description Length"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This article reviews the principle of minimum description length (MDL) for problems of model selection, and illustrates the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24063796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6db07f39446bdf74d0178a75f526baffee1a0369",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare the performance of three types of neural network-based ensemble techniques to that of a single neural network. The ensemble algorithms are two versions of boosting and committees of neural networks trained independently. For each of the four algorithms, we experimentally determine the test and training error curves in an optical character recognition (OCR) problem as both a function of training set size and computational cost using three architectures. We show that a single machine is best for small training set size while for large training set size some version of boosting is best. However, for a given computational cost, boosting is always best. Furthermore, we show a surprising result for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate. This has potential implications in the search for better training algorithms."
            },
            "slug": "Boosting-and-Other-Ensemble-Methods-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting and Other Ensemble Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A surprising result is shown for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120727315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cce218b91cf634413ef9a71f702bd37b1a9ad2a6",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ..."
            },
            "slug": "Exploratory-Projection-Pursuit-Friedman",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods and the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37365552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11656f389fabe0c8ab987ded90372d06c6591008",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-nonlinear-PCA-learning-rule-in-independent-Oja",
            "title": {
                "fragments": [],
                "text": "The nonlinear PCA learning rule in independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31887966"
                        ],
                        "name": "M. Hatef",
                        "slug": "M.-Hatef",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hatef",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hatef"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1991617,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "54801c260df5221a9de533d371d3edcc358b4050",
            "isKey": false,
            "numCitedBy": 5738,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a common theoretical framework for combining classifiers which use distinct pattern representations and show that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision. An experimental comparison of various classifier combination schemes demonstrates that the combination rule developed under the most restrictive assumptions-the sum rule-and its derivatives consistently outperform other classifier combinations schemes. A sensitivity analysis of the various schemes to estimation errors is carried out to show that this finding can be justified theoretically."
            },
            "slug": "Combining-classifiers-Kittler-Hatef",
            "title": {
                "fragments": [],
                "text": "On Combining Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A common theoretical framework for combining classifiers which use distinct pattern representations is developed and it is shown that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421006"
                        ],
                        "name": "R. Michalski",
                        "slug": "R.-Michalski",
                        "structuredName": {
                            "firstName": "Ryszard",
                            "lastName": "Michalski",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Michalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070811"
                        ],
                        "name": "R. Stepp",
                        "slug": "R.-Stepp",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Stepp",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stepp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7966553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "644b44b29a0ebf08d7cdbbd0892334b63486ac8c",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for automated construction of classifications called conceptual clustering is described and compared to methods used in numerical taxonomy. This method arranges objects into classes representing certain descriptive concepts, rather than into classes defined solely by a similarity metric in some a priori defined attribute space. A specific form of the method is conjunctive conceptual clustering, in which descriptive concepts are conjunctive statements involving relations on selected object attributes and optimized according to an assumed global criterion of clustering quality. The method, implemented in program CLUSTER/2, is tested together with 18 numerical taxonomy methods on two exemplary problems: 1) a construction of a classification of popular microcomputers and 2) the reconstruction of a classification of selected plant disease categories. In both experiments, the majority of numerical taxonomy methods (14 out of 18) produced results which were difficult to interpret and seemed to be arbitrary. In contrast to this, the conceptual clustering method produced results that had a simple interpretation and corresponded well to solutions preferred by people."
            },
            "slug": "Automated-Construction-of-Classifications:-Versus-Michalski-Stepp",
            "title": {
                "fragments": [],
                "text": "Automated Construction of Classifications: Conceptual Clustering Versus Numerical Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method for automated construction of classifications called conceptual clustering is described and compared to methods used in numerical taxonomy, in which descriptive concepts are conjunctive statements involving relations on selected object attributes and optimized according to an assumed global criterion of clustering quality."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144821969"
                        ],
                        "name": "A. Wong",
                        "slug": "A.-Wong",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Wong",
                            "middleNames": [
                                "K.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111257331"
                        ],
                        "name": "David C. Wang",
                        "slug": "David-C.-Wang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wang",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15409081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef7b7c7580c9bf3b8e4ffd842b9b3ceba1236e84",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new clustering algorithm for analyzing unordered discrete-valued data. This algorithm consists of a cluster initiation phase and a sample regrouping phase. The first phase is based on a data-directed valley detection process utilizing the optimal second-order product approximation of high-order discrete probability distribution, together with a distance measure for discrete-valued data. As for the second phase, it involves the iterative application of the Bayes' decision rule based on subgroup discrete distributions. Since probability is used as its major decision criterion, the proposed method minimizes the disadvantages of yielding solutions sensitive to the arbitrary distance measure adopted. The performance of the proposed algorithm is evaluated by applying it to four different sets of simulated data and a set of clinical data. For performance comparison, the decision-directed algorithm [11] is also applied to the same set of data. These evaluation experiments fully demonstrate the validity and the operational feasibility of the proposed algorithm and its superiority as compared to the decision-directed algorithm."
            },
            "slug": "DECA:-A-Discrete-Valued-Data-Clustering-Algorithm-Wong-Wang",
            "title": {
                "fragments": [],
                "text": "DECA: A Discrete-Valued Data Clustering Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This paper presents a new clustering algorithm for analyzing unordered discrete-valued data that consists of a cluster initiation phase and a sample regrouping phase based on a data-directed valley detection process utilizing the optimal second-order product approximation of high-order discrete probability distribution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 411526,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "50a42ed2f81b9fe150883a6c89194c88a9647106",
            "isKey": false,
            "numCitedBy": 42029,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples."
            },
            "slug": "A-new-look-at-the-statistical-model-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "A new look at the statistical model identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144905110"
                        ],
                        "name": "L. Lam",
                        "slug": "L.-Lam",
                        "structuredName": {
                            "firstName": "Louisa",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15777546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00461762996c4ee6f69c2c17a16ff3c6a8fac5b6",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-combinations-of-pattern-classifiers-Lam-Suen",
            "title": {
                "fragments": [],
                "text": "Optimal combinations of pattern classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763529"
                        ],
                        "name": "P. Cosman",
                        "slug": "P.-Cosman",
                        "structuredName": {
                            "firstName": "Pamela",
                            "lastName": "Cosman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cosman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830639"
                        ],
                        "name": "K. Oehler",
                        "slug": "K.-Oehler",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Oehler",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Oehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788775"
                        ],
                        "name": "E. Riskin",
                        "slug": "E.-Riskin",
                        "structuredName": {
                            "firstName": "Eve",
                            "lastName": "Riskin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17304149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fd37391b675db8b7751d00780abfbed7596fedf",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "A review is presented of vector quantization, the mapping of pixel intensity vectors into binary vectors indexing a limited number of possible reproductions, which is a popular image compression algorithm. Compression has traditionally been done with little regard for image processing operations that may precede or follow the compression step. Recent work has used vector quantization both to simplify image processing tasks, such as enhancement classification, halftoning, and edge detection, and to reduce the computational complexity by performing the tasks simultaneously with the compression. The fundamental ideas of vector quantization are explained, and vector quantization algorithms that perform image processing are surveyed. >"
            },
            "slug": "Using-vector-quantization-for-image-processing-Cosman-Oehler",
            "title": {
                "fragments": [],
                "text": "Using vector quantization for image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A review is presented of vector quantization, the mapping of pixel intensity vectors into binary vectors indexing a limited number of possible reproductions, which is a popular image compression algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492516"
                        ],
                        "name": "K. Woods",
                        "slug": "K.-Woods",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Woods",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Woods"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757085"
                        ],
                        "name": "W. Kegelmeyer",
                        "slug": "W.-Kegelmeyer",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Kegelmeyer",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kegelmeyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1693363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d08ce08eeb9c6376f8573d00e9138d33d2d54e2",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Combination of multiple classifiers (CMC) has recently drawn attention as a method of improving classification accuracy. This paper presents a method for combining classifiers that use estimates of each individual classifier's local accuracy in small regions of feature space surrounding an unknown test sample. Only the output of the most locally accurate classifier is considered. We address issues of (1) optimization of individual classifiers, and (2) the effect of varying the sensitivity of the individual classifiers on the CMC algorithm. Our algorithm performs better on data from a real problem in mammogram image analysis than do other recently proposed CMC techniques."
            },
            "slug": "Combination-of-multiple-classifiers-using-local-Woods-Bowyer",
            "title": {
                "fragments": [],
                "text": "Combination of multiple classifiers using local accuracy estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A method for combining classifiers that use estimates of each individual classifier's local accuracy in small regions of feature space surrounding an unknown test sample that performs better on data from a real problem in mammogram image analysis than do other recently proposed CMC techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46780537"
                        ],
                        "name": "G. Trunk",
                        "slug": "G.-Trunk",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Trunk",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Trunk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "As expected, the quality of the selected feature subset for small training sets is poor, but improves as the training set size increases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "Trunk [157] provided a simple example to illustrate the curse of dimensionality which we reproduce below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13086902,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "26cfe4cb725e8cb939cd968f44218b2d9a36a794",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error. A simple problem has been formulated where the probability of error approaches zero as the dimensionality increases and all the parameters are known; on the other hand, the probability of error approaches one-half as the dimensionality increases and parameters are estimated."
            },
            "slug": "A-Problem-of-Dimensionality:-A-Simple-Example-Trunk",
            "title": {
                "fragments": [],
                "text": "A Problem of Dimensionality: A Simple Example"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error, so the probability of error approaches one-half as the dimensionality increases and parameters are estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2356968"
                        ],
                        "name": "S. Bhatia",
                        "slug": "S.-Bhatia",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Bhatia",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhatia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727747"
                        ],
                        "name": "J. Deogun",
                        "slug": "J.-Deogun",
                        "structuredName": {
                            "firstName": "Jitender",
                            "lastName": "Deogun",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deogun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5640476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a4ba2de87f9e0d68639c20ee2a308739fa26d",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is used in information retrieval systems to enhance the efficiency and effectiveness of the retrieval process. Clustering is achieved by partitioning the documents in a collection into classes such that documents that are associated with each other are assigned to the same cluster. This association is generally determined by examining the index term representation of documents or by capturing user feedback on queries on the system. In cluster-oriented systems, the retrieval process can be enhanced by employing characterization of clusters. In this paper, we present the techniques to develop clusters and cluster characterizations by employing user viewpoint. The user viewpoint is elicited through a structured interview based on a knowledge acquisition technique, namely personal construct theory. It is demonstrated that the application of personal construct theory results in a cluster representation that can be used during query as well as to assign new documents to the appropriate clusters."
            },
            "slug": "Conceptual-clustering-in-information-retrieval-Bhatia-Deogun",
            "title": {
                "fragments": [],
                "text": "Conceptual clustering in information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the application of personal construct theory results in a cluster representation that can be used during query as well as to assign new documents to the appropriate clusters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221802"
                        ],
                        "name": "C. Metz",
                        "slug": "C.-Metz",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Metz",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Metz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3842413,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "5c2f4494f7ff8b93433f3432a4ce271586f6d56f",
            "isKey": false,
            "numCitedBy": 5365,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-principles-of-ROC-analysis.-Metz",
            "title": {
                "fragments": [],
                "text": "Basic principles of ROC analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Seminars in nuclear medicine"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758284"
                        ],
                        "name": "A. Djouadi",
                        "slug": "A.-Djouadi",
                        "structuredName": {
                            "firstName": "Abdelhamid",
                            "lastName": "Djouadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Djouadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064200"
                        ],
                        "name": "E. Bouktache",
                        "slug": "E.-Bouktache",
                        "structuredName": {
                            "firstName": "Essaid",
                            "lastName": "Bouktache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bouktache"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11883434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e559e85029ad60eb7b169642a5884103d8f9275",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast algorithm that finds the nearest neighbor (NN) of an unknown sample from a design set of labeled samples is proposed. This algorithm requires a quite moderate preprocessing effort and a rather excessive storage, but it accomplishes substantial computational savings during classification. The performance of the algorithm is described and compared to the performance of the conventional one. Results on simulated data are provided to illustrate the computational savings that may be achieved using this fast algorithm."
            },
            "slug": "A-Fast-Algorithm-for-the-Nearest-Neighbor-Djouadi-Bouktache",
            "title": {
                "fragments": [],
                "text": "A Fast Algorithm for the Nearest-Neighbor Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A fast algorithm that finds the nearest neighbor (NN) of an unknown sample from a design set of labeled samples is proposed and it accomplishes substantial computational savings during classification."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39755829"
                        ],
                        "name": "G. Castellano",
                        "slug": "G.-Castellano",
                        "structuredName": {
                            "firstName": "Giovanna",
                            "lastName": "Castellano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Castellano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724997"
                        ],
                        "name": "A. Fanelli",
                        "slug": "A.-Fanelli",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Fanelli",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fanelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8111020"
                        ],
                        "name": "M. Pelillo",
                        "slug": "M.-Pelillo",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Pelillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pelillo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6014155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e26b50da084d369693998bf99c90fb4fc047ffa",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of determining the proper size of an artificial neural network is recognized to be crucial, especially for its practical implications in such important issues as learning and generalization. One popular approach for tackling this problem is commonly known as pruning and it consists of training a larger than necessary network and then removing unnecessary weights/nodes. In this paper, a new pruning method is developed, based on the idea of iteratively eliminating units and adjusting the remaining weights in such a way that the network performance does not worsen over the entire training set. The pruning problem is formulated in terms of solving a system of linear equations, and a very efficient conjugate gradient algorithm is used for solving it, in the least-squares sense. The algorithm also provides a simple criterion for choosing the units to be removed, which has proved to work well in practice. The results obtained over various test problems demonstrate the effectiveness of the proposed approach."
            },
            "slug": "An-iterative-pruning-algorithm-for-feedforward-Castellano-Fanelli",
            "title": {
                "fragments": [],
                "text": "An iterative pruning algorithm for feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new pruning method is developed, based on the idea of iteratively eliminating units and adjusting the remaining weights in such a way that the network performance does not worsen over the entire training set."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830639"
                        ],
                        "name": "K. Oehler",
                        "slug": "K.-Oehler",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Oehler",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Oehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30174001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bde43668a483bb270e79dcfe6e57e62d8b9fc3cd",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method of combining classification and compression into a single vector quantizer by incorporating a Bayes risk term into the distortion measure used in the quantizer design algorithm. Once trained, the quantizer can operate to minimize the Bayes risk weighted distortion measure if there is a model providing the required posterior probabilities, or it can operate in a suboptimal fashion by minimizing the squared error only. Comparisons are made with other vector quantizer based classifiers, including the independent design of quantization and minimum Bayes risk classification and Kohonen's LVQ. A variety of examples demonstrate that the proposed method can provide classification ability close to or superior to learning VQ while simultaneously providing superior compression performance. >"
            },
            "slug": "Combining-Image-Compression-and-Classification-Oehler-Gray",
            "title": {
                "fragments": [],
                "text": "Combining Image Compression and Classification Using Vector Quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A variety of examples demonstrate that the proposed method can provide classification ability close to or superior to learning VQ while simultaneously providing superior compression performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4929024"
                        ],
                        "name": "J. Bezdek",
                        "slug": "J.-Bezdek",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bezdek",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bezdek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "In syntactic pattern recognition, a formal analogy is drawn between the structure of patterns and the syntax of a language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30806637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24a85e28954871d30ebefac06b459f8c2701e7a0",
            "isKey": false,
            "numCitedBy": 15536,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "New updated! The latest book from a very famous author finally comes out. Book of pattern recognition with fuzzy objective function algorithms, as an amazing reference becomes what you need to get. What's for is this book? Are you still thinking for what the book is? Well, this is what you probably will get. You should have made proper choices for your better life. Book, as a source that may involve the facts, opinion, literature, religion, and many others are the great friends to join with."
            },
            "slug": "Pattern-Recognition-with-Fuzzy-Objective-Function-Bezdek",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition with Fuzzy Objective Function Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Books, as a source that may involve the facts, opinion, literature, religion, and many others are the great friends to join with, becomes what you need to get."
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Applications in Pattern Recognition"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40000333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de86c61defbb8c259583074f3cf63afe13571ce1",
            "isKey": false,
            "numCitedBy": 856,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principal-components,-minor-components,-and-linear-Oja",
            "title": {
                "fragments": [],
                "text": "Principal components, minor components, and linear neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40941894"
                        ],
                        "name": "Soumen Chakrabarti",
                        "slug": "Soumen-Chakrabarti",
                        "structuredName": {
                            "firstName": "Soumen",
                            "lastName": "Chakrabarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumen Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688317"
                        ],
                        "name": "P. Indyk",
                        "slug": "P.-Indyk",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Indyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Indyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207226010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3261bb81085f59efae1e1c72453c47daaee777ac",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%."
            },
            "slug": "Enhanced-hypertext-categorization-using-hyperlinks-Chakrabarti-Dom",
            "title": {
                "fragments": [],
                "text": "Enhanced hypertext categorization using hyperlinks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work has developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained and its technique also adapts gracefully to the fraction of neighboring documents having known topics."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682325"
                        ],
                        "name": "F. Ferri",
                        "slug": "F.-Ferri",
                        "structuredName": {
                            "firstName": "Francesc",
                            "lastName": "Ferri",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ferri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31887966"
                        ],
                        "name": "M. Hatef",
                        "slug": "M.-Hatef",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hatef",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hatef"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 69632291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5523bedd0b1b74fa34509d56cb62f0e7b386a180",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comparative-study-of-techniques-for-large-scale-was-Ferri-Pudil",
            "title": {
                "fragments": [],
                "text": "Comparative study of techniques for large-scale feature selection* *This work was suported by a SERC grant GR/E 97549. The first author was also supported by a FPI grant from the Spanish MEC, PF92 73546684"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681667"
                        ],
                        "name": "B. Lerner",
                        "slug": "B.-Lerner",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lerner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341417"
                        ],
                        "name": "H. Guterman",
                        "slug": "H.-Guterman",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Guterman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Guterman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9262614"
                        ],
                        "name": "M. Aladjem",
                        "slug": "M.-Aladjem",
                        "structuredName": {
                            "firstName": "Mayer",
                            "lastName": "Aladjem",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aladjem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686153"
                        ],
                        "name": "I. Dinstein",
                        "slug": "I.-Dinstein",
                        "structuredName": {
                            "firstName": "Its'hak",
                            "lastName": "Dinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5366418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c8543f707d1ee56692deefb8f777dc88f2e3869",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparative-study-of-neural-network-based-feature-Lerner-Guterman",
            "title": {
                "fragments": [],
                "text": "A comparative study of neural network based feature extraction paradigms"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "The topic of unsupervised classification or clustering is covered in Section 8."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "The rigid template\nmatching mentioned above, while effective in some\napplication domains, has a number of disadvantages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10408361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "595640253ffdfd12e04ac57bd78753f936a7cfad",
            "isKey": false,
            "numCitedBy": 899,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents a general theoretical framework for ensemble methods of constructing significantly improved regression estimates. Given a population of regression estimators, the authors construct a hybrid estimator that is as good or better in the mean square error sense than any estimator in the population. They argue that the ensemble method presented has several properties: (1) it efficiently uses all the networks of a population -- none of the networks need to be discarded; (2) it efficiently uses all of the available data for training without over-fitting; (3) it inherently performs regularization by smoothing in functional space, which helps to avoid over-fitting; (4) it utilizes local minima to construct improved estimates whereas other neural network algorithms are hindered by local minima; (5) it is ideally suited for parallel computation; (6) it leads to a very useful and natural measure of the number of distinct estimators in a population; and (7) the optimal parameters of the ensemble estimator are given in closed form. Experimental results show that the ensemble method dramatically improves neural network performance on difficult real-world optical character recognition tasks."
            },
            "slug": "When-Networks-Disagree:-Ensemble-Methods-for-Hybrid-Perrone-Cooper",
            "title": {
                "fragments": [],
                "text": "When Networks Disagree: Ensemble Methods for Hybrid Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the ensemble method dramatically improves neural network performance on difficult real-world optical character recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207661551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0899a6b62251ebb4af1ed35f0c6f9d63bed8c8e9",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel fast algorithm for independent component analysis, which can be used for blind source separation and feature extraction. We show how a neural network learning rule can be transformed into a fixedpoint iteration, which provides an algorithm that is very simple, does not depend on any user-defined parameters, and is fast to converge to the most accurate solution allowed by the data. The algorithm finds, one at a time, all nongaussian independent components, regardless of their probability distributions. The computations can be performed in either batch mode or a semiadaptive manner. The convergence of the algorithm is rigorously proved, and the convergence speed is shown to be cubic. Some comparisons to gradient-based algorithms are made, showing that the new algorithm is usually 10 to 100 times faster, sometimes giving the solution in just a few iterations."
            },
            "slug": "A-Fast-Fixed-Point-Algorithm-for-Independent-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "A Fast Fixed-Point Algorithm for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A novel fast algorithm for independent component analysis is introduced, which can be used for blind source separation and feature extraction, and the convergence speed is shown to be cubic."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5895004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1291583873fb890e7922ec0dfefd4846df46c9",
            "isKey": false,
            "numCitedBy": 5479,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stacked-generalization-Wolpert",
            "title": {
                "fragments": [],
                "text": "Stacked generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403825430"
                        ],
                        "name": "M. Whindham",
                        "slug": "M.-Whindham",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Whindham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Whindham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484933"
                        ],
                        "name": "Adele Cutler",
                        "slug": "Adele-Cutler",
                        "structuredName": {
                            "firstName": "Adele",
                            "lastName": "Cutler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adele Cutler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56608477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f80213518f8b7711ee2b8d84eb12ca0e9d0c9e6",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Determining the number of components in a mixture of distributions is an important but difficult problem. This article introduces a procedure called minimum information ratio estimation and validation (MIREV), which is based on a ratio of Fisher information matrices. The smallest eigenvalue of the information ratio matrix is used to determine the number of components. A measure of uncertainty may be obtained using a bootstrap technique. Simulations illustrate the effectiveness of the procedure. For mixtures of exponential families, an expression for the observed information ratio matrix provides insight to the success of the procedure. Cluster analysis attempts to identify and characterize subpopulations believed to be present in a population. A wide variety of methods, are available, including criterion optimization, hierarchical methods, and various heuristic methods. Criterion optimization techniques, such as mixture analysis, fuzzy clustering, and partitioning methods are popular because they..."
            },
            "slug": "Information-Ratios-for-Validating-Mixture-Analysis-Whindham-Cutler",
            "title": {
                "fragments": [],
                "text": "Information Ratios for Validating Mixture Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This article introduces a procedure called minimum information ratio estimation and validation (MIREV), which is based on a ratio of Fisher information matrices, which is used to determine the number of components in a mixture of distributions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145214184"
                        ],
                        "name": "Takayuki Ito",
                        "slug": "Takayuki-Ito",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Ito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8235461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71ea46c9266f5104f79ea27fdfb4c5686677695a",
            "isKey": false,
            "numCitedBy": 755,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition. The model consists of nine layers of cells. The authors demonstrate that the model can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape. A learning-with-a-teacher process is used for the reinforcement of the modifiable synapses in the new large-scale model, instead of the learning-without-a-teacher process applied to a previous model. The authors focus on the mechanism for pattern recognition rather than that for self-organization."
            },
            "slug": "Neocognitron:-A-neural-network-model-for-a-of-Fukushima-Miyake",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A neural network model for a mechanism of visual pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition and can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2184725"
                        ],
                        "name": "Tianping Chen",
                        "slug": "Tianping-Chen",
                        "structuredName": {
                            "firstName": "Tianping",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianping Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33315484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7a64bc8947a1889e8f827917e3bca5babc4369f",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stability-Analysis-of-Learning-Algorithms-for-Blind-Amari-Chen",
            "title": {
                "fragments": [],
                "text": "Stability Analysis of Learning Algorithms for Blind Source Separation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295483"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145623308"
                        ],
                        "name": "S. Lakshmanan",
                        "slug": "S.-Lakshmanan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Lakshmanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lakshmanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206418422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c752b836f019b37107b68b230a91be70dd2c32bf",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general object localization and retrieval scheme based on object shape using deformable templates. Prior knowledge of an object shape is described by a prototype template which consists of the representative contour/edges, and a set of probabilistic deformation transformations on the template. A Bayesian scheme, which is based on this prior knowledge and the edge information in the input image, is employed to find a match between the deformed template and objects in the image. Computational efficiency is achieved via a coarse-to-fine implementation of the matching algorithm. Our method has been applied to retrieve objects with a variety of shapes from images with complex background. The proposed scheme is invariant to location, rotation, and moderate scale changes of the template."
            },
            "slug": "Object-Matching-Using-Deformable-Templates-Jain-Zhong",
            "title": {
                "fragments": [],
                "text": "Object Matching Using Deformable Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Bayesian scheme, which is based on prior knowledge and the edge information in the input image, is employed to find a match between the deformed template and objects in the image and computational efficiency is achieved via a coarse-to-fine implementation of the matching algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7055940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f04029d1d83f41eaebf5a216ebecf2a61ff6dc0",
            "isKey": false,
            "numCitedBy": 3271,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Selection-of-Relevant-Features-and-Examples-in-Blum-Langley",
            "title": {
                "fragments": [],
                "text": "Selection of Relevant Features and Examples in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97306284"
                        ],
                        "name": "K. Fu",
                        "slug": "K.-Fu",
                        "structuredName": {
                            "firstName": "King-Sun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14954458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ed2ab133e48f34c59c14b4cc362cfa0f363392",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of pattern recognition is discussed in terms of single-entity representation versus multiple-entity representation. A combined syntactic-semantic approach based on attributed grammars is suggested. Syntax-semantics tradeoff in pattern representation is demonstrated. This approach is intended to be an initial step toward unification of syntactic and statistical approaches to pattern recognition."
            },
            "slug": "A-Step-Towards-Unification-of-Syntactic-and-Pattern-Fu",
            "title": {
                "fragments": [],
                "text": "A Step Towards Unification of Syntactic and Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A combined syntactic-semantic approach based on attributed grammars is suggested, intended to be an initial step toward unification of syntactic and statistical approaches to pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the maximum margin objective was introduced in the context of support vector machines [23] based on structural risk minimization theory [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Structural risk minimization based on the notion of VC dimension has also been used for model selection where the best model is the one with the best worst-case performance (upper bound on the generalization error) [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "classifier by Vapnik [162] which has also been studied by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A frequently used measure for the capacity is the Vapnik-Chervonenkis (VC) dimensionality [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The classical studies by Cover [33] and Vapnik [162] on classifier capacity and complexity provide a good understanding of the mechanisms behind overtraining."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "approaches are supported by Vapnik's philosophy [162]: aIf"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the recent development of support vector machines [162], however, these results have proved to be quite useful."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8757,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14754287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a5952532cdd48eec5e3dc326907c36a70e0a24",
            "isKey": false,
            "numCitedBy": 2921,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications."
            },
            "slug": "Vector-quantization-Gray",
            "title": {
                "fragments": [],
                "text": "Vector quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2342718"
                        ],
                        "name": "S. Sclove",
                        "slug": "S.-Sclove",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Sclove",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclove"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5685748,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b4135508114fc08539d72fdb4036332a9ac81c1e",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of image segmentation is considered in the context of a mixture of probability distributions. The segments fall into classes. A probability distribution is associated with each class of segment. Parametric families of distributions are considered, a set of parameter values being associated with each class. With each observation is associated an unobservable label, indicating from which class the observation arose. Segmentation algorithms are obtained by applying a method of iterated maximum likelihood to the resulting likelihood function. A numerical example is given. Choice of the number of classes, using Akaike's information criterion (AIC) for model identification, is illustrated."
            },
            "slug": "Application-of-the-Conditional-Population-Mixture-Sclove",
            "title": {
                "fragments": [],
                "text": "Application of the Conditional Population-Mixture Model to Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The problem of image segmentation is considered in the context of a mixture of probability distributions, where segments fall into classes and each class is associated an unobservable label, indicating from which class the observation arose."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14716061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f815573475053a62d78e24dcb42348d6e3a5eca",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "The connection between compression and the estimation of probability distributions has long been known for the case of discrete alphabet sources and lossless coding. A universal lossless code which does a good job of compressing must implicitly also do a good job of modeling. In particular, with a collection of codebooks, one for each possible class or model, if codewords are chosen from among the ensemble of codebooks so as to minimize bit rate, then the codebook selected provides an implicit estimate of the underlying class. Less is known about the corresponding connections between lossy compression and continuous sources. We consider aspects of estimating conditional and unconditional densities in conjunction with Bayes-risk weighted vector quantization for joint compression and classification."
            },
            "slug": "Vector-quantization-and-density-estimation-Gray-Olshen",
            "title": {
                "fragments": [],
                "text": "Vector quantization and density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work considers aspects of estimating conditional and unconditional densities in conjunction with Bayes-risk weighted vector quantization for joint compression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[18], [28], [137], and also Bayesian learning [113]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3641,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2317429"
                        ],
                        "name": "Michiaki Taniguchi",
                        "slug": "Michiaki-Taniguchi",
                        "structuredName": {
                            "firstName": "Michiaki",
                            "lastName": "Taniguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michiaki Taniguchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11043212,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7de82963ab9dcb775ae0e14a14ed83079fce98b2",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the linearly weighted combination of estimators in which the weighting functions are dependent on the input. We show that the weighting functions can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator has seen data in the region of the input space close to the input pattern. The latter solution is closely related to the mixture of experts approach and we show how learning rules for the mixture of experts can be derived from the theory about learning with missing features. The presented approaches are modular since the weighting functions can easily be modified (no retraining) if more estimators are added. Furthermore, it is easy to incorporate estimators which were not derived from data such as expert systems or algorithms."
            },
            "slug": "Combining-Estimators-Using-Non-Constant-Weighting-Tresp-Taniguchi",
            "title": {
                "fragments": [],
                "text": "Combining Estimators Using Non-Constant Weighting Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper discusses the linearly weighted combination of estimators in which the weighting functions are dependent on the input and shows how learning rules for the mixture of experts can be derived from the theory about learning with missing features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055457772"
                        ],
                        "name": "Qiaobing Xie",
                        "slug": "Qiaobing-Xie",
                        "structuredName": {
                            "firstName": "Qiaobing",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiaobing Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2227174"
                        ],
                        "name": "C. Laszlo",
                        "slug": "C.-Laszlo",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Laszlo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Laszlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39079344"
                        ],
                        "name": "R. Ward",
                        "slug": "R.-Ward",
                        "structuredName": {
                            "firstName": "Rabab",
                            "lastName": "Ward",
                            "middleNames": [
                                "Kreidieh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2594144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df595fee0daf7637748fb678914d31c19a693d1d",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective data reduction technique based on vector quantization is introduced for nonparametric classifier design. Two new nonparametric classifiers are developed, and their performance is evaluated using various examples. The new methods maintain a classification accuracy that is competitive with that of classical methods but, at the same time, yields very high data reduction rates. >"
            },
            "slug": "Vector-Quantization-Technique-for-Nonparametric-Xie-Laszlo",
            "title": {
                "fragments": [],
                "text": "Vector Quantization Technique for Nonparametric Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An effective data reduction technique based on vector quantization is introduced for nonparametric classifier design and maintains a classification accuracy that is competitive with that of classical methods but yields very high data reduction rates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34659351"
                        ],
                        "name": "M\u00e1rio A. T. Figueiredo",
                        "slug": "M\u00e1rio-A.-T.-Figueiredo",
                        "structuredName": {
                            "firstName": "M\u00e1rio",
                            "lastName": "Figueiredo",
                            "middleNames": [
                                "A.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rio A. T. Figueiredo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102270"
                        ],
                        "name": "J. Leit\u00e3o",
                        "slug": "J.-Leit\u00e3o",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Leit\u00e3o",
                            "middleNames": [
                                "M.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Leit\u00e3o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15700214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "695a2b2fa6cfd4579a1fa128ba7aaf014c581698",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider the problem of fitting a finite Gaussian mixture, with an unknown number of components, to observed data. This paper proposes a new minimum description length (MDL) type criterion, termed MMDL(f or mixture MDL), to select the number of components of the model. MMDLis based on the identification of an \"equivalent sample size\", for each component, which does not coincide with the full sample size. We also introduce an algorithm based on the standard expectation-maximization (EM) approach together with a new agglomerative step, called agglomerative EM (AEM). The experiments here reported have shown that MMDLo utperforms existing criteria of comparable computational cost. The good behavior of AEM, namely its good robustness with respect to initialization, is also illustrated experimentally."
            },
            "slug": "On-Fitting-Mixture-Models-Figueiredo-Leit\u00e3o",
            "title": {
                "fragments": [],
                "text": "On Fitting Mixture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper proposes a new minimum description length (MDL) type criterion, termed MMDL(f or mixture MDL), to select the number of components of the model, based on the identification of an \"equivalent sample size\", for each component, which does not coincide with the full sample size."
            },
            "venue": {
                "fragments": [],
                "text": "EMMCVPR"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34898135"
                        ],
                        "name": "R. Hayes",
                        "slug": "R.-Hayes",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Hayes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hayes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5434170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3145cc49e77cd853d4093257113023369b4cdd96",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The effect of finite sample-size on parameter estimates and their subsequent use in a family of functions are discussed. General and parameter-specific expressions for the expected bias and variance of the functions are derived. These expressions are then applied to the Bhattacharyya distance and the analysis of the linear and quadratic classifiers, providing insight into the relationship between the number of features and the number of training samples. Because of the functional form of the expressions, an empirical approach is presented to enable asymptotic performance to be accurately estimated using a very small number of samples. Results were experimentally verified using artificial data in controlled cases and using real, high-dimensional data. >"
            },
            "slug": "Effects-of-Sample-Size-in-Classifier-Design-Fukunaga-Hayes",
            "title": {
                "fragments": [],
                "text": "Effects of Sample Size in Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The effect of finite sample-size on parameter estimates and their subsequent use in a family of functions are discussed, and an empirical approach is presented to enable asymptotic performance to be accurately estimated using a very small number of samples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Hidden Markov Models (HMM), have been a popular statistical tool for modeling and recognizing sequential data, in particular, speech data [130], [86]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12495425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "231f6de83cfa4d641da1681e97a11b689a48e3aa",
            "isKey": false,
            "numCitedBy": 2251,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method."
            },
            "slug": "Statistical-methods-for-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Statistical methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7882,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2198160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e23c634a7beb02a127ecb11551fd0333491c602",
            "isKey": false,
            "numCitedBy": 2303,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved."
            },
            "slug": "Regularized-Discriminant-Analysis-Friedman",
            "title": {
                "fragments": [],
                "text": "Regularized Discriminant Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Alternatives to the usual maximum likelihood estimates for the covariance matrices are proposed, characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129654"
                        ],
                        "name": "S. Raudys",
                        "slug": "S.-Raudys",
                        "structuredName": {
                            "firstName": "Sarunas",
                            "lastName": "Raudys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12482517,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a74f3c9a67dfcc3515935e533d829a89fd5e1596",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Expected-classification-error-of-the-Fisher-linear-Raudys-Duin",
            "title": {
                "fragments": [],
                "text": "Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Picard [125] has identified a novel application of pattern recognition, called affective computing which will give a computer the ability to recognize and express emotions, to"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1101992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35858c5a69cf765fd2874e96553f693187da6d66",
            "isKey": false,
            "numCitedBy": 3512,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers are beginning to acquire the ability to express and recognize affect, and may soon be given the ability to \" have emotions. \" The essential role of emotion in both human cognition and perception, as demonstrated by recent neurological studies, indicates that affective computers should not only provide better performance in assisting humans, but also might enhance computers' abilities to make decisions. This paper presents and discusses key issues in \" affective computing, \" computing that relates to, arises from, or influences emotions. Models are suggested for computer recognition of human emotion, and new applications are presented for computer-assisted learning, perceptual information retrieval, arts and entertainment, and human health and interaction. Affective computing, coupled with new wear-able computers, will also provide the ability to gather new data necessary for advances in emotion and cog-nition theory. Nothing in life is to be feared. It is only to be understood. \u2013 Marie Curie Emotions have a stigma in science; they are believed to be inherently non-scientific. Scientific principles are derived from rational thought, logical arguments, testable hypotheses, and repeatable experiments. There is room alongside science for \" non-interfering \" emotions such as those involved in curiosity, frustration, and the pleasure of discovery. In fact, much scientific research has been prompted by fear. Nonetheless, the role of emotions is marginalized at best. Why bring \" emotion \" or \" affect \" into any of the deliberate tools of science? Moreover, shouldn't it be completely avoided when considering properties to design into computers? After all, computers control significant parts of our lives \u2013 the phone system, the stock market, nuclear power plants, jet landings, and more. Who wants a computer to be able to \" feel angry \" at them? To feel contempt for any living thing? In this essay I will submit for discussion a set of ideas on what I call \" affective computing, \" computing that relates to, arises from, or influences emotions. This will need some further clarification which I shall attempt below. I should say up front that I am not proposing the pursuit of computerized cingulotomies 1 or even into the business of building \" emotional computers \". 1 The making of small wounds in the ridge of the limbic system known as the cingulate gyrus, a surgical procedure to aid severely depressed patients. Nor will I propose answers to the difficult and intriguing questions , \" \u2026"
            },
            "slug": "Affective-Computing-Picard",
            "title": {
                "fragments": [],
                "text": "Affective Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Key issues in affective computing, \" computing that relates to, arises from, or influences emotions\", are presented and new applications are presented for computer-assisted learning, perceptual information retrieval, arts and entertainment, and human health and interaction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2882099"
                        ],
                        "name": "Jesper Vedelsby",
                        "slug": "Jesper-Vedelsby",
                        "structuredName": {
                            "firstName": "Jesper",
                            "lastName": "Vedelsby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesper Vedelsby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5846986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "910688d01c01856dd20715907af44157de8d3d1d",
            "isKey": false,
            "numCitedBy": 1971,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme."
            },
            "slug": "Neural-Network-Ensembles,-Cross-Validation,-and-Krogh-Vedelsby",
            "title": {
                "fragments": [],
                "text": "Neural Network Ensembles, Cross Validation, and Active Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how to estimate the optimal weights of the ensemble members using unlabeled data and how the ambiguity can be used to select new training data to be labeled in an active learning scheme."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2623469"
                        ],
                        "name": "J. Hoffbeck",
                        "slug": "J.-Hoffbeck",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Hoffbeck",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hoffbeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18072152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53b1f543ea8bcff02005f1322625f55ec52750a9",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A new covariance matrix estimator useful for designing classifiers with limited training data is developed. In experiments, this estimator achieved higher classification accuracy than the sample covariance matrix and common covariance matrix estimates. In about half of the experiments, it achieved higher accuracy than regularized discriminant analysis, but required much less computation."
            },
            "slug": "Covariance-Matrix-Estimation-and-Classification-Hoffbeck-Landgrebe",
            "title": {
                "fragments": [],
                "text": "Covariance Matrix Estimation and Classification With Limited Training Data"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A new covariance matrix estimator useful for designing classifiers with limited training data is developed, and in experiments, this estimator achieved higher classification accuracy than the sample covariance matrices and common covariance Matrix estimates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711320"
                        ],
                        "name": "M. Golfarelli",
                        "slug": "M.-Golfarelli",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Golfarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Golfarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747625"
                        ],
                        "name": "D. Maio",
                        "slug": "D.-Maio",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Maio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Maio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687735"
                        ],
                        "name": "D. Maltoni",
                        "slug": "D.-Maltoni",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Maltoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Maltoni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10795222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43794a59741c3a88fe448e4de7de54f3dc20c1dd",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of performance evaluation in biometric verification systems. By formulating the optimum Bayesian decision criterion for a verification system and by assuming the data distributions to be multinormals, we derive two statistical expressions for calculating theoretically the false acceptance and false rejection rates. Generally, the adoption of a Bayesian parametric model does not allow for obtaining explicit expressions for the calculation of the system errors. As far as biometric verification systems are concerned, some hypotheses can be reasonably adopted, thus allowing simple and affordable expressions to be derived. By using two verification system prototypes. Based on hand shape and human face, respectively, we show our results are well founded."
            },
            "slug": "On-the-Error-Reject-Trade-Off-in-Biometric-Systems-Golfarelli-Maio",
            "title": {
                "fragments": [],
                "text": "On the Error-Reject Trade-Off in Biometric Verification Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "By formulating the optimum Bayesian decision criterion for a verification system and by assuming the data distributions to be multinormals, two statistical expressions are derived for calculating theoretically the false acceptance and false rejection rates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17652870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6972ae4e0a5d1f5c06a56e10539133a0cf5b05e7",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-note-on-comparing-classifiers-Duin",
            "title": {
                "fragments": [],
                "text": "A note on comparing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 572361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
            "isKey": false,
            "numCitedBy": 4007,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network."
            },
            "slug": "Adaptive-Mixtures-of-Local-Experts-Jacobs-Jordan",
            "title": {
                "fragments": [],
                "text": "Adaptive Mixtures of Local Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases, which is demonstrated to be able to be solved by a very simple expert network."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102673564"
                        ],
                        "name": "T. Ens",
                        "slug": "T.-Ens",
                        "structuredName": {
                            "firstName": "Toulouse",
                            "lastName": "Ens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10175271,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "61bb90c1769d2076e9a5538bd3e4bffcb3f6a1b7",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind signal separation (BSS) and independent component analysis (ICA) are emerging techniques of array processing and data analysis, aiming at recovering unobserved signals or \u2018sources\u2019 from observed mixtures (typically, the output of an array of sensors), exploiting only the assumption of mutual independence between the signals. The weakness of the assumptions makes it a powerful approach but requires to venture beyond familiar second order statistics. The objective of this paper is to review some of the approaches that have been recently developed to address this exciting problem, to show how they stem from basic principles and how they relate to each other. Keywords\u2014 Signal separation, blind source separation, independent component analysis."
            },
            "slug": "Blind-signal-separation-:-statistical-principles-Ens",
            "title": {
                "fragments": [],
                "text": "Blind signal separation : statistical principles"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34898135"
                        ],
                        "name": "R. Hayes",
                        "slug": "R.-Hayes",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Hayes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hayes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43026843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d0b6c4e1c121c92855e2a13e8188ba88e54ba6e",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Parzen density estimate is known to be an effective tool for estimating the Bayes error, given a set of training samples from the class distributions. An algorithm is developed to select a given number of representative samples whose Parzen density estimate closely matches that of the entire sample set. Using this reduced representative set, a piecewise quadratic classifier which provides nearly optimal performance is designed. >"
            },
            "slug": "The-Reduced-Parzen-Classifier-Fukunaga-Hayes",
            "title": {
                "fragments": [],
                "text": "The Reduced Parzen Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm is developed to select a given number of representative samples whose Parzen density estimate closely matches that of the entire sample set, and a piecewise quadratic classifier which provides nearly optimal performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144023920"
                        ],
                        "name": "Michael Thompson",
                        "slug": "Michael-Thompson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40423501"
                        ],
                        "name": "Satosi Watanabe",
                        "slug": "Satosi-Watanabe",
                        "structuredName": {
                            "firstName": "Satosi",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satosi Watanabe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 193123221,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "fd14d87a33b7bd94008a7fb4f9f5919bff7ddfb5",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "No wonder you activities are, reading will be always needed. It is not only to fulfil the duties that you need to finish in deadline time. Reading will encourage your mind and thoughts. Of course, reading will greatly develop your experiences about everything. Reading frontiers of pattern recognition is also a way as one of the collective books that gives many advantages. The advantages are not only for you, but for the other peoples with those meaningful benefits."
            },
            "slug": "Frontiers-of-Pattern-Recognition-Thompson-Watanabe",
            "title": {
                "fragments": [],
                "text": "Frontiers of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Reading frontiers of pattern recognition is also a way as one of the collective books that gives many advantages, not only for you, but for the other peoples with those meaningful benefits."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38011004"
                        ],
                        "name": "D. Judd",
                        "slug": "D.-Judd",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Judd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Judd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700396"
                        ],
                        "name": "P. McKinley",
                        "slug": "P.-McKinley",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "McKinley",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. McKinley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5944314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98af05675a3293e398b0eb12c1fe8632abdd223b",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithmic enhancements are described that allow large reduction (for some data sets, over 95 percent) in the number of floating point operations in mean square error data clustering. These improvements are incorporated into a parallel data clustering tool, P-CLUSTER, developed in an earlier study. Experiments on segmenting standard texture images show that the proposed enhancements enable clustering of an entire 512/spl times/512 image at approximately the same computational cost as that of previous methods applied to only 5 percent of the image pixels."
            },
            "slug": "Large-scale-parallel-data-clustering-Judd-McKinley",
            "title": {
                "fragments": [],
                "text": "Large-scale parallel data clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments on segmenting standard texture images show that the proposed enhancements enable clustering of an entire 512/spl times/512 image at approximately the same computational cost as that of previous methods applied to only 5 percent of the image pixels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729122"
                        ],
                        "name": "A. Antos",
                        "slug": "A.-Antos",
                        "structuredName": {
                            "firstName": "Andr\u00e1s",
                            "lastName": "Antos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29949701,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "7fb56a65a2da533643514605b7a120f9e7463c17",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a short proof of the following result. Let (X,Y) be any distribution on N/spl times/{0,1}, and let (X/sub 1/,Y/sub 1/),...,(X/sub n/,Y/sub n/) be an i.i.d. sample drawn from this distribution. In discrimination, the Bayes error L*=inf/sub g/P{g(X)/spl ne/Y} is of crucial importance. Here we show that without further conditions on the distribution of (X,Y), no rate-of-convergence results can be obtained. Let /spl phi//sub n/(X/sub 1/,Y/sub 1/,...,X/sub n/,Y/sub n/) be an estimate of the Bayes error, and let {/spl phi//sub n/(.)} be a sequence of such estimates. For any sequence {a/sub n/} of positive numbers converging to zero, a distribution of (X,Y) may be found such that E{|L*-/spl phi//sub n/(X/sub 1/,Y/sub 1/,...,X/sub n/,Y/sub n/)|}/spl ges/a/sub n/ often converges infinitely."
            },
            "slug": "Lower-Bounds-for-Bayes-Error-Estimation-Antos-Devroye",
            "title": {
                "fragments": [],
                "text": "Lower Bounds for Bayes Error Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that without further conditions on the distribution of (X,Y), no rate-of-convergence results can be obtained and the Bayes error is of crucial importance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204972"
                        ],
                        "name": "M. V. Rossum",
                        "slug": "M.-V.-Rossum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rossum",
                            "middleNames": [
                                "C.",
                                "W.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Rossum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2281536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Lecture Notes for the MSc/DTC module. The brain is a complex computing machine which has evolved to give the ttest output to a given input. Neural computation has as goal to describe the function of the nervous system in mathematical and computational terms. By analysing or simulating the resulting equations, one can better understand its function, research how changes in parameters would eect the function, and try to mimic the nervous system in hardware or software implementations. Neural Computation is a bit like physics, that has been successful in describing numerous physical phenomena. However, approaches developed in those elds not always work for neural computation, because: 1. Physical systems are best studied in reduced, simplied circumstances, but the nervous system is hard to study in isolation. Neurons require a narrow range of operating conditions (temperature, oxygen, presence of other neurons, ion concentrations, ...) under which they work as they should. These conditions are hard to reproduce outside the body. Secondly, the neurons form a highly interconnected network. The function of the nervous systems depends on this connectivity and interaction, by trying to isolate the components, you are likely to alter the function. 2. It is not clear how much detail one needs to describe the computations in the brain. In these lectures we shall see various description levels. 3. Neural signals and neural connectivity are hard to measure, especially, if disturbance and damage to the nervous system is to be kept minimal. Perhaps Neural Computation has more in common with trying to gure out how a complicated machine, such as a computer or car works. Knowledge of the basic physics helps, but is not sucient. Luckily there are factors which perhaps make understanding the brain easier than understanding an arbitrary complicated machine: 1. There is a high degree of conservation across species. This means that animal studies can be used to gain information about the human brain. Furthermore, study of, say, the visual system might help to understand the auditory system. 2. The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives. Therefore it might be possible to nd the organising principles and develop a brain from there. This would be easier than guring out the complete 'wiring diagram'. 3. The nervous system is exible and robust, neurons die everyday. This stands \u2026"
            },
            "slug": "Neural-Computation-Rossum",
            "title": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives, and it might be possible to develop a brain from there."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144780498"
                        ],
                        "name": "D. Lovell",
                        "slug": "D.-Lovell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lovell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lovell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145387873"
                        ],
                        "name": "M. Niranjan",
                        "slug": "M.-Niranjan",
                        "structuredName": {
                            "firstName": "Mahesan",
                            "lastName": "Niranjan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Niranjan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999666"
                        ],
                        "name": "K. J. Dalton",
                        "slug": "K.-J.-Dalton",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Dalton",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J. Dalton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824203"
                        ],
                        "name": "R. Derom",
                        "slug": "R.-Derom",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Derom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Derom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19431484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "236094cafc9d09ae766a52631e615e97b44aef00",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-selection-using-expected-attainable-Lovell-Dance",
            "title": {
                "fragments": [],
                "text": "Feature selection using expected attainable discrimination"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706323"
                        ],
                        "name": "H. Chernoff",
                        "slug": "H.-Chernoff",
                        "structuredName": {
                            "firstName": "Herman",
                            "lastName": "Chernoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chernoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121905989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4ad944f76e5eb48ddb4fb87c26ad18e99800d3",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A novel method of representing multivariate data is presented. Each point in k-dimensional space, k\u226418, is represented by a cartoon of a face whose features, such as length of nose and curvature of mouth, correspond to components of the point. Thus every multivariate observation is visualized as a computer-drawn face. This presentation makes it easy for the human mind to grasp many of the essential regularities and irregularities present in the data. Other graphical representations are described briefly."
            },
            "slug": "The-Use-of-Faces-to-Represent-Points-in-k-Space-Chernoff",
            "title": {
                "fragments": [],
                "text": "The Use of Faces to Represent Points in k- Dimensional Space Graphically"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Every multivariate observation is visualized as a computer-drawn face that makes it easy for the human mind to grasp many of the essential regularities and irregularities present in the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775720"
                        ],
                        "name": "D. Hummels",
                        "slug": "D.-Hummels",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hummels",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hummels"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12610611,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0553c2669fc2945515f0997123fae3198c096d08",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of nonparametric error estimates may lead to biased results if the kernel covariances are estimated from the same data as are used to form the error estimate. If additional design samples are available, one may eliminate this bias by estimating the class covariances using an independent set of data. If, however, additional samples are not available, one may resort to leave-one-out type estimates of the kernel (for Parzen estimates) or metric (for nearest-neighbor estimates) for every sample being tested. The authors present an efficient algorithm for computation of these leave-one-out type estimates that requires little additional computational burden over procedures currently in use. The presentation is applicable to both Parzen and k-nearest neighbor (k-NN) type estimates. Experimental results demonstrating the efficiency of the algorithm are provided. >"
            },
            "slug": "Leave-One-Out-Procedures-for-Nonparametric-Error-Fukunaga-Hummels",
            "title": {
                "fragments": [],
                "text": "Leave-One-Out Procedures for Nonparametric Error Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors present an efficient algorithm for computation of leave-one-out type estimates that requires little additional computational burden over procedures currently in use and is applicable to both Parzen and k-nearest neighbor type estimates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11414801,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "42f711630779310444d52ced7bc23a236ae8d5c9",
            "isKey": false,
            "numCitedBy": 1286,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind signal separation (BSS) and independent component analysis (ICA) are emerging techniques of array processing and data analysis that aim to recover unobserved signals or \"sources\" from observed mixtures (typically, the output of an array of sensors), exploiting only the assumption of mutual independence between the signals. The weakness of the assumptions makes it a powerful approach, but it requires us to venture beyond familiar second order statistics, The objectives of this paper are to review some of the approaches that have been developed to address this problem, to illustrate how they stem from basic principles, and to show how they relate to each other."
            },
            "slug": "Blind-signal-separation:-statistical-principles-Cardoso",
            "title": {
                "fragments": [],
                "text": "Blind signal separation: statistical principles"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The objectives of this paper are to review some of the approaches that have been developed to address blind signal separation and independent component analysis, to illustrate how they stem from basic principles, and to show how they relate to each other."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143677016"
                        ],
                        "name": "K. Mohiuddin",
                        "slug": "K.-Mohiuddin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohiuddin",
                            "middleNames": [
                                "Moidin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mohiuddin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30193862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1b6328525693f31c937906a3b6f6da2b01015b6",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-OCR-performance-using-character-models-Mao-Mohiuddin",
            "title": {
                "fragments": [],
                "text": "Improving OCR performance using character degradation models and boosting algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49862983"
                        ],
                        "name": "A. Pellionisz",
                        "slug": "A.-Pellionisz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Pellionisz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pellionisz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061358113"
                        ],
                        "name": "Edward Rosenfeld",
                        "slug": "Edward-Rosenfeld",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142192775,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c4ba5479cc4242ffad927425fc107dcb7ab4bea6",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Network architecture: \"De memoria et reminiscentia\", Aristotle \"Cybernetics\", Norbert Wiener \"Outline of a theory of thought-processes and thinking machines\", E.R. Caianiello \"Adaptive systems using learning matrices\", K. Steinbuch and E. Schmitt \"A memory storage model utilizing spatial correlation functions\", James A. Anderson \"Associatron - a model of associative memory\", Kaoru Nakano \"The holographic hypothesis of memory structure in brain function and perception\", Karl H. Pribram et al \"How patterned neural connections can be set up by self-organization\", D.J. Willshaw and C. von der Malsburg \"Topographic organization of nerve fields\", Shun-ichi Amari \"ART 2 - self-organization of stable category recognition codes for analog input patterns\", Gail A. Carpenter and Stephen Grossberg \"Bidirectional associative memories\", Bart Kosko \"Sparse distributed memory\", Pentti Kanerva. Part 2 Computation and neurobiology: \"What the frog's eye tells the frog's brain\", J.Y. Lettvin et al \"Single units and sensation - a neuron doctrine for perceptual psychology?\", H.B. Barlow \"Large receptive fields and spatial transformations in the visual system\", J.T. McIlwain \"The extent to which biosonar information is represented in the bat auditory context\", Nobuo Suga \"Learning by selection\", J.-P. Changeux et al \"Neuronal group selection in the cerebral cortex\", Gerald M. Edelman and Leif H. Finkel \"Plasticity in the organization of adult cerebral cortical maps - a computer simulation based on neuronal group selection\", John C. Pearson et al \"Tensor network theory of the metaorganization of functional geometries in the central nervous system\", A. Pellionisz and R. Llinas \"How brains make chaos in order to make sense of the world\", Christine A. Skarda and Walter J. Freeman \"Computational maps in the brain\", Eric I. Knudsen et al \"A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons\", David Zipser and Richard A. Andersen \"Long-term synaptic potentiation\", Thomas H. Brown et al. Part 3 Statistics and pattern classification: \"Learning machines\", Nils Nilsson \"Nearest neighbor pattern classification\", T.M. Cover and P.E. Hart \"Practical techniques for pattern recognition\", Bruce G. Batchelor \"A neural model for category learning\", Douglas L. Reilly et al \"A relaxation model for memory with high storage density\", Charles M. Bachmann et al \"Statistical pattern recognition with neural networks - benchmarking studies\", Teuvo Kohonen et al \"Self-organization in a perceptual network\", Ralph Linsker \"Image compression by back propagation - an example of extensional programming\", Garrison W. Cottrell et al \"Neural networks and principal component analysis - learning from examples without local minima\", Pierre Baldi and Kurt Hornik. Part 4 Current applications and future problems: \"Perceptrons\", Marvin L. Minsky and Seymour A. Papert. (Part contents)."
            },
            "slug": "Directions-for-research-Anderson-Pellionisz",
            "title": {
                "fragments": [],
                "text": "Directions for research"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Part 1 Network architecture: \"De memoria et reminiscentia\", Aristotle \"Cybernetics\", Norbert Wiener \"Outline of a theory of thought-processes and thinking machines\", E.R. Steinbuch and E.M. Caianiello \"Adaptive systems using learning matrices\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4453577"
                        ],
                        "name": "D. Ridder",
                        "slug": "D.-Ridder",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Ridder",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ridder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11229430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f08e13ef069e54f8777a0a61506decd323e65640",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sammon's-mapping-using-neural-networks:-A-Ridder-Duin",
            "title": {
                "fragments": [],
                "text": "Sammon's mapping using neural networks: A comparison"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144972322"
                        ],
                        "name": "J. Anderson",
                        "slug": "J.-Anderson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Anderson",
                            "middleNames": [
                                "A.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120679333,
            "fieldsOfStudy": [
                "Business",
                "Biology"
            ],
            "id": "acb9df924d56ee66ca68b4b878c90847aac04292",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "7-Logistic-discrimination-Anderson",
            "title": {
                "fragments": [],
                "text": "7 Logistic discrimination"
            },
            "venue": {
                "fragments": [],
                "text": "Classification, Pattern Recognition and Reduction of Dimensionality"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3345095"
                        ],
                        "name": "W. Siedlecki",
                        "slug": "W.-Siedlecki",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Siedlecki",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Siedlecki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765522"
                        ],
                        "name": "J. Sklansky",
                        "slug": "J.-Sklansky",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Sklansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sklansky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38276676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "113d488bf37d1ff61c51a4fc2704cfbf637ecaca",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-note-on-genetic-algorithms-for-large-scale-Siedlecki-Sklansky",
            "title": {
                "fragments": [],
                "text": "A note on Genetic Algorithms for Large-Scale Feature Selection"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40423501"
                        ],
                        "name": "Satosi Watanabe",
                        "slug": "Satosi-Watanabe",
                        "structuredName": {
                            "firstName": "Satosi",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satosi Watanabe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "We take this ability for granted until we face the task of teaching a machine how to do the same."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "Moreover, as stated earlier, a small number of features can alleviate the curse of dimensionality when the number of training samples is limited."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56583421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72323fd334f851363eaab229bb5933fa98e59040",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Recognition:-Human-and-Mechanical-Watanabe",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition: Human and Mechanical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 13948283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6fa2a069744f7ab4ca58b1177134ca3f88f2601",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This correspondence calls attention to several frequently used assumptions and techniques culled from the pattern recognition literature."
            },
            "slug": "Candide's-Practical-Principles-of-Experimental-Nagy",
            "title": {
                "fragments": [],
                "text": "Candide's Practical Principles of Experimental Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This correspondence calls attention to several frequently used assumptions and techniques culled from the pattern recognition literature."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145483797"
                        ],
                        "name": "E. Backer",
                        "slug": "E.-Backer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Backer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Backer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "A big frustration in using clustering programs is the lack of guidelinesavailable forchoosingK, initialpartition,updating the partition, adjusting the number of clusters, and the stopping criterion [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58398847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79b7c59492b92173e0c2ec07adad4918e86440",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "RECONNAISANCE OF THE DOMAIN. Cluster Analysis: Why, What, How, When. Methods in Clustering Data. Examples and Discussion. Framing of Cluster Analysis Studies. Fuzzy Clustering: A Branch in Fuzzy Logic. RECONSIDERATION OF THE TASK. Indetermination and Uncertainty in Cluster Analysis. The Need and Relevance of Computer-assisted Reasoning in Cluster Analysis. COMPUTER-ASSISTED DECISION SUPPORT IN CLUSTER ANALYSIS. Knowledge-based Expert Systems: An Introduction. Rule Base Developments for Cluster Analysis. Case Study: Analysis of Delphind Sonar Sound Signals."
            },
            "slug": "Computer-assisted-reasoning-in-cluster-analysis-Backer",
            "title": {
                "fragments": [],
                "text": "Computer-assisted reasoning in cluster analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper discusses the development of rule base Developments for Cluster Analysis, and the need and Relevance of Computer-assisted Reasoning in Cluster Analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410228"
                        ],
                        "name": "N. Choakjarernwanit",
                        "slug": "N.-Choakjarernwanit",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Choakjarernwanit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Choakjarernwanit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27187885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aa95b6982886b0b8d5e71cee3f4315c8228811e",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-selection-based-on-the-approximation-of-by-Pudil-Novovicov\u00e1",
            "title": {
                "fragments": [],
                "text": "Feature selection based on the approximation of class densities by finite mixtures of special type"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144923779"
                        ],
                        "name": "Bin Yu",
                        "slug": "Bin-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1825087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65d34977d9055f42e51dc1e7d9b4ca2f36c17537",
            "isKey": false,
            "numCitedBy": 1094,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "We review the principles of minimum description length and stochastic complexity as used in data compression and statistical modeling. Stochastic complexity is formulated as the solution to optimum universal coding problems extending Shannon's basic source coding theorem. The normalized maximized likelihood, mixture, and predictive codings are each shown to achieve the stochastic complexity to within asymptotically vanishing terms. We assess the performance of the minimum description length criterion both from the vantage point of quality of data compression and accuracy of statistical inference. Context tree modeling, density estimation, and model selection in Gaussian linear regression serve as examples."
            },
            "slug": "The-Minimum-Description-Length-Principle-in-Coding-Barron-Rissanen",
            "title": {
                "fragments": [],
                "text": "The Minimum Description Length Principle in Coding and Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The normalized maximized likelihood, mixture, and predictive codings are each shown to achieve the stochastic complexity to within asymptotically vanishing terms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143677016"
                        ],
                        "name": "K. Mohiuddin",
                        "slug": "K.-Mohiuddin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohiuddin",
                            "middleNames": [
                                "Moidin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mohiuddin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5132449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0a84918e9243eaaafc308f2ebe0c248b5111b52",
            "isKey": false,
            "numCitedBy": 2573,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural nets (ANNs) are massively parallel systems with large numbers of interconnected simple processors. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model. It outlines network architectures and learning processes, and presents some of the most commonly used ANN models. It concludes with character recognition, a successful ANN application."
            },
            "slug": "Artificial-Neural-Networks:-A-Tutorial-Jain-Mao",
            "title": {
                "fragments": [],
                "text": "Artificial Neural Networks: A Tutorial"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model, and outlines network architectures and learning processes, and presents some of the most commonly used ANN models."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878706"
                        ],
                        "name": "D. Michie",
                        "slug": "D.-Michie",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Michie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Michie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107314775"
                        ],
                        "name": "C. C. Taylor",
                        "slug": "C.-C.-Taylor",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15773445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fad1bd501aa769f7701c1016f8a4d1473ca77601",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "Survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning."
            },
            "slug": "Machine-Learning,-Neural-and-Statistical-Michie-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Machine Learning, Neural and Statistical Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50130827"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7980390,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ce7876c0b2acb52b88610ce9cfe21d239c28922",
            "isKey": false,
            "numCitedBy": 1929,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context."
            },
            "slug": "On-Bayesian-Analysis-of-Mixtures-with-an-Unknown-of-Richardson-Green",
            "title": {
                "fragments": [],
                "text": "On Bayesian Analysis of Mixtures with an Unknown Number of Components (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143677016"
                        ],
                        "name": "K. Mohiuddin",
                        "slug": "K.-Mohiuddin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohiuddin",
                            "middleNames": [
                                "Moidin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mohiuddin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46941514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a6ccfcbe4eb16a521b08233ea302c935167ce16",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes a node saliency measure and a backpropagation type of algorithm to compute the node saliencies. A node-pruning procedure is then presented to remove insalient nodes in the network to create a parsimonious network. The optimal/suboptimal subset of features are simultaneously selected by the network. The performance of the proposed approach for feature selection is compared with Whitney's feature selection method. One advantage of the node-pruning procedure over classical feature selection methods is that the node-pruning procedure can simultaneously \"optimize\" both the feature set and the classifier, while classical feature selection methods select the \"best\" subset of features with respect to a fixed classifier."
            },
            "slug": "Parsimonious-network-design-and-feature-selection-Mao-Mohiuddin",
            "title": {
                "fragments": [],
                "text": "Parsimonious network design and feature selection through node pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A node-pruning procedure is presented to remove insalient nodes in the network to create a parsimonious network and the optimal/suboptimal subset of features are simultaneously selected by the network."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12874899"
                        ],
                        "name": "J. Sammon",
                        "slug": "J.-Sammon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sammon",
                            "middleNames": [
                                "W."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sammon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43151050,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "154f8a9906bcc99fca9b17aa521649b1c3734093",
            "isKey": false,
            "numCitedBy": 3461,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results. The algorithm is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "slug": "A-Nonlinear-Mapping-for-Data-Structure-Analysis-Sammon",
            "title": {
                "fragments": [],
                "text": "A Nonlinear Mapping for Data Structure Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results that is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662980"
                        ],
                        "name": "R. Winder",
                        "slug": "R.-Winder",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Winder",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Winder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40765117,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "84fdfb9d1d19a57d119ce96210b03a6227428ee7",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A tabulation of the 2470 representative threshold functions of seven arguments has been prepared by the author. This paper discusses the methods used in, and the threshold logic implications of, the enumeration. The self-dual classification method of Goto-Takahasi was employed. A lattice was defined on the 8-cube in terms of which all 2-monotonic, canonical, self-dual functions of eight arguments were directly generated. Each such representative function was then treated by a modified form of the Muroga-Toda-Takasu linear programming test-synthesis procedure to obtain minimal 1-realizations. The Chow parameters for each function were calculated, and the final enumeration was ordered lexicographically by these parameters to afford a trivial test-synthesis procedure for n?7. The enumeration demonstrated that minimal 1-realizations are still integral for n?7; it corroborated Cobham's result that complete monotonicity is equivalent to 1-realizability, and established hyper-2-monotonicity as a useful characterization, for n?7. It significantly extended our knowledge of the number of threshold functions and the various symmetry types, the size of weights and threshold required, the number of iterations required by the linear program, and similar statistics."
            },
            "slug": "Enumeration-of-Seven-Argument-Threshold-Functions-Winder",
            "title": {
                "fragments": [],
                "text": "Enumeration of Seven-Argument Threshold Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The enumeration demonstrated that minimal 1-realizations are still integral for n?7; it corroborated Cobham's result that complete monotonicity is equivalent to 1- realizability, and established hyper-2-monotonicity as a useful characterization, for n?:7."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060471826"
                        ],
                        "name": "Manish Mehta",
                        "slug": "Manish-Mehta",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Mehta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manish Mehta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144947410"
                        ],
                        "name": "R. Agrawal",
                        "slug": "R.-Agrawal",
                        "structuredName": {
                            "firstName": "Rakesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Agrawal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "435384cc048ce88afa4e9cfb51e38297c4e7a255",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the application of the Minimum Description Length principle for pruning decision trees. We present a new algorithm that intuitively captures the primary goal of reducing the misclassification error. An experimental comparison is presented with three other pruning algorithms. The results show that the MDL pruning algorithm achieves good accuracy, small trees, and fast execution times."
            },
            "slug": "MDL-Based-Decision-Tree-Pruning-Mehta-Rissanen",
            "title": {
                "fragments": [],
                "text": "MDL-Based Decision Tree Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new algorithm is presented that intuitively captures the primary goal of reducing the misclassification error and achieves good accuracy, small trees, and fast execution times in the MDL pruning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3303987"
                        ],
                        "name": "J. V. Campenhout",
                        "slug": "J.-V.-Campenhout",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Campenhout",
                            "middleNames": [
                                "M.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Campenhout"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20060045,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "244b25c97bd53a235a1fca458c410a81c95f019d",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "An aspect of the measurement selection problem\u00bfthe existence of anomalous orderings on the probability of error obtained by selected subsets of measurements\u00bfis discussed. It is shown that for any ordering on the probability of error as a function of the subset of measurements (subject to an obvious set monotonicity condition), there exists a multivariate normal two-hypothesis problem N(\u00bf,K) versus N(\u00bf\u00bf,K) that exhibits this ordering. Thus no known nonexhaustive sequential k-measurement selection procedure is optimal, even for jointly normal measurements."
            },
            "slug": "On-the-Possible-Orderings-in-the-Measurement-Cover-Campenhout",
            "title": {
                "fragments": [],
                "text": "On the Possible Orderings in the Measurement Selection Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that for any ordering on the probability of error as a function of the subset of measurements (subject to an obvious set monotonicity condition), there exists a multivariate normal two-hypothesis problem N(K) versus N(\u00bf\u00bf,K) that exhibits this ordering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49806057"
                        ],
                        "name": "W. Bean",
                        "slug": "W.-Bean",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bean",
                            "middleNames": [
                                "Bennett"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71208322,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "6b6f7ee9109fc4514e813a7b7745275b408da60f",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book with the flashy title and the explosive dust cover consists of an effort on the part of A. B. Garrett to set forth what he conceives to be the major discoveries of the modern world. Insofar as it is possible to find and analyze those forces which have led to discovery, he aims to do so. Hardly any aspect of psychological and intellectual study defies critical and satisfying analysis more obdurately than does the elusive matter of inspiration, creativity, and discovery. What is it that impels, excites, or makes luminous those creative people who can see a new truth which everyone suddenly recognizes as having been all too self-evident? In fact they decry it as already known or unimportant and, after a while as old. What is it that ignites the flame of inspiration in a poet, a dramatist, or a novelist? What enables the creative artist to"
            },
            "slug": "The-Flash-of-Genius.-Bean",
            "title": {
                "fragments": [],
                "text": "The Flash of Genius."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book with the flashy title and the explosive dust cover consists of an effort on the part of A. B. Garrett to set forth what he conceives to be the major discoveries of the modern world."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3176394"
                        ],
                        "name": "P. Somol",
                        "slug": "P.-Somol",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Somol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Somol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1897550"
                        ],
                        "name": "P. Pacl\u00edk",
                        "slug": "P.-Pacl\u00edk",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pacl\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pacl\u00edk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13216311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9aaa7dcee5657b8d6e73ba89b2ff9723b45f093",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-floating-search-methods-in-feature-Somol-Pudil",
            "title": {
                "fragments": [],
                "text": "Adaptive floating search methods in feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3332651,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8566d8dcf52f59dd157803283971f0145f832614",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "From a Bayesian decision theoretic framework, we show that the reason why the usual statistical approaches do not take context into account is because of the assumptions made on the joint prior probability function and because of the simplistic loss function chosen. We illustrate how the constraints sometimes employed by artificial intelligence researchers constitute a different kind of assumption on the joint prior probability function. We discuss a couple of loss functions which do take context into account and when combined with the joint prior probability constraint create a decision problem requiring a combinatorial state space search. We also give a theory for how probabilistic relaxation works from a Bayesian point of view."
            },
            "slug": "Decision-Making-in-Context-Haralick",
            "title": {
                "fragments": [],
                "text": "Decision Making in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "From a Bayesian decision theoretic framework, it is shown that the reason why the usual statistical approaches do not take context into account is because of the assumptions made on the joint prior probability function andBecause of the simplistic loss function chosen."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12795832,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c47e6bc5ca7c90abd7b352b34529e33213f3c8ae",
            "isKey": false,
            "numCitedBy": 3059,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Floating-search-methods-in-feature-selection-Pudil-Novovicov\u00e1",
            "title": {
                "fragments": [],
                "text": "Floating search methods in feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784213"
                        ],
                        "name": "R. Bajcsy",
                        "slug": "R.-Bajcsy",
                        "structuredName": {
                            "firstName": "Ruzena",
                            "lastName": "Bajcsy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bajcsy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802545"
                        ],
                        "name": "S. Kovacic",
                        "slug": "S.-Kovacic",
                        "structuredName": {
                            "firstName": "Stanislav",
                            "lastName": "Kovacic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kovacic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19718946,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "36c9356db20ab4eace62d86e0a576fa6382d84b9",
            "isKey": false,
            "numCitedBy": 1217,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiresolution-elastic-matching-Bajcsy-Kovacic",
            "title": {
                "fragments": [],
                "text": "Multiresolution elastic matching"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1958412,
            "fieldsOfStudy": [
                "Education",
                "Mathematics"
            ],
            "id": "274ba2f97f4ee527997cd5cd7080ffe600bd8264",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider an item that belongs to one of two classes, ? = 0 or ? = 1, with equal probability. Suppose also that there are two measurement experiments E1 and E2 that can be performed, and suppose that the outcomes are independent (given ?). Let Ei? denote an independent performance of experiment Ei. Let Pe(E) denote the probability of error resulting from the performance of experiment E. Elashoff [1] gives an example of three experiments E1,E2,E3 such that Pe(E1) < Pe(E2) < Pe(E3), but Pe(E1,E3) < Pe(E1,E2). Toussaint [2] exhibits binary valued experiments satisfying Pe(E1) < Pe(E2) < Pe(E3), such that Pe(E2,E3) < Pe(E1,E3) < Pe(E1,E2). We shall give an example of binary valued experiments E1 and E2 such that Pe(E1) < Pe(E2), but Pe(E2,E2?) < Pe(E1,E2) < Pe(E1,E1?). Thus if one observation is allowed, E1 is the best experiment. If two observations are allowed, then two independent copies of the ``worst'' experiment E2 are preferred. This is true despite the conditional independence of the observations."
            },
            "slug": "The-Best-Two-Independent-Measurements-Are-Not-the-Cover",
            "title": {
                "fragments": [],
                "text": "The Best Two Independent Measurements Are Not the Two Best"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "If two observations are allowed, then two independent copies of the ``worst'' experiment E2 are preferred, and if one observation is allowed, E1 is the best experiment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31129825"
                        ],
                        "name": "A. Webb",
                        "slug": "A.-Webb",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Webb",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Webb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205014399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "003739d648c7285c60d8aa9558f8efc713cd4fb0",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multidimensional-scaling-by-iterative-majorization-Webb",
            "title": {
                "fragments": [],
                "text": "Multidimensional scaling by iterative majorization using radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48403,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695784"
                        ],
                        "name": "U. Fayyad",
                        "slug": "U.-Fayyad",
                        "structuredName": {
                            "firstName": "Usama",
                            "lastName": "Fayyad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Fayyad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398381803"
                        ],
                        "name": "G. Piatetsky-Shapiro",
                        "slug": "G.-Piatetsky-Shapiro",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Piatetsky-Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piatetsky-Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5708816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83ea8d14d2e9bc793255758ccc77fb7929df47a9",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a first step towards a unifying framework for Knowledge Discovery in Databases. We describe links between data mining, knowledge discovery, and other related fields. We then define the KDD process and basic data mining algorithms, discuss application issues and conclude with an analysis of challenges facing practitioners in the field."
            },
            "slug": "Knowledge-Discovery-and-Data-Mining:-Towards-a-Fayyad-Piatetsky-Shapiro",
            "title": {
                "fragments": [],
                "text": "Knowledge Discovery and Data Mining: Towards a Unifying Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The KDD process and basic data mining algorithms are defined, links between data mining, knowledge discovery, and other related fields are described, and an analysis of challenges facing practitioners in the field is analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781982"
                        ],
                        "name": "D. Hand",
                        "slug": "D.-Hand",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hand",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26393105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12bd94c631b4751439cb17012feca227d6fb15c2",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-advances-in-error-rate-estimation-Hand",
            "title": {
                "fragments": [],
                "text": "Recent advances in error rate estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144290244"
                        ],
                        "name": "H. Niemann",
                        "slug": "H.-Niemann",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Niemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Niemann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10218434,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "44ae199fa97f90239fc0500d5048887fffeb8b61",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Linear-and-nonlinear-mapping-of-patterns-Niemann",
            "title": {
                "fragments": [],
                "text": "Linear and nonlinear mapping of patterns"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064699986"
                        ],
                        "name": "D. Chakrabarti",
                        "slug": "D.-Chakrabarti",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chakrabarti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068766142"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "ICA has been successfully used for blind-source separation [78]; extracting linear feature combinations that define independent sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118274211,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6ade6139ee56684cdf190f7f1212541fcb5ffb69",
            "isKey": false,
            "numCitedBy": 2269,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An apparatus for hydrolytic degradation of plastics in which plastic material is deposited into a tubular housing via a feed hopper. An elongated screw shaft has a first section in the form of a high pitch screw thread disposed below the feed hopper to receive and advance the material to a second section. The second section of the screw shaft is in the form of a lower pitch thread for compressing the plastic material and transferring it to a longer, third section in the form of kneading discs, from which material passes through an outlet nozzle section to a cyclone separator where trapped gases and liquid may be withdrawn. The tubular housing is vented upstream of the feed hopper and a water inlet pipe is disposed adjacent to the second section of the screw shaft, downstream of the feed hopper. The outlet nozzle section is provided with pressure measuring and regulating means and a liquid level measuring and regulating device."
            },
            "slug": "A-fast-fixed-point-algorithm-for-independent-Chakrabarti-Hoyer",
            "title": {
                "fragments": [],
                "text": "A fast fixed - point algorithm for independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "An appropriate kernel function K (as in kernel PCA, Section 4.1) needs to be selected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "The best known linear feature extractor is the principal component analysis (PCA) or Karhunen-Loe\u00c1ve expansion, that computes the m largest eigenvectors of the d d covariance matrix of the n d-dimensional patterns."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "The basic idea of kernel PCA is to first map input data into some new feature space F typically via a nonlinear function (e.g., polynomial of degree p) and then perform a linear PCA in the mapped space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "6 shows an example of the feature selection procedure using the floating search technique on the PCA features in the digit dataset for two different training set sizes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "These so-called autoassociative, or nonlinear PCA networks offer a powerful tool to train and describe nonlinear subspaces [98]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "4a shows the architecture of a network which is able to find the PCA subspace [117]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "The linear PCA in the F space solves the eigenvectors of the correlation matrix X X T , which is also called the kernel matrix K X"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "In discriminant analysis, interclass separation is emphasized by replacing the total covariance matrix in PCA by a general separability measure like the Fisher criterion, which results in finding the eigenvectors of S\u00ff1w Sb (the product of the inverse of the within-class scatter matrix, Sw, and the between-class\nscatter matrix, Sb) [58]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Since PCA uses the most expressive features (eigenvectors with the largest eigenvalues), it effectively approximates the data by a linear subspace using the mean squared error criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "In kernel PCA, the first m eigenvectors of K X"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "Note that for a complete representation, up to m eigenvectors in E may be needed (depending on the kernel function) by kernel PCA, while in linear PCA a set of d eigenvectors represents the original feature space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Whereas PCA is an unsupervised linear feature extraction method, discriminant analysis uses the category information associated with each pattern for (linearly) extracting the most discriminatory features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "One such method which is directly related to PCA is called the Kernel PCA [73], [145]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "To avoid computing the mapping explicitly, kernel PCA employs only Mercer kernels which can be decomposed into a dot product,\nK x; y x y : As a result, the kernel space has a well-defined metric."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": true,
            "numCitedBy": 9897,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40574103"
                        ],
                        "name": "S. Klinke",
                        "slug": "S.-Klinke",
                        "structuredName": {
                            "firstName": "Sigbert",
                            "lastName": "Klinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Klinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549124"
                        ],
                        "name": "J. Polzehl",
                        "slug": "J.-Polzehl",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Polzehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Polzehl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60628897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43d7cce568c4d6ec9cc2d95fe54dd5fa8f51e936",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques. This class contains methods designed for analyzing high dimensional data using low-dimensional projections. The main idea is to describe \u201cinteresting\u201d projections by maximizing an objective function or projection pursuit index."
            },
            "slug": "Exploratory-Projection-Pursuit-Klinke-Polzehl",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques that contains methods designed for analyzing high dimensional data using low-dimensional projections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "At these places, we have to rely on the background knowledge which may be available only to the more experienced readers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115386250,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a68fff5d07699b33af739e88c59ff5b0dd4ee874",
            "isKey": false,
            "numCitedBy": 7015,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The Jackknife Estimate of Bias The Jackknife Estimate of Variance Bias of the Jackknife Variance Estimate The Bootstrap The Infinitesimal Jackknife The Delta Method and the Influence Function Cross-Validation, Jackknife and Bootstrap Balanced Repeated Replications (Half-Sampling) Random Subsampling Nonparametric Confidence Intervals."
            },
            "slug": "The-jackknife,-the-bootstrap,-and-other-resampling-Efron",
            "title": {
                "fragments": [],
                "text": "The jackknife, the bootstrap, and other resampling plans"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402942759"
                        ],
                        "name": "H. Avi-Itzhak",
                        "slug": "H.-Avi-Itzhak",
                        "structuredName": {
                            "firstName": "Hadar",
                            "lastName": "Avi-Itzhak",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Avi-Itzhak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32530891"
                        ],
                        "name": "Thanh A. Diep",
                        "slug": "Thanh-A.-Diep",
                        "structuredName": {
                            "firstName": "Thanh",
                            "lastName": "Diep",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thanh A. Diep"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7115875,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3c8c9f6919cc91c1b6c14e86594d521e2386be55",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents new upper and lower bounds on the minimum probability of error of Bayesian decision systems for the two-class problem. These bounds can be made arbitrarily close to the exact minimum probability of error, making them tighter than any previously known bounds."
            },
            "slug": "Arbitrarily-Tight-Upper-and-Lower-Bounds-on-the-of-Avi-Itzhak-Diep",
            "title": {
                "fragments": [],
                "text": "Arbitrarily Tight Upper and Lower Bounds on the Bayesian Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "New upper and lower bounds on the minimum probability of error of Bayesian decision systems for the two-class problem are presented, making them tighter than any previously known bounds."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759817"
                        ],
                        "name": "I. Borg",
                        "slug": "I.-Borg",
                        "structuredName": {
                            "firstName": "Ingwer",
                            "lastName": "Borg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Borg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2594542"
                        ],
                        "name": "P. Groenen",
                        "slug": "P.-Groenen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Groenen",
                            "middleNames": [
                                "J.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Groenen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "However, the number of possible subsets grows combinatorially, making this exhaustive search impractical for even moderate values of m and d. Cover and Van Campenhout [35] showed that no nonexhaustive sequential feature selection procedure can be guaranteed to produce the optimal subset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124336568,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "eca9dc218b5a235f3f8d2f94d1decaa2a443db7d",
            "isKey": false,
            "numCitedBy": 1122,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern multidimensional scalin , Modern multidimensional scalin , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u062c\u0646\u062f\u06cc \u0634\u0627\u067e\u0648\u0631 \u0627\u0647\u0648\u0627\u0632"
            },
            "slug": "Modern-multidimensional-scaling-Borg-Groenen",
            "title": {
                "fragments": [],
                "text": "Modern multidimensional scaling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51078936"
                        ],
                        "name": "H. Demuth",
                        "slug": "H.-Demuth",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Demuth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Demuth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5701055"
                        ],
                        "name": "M. Beale",
                        "slug": "M.-Beale",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Beale",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Beale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62228050,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "44463fe256cb0889d907592ffe84abf0b8f93f98",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Note: Includes index + release notes version 2.0a Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Neutral-network-toolbox-for-use-with-Matlab-Demuth-Beale",
            "title": {
                "fragments": [],
                "text": "Neutral network toolbox for use with Matlab"
            },
            "tldr": {
                "abstractSimilarityScore": 33,
                "text": "This research presents a meta-modelling framework that automates the very labor-intensive and therefore time-heavy and expensive and therefore expensive and expensive process of manually cataloging and updating reference records for this type of research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Some of the well-known clustering algorithms are listed in Table 10 [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "While several algorithms are available to estimate the intrinsic dimensionality [81], they do not indicate how a subspace of the identified dimensionality can be easily identified."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": ", model-based) approaches to unsupervised classification [81]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 284
                            }
                        ],
                        "text": "The problem of partitional clustering can be formally stated as follows: Given n patterns in a d-dimensional metric space, determine a partition of the patterns into K clusters, such that the patterns in a cluster are more similar to each other than to patterns in different clusters [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dubes, Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, N.J.: Prentice Hall,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 94
                            }
                        ],
                        "text": "An obvious choice in this class is to use the minimum description length (MDL) criterion [10] [138], but several other model selection criteria have been proposed: Schwarz's Bayesian inference criterion (BIC), the minimum message length (MML) criterion, and Akaike's information criterion (AIC) [2], [148], [167]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "A number of techniques, such as the minimum description length (MDL) principle [138], can be used to select this parameter (see Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "A number of techniques, such as the minimum description length (MDL) principle [138], can be used to select this parameter (see Section 8.2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 150
                            }
                        ],
                        "text": "To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry, Singapore"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Table 5 lists most of the well-known feature selection methods which have been proposed in the literature [85]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 22
                            }
                        ],
                        "text": "Ferri et al. [50] and\nJain and Zongker [85] have compared several of the feature\nselection algorithms in terms of classification error and run time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Jain and Zongker [85] illustrate this phenomenon for a two-class classification problem involving 20-dimensional Gaussian class-conditional densities (the same data was also used by Trunk [157] to demonstrate the curse of dimensionality phenomenon)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Jain and Zongker [85] have compared several of the feature 16 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aFeature Selection: Evaluation, Application, and Small Sample Performance,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 281
                            }
                        ],
                        "text": "In addition to the excellent textbooks by Duda and Hart [44],1 Fukunaga [58], Devijver and Kittler [39], Devroye et al. [41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "[41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Kanal's review also contained a large section on structural methods and pattern grammars."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Kanal placed less emphasis on applications, but more on modeling and design of pattern recognition systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "By the time Kanal wrote his survey paper, more than 500 papers and about half a dozen books on pattern recognition were already published."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "The discussion on automatic feature extraction in [89] was based on various distance measures between classconditional probability density functions and the resulting error bounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "In comparison to the state of the pattern recognition field as described by Nagy and Kanal in the 1960s and 1970s, today a number of commercial pattern recognition systems are available which even individuals can buy for personal use (e.g., machine printed character recognition and isolated spoken word recognition)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aPatterns in Pattern Recognition: 1968-1974,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Information Theory, vol. 20,"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97306284"
                        ],
                        "name": "K. Fu",
                        "slug": "K.-Fu",
                        "structuredName": {
                            "firstName": "King-Sun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fu [56] introduced the notion of attributed grammars which unifies syntactic and statistical pattern recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In many recognition problems involving complex patterns, it is more appropriate to adopt a hierarchical perspective where a pattern is viewed as being composed of simple subpatterns which are themselves built from yet simpler subpatterns [56], [121]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This paradigm has been used in situations where the patterns have a definite structure which can be captured in terms of a set of rules, such as EKG waveforms, textured images, and shape analysis of contours [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62636588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2982aba3ad1a424ccd342895c308c9744f06ea6f",
            "isKey": true,
            "numCitedBy": 985,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Syntactic-Pattern-Recognition-And-Applications-Fu",
            "title": {
                "fragments": [],
                "text": "Syntactic Pattern Recognition And Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "In addition to the excellent textbooks by Duda and Hart [44],(1) Fukunaga [58], Devijver and Kittler [39], Devroye et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "is minimum, where L\u0085!i; !j\u0086 is the loss incurred in deciding !i when the true class is !j and P \u0085!jjx\u0086 is the posterior probability [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 42
                            }
                        ],
                        "text": "In addition to the excellent textbooks by Duda and Hart [44],1 Fukunaga [58], Devijver and Kittler [39], Devroye et al. [41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Its second edition by Duda, Hart, and Stork [45] is in press."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene Analysis, New York"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 139
                            }
                        ],
                        "text": "Model selection based on stochastic complexity has been applied to feature selection in both supervised learning and unsupervised learning [159] and pruning in decision 32 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 116
                            }
                        ],
                        "text": "2), which was proposed in 1977 [36], and which is now a very popular approach for density estimation and clustering [159], due to the computing power available today."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 54
                            }
                        ],
                        "text": "model and 2) how to estimate the number of components [159]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 231
                            }
                        ],
                        "text": "To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aModel Selection in Unsupervised Learning with Applications to Document Clustering,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Sixth Int'l Conf. Machine Learning,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ICA has been successfully used for blind-source separation [78]; extracting linear feature combinations that define independent sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Oja [118] shows how autoassociative networks can be used for ICA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": ": STATISTICAL PATTERN RECOGNITION: A REVIEW 19\n4. http://www.gmd.de/ml-archive/\nA larger hidden layer may result in overtraining."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aIndependent Component Analysis, a New Concept?,o"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "Now the probability of error which is a function of both n and d can be written as:\nPe n; d Z 1 d 1 2 p e\u00ff12z2dz;where 5\nd Pd\ni 1 1i 1 1n Pd i 1 1i dn q : 6 Trunk showed that limd!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Trunk considered the following two cases:\n1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "Trunk [157] provided a simple example to illustrate the curse of dimensionality which we reproduce below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "Jain and Zongker [85] illustrate this phenomenon for a two-class classification problem involving 20-dimensional Gaussian class-conditional densities (the same data was also used by Trunk [157] to demonstrate the curse of dimensionality phenomenon)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Trunk found the maximum likelihood estimate m\u0302 of m and used the plug-in decision rule (substitute m\u0302 for m in the optimal Bayes decision rule)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Problem of Dimensionality: A Simple Example,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "It is now understood that these problems can, to some extent, be circumvented using regularization, or can even be completely resolved by a proper design of classification procedures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "In the classification mode, the trained classifier assigns the input pattern to one of the pattern classes under consideration based on the measured features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "The mean vectors for the two classes have the following components\nm1 1; 1 2 p ; 1 3 p ; ; 1 d p and m2 \u00ff1;\u00ff 1 2 p ;\u00ff 1 3 p ; ;\u00ff 1 d p :\nNote that the features are statistically independent and the discriminating power of the successive features decreases monotonically with the first feature\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaDimensionality and Sample Size Considerations in Pattern Recognition Practice,\u00ba Handbook of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaDimensionality and Sample Size Considerations in Pattern Recognition Practice,\u00ba Handbook of Statistics"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 255
                            }
                        ],
                        "text": "In addition to the excellent textbooks by Duda and Hart [44],1 Fukunaga [58], Devijver and Kittler [39], Devroye et al. [41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 140
                            }
                        ],
                        "text": "[41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Nagy described the early roots of pattern recognition, which at that time was shared with researchers in artificial intelligence and perception."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In comparison to the state of the pattern recognition field as described by Nagy and Kanal in the 1960s and 1970s, today a number of commercial pattern recognition systems are available which even individuals can buy for personal use (e.g., machine printed character recognition and isolated spoken word recognition)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "A large part of Nagy's paper introduced a number of potential applications of pattern recognition and the interplay between feature definition and the application domain knowledge."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aState of the Art in Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE,"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "Now the probability of error which is a function of both n and d can be written as:\nPe n; d Z 1 d 1 2 p e\u00ff12z2dz;where 5\nd Pd\ni 1 1i 1 1n Pd i 1 1i dn q : 6 Trunk showed that limd!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Trunk considered the following two cases:\n1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "Trunk [157] provided a simple example to illustrate the curse of dimensionality which we reproduce below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "Jain and Zongker [85] illustrate this phenomenon for a two-class classification problem involving 20-dimensional Gaussian class-conditional densities (the same data was also used by Trunk [157] to demonstrate the curse of dimensionality phenomenon)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Trunk found the maximum likelihood estimate m\u0302 of m and used the plug-in decision rule (substitute m\u0302 for m in the optimal Bayes decision rule)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Problem of Dimensionality: A Simple Example,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 294
                            }
                        ],
                        "text": "For the first question, the standard answer is the expectation-maximization (EM) algorithm (which, under mild conditions, converges to the maximum likelihood (ML) estimate of the mixture parameters); several authors have also advocated the (computationally demanding) Markov chain Monte-Carlo (MCMC) method [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 72
                            }
                        ],
                        "text": "Stochastic approaches generally involve Markov chain Monte Carlo (MCMC) [135] sampling and are far more computationally intensive than EM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 86
                            }
                        ],
                        "text": "also advocated the (computationally demanding) Markov chain Monte-Carlo (MCMC) method [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Analysis of Mixtures with Unknown Number of Components,o"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Soc. (B),"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102399764"
                        ],
                        "name": "L. Low",
                        "slug": "L.-Low",
                        "structuredName": {
                            "firstName": "Leone",
                            "lastName": "Low",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Low"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222289739,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "77311b0d5918b95ff3b1219754b24b65e6e316ac",
            "isKey": false,
            "numCitedBy": 2416,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Jackknife:-The-Bootstrap-and-Other-Resampling-Low-Efron",
            "title": {
                "fragments": [],
                "text": "The Jackknife: The Bootstrap and Other Resampling Plans."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91408809"
                        ],
                        "name": "A. B. Garrett",
                        "slug": "A.-B.-Garrett",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Garrett",
                            "middleNames": [
                                "Benjamin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. B. Garrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 190890733,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "c008aa6cc2e54c9d01655e7f0bd6546f595d3e47",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Flash-of-Genius-Garrett",
            "title": {
                "fragments": [],
                "text": "The Flash of Genius"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152633671"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72847318"
                        ],
                        "name": "J. Schurmann",
                        "slug": "J.-Schurmann",
                        "structuredName": {
                            "firstName": "Jurgen",
                            "lastName": "Schurmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schurmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126019720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d21120ddfdc9e6b52f6207f8a177052465d22de1",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification:-A-Unified-View-of-and-McLachlan-Schurmann",
            "title": {
                "fragments": [],
                "text": "Pattern Classification: A Unified View of Statistical and Neural Approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60735762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71c0f082a41c7f0b102c3ca9e4cf6b31f361d06a",
            "isKey": false,
            "numCitedBy": 4228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-statistical-pattern-recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to statistical pattern recognition (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922810"
                        ],
                        "name": "P. Devijver",
                        "slug": "P.-Devijver",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Devijver",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Devijver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "An extensive comparison of a large set of classifiers over many different problems is the StatLog project [109] which showed a large variability over their relative performances, illustrating that there is no such thing as an overall optimal classification rule."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61074523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc5a5cf6aa29ae0847dbd88bcc0ac042a9ee71fb",
            "isKey": false,
            "numCitedBy": 3014,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-:-a-statistical-approach-Devijver-Kittler",
            "title": {
                "fragments": [],
                "text": "Pattern recognition : a statistical approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4929024"
                        ],
                        "name": "J. Bezdek",
                        "slug": "J.-Bezdek",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bezdek",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bezdek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736103"
                        ],
                        "name": "S. Pal",
                        "slug": "S.-Pal",
                        "structuredName": {
                            "firstName": "Sankar",
                            "lastName": "Pal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60958185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62001bdde560ae4a258e7b38e83908685325b93e",
            "isKey": false,
            "numCitedBy": 517,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fuzzy-models-for-pattern-recognition-:-methods-that-Bezdek-Pal",
            "title": {
                "fragments": [],
                "text": "Fuzzy models for pattern recognition : methods that search for structures in data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60478593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46909441dd40badf6f2f1a815dcdfb295bb194ef",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-information-maximization-approach-to-blind-and-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "An information-maximization approach to blind separation and blind deconvolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57100530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e70b4af0c13eac9bbb4445b9822350a60aad15b3",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Systems-That-Learn-Weiss",
            "title": {
                "fragments": [],
                "text": "Computer Systems That Learn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56833645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8c4869392e5a52c3511fb907bd719391d9b9726",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-aspects-of-neural-networks-Ripley",
            "title": {
                "fragments": [],
                "text": "Statistical aspects of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "During training, the test set error and the\ntraining set error are initially almost equal, but after a certain point (three epochs5) the test set error starts to\nincrease while the training error keeps on decreasing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58166686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "718c68c25886484c922d6d33702a726cdd27ca3a",
            "isKey": false,
            "numCitedBy": 840,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Subspace-methods-of-pattern-recognition-Oja",
            "title": {
                "fragments": [],
                "text": "Subspace methods of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49862983"
                        ],
                        "name": "A. Pellionisz",
                        "slug": "A.-Pellionisz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Pellionisz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pellionisz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061358113"
                        ],
                        "name": "Edward Rosenfeld",
                        "slug": "Edward-Rosenfeld",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53887361,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1dd8675add2e0ffbc8d34a7886f1596f488dd2e8",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neurocomputing-(vol.-2):-directions-for-research-Anderson-Pellionisz",
            "title": {
                "fragments": [],
                "text": "Neurocomputing (vol. 2): directions for research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42797038,
            "fieldsOfStudy": [],
            "id": "a5d691a683ddf08c2f73b22548440636959df9b3",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simplifying Decision Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Man Mach. Stud."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114788213"
                        ],
                        "name": "D. Signorini",
                        "slug": "D.-Signorini",
                        "structuredName": {
                            "firstName": "DavidF.",
                            "lastName": "Signorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Signorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4137433"
                        ],
                        "name": "J. Slattery",
                        "slug": "J.-Slattery",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058057862"
                        ],
                        "name": "S. Dodds",
                        "slug": "S.-Dodds",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Dodds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dodds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51934565"
                        ],
                        "name": "V. Lane",
                        "slug": "V.-Lane",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095059657"
                        ],
                        "name": "P. Littlejohns",
                        "slug": "P.-Littlejohns",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Littlejohns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Littlejohns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2878979,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "20b844e395355b40fa5940c61362ec40e56027aa",
            "isKey": false,
            "numCitedBy": 4706,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-Signorini-Slattery",
            "title": {
                "fragments": [],
                "text": "Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107314775"
                        ],
                        "name": "C. C. Taylor",
                        "slug": "C.-C.-Taylor",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1625830,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ca23e7a71ace53d6a5b2a553ff37c63365d22b8a",
            "isKey": false,
            "numCitedBy": 5721,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Recognition-Ripley-Taylor",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9365056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72247e4e34de0dd2d0428522ded24b49fb1632be",
            "isKey": false,
            "numCitedBy": 1772,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Complexity-in-Statistical-Inquiry-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific Series in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066721"
                        ],
                        "name": "Shivakumar Vaithyanathan",
                        "slug": "Shivakumar-Vaithyanathan",
                        "structuredName": {
                            "firstName": "Shivakumar",
                            "lastName": "Vaithyanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shivakumar Vaithyanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31122868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80fee4cf28089dfc072a92eaae0f1435d690ffab",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-Selection-in-Unsupervised-Learning-with-To-Vaithyanathan-Dom",
            "title": {
                "fragments": [],
                "text": "Model Selection in Unsupervised Learning with Applications To Document Clustering"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087848"
                        ],
                        "name": "R. Dubes",
                        "slug": "R.-Dubes",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dubes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dubes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "\u2026patterns along a reasonably smooth curve have an intrinsic dimensionality of one, irrespective of the value of d. Note that the intrinsic dimensionality is not the same as the linear dimensionality which is a global property of the data involving the number of significant eigenvalues of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "By the time Kanal wrote his survey paper, more than 500 papers and about half a dozen books on pattern recognition were already published."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29535089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa4bddbd10eafd8e1b54338517eedfee408f03ae",
            "isKey": false,
            "numCitedBy": 10558,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Clustering-Data-Jain-Dubes",
            "title": {
                "fragments": [],
                "text": "Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42756182,
            "fieldsOfStudy": [],
            "id": "9b688fd4be93dd8bd461dd7ab0b78427e4ba3a47",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large-Scale Parallel Data Clustering"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205072197,
            "fieldsOfStudy": [],
            "id": "c2d0456c9d2a987dca778b44da93f6312c7c7350",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A note on genetic algorithms for large-scale feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207657152,
            "fieldsOfStudy": [],
            "id": "7bcdf9e7c9d072d94be325f8a0d3e7db60ac1b6c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644344103"
                        ],
                        "name": "J. C. BurgesChristopher",
                        "slug": "J.-C.-BurgesChristopher",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "BurgesChristopher",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. BurgesChristopher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215966761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6716697767fc601efc7690f40820d9ea7a7bf57c",
            "isKey": false,
            "numCitedBy": 13527,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w..."
            },
            "slug": "A-Tutorial-on-Support-Vector-Machines-for-Pattern-BurgesChristopher",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Support Vector Machines for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This tutorial starts with an overview of the concepts of VC dimension and structural risk minimization and describes linear Support Vector Machines (SVMs) for separable and non-separable data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mctliods lor Coiiibinitig Multiple Classificrs and Thcir hpplicaiiuns in Ihidrvritteir Character l<eecognition"
            },
            "venue": {
                "fragments": [],
                "text": "Mctliods lor Coiiibinitig Multiple Classificrs and Thcir hpplicaiiuns in Ihidrvritteir Character l<eecognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Uso nf Cniilext in I'nttcrn Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IWtcrii Rccqyniiic~t"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal i'artitinning for Classification nncl lbgrcssinn Trees IECC Trnris. Pfltterii Aiidys'is nnd Mnrliim h t t"
            },
            "venue": {
                "fragments": [],
                "text": "Optimal i'artitinning for Classification nncl lbgrcssinn Trees IECC Trnris. Pfltterii Aiidys'is nnd Mnrliim h t t"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BaEziiiR Predictors Mnclii~re Lcm~riiq~"
            },
            "venue": {
                "fragments": [],
                "text": "Closs$c.ntioil rind Rrpssimi Trees. Wadswortli, Calif"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Van Campeiihuut, \"On the l'ussible"
            },
            "venue": {
                "fragments": [],
                "text": "'I'wo Hcst,\" I E G E Trims Sys&s, Mmr, t ud Cybcriirlics,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aFeature Extraction Based on Decision Boundaries,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMethods for Combining Multiple Classifiers and Their Applications in Handwritten Character Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mctliods lor Coiiibinitig Multiple Classificrs and Thcir hpplicaiiuns in Ihidrvritteir Character l<eecognition"
            },
            "venue": {
                "fragments": [],
                "text": "Mctliods lor Coiiibinitig Multiple Classificrs and Thcir hpplicaiiuns in Ihidrvritteir Character l<eecognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Uso nf Cniilext in I'nttcrn Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IWtcrii Rccqyniiic~t"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal i'artitinning for Classification nncl lbgrcssinn Trees IECC Trnris. Pfltterii Aiidys'is nnd Mnrliim h t t"
            },
            "venue": {
                "fragments": [],
                "text": "Optimal i'artitinning for Classification nncl lbgrcssinn Trees IECC Trnris. Pfltterii Aiidys'is nnd Mnrliim h t t"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BaEziiiR Predictors Mnclii~re Lcm~riiq~"
            },
            "venue": {
                "fragments": [],
                "text": "Closs$c.ntioil rind Rrpssimi Trees. Wadswortli, Calif"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Van Campeiihuut, \"On the l'ussible"
            },
            "venue": {
                "fragments": [],
                "text": "'I'wo Hcst,\" I E G E Trims Sys&s, Mmr, t ud Cybcriirlics,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aFeature Extraction Based on Decision Boundaries,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aExpected Classification Error of the Fisher Linear Classifier with Pseudoinverse Covariance Matrix,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Other possibilities like table-look-up and branch-and-bound methods [42] are less efficient for large dimensionalities."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bouktache, aA Fast Algorithm for the Nearest- Neighbor Classifier,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 126
                            }
                        ],
                        "text": "Various training algorithms have been proposed in the literature [23], including chunking [161], Osuna's decomposition method [119], and sequential minimal optimization [124]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Training Algorithm for Support Vector Machines,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Workshop Neural Networks for Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aNeural-Network Feature Selector,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "[69] or rubber sheet deformations [9] can be used to match"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMultiresolution Elastic Matching,o"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision Graphics Image Processing,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "This paradoxical behavior is referred to as the peaking phenomenon(3) [80], [131], [132]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSmall Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of Learning Algorithms for Blind Source Separation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "/by Nr?i/m/ Nr:two~ks"
            },
            "venue": {
                "fragments": [],
                "text": "/by Nr?i/m/ Nr:two~ks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Interest in the area of pattern recognition has been renewed recently due to emerging applications which are not only challenging but also computationally more demanding (see Table 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMultiresolution Elastic Matching,\u00ba Computer Vision Graphics Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaMultiresolution Elastic Matching,\u00ba Computer Vision Graphics Image Processing"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a Funturelcss Approach to I'attcrri I<ccogtiition,\" I'rrtkni Ili?cupitiun I.i?ttcrs"
            },
            "venue": {
                "fragments": [],
                "text": "Experiments with a Funturelcss Approach to I'attcrri I<ccogtiition,\" I'rrtkni Ili?cupitiun I.i?ttcrs"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "Examples are stacking [168], bagging [21], and boosting (or ARCing) [142]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aStacked Generalization,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaOptimal Combinations of Pattern Classifiers,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaOptimal Combinations of Pattern Classifiers,\u00ba Pattern Recognition Letters"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "This paradoxical behavior is referred to as the peaking phenomenon(3) [80], [131], [132]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSmall Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "All other strategies are suboptimal due to the fact that the best pair of features need not contain the best single feature [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aThe Best Two Independent Measurements are not the Two Best,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aStability Analysis of Learning Algorithms for Blind Source Separation,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "The speed, reliability, and consistency with which a clustering algorithm can organize large amounts of data constitute overwhelming reasons to use it in applications such as data mining [88], information retrieval [17], [25], image segmentation [55], signal compression and coding [1], and machine learning [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deogun, aConceptual Clustering in Information Retrieval,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mohiuddin, aImproving OCR Performance Using Character Degradation Models and Boosting Algorithm,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaVector Quantization and Density Estimation,\u00ba Proc. Int'l Conf. Compression and Complexity of Sequences"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaVector Quantization and Density Estimation,\u00ba Proc. Int'l Conf. Compression and Complexity of Sequences"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Dimcnsioiialily, Sartiplc Size, Classification Frror, and Complexity of Clansificntion Algurithms in Pallern Recognition,\" lEL'11 Troris. I'ffftcvn Atidy.si.~ ortd Mncfririr iirte!/igcrm"
            },
            "venue": {
                "fragments": [],
                "text": "On Dimcnsioiialily, Sartiplc Size, Classification Frror, and Complexity of Clansificntion Algurithms in Pallern Recognition,\" lEL'11 Troris. I'ffftcvn Atidy.si.~ ortd Mncfririr iirte!/igcrm"
            },
            "year": 1934
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Information-h.IaxiiiiizaIiu~i Appmach to Blind Separation"
            },
            "venue": {
                "fragments": [],
                "text": "Nerrvni Comptrrlion"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "For finite sample sizes and unknown distributions, however, such bounds are impossible [6], [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aLower Bounds for Bayes Error Estimation,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "13 shows an example of mixture decomposition, where K is selected using a modified MDL criterion [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOn Fitting Mixture Models,o Energy Minimization Methods in Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "For some families of distributions tight bounds for the Bayes error may be obtained [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aArbitrarily Tight Upper and Lower Bounds on the Bayesian Probability of Error,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ilctcrriiinistic Antwaling for Clusturing, Comprcssion, Clilssificnlion, Ikgrcssioii and Helated Optimizntinn l~'rol~lctns"
            },
            "venue": {
                "fragments": [],
                "text": "Pwr. IEEC"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aBlind Signal Separation: Statistical Principles,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Siiigapure: World Scientific"
            },
            "venue": {
                "fragments": [],
                "text": "Siiigapure: World Scientific"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kiplcy, I'nlhiwi X m p i t i m r ntrd Nmrd Nct7~wk.s. Cantbridgc, Mass"
            },
            "venue": {
                "fragments": [],
                "text": "Kiplcy, I'nlhiwi X m p i t i m r ntrd Nmrd Nct7~wk.s. Cantbridgc, Mass"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "The original algorithm was proposed by Schapire [142], who showed that, in principle, it is possible for a combination of weak classifiers (whose performances are only slightly better than random guessing) to achieve an error rate which is arbitrarily small on the training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. [143] proposed a different explanation for the effectiveness of voting (weighted average, in fact) methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "Examples are stacking [168], bagging [21], and boosting (or ARCing) [142]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aThe Strength of Weak Learnability,o"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "llr Making in Context"
            },
            "venue": {
                "fragments": [],
                "text": "llr Making in Context"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Cooper [123] also proposed a generalized ensemble, an optimal linear combiner in the least square error sense."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Cooper [123] showed that under the zero-mean and independence assumption on the misfit (difference between"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aWhen Networks Disagree: Ensemble Methods for Hybrid Neural Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Speech and Image Processing. R.J. Mammone, ed., Chapman-Hall,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "The classical studies by Cover [33] and Vapnik [162] on classifier capacity and complexity provide a good understanding of the mechanisms behind overtraining."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "However, the number of possible subsets grows combinatorially, making this exhaustive search impractical for even moderate values of m and d. Cover and Van Campenhout [35] showed that no nonexhaustive sequential feature selection procedure can be guaranteed to produce the optimal subset."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aGeometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electronic Computers,"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "But, if a multilayer feed forward network is used for pattern classification, then the node-pruning method simultaneously determines both the optimal feature subset and the optimal network classifier [26], [103]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative Pruning Algorithm for Feedforward Neural Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Cooper [123] also proposed a generalized ensemble, an optimal linear combiner in the least square error sense."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Cooper [123] showed that under the zero-mean and independence assumption on the misfit (difference between"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aWhen Networks Disagree: Ensemble Methods for Hybrid Neural Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Speech and Image Processing. R.J. Mammone, ed., Chapman-Hall,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "SVM has been extended to perform semisupervised learning [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSemi-Supervised Support Vector Machines,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Neural Information Processing Systems, Denver,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting atid Other Enscmblc Mcthnds AL~rml Cumprtntion"
            },
            "venue": {
                "fragments": [],
                "text": "Boosting atid Other Enscmblc Mcthnds AL~rml Cumprtntion"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Pitting Mixture Models,\" E i i i ~ g y Minimiztftion Metlds irt (1otupirftr Vision nfid I'ntiem I<ccopitioii"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "avoided by using a pruning stage [63], [106], [128]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMDL-Based Decision Tree Pruning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Conf. Knowledge Discovery in Databases and Data"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is\npossible that the available information is sufficient for a\ndirect solution but is insufficient for solving a more general\nintermediate problem.\u00ba"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neurocomputing 2: Directions for Research"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing 2: Directions for Research"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wdpcrt , \" Stacked Ccncralization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "At these places, we have to rely on the background knowledge which may be available only to the more experienced readers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Systems that Learn. California"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Systems that Learn. California"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gcomctrical mil Stiltistical Propcrtius of Systcnis of Litwar lncqualitics with Applications in I'attcrn kccognition,"
            },
            "venue": {
                "fragments": [],
                "text": "I B E E Trnns. CIrcIruriic Coiiipirtels,"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hondwrittcn Digit Recognition by Combinccl Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Hondwrittcn Digit Recognition by Combinccl Classifiers"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Snpport Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "L' IDC, Ncrdrnl Ii$\"rf ion Pnwssiiig Systems"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Network Toolbox for Use with Matlab. version 3, Mathworks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Qiiiilm, \"Simplilying Dccision Trees,\" Iifi'l"
            },
            "venue": {
                "fragments": [],
                "text": "MowMnclf iw Studics,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSelection of Relevant Features and Examples in Machine Learning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAn Information-Maximization Approach to Blind Separation,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fast Algorithm for Ihe Nearest- Neighbor Classifier"
            },
            "venue": {
                "fragments": [],
                "text": "A Fast Algorithm for Ihe Nearest- Neighbor Classifier"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5: Proprws jou Mrichiirt. l,i!ririiiticq"
            },
            "venue": {
                "fragments": [],
                "text": "5: Proprws jou Mrichiirt. l,i!ririiiticq"
            },
            "year": 193
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "Oja [118] shows how autoassociative networks can be used for ICA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear PCA Learning Rule in Independent Component Analysis,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing, vol. 17,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 138
                            }
                        ],
                        "text": "Hidden Markov Models (HMM), have been a popular statistical tool for modeling and recognizing sequential data, in particular, speech data [130], [86]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "The speed, reliability, and consistency with which a clustering algorithm can organize large amounts of data constitute overwhelming reasons to use it in applications such as data mining [88], information retrieval [17], [25], image segmentation [55], signal compression and coding [1], and machine learning [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[88] show that a combination of algorithmic enhancements to a square-error clustering algorithm and distribution of the computations over a network of workstations can be used to cluster hundreds of thousands of multidimensional patterns in just a few minutes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aLarge-Scale Parallel Data Clustering,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "Details of this dataset are available in [160]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aHandwritten Digit Recognition by Combined Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika, vol. 34,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Examples of adaptive combiners include adaptive weighting [156], associative switch, mixture of local experts (MLE) [79], and hierarchical MLE [87]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aHierarchical Mixtures of Experts and the EM Algorithm,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Examples of the use of ROC analysis are combining classifiers [170] and feature selection [99]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aFeature Selection Using Expected Attainable Discrimination,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 138
                            }
                        ],
                        "text": "Hidden Markov Models (HMM), have been a popular statistical tool for modeling and recognizing sequential data, in particular, speech data [130], [86]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "Oja [118] shows how autoassociative networks can be used for ICA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear PCA Learning Rule in Independent Component Analysis,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing, vol. 17,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "The speed, reliability, and consistency with which a clustering algorithm can organize large amounts of data constitute overwhelming reasons to use it in applications such as data mining [88], information retrieval [17], [25], image segmentation [55], signal compression and coding [1], and machine learning [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[88] show that a combination of algorithmic enhancements to a square-error clustering algorithm and distribution of the computations over a network of workstations can be used to cluster hundreds of thousands of multidimensional patterns in just a few minutes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aLarge-Scale Parallel Data Clustering,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A f l d i v c Cwp>t t i f ix"
            },
            "venue": {
                "fragments": [],
                "text": "MI'I' I'rcss,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiwsnlutinn Glaslic Matching Coriipiriev VisioH Gmphii!s Imye Proc~ssiq"
            },
            "venue": {
                "fragments": [],
                "text": "Multiwsnlutinn Glaslic Matching Coriipiriev VisioH Gmphii!s Imye Proc~ssiq"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaArtificial Neural Networks for Feature Extraction and Multivariate Data Projection,\u00ba IEEE Trans"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Srihari, \u00aaDecision Combination in Multiple Classifier Systems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Srihari, \u00aaDecision Combination in Multiple Classifier Systems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiwsnlutinn Glaslic Matching Coriipiriev VisioH Gmphii!s Imye Proc~ssiq"
            },
            "venue": {
                "fragments": [],
                "text": "Multiwsnlutinn Glaslic Matching Coriipiriev VisioH Gmphii!s Imye Proc~ssiq"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aBoosting and Other Ensemble Methods,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sarvarayudu, aHierarchical Classifier Design Using Mutual Information,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Recognition and Machine Intelligence,"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "2), which was proposed in 1977 [36], and which is now a very popular approach for density estimation and clustering [159], due to the computing power available today."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMaximum Likelihood from Incomplete Data via the (EM) Algorithm,o"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Soc.,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 58
                            }
                        ],
                        "text": "Examples of adaptive combiners include adaptive weighting [156], associative switch, mixture of local experts (MLE) [79], and hierarchical MLE [87]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aCombining Estimators Using Non- Constant Weighting Functions,o"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOn Self-Organizing Algorithms and Networks for Class-Separability Features,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[172] grouped these expectations into three levels: 1) measurement (or confidence), 2) rank, and 3) abstract."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMethods for Combining Multiple Classifiers and Their Applications in Handwritten Character Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAutomatic Pattern Recognition: A Study of the Probability of Error,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 90
                            }
                        ],
                        "text": "Various training algorithms have been proposed in the literature [23], including chunking [161], Osuna's decomposition method [119], and sequential minimal optimization [124]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data, Berlin: Springer-Verlag,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Siiiall Sample Sisc F:ffccts in Statistical Pattern I<ccngnition: llcrorirmci~dntioiit; For"
            },
            "venue": {
                "fragments": [],
                "text": "Siiiall Sample Sisc F:ffccts in Statistical Pattern I<ccngnition: llcrorirmci~dntioiit; For"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFeature Selection Based on the Approximation of Class Densities by Finite Mixtures of the Special Type,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFeature Selection Based on the Approximation of Class Densities by Finite Mixtures of the Special Type,\u00ba Pattern Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "This paradoxical behavior is referred to as the peaking phenomenon(3) [80], [131], [132]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "too large relative to the number of training samples (curse of dimensionality [80]), 2) the number of unknown"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "It is generally accepted that using at least ten times as many training samples per class as the number of features (n=d > 10) is a good practice to follow in classifier design [80]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aDimensionality and Sample Size Considerations in Pattern Recognition Practice,o"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Statistics. P.R. Krishnaiah and L.N. Kanal, eds.,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "optimize the resulting partition [110], [139], and 3) mapping it onto a neural network [103] for possibly efficient implementation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aDeterministic Annealing for Clustering, Compression, Classification, Regression and Related Optimization Problems,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ilctcrriiinistic Antwaling for Clusturing, Comprcssion, Clilssificnlion"
            },
            "venue": {
                "fragments": [],
                "text": "Ikgrcssioii and Helated Optimizntinn l~'rol~lctns,\" Pwr. IEEC,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stntisticnl Aspccts of Ncuml Nctwnrks Akfzuovks 011 Choos: Stofictiiinl imI Probabilistic Aspiiuls. U. ~umndul.f~-Niclsen"
            },
            "venue": {
                "fragments": [],
                "text": "J. jrwscn, ani1 W. Kundnl, ctls., Clrapinan and Hall"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "Tumer and Ghosh [158] provided a quantitative analysis of the improvements in classification accuracy by combining multiple neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 182
                            }
                        ],
                        "text": "In general, classifier combination could refine decision boundary such that its variance with respect to Bayes decision boundary is reduced, leading to improved recognition accuracy [158]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAnalysis of Decision Boundaries in Linearly Combined Neural Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting the hlargin: A h' ew Explanation lor the Eikctivciicss d Vniing Methods Aruinls qf Sfolislii:s, ' I 990"
            },
            "venue": {
                "fragments": [],
                "text": "Boosting the hlargin: A h' ew Explanation lor the Eikctivciicss d Vniing Methods Aruinls qf Sfolislii:s, ' I 990"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1,'rincipal Compoiienls, Minor Componenls, and I ,inear Ncririll Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nt:irml Nctruorks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "It is found that the boosting algorithm [143] also improves the margin distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[143] proposed a different explanation for"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aBoosting the Margin: A New Explanation for the Effectiveness of Voting Methods,o"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advances in Error Rate Estimation,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Error Rate Estimation,\u00ba Pattern Recognition Letters"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Attempts have been made to design hybrid systems involving multiple models [57]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Step Towards Unification of Syntactic and Statistical Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sclcction of Rclewut Feattircs and Examples in Machine Learning Arl@cid bitelligi?ria: vul"
            },
            "venue": {
                "fragments": [],
                "text": "Sclcction of Rclewut Feattircs and Examples in Machine Learning Arl@cid bitelligi?ria: vul"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1,'rincipal Compoiienls, Minor Componenls, and I ,inear Ncririll Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nt:irml Nctruorks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arbitrarily Tight Upper and Lower I3ou1~ds on ttw tlaycsian Probability ol Error"
            },
            "venue": {
                "fragments": [],
                "text": "Arbitrarily Tight Upper and Lower I3ou1~ds on ttw tlaycsian Probability ol Error"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An hnpiricnl Study of thr Pcrformoncc of Fleuristic Methods lor Clustering"
            },
            "venue": {
                "fragments": [],
                "text": "An hnpiricnl Study of thr Pcrformoncc of Fleuristic Methods lor Clustering"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "A popular analysis of combination schemes is based on the well-known bias-variance dilemma [64], [93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aNeural Networks and the Bias/Variance Dilemma,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Its second edition by Duda, Hart, and Stork [45] is in press."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene Analysis. second ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion,\" IECE Trms. Artttrrri Aiiirlysis r n n l M d i i i i c Ilrti:/liprrw"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 5,110"
            },
            "year": 1911
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "A popular analysis of combination schemes is based on the well-known bias-variance dilemma [64], [93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aNeural Networks and the Bias/Variance Dilemma,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 124
                            }
                        ],
                        "text": "It is also possible to redefine the MDS algorithm so that it directly produces a map that may be used for new test patterns [165]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMultidimensional Scaling by Iterative Majorization Using Radial Basis Functions,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 119
                            }
                        ],
                        "text": "As a consequence, nonlinear procedures and subspace approaches have become popular, both for dimensionality reduction (Section 4) and for building classifiers (Section 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Deformable template models\n[69] or rubber sheet deformations [9] can be used to match\npatterns when the deformation cannot be easily explained\nor modeled directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik, Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Vapnik, Statistical Learning Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Annals of Math. and Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Math. and Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Whcn Networks Uisngrfc: Eiiseirible Methuds for Hybrid Ncural NPtwurks Ncwd h W mnvks for Spccdi nird b ~ i n p ! I'uorrssirrg"
            },
            "venue": {
                "fragments": [],
                "text": "K.J. Mammotw, cd"
            },
            "year": 1903
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Its second edition by Duda, Hart, and Stork [45] is in press."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene Analysis. second ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion,\" IECE Trms. Artttrrri Aiiirlysis r n n l M d i i i i c Ilrti:/liprrw"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 5,110"
            },
            "year": 1911
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Veptiik, \"Boosting atid Other Enscmblc Mcthnds,"
            },
            "venue": {
                "fragments": [],
                "text": "AL~rml Cumprtntion,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "decisions (See [66] for an applied example of the use of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOn the Error-Reject Trade-Off in Biometric Verification System,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFeature Selection Using Expected Attainable Discrimination,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFeature Selection Using Expected Attainable Discrimination,\u00ba Pattern Recognition Letters"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Examples are the nearest neighbor classifier using tangent distance [152] and deformable template matching [84]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aObject Matching Using Deformable Templates,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Benneil, \"Semi-Supervised Snpport Vector Machines,\" L ' I D C"
            },
            "venue": {
                "fragments": [],
                "text": "Ncrdrnl Ii$\"rf ion Pnwssiiig Systems, Ucnvcr,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MI31 ,-lhsccl Ikcision 'I'rcc I'riining,\" Pmc. Firsl Ziit'l Cui$ Krfuwlrd~t: Disr:oui:r!y itr Uptoboscs orrd D n h hfiriiq"
            },
            "venue": {
                "fragments": [],
                "text": "MI31 ,-lhsccl Ikcision 'I'rcc I'riining,\" Pmc. Firsl Ziit'l Cui$ Krfuwlrd~t: Disr:oui:r!y itr Uptoboscs orrd D n h hfiriiq"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 78
                            }
                        ],
                        "text": "4a shows the architecture of a network which is able to find the PCA subspace [117]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aPrincipal Components, Minor Components, and Linear Neural Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire, \u00aaThe Strength of Weak Learnability"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modcl Selection in Unsripcrvised 1 carnitrg with Applications tu nocumcnt Cluskrmg"
            },
            "venue": {
                "fragments": [],
                "text": "Modcl Selection in Unsripcrvised 1 carnitrg with Applications tu nocumcnt Cluskrmg"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Componcnt Analysis as ' 1 Kcrncl Eigciwaluc l'roblunl"
            },
            "venue": {
                "fragments": [],
                "text": "Neirrril Comptutiivr"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Best Twu Indeaeudcnt b1easureiiwnts arc not vol Apr, '1'991. .~ the 'I'wo Hcst"
            },
            "venue": {
                "fragments": [],
                "text": "IEGE Trims Sys&s"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A New 1 .uok at Statistical Motlcl Idcntificntion,\" IEEE ' I n \" Autoinntic I h l r 0 1"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 36,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", \" A Nntr on Gcnctic Algorithms for Large - Scale l h i tire Selection"
            },
            "venue": {
                "fragments": [],
                "text": "; ition of t l l c Coiiditivnal Population Mixture Modcl 10 lmagc Srginciihtioii , \" ICTL ' / ' rnt / s . IWwi Kccngiritinii nird Mndriric IrileUipice"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tllind Signdl Supardtion: Statistical I'rir~ciplcs,\" I h c"
            },
            "venue": {
                "fragments": [],
                "text": "tllind Signdl Supardtion: Statistical I'rir~ciplcs,\" I h c"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "It>formniioii Ratios for Validating Mixtnrc Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "J, An!. St,itistiunI Assoa"
            },
            "year": 1492
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparing Support Vectur Machines with Caussiaii Kcriicls to Ilailial t h i s Function Cla Trnris"
            },
            "venue": {
                "fragments": [],
                "text": "Sixid Prcicrssiyy"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Several techniques have been investigated to address this deficiency which range from linear interpolation to training a neural network [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSammon's Mapping Using Neural Networks: Comparison,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "One of the most rigorous theories on classifier combination is presented by Kleinberg [91]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aStochastic Discrimination,o"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Math. and Artificial Intelligence,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 65
                            }
                        ],
                        "text": "The technique of conceptual clustering or learning from examples [108] can be used with patterns represented by nonnumeric or symbolic descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAutomated Construction of Classifications: Conceptual Clustering versus Numerical Taxonomy,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simplilying Dccision Trees"
            },
            "venue": {
                "fragments": [],
                "text": "MowMnclfiw Studics"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuvutrowpiiting 2: Divcctiunsfov RCSPWCJEII. Cambridge Mass.: MIT PrcssImwcr 13ounds for Bayes Error Estiination"
            },
            "venue": {
                "fragments": [],
                "text": "Neuvutrowpiiting 2: Divcctiunsfov RCSPWCJEII. Cambridge Mass.: MIT PrcssImwcr 13ounds for Bayes Error Estiination"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaOn Bayesian Analysis of Mixtures with Unknown Number of Components"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Soc. (B)"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pntienr Clnssqicntiotr: A Iliiifccl Virw ojStotislicnl iirirl \" x i rlppror~dii?s"
            },
            "venue": {
                "fragments": [],
                "text": "Pntienr Clnssqicntiotr: A Iliiifccl Virw ojStotislicnl iirirl \" x i rlppror~dii?s"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "R.P.W. Duin, \"A Note on Comparing Classifiers,\" Przlterii R c c r p i i t h Lrltcrs"
            },
            "venue": {
                "fragments": [],
                "text": "New York: John Wilcy & Sons,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Cover and Van Campenhout [35] showed that no nonexhaustive sequential feature selection procedure can be guaranteed to produce the optimal subset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 142
                            }
                        ],
                        "text": "However, the number of possible subsets grows combinatorially, making this exhaustive search impractical for even moderate values of m and d. Cover and Van Campenhout [35] showed that no nonexhaustive sequential feature selection procedure can be guaranteed to produce the optimal subset."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOn the Possible Orderings in the Measurement Selection Problem,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "optimize the resulting partition [110], [139], and 3) mapping it onto a neural network [103] for possibly efficient implementation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAn Empirical Study of the Performance of Heuristic Methods for Clustering,o Pattern Recognition in Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1,'rincipal Compoiienls, Minor Componenls, and I ,inear Ncririll Networks,\" Nt:irml Nctruorks, vol. 5, no. h, p p 927-936"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coiiunrirum cif Cuinbinnturial Complexity"
            },
            "venue": {
                "fragments": [],
                "text": "Coiiunrirum cif Cuinbinnturial Complexity"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "ICA has been successfully used for blind-source separation [78]; extracting linear feature combinations that define independent sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Fast Fixed-Point Algorithm for Independent Component Analysis,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "h s t 'fraining of Support Vector Macliiiics Using Sequential Miniinal Oplimizaliim Ailniiriucr: in KWWI Mr!thurls--- Suppout Veifou 1,cnrlthg"
            },
            "venue": {
                "fragments": [],
                "text": "I1241 1. I'lntt Mrl' l'rcss, 'I'J99"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and 1.S. I h g u nCoiiccptiial Clustering in Informaiinn Rctricval K E E Trims. S p t ~ i n s , Mrrri, ~ i i i Cybrwrtks"
            },
            "venue": {
                "fragments": [],
                "text": "and 1.S. I h g u nCoiiccptiial Clustering in Informaiinn Rctricval K E E Trims. S p t ~ i n s , Mrrri, ~ i i i Cybrwrtks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support Vcctnr Imriiing,\" 1'Ii.l). thesis, Tcclmisctic Univcrsiliil"
            },
            "venue": {
                "fragments": [],
                "text": "Support Vcctnr Imriiing,\" 1'Ii.l). thesis, Tcclmisctic Univcrsiliil"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sammon's Mapping Using Neiird Netwurks: Cumpiriwn,"
            },
            "venue": {
                "fragments": [],
                "text": "Pnt t tun k!ci)p!itiou I.cttcr.s,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriiiriiirriif Aimlysis m i i f Sfntisfiml I'dtcr.fr R ~ o p tioir"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Juhn Wile): & Sons,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "Examples are the nearest neighbor classifier using tangent distance [152] and deformable template matching [84]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aEfficient Pattern Recognition Using a New Transformation Distance,o"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaNeural Networks and the Bias/Variance Dilemma,\u00ba Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaNeural Networks and the Bias/Variance Dilemma,\u00ba Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 45
                            }
                        ],
                        "text": ", word segmentation) of a recognition system [174]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOptical Character Recognition,o Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "venue": {
                "fragments": [],
                "text": "J.G. Webster, ed.,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aEvolution and Generalization of a Single Neuron; Single-Layer Perceptron as Seven Statistical Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaIndependent Component Analysis, a New Concept?,\u00ba Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaIndependent Component Analysis, a New Concept?,\u00ba Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[153] proposed an algorithm named Tangent-Prop to minimize the derivative of the classifier outputs with respect to distortion parameters, i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aTangent Prop\u00d0A Formalism for Specifying Selected Invariances in an Adaptive Network,o"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "More advanced techniques for computing prototypes are vector quantization [115], [171] and learning vector quantization [92], and the data reduction methods associated with the one-nearest neighbor decision rule (1-NN), such as editing and condensing [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aCombining Image Compression and Classification Using Vector Quantization,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oil Ihiycsian Analysis of Mixtures with Uiiknorvn Numbcr of Compuncnts"
            },
            "venue": {
                "fragments": [],
                "text": "Oil Ihiycsian Analysis of Mixtures with Uiiknorvn Numbcr of Compuncnts"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Flnnting Scarrh Ivlcthncls in Teatiire Selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pnllcnr Recogirilim LcIIers"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 45
                            }
                        ],
                        "text": ", word segmentation) of a recognition system [174]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOptical Character Recognition,o Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "venue": {
                "fragments": [],
                "text": "J.G. Webster, ed.,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aEvolution and Generalization of a Single Neuron; Single-Layer Perceptron as Seven Statistical Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaIndependent Component Analysis, a New Concept?,\u00ba Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaIndependent Component Analysis, a New Concept?,\u00ba Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSurvey on Independent Component Analysis,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computing Surveys,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Examples are the optimization of the covariance estimates for the Parzen kernel [76] and discriminant analysis [61], and the use of bootstrapping"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": ", by vector quantization techniques possibly combined with an optimized metric or kernel [60], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hummels, aLeave-One-Out Procedures for Nonparametric Error Estimates,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "For example, Chernoff [29] represents each pattern as a cartoon face whose facial characteristics, such as nose length, mouth curvature, and eye size, are made to correspond to individual features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aThe Use of Faces to Represent Points in k-Dimensional Space Graphically,o"
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Statistical Assoc.,"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aCandide's Practical Principles of Experimental Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Improved 'I'winiilg Algorithm for Stippnrt Vcctor Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IECC Workshop NczrrnI .?Jrlii:rlrks ,fur Siayrid Pmci!ssirig"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 76
                            }
                        ],
                        "text": "This paradoxical behavior is referred to as the peaking phenomenon(3) [80], [131], [132]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pikelis, aOn Dimensionality, Sample Size, Classification Error, and Complexity of Classification Algorithms in Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A f l d i v c Cwp>ttifix. MI'I' I'rcss"
            },
            "venue": {
                "fragments": [],
                "text": "A f l d i v c Cwp>ttifix. MI'I' I'rcss"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aKnowledge Discovery and Data Mining: Towards a Unifying Framework,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Second Int'l Conf. Knowledge Discovery and Data Mining,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaSurvey on Independent Component Analysis,\u00ba Neural Computing Surveys"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaSurvey on Independent Component Analysis,\u00ba Neural Computing Surveys"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aEffects of Sample Size in Classifier Design,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 34
                            }
                        ],
                        "text": "For example, the networks used by Fukushima [62] et al. and Le Cun et al. [95] have the so called shared weight layers that are in fact filters for extracting features in two-dimensional images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "For example, the networks used by Fukushima [62] et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aNeocognitron: A Neural Network Model for a Mechanism of Visual Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Systems, Man, and Cybernetics,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMarkovian Models for Sequential Data,\u00ba Neural Computing Surveys"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaMarkovian Models for Sequential Data,\u00ba Neural Computing Surveys"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "A special type of classifier is the decision tree [22], [30],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOptimal Partitioning for Classification and Regression Trees,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaEnhanced Hypertext Categorization Using Hyperlinks,\u00ba ACM SIGMOD"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaEnhanced Hypertext Categorization Using Hyperlinks,\u00ba ACM SIGMOD"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods tirflt SCIIITJI @or Str'wtirrts iir U m"
            },
            "venue": {
                "fragments": [],
                "text": "Methods tirflt SCIIITJI @or Str'wtirrts iir U m"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "a An Improved Training Algorithm for Support Vector Machines , o Proc . IEEE Workshop Neural Networks for Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Agrawal, \u00aaMDL-Based Decision Tree Pruning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Conf. Knowledge Discovery in Databases and Data Mining"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 300
                            }
                        ],
                        "text": "An obvious choice in this class is to use the minimum description length (MDL) criterion [10] [138], but several other model selection criteria have been proposed: Schwarz's Bayesian inference criterion (BIC), the minimum message length (MML) criterion, and Akaike's information criterion (AIC) [2], [148], [167]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aApplication of the Conditional Population Mixture Model to Image Segmentation,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Recognition and Machine Intelligence,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Examples are the optimization of the covariance estimates for the Parzen kernel [76] and discriminant analysis [61], and the use of bootstrapping"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 270
                            }
                        ],
                        "text": "In addition to the commonly used maximum likelihood estimator of the covariance matrix, various regularization techniques [54] are available to obtain a robust estimate in small sample size situations and the leave-one-out estimator is available for minimizing the bias [76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aCovariance Matrix Estimation and Classification with Limited Training Data,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vcctnr Quantixatinn 'I'cdinique fur Nonpnminetric Classificr Design"
            },
            "venue": {
                "fragments": [],
                "text": "Vcctnr Quantixatinn 'I'cdinique fur Nonpnminetric Classificr Design"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sirbspocc Mctlicds o, f Pnitcni Rccupitiuir"
            },
            "venue": {
                "fragments": [],
                "text": "Sirbspocc Mctlicds o, f Pnitcni Rccupitiuir"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaAdaptive Mixtures of Local Experts,\u00ba Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaAdaptive Mixtures of Local Experts,\u00ba Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aComparative Study of Techniques for Large Scale Feature Selection,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition in Practice IV,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaOn Fitting Mixture Models,\u00ba Energy Minimization Methods in Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaOn Fitting Mixture Models,\u00ba Energy Minimization Methods in Computer Vision and Pattern Recognition"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "McLacl-ilan, Discriiiriiirriif Aimlysis m i i f Sfntisfiml I'dtcr.fr R ~ o p tioir"
            },
            "venue": {
                "fragments": [],
                "text": "Juhn Wile): & Sons"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aComparative Study of Techniques for Large Scale Feature Selection,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition in Practice IV,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaOn Fitting Mixture Models,\u00ba Energy Minimization Methods in Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaOn Fitting Mixture Models,\u00ba Energy Minimization Methods in Computer Vision and Pattern Recognition"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "McLacl-ilan, Discriiiriiirriif Aimlysis m i i f Sfntisfiml I'dtcr.fr R ~ o p tioir"
            },
            "venue": {
                "fragments": [],
                "text": "Juhn Wile): & Sons"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cnndidc's Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion IECE Trms. Artttrrri Aiiirlysis rnnl M d i i i i c Ilrti:/liprrw"
            },
            "venue": {
                "fragments": [],
                "text": "Cnndidc's Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion IECE Trms. Artttrrri Aiiirlysis rnnl M d i i i i c Ilrti:/liprrw"
            },
            "year": 1911
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Deformable template models\n[69] or rubber sheet deformations [9] can be used to match\npatterns when the deformation cannot be easily explained\nor modeled directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "An obvious choice in this class is to use the minimum description length (MDL) criterion [10] [138], but several other model selection criteria have been proposed: Schwarz's Bayesian inference criterion (BIC), the minimum message length (MML) criterion, and Akaike's information criterion (AIC) [2], [148], [167]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minimum Description Length Principle in Coding and Modeling,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Information Theory, vol. 44,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "An example is the random subspace method [75]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aThe Random Subspace Method for Constructing Decision Forests,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is\npossible that the available information is sufficient for a\ndirect solution but is insufficient for solving a more general\nintermediate problem.\u00ba"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaStatistical Aspects of Neural Networks,\u00ba Networks on Chaos: Statistical and Probabilistic Aspects"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaStatistical Aspects of Neural Networks,\u00ba Networks on Chaos: Statistical and Probabilistic Aspects"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "I.cCun, and 1. I h i k c r , \"'l'iingcnt I'rop"
            },
            "venue": {
                "fragments": [],
                "text": "4nalysis of Decision Botiiidarics in Liiwarly C:onitriiied Ncural Classifiers,\" I'/~tler~ Recop iliotr"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE ASSP"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE ASSP"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "avoided by using a pruning stage [63], [106], [128]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSimplifying Decision Trees,o"
            },
            "venue": {
                "fragments": [],
                "text": "Int'l J. Man-Machine Studies,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cnndidc's Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion IECE Trms. Artttrrri Aiiirlysis rnnl M d i i i i c Ilrti:/liprrw"
            },
            "venue": {
                "fragments": [],
                "text": "Cnndidc's Practical I'rinciplm of Expfrirneiital Pnttcrn Recognilion IECE Trms. Artttrrri Aiiirlysis rnnl M d i i i i c Ilrti:/liprrw"
            },
            "year": 1911
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cyoifi, \"Imwcr 13ounds for Bayes Error Estiination,\" /EEL 'Y 'WYIS"
            },
            "venue": {
                "fragments": [],
                "text": "I'ottert~ ArmZysis nwd M I I E J ~ ~ I I Z Iirfdlig~nci~,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "indepcndcnt Component Analysis, a Ncw Concept?,\" S i p 1 I'rorwirisUsing Veclor Qtianlizalion fur Iiiiage Prucessiug"
            },
            "venue": {
                "fragments": [],
                "text": "Pwc. IEEE"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aModel Selection and the Principle of Minimum Description Length,o"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cotnparativc SLudy of Tcchniqucs for Large Sciitc 1:calurc Sclecfioti"
            },
            "venue": {
                "fragments": [],
                "text": "Cotnparativc SLudy of Tcchniqucs for Large Sciitc 1:calurc Sclecfioti"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The only \u00aaoptimal\u00ba (in terms of a class of monotonic criterion functions) feature selection method which avoids the exhaustive search is based on the branch and bound algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Duin, \u00aaSammon's Mapping Using Neural Networks: Comparison,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "Duin, \u00aaSammon's Mapping Using Neural Networks: Comparison,\u00ba Pattern Recognition Letters"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLogistic Discrimination,\u00ba Handbook of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaLogistic Discrimination,\u00ba Handbook of Statistics"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "and the resubstitution estimates of the error rate [82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "for designing classifiers [48], and for error estimation [82]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aBootstrap Techniques for Error Estimation,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sammon's Mapping Using Neiird Netwurks: Cumpiriwn,\" Pntttun k!ci)p!itiou I.cttcr.s"
            },
            "venue": {
                "fragments": [],
                "text": "Sammon's Mapping Using Neiird Netwurks: Cumpiriwn,\" Pntttun k!ci)p!itiou I.cttcr.s"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Whcn Networks Uisngrfc: Eiiseirible Methuds for Hybrid Ncural NPtwurks,\" Ncwd h W mnvks f o r Spccdi nird b ~ i n p ! I'uorrssirrg"
            },
            "venue": {
                "fragments": [],
                "text": "pp. hdb-h70,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Duin, \u00aaExpected Classification Error of the Fisher Linear Classifier with Pseudoinverse Covariance Matrix,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "Duin, \u00aaExpected Classification Error of the Fisher Linear Classifier with Pseudoinverse Covariance Matrix,\u00ba Pattern Recognition Letters"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural-Nclwork ikatiirc Selcctor IEEE Tmrrs. Ni!tm/ Ni!h~~i)rk$"
            },
            "venue": {
                "fragments": [],
                "text": "Neural-Nclwork ikatiirc Selcctor IEEE Tmrrs. Ni!tm/ Ni!h~~i)rk$"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "den Hartog, \u00aaHandwritten Digit Recognition by Combined Classifiers,\u00ba Kybernetika"
            },
            "venue": {
                "fragments": [],
                "text": "den Hartog, \u00aaHandwritten Digit Recognition by Combined Classifiers,\u00ba Kybernetika"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sirbspocc Mctlicds o,f Pnitcni Rccupitiuir, Leicliworh, Herthrdsliirc"
            },
            "venue": {
                "fragments": [],
                "text": "Eriglaud: Research Studies Press,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sluirulirunl Pollern Xccr>piliuii"
            },
            "venue": {
                "fragments": [],
                "text": "N e w York SpringerVerlaz,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aExperiments with a Featureless Approach to Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiplc Classifiers Using Local Accuracy Iktifnates,\" I /'/Nerti h n l y s i s rwd Moulriric Itrtellixcrw"
            },
            "venue": {
                "fragments": [],
                "text": "Multiplc Classifiers Using Local Accuracy Iktifnates,\" I /'/Nerti h n l y s i s rwd Moulriric Itrtellixcrw"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSupport Vector Learning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, Technische UniversitaE\u0300t, Berlin,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiplc Classifiers Using Local Accuracy Iktifnates,\" I /'/Nerti h n l y s i s rwd Moulriric Itrtellixcrw"
            },
            "venue": {
                "fragments": [],
                "text": "Multiplc Classifiers Using Local Accuracy Iktifnates,\" I /'/Nerti h n l y s i s rwd Moulriric Itrtellixcrw"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aSupport Vector Learning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, Technische UniversitaE\u0300t, Berlin,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\" Oil Ihiycsian Analysis of Mixtures with Uiiknorvn Numbcr of Compuncnts , \" 1"
            },
            "venue": {
                "fragments": [],
                "text": "Royrrl Sttit istici ~ I Soc , [ I 361 E . Ripley , \" Stntisticnl Aspccts of Ncuml Nctwnrks , \" Akfzuovks 011 Choos : Stofictiiinl imI Probabilistic Aspiiuls"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOptimal Combinations of Pattern Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Letters,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "In the\nstatistical decision theoretic approach, the decision bound-\naries are determined by the probability distributions of the\npatterns belonging to each class, which must either be\nspecified or learned [41], [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaArtificial Neural Networks: A Tutorial,\u00ba Computer"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaArtificial Neural Networks: A Tutorial,\u00ba Computer"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sclove , a Application of the Conditional Population Mixture Model to Image Segmentation , o IEEE Trans"
            },
            "venue": {
                "fragments": [],
                "text": "o IEEE Trans . Signal Processing Pattern Classification : A Unified View of Statistical and Neural Approaches"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The\nfinal classifier after 50 epochs has clearly adapted to the\nnoise in the dataset: it tries to separate isolated patterns in a\nway that does not contribute to its generalization ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 63
                            }
                        ],
                        "text": "In this sense, multilayer networks serve as feature extractors [100]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOptimized Feature Extraction and the Bayes Decision in Feed-Forward Classifier Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fcnturc Scluctiuu Ihscrl on thc hpproxirnntion of Class Ilcnsitics by Fiiritc Mixturcs of flip Spccial 'I'ppc,\" I'ntttm Kmugiritiou"
            },
            "venue": {
                "fragments": [],
                "text": "Fcnturc Scluctiuu Ihscrl on thc hpproxirnntion of Class Ilcnsitics by Fiiritc Mixturcs of flip Spccial 'I'ppc,\" I'ntttm Kmugiritiou"
            },
            "year": 1945
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hearst, \u00aaSupport Vector Machines,\u00ba IEEE Intelligent Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Hearst, \u00aaSupport Vector Machines,\u00ba IEEE Intelligent Systems"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 62
                            }
                        ],
                        "text": "Examples of the use of ROC analysis are combining classifiers [170] and feature selection [99]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aCombination of Multiple Classifiers Using Local Accuracy Estimates,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Note on Comparing Classifiers Przlterii R c c r p i i t h Lrltcrs"
            },
            "venue": {
                "fragments": [],
                "text": "A Note on Comparing Classifiers Przlterii R c c r p i i t h Lrltcrs"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Boosting [52] is another resampling technique for generating a sequence of training data sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aExperiments with a New Boosting Algorithm,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 13th Int'l Conf. Machine Learning,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the l'ussible Urclcrin@ i n the M e m \" c i i t Scfcction Problen~,'' I A. Ilcmpster, N. h i d , and 11. KubinMaximum 1 ,ikcliliood from IncoInpLctc Data via the (EM) Algoiitlm"
            },
            "venue": {
                "fragments": [],
                "text": "IY77. ti. tlcmuth and H.M, I%ualu, Nciird Nct7uork Tunlliuxfiir llse iuitlr Mnthfb. version 3"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaA Note on Comparing Classifiers,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaA Note on Comparing Classifiers,\u00ba Pattern Recognition Letters"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mmkoviori Modcls for Scquetrtiol Data"
            },
            "venue": {
                "fragments": [],
                "text": "Neirnd Corrrprrtirfg Srirueys"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kittlcr, \"Cotnparativc SLudy of Tcchniqucs for Large Sciitc 1:calurc Sclecfioti,\" P n l t m i Xmgr i i t iuu irr Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 163
                            }
                        ],
                        "text": "The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aConundrum of Combinatorial Complexity,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "N w r d Nrliuo~kks j i r Rrtteni RccogtiiyHition. Oxford; Clawndon I'rcss"
            },
            "venue": {
                "fragments": [],
                "text": "N w r d Nrliuo~kks j i r Rrtteni RccogtiiyHition. Oxford; Clawndon I'rcss"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Minimum Description i-mgth l'rinciplc in Coding and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Tvwrs. Injoririntiotz Tlmry"
            },
            "year": 199
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stacked Ccncralization"
            },
            "venue": {
                "fragments": [],
                "text": "Ncirrd Nctruorks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 224
                            }
                        ],
                        "text": "To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evidence Framework Applied to Classification Networks,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pltitruiz Kccopiitiu!!: Htrnwr n d M w h i c o l"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Wiley,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[90] developed a common theoretical framework for a class of combination schemes where"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aOn Combining Classifiers,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stofislicol Lcorriing 7%mr!"
            },
            "venue": {
                "fragments": [],
                "text": "Stofislicol Lcorriing 7%mr!"
            },
            "year": 199
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Potkm Aiti!ysis niid M d i w I r i t d i i p c c , vd"
            },
            "venue": {
                "fragments": [],
                "text": "Potkm Aiti!ysis niid M d i w I r i t d i i p c c , vd"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 1
                            }
                        ],
                        "text": "The decision making process in statistical pattern recognition can be summarized as follows: A given pattern is to be assigned to one of c categories !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sample Size, Classification Error, and Complexity of Classification Algorithms in Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "The Receiver Operating Characteristic (ROC) Curve [107] is a plot of FAR versus FRR which permits the system designer to assess the performance of the recognition system at various operating points (thresholds in the decision rule)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aBasic Principles of ROC Analysis,o"
            },
            "venue": {
                "fragments": [],
                "text": "Seminars in Nuclear Medicine, vol. VIII,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mriltidimeiisional Scaling by Iterative Mwjorization Using Radial Basis Futictions"
            },
            "venue": {
                "fragments": [],
                "text": "Mriltidimeiisional Scaling by Iterative Mwjorization Using Radial Basis Futictions"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Tutorial nn Support Vcctor Machitics for Pattern Recrignilinn"
            },
            "venue": {
                "fragments": [],
                "text": "Dntn Miiiirig nrid Knorriltdge Discovery"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Piritc'rir Rw#!iitjwi: A Stotisticii! Appuor~lt. Imdon: Prciilicc I-hll"
            },
            "venue": {
                "fragments": [],
                "text": "Piritc'rir Rw#!iitjwi: A Stotisticii! Appuor~lt. Imdon: Prciilicc I-hll"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stofislicol Lcorriing 7%mr!"
            },
            "venue": {
                "fragments": [],
                "text": "Stofislicol Lcorriing 7%mr!"
            },
            "year": 199
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Potkm Aiti!ysis niid M d i w I r i t d i i p c c , vd"
            },
            "venue": {
                "fragments": [],
                "text": "Potkm Aiti!ysis niid M d i w I r i t d i i p c c , vd"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mriltidimeiisional Scaling by Iterative Mwjorization Using Radial Basis Futictions"
            },
            "venue": {
                "fragments": [],
                "text": "Mriltidimeiisional Scaling by Iterative Mwjorization Using Radial Basis Futictions"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 206
                            }
                        ],
                        "text": "But, if a multilayer feed forward network is used for pattern classification, then the node-pruning method simultaneously determines both the optimal feature subset and the optimal network classifier [26], [103]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "optimize the resulting partition [110], [139], and 3) mapping it onto a neural network [103] for possibly efficient implementation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aParsimonious Network Design and Feature Selection through Node Pruning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 12th Int'l Conf. Pattern on Recognition,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "a Feature Selection : Evaluation , Application , and Small Sample Performance , o IEEE Trans"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gcomctrical mil Stiltistical Propcrtius of Systcnis of Litwar lncqualitics with Applications in I'attcrn kccognition"
            },
            "venue": {
                "fragments": [],
                "text": "IBEE Trnns. CIrcIruriic Coiiipirtels"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Philadelphia: SIAM"
            },
            "venue": {
                "fragments": [],
                "text": "Philadelphia: SIAM"
            },
            "year": 1482
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Borr mid 1' . Groenen, Modwun Mi~llinitrrgrisiot~oI Scdin.i~, Berlin: nos"
            },
            "venue": {
                "fragments": [],
                "text": "Borr mid 1' . Groenen, Modwun Mi~llinitrrgrisiot~oI Scdin.i~, Berlin: nos"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opticnl Ctmxlcr Recognition Wilry Cncyclopcdin a j Elrctrkd o d Elrxc\\roiric Cizgiwi:ririg. 1.G. Webstcr, ~ d"
            },
            "venue": {
                "fragments": [],
                "text": "\\JOL 15"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Most commonly used criterion functions do not satisfy this monotonicity property."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jackel, \u00aaBackpropagation Applied to Handwritten Zip Code Recognition,\u00ba Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Jackel, \u00aaBackpropagation Applied to Handwritten Zip Code Recognition,\u00ba Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 295
                            }
                        ],
                        "text": "An obvious choice in this class is to use the minimum description length (MDL) criterion [10] [138], but several other model selection criteria have been proposed: Schwarz's Bayesian inference criterion (BIC), the minimum message length (MML) criterion, and Akaike's information criterion (AIC) [2], [148], [167]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 192
                            }
                        ],
                        "text": "To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA New Look at Statistical Model Identification,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Automatic Control,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contptcr Syslems thnt Lrnrri. Califnrnia: Mu~gan KaLdmann Piiblislirrs"
            },
            "venue": {
                "fragments": [],
                "text": "Contptcr Syslems thnt Lrnrri. Califnrnia: Mu~gan KaLdmann Piiblislirrs"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "or the nearest mean classifier can be viewed as finding the nearest subspace [116]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Subspace Methods of Pattern Recognition, Letchworth"
            },
            "venue": {
                "fragments": [],
                "text": "Hertfordshire, England: Research Studies Press,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Nntr on Gcnctic Algorithms for Large-Scale l h i tire Selection Piillcm I<rcuxriititvr L ~ t / r r s"
            },
            "venue": {
                "fragments": [],
                "text": "A Nntr on Gcnctic Algorithms for Large-Scale l h i tire Selection Piillcm I<rcuxriititvr L ~ t / r r s"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Noiiliiienr Mappitrg for Unta Structirrc Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Tmirs. Gmipiitcr"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Rccuguitioii Using a Ncrv 'I'ransforitiatinl~ l>istmu:c,\" Ahniici!s iif Ncirnil I$miirrtiuii Prrrcrmi~ig Spkrirs"
            },
            "venue": {
                "fragments": [],
                "text": "S.J. Haiison, J.l) Cowan, and C.I.. Gilcs, cds"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character Recognition,\u00ba Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "venue": {
                "fragments": [],
                "text": "Character Recognition,\u00ba Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal i ' artitinning for Classification nncl lbgrcs - sinn Trees , \" I E C C Trnris"
            },
            "venue": {
                "fragments": [],
                "text": "\" tllind Signdl Supardtion : Statistical I ' rir ~ ciplcs , \" I h c . IEEE"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Vector quantization also provides an efficient tool for density estimation [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aVector Quantization and Density Estimation,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. Compression and Complexity of Sequences,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A popular analysis of combination schemes is based on the well-known bias-variance dilemma [64], [93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vedelsby, aNeural Network Ensembles, Cross Validation, and Active Learning,o"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal i ' artitinning for Classification nncl lbgrcs - sinn Trees , \" I E C C Trnris"
            },
            "venue": {
                "fragments": [],
                "text": "\" tllind Signdl Supardtion : Statistical I ' rir ~ ciplcs , \" I h c . IEEE"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaModel Selection and the Principle of Minimum Description Length,\u00ba technical report"
            },
            "venue": {
                "fragments": [],
                "text": "N.J"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Rccuguitioii Using a Ncrv 'I'ransforitiatinl~ l>istmu:c,\" Ahniici!s iif Ncirnil I$miirrtiuii Prrrcrmi~ig Spkrirs"
            },
            "venue": {
                "fragments": [],
                "text": "S.J. Haiison, J.l) Cowan, and C.I.. Gilcs, cds"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character Recognition,\u00ba Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "venue": {
                "fragments": [],
                "text": "Character Recognition,\u00ba Wiley Encyclopedia of Electrical and Electronic Engineering"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "One such method which is directly related to PCA is called the Kernel PCA [73], [145]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aNonlinear Component Analysis as a Kernel Eigenvalue Problem,o"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 167
                            }
                        ],
                        "text": "Various stress functions are used for measuring the performance of this mapping [20]; the most popular\ncriterion is the stress function introduced by Sammon [141] and Niemann [114]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 175
                            }
                        ],
                        "text": "Various stress functions are used for measuring the performance of this mapping [20]; the most popular criterion is the stress function introduced by Sammon [141] and Niemann [114]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aLinear and Nonlinear Mappings of Patterns,o"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition,"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 2
                            }
                        ],
                        "text": "They further showed that any ordering of the classification errors of each of the 2d feature subsets is possible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLinear and Nonlinear Mappings of Patterns,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaLinear and Nonlinear Mappings of Patterns,\u00ba Pattern Recognition"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applic;ition of tllc Coiiditivnal Population Mixture Modcl 10 lmagc Srginciihtioii,\" ICTL '/'rnt/s. IWwi Kccngiritinii nird Mndriric IrileUipice"
            },
            "venue": {
                "fragments": [],
                "text": "Applic;ition of tllc Coiiditivnal Population Mixture Modcl 10 lmagc Srginciihtioii,\" ICTL '/'rnt/s. IWwi Kccngiritinii nird Mndriric IrileUipice"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Various training algorithms have been proposed in the literature [23], including chunking [161], Osuna's decomposition method [119], and sequential minimal optimization [124]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "For example, the maximum margin objective was introduced in the context of support vector machines [23] based on structural risk minimization theory [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Tutorial on Support Vector Machines for Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Data Mining and Knowledge Discovery,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 2
                            }
                        ],
                        "text": "They further showed that any ordering of the classification errors of each of the 2d feature subsets is possible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLinear and Nonlinear Mappings of Patterns,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaLinear and Nonlinear Mappings of Patterns,\u00ba Pattern Recognition"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 138
                            }
                        ],
                        "text": "While this is true for feature sets of moderate size, several recent applications, particularly those in data mining and document classification, involve thousands of features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minor Components, and Linear Neural Networks,\u00ba Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Minor Components, and Linear Neural Networks,\u00ba Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aStatistical Themes and Lessons for Data Mining,o"
            },
            "venue": {
                "fragments": [],
                "text": "Data Mining and Knowledge Discovery,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": ", by vector quantization techniques possibly combined with an optimized metric or kernel [60], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reduced Parzen Classifier,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applic;ition of tllc Coiiditivnal Population Mixture Modcl 10 lmagc Srginciihtioii,\" ICTL '/'rnt/s. IWwi Kccngiritinii nird Mndriric IrileUipice"
            },
            "venue": {
                "fragments": [],
                "text": "Applic;ition of tllc Coiiditivnal Population Mixture Modcl 10 lmagc Srginciihtioii,\" ICTL '/'rnt/s. IWwi Kccngiritinii nird Mndriric IrileUipice"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Various training algorithms have been proposed in the literature [23], including chunking [161], Osuna's decomposition method [119], and sequential minimal optimization [124]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "For example, the maximum margin objective was introduced in the context of support vector machines [23] based on structural risk minimization theory [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aA Tutorial on Support Vector Machines for Pattern Recognition,o"
            },
            "venue": {
                "fragments": [],
                "text": "Data Mining and Knowledge Discovery,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Combiners, such as voting, averaging (or sum), and Borda count [74] are static, with no training required, while others are trainable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aDecision Combination in Multiple Classifier Systems,o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence,"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 99,
            "methodology": 63,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 477,
        "totalPages": 48
    },
    "page_url": "https://www.semanticscholar.org/paper/Statistical-Pattern-Recognition:-A-Review-Jain-Duin/3626f388371b678b2f02f6eefc44fa5abc53ceb3?sort=total-citations"
}