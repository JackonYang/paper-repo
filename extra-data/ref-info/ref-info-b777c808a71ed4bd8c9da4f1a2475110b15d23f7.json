{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145887845"
                        ],
                        "name": "D. M. Campbell",
                        "slug": "D.-M.-Campbell",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Campbell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158090475"
                        ],
                        "name": "Y. S. Jiang",
                        "slug": "Y.-S.-Jiang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jiang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. S. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742211"
                        ],
                        "name": "Stephen W. Liddle",
                        "slug": "Stephen-W.-Liddle",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Liddle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen W. Liddle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144259075"
                        ],
                        "name": "D. Quass",
                        "slug": "D.-Quass",
                        "structuredName": {
                            "firstName": "Dallan",
                            "lastName": "Quass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Quass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985589"
                        ],
                        "name": "Randy D. Smith",
                        "slug": "Randy-D.-Smith",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Smith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randy D. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3217704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "156a5992d338d2bf7ba8f5af50be8985fabe93fb",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conceptual-Model-Based-Data-Extraction-from-Web-Embley-Campbell",
            "title": {
                "fragments": [],
                "text": "Conceptual-Model-Based Data Extraction from Multiple-Record Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": "Data Knowl. Eng."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161245261"
                        ],
                        "name": "S. Lim",
                        "slug": "S.-Lim",
                        "structuredName": {
                            "firstName": "Seung",
                            "lastName": "Lim",
                            "middleNames": [
                                "Jin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1999678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c1a5b3400848c7156511851f2f6ca8a3b561c62",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Among the HTML elements, HTML tables [RHJ98] encapsulate hierarchically structured data (hierarchical data in short) in a tabular structure. HTML tables do not come with a rigid schema and almost any forms of two-dimensional tables are acceptable according to the HTML grammar. This relaxation complicates the process of retrieving hierarchical data from HTML tables. In this paper, we propose an automated approach for retrieving hierarchical data from HTML tables. The proposed approach constructs the content tree of an HTML table, which captures the intended hierarchy of the data content of the table, without requiring the internal structure of the table to be known beforehand. Also, the user of the content tree does not deal with HTML tags while retrieving the desired data from the content tree. Our approach can be employed by (i) a query language written for retrieving hierarchically structured data, extracted from either the contents of HTML tables or other sources, (ii) a processor for converting HTML tables to XML documents, and (iii) a data warehousing repository for collecting hierarchical data from HTML tables and storing materialized views of the tables. The time complexity of the proposed retrieval approach is proportional to the number of HTML elements in an HTML table."
            },
            "slug": "An-automated-approach-for-retrieving-hierarchical-Lim-Ng",
            "title": {
                "fragments": [],
                "text": "An automated approach for retrieving hierarchical data from HTML tables"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes an automated approach for retrieving hierarchical data from HTML tables that constructs the content tree of an HTML table, which captures the intended hierarchy of the data content of the table, without requiring the internal structure of thetable to be known beforehand."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707920"
                        ],
                        "name": "J. Biskup",
                        "slug": "J.-Biskup",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Biskup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Biskup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2286967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68380580fce82c538ed8389d85ad5591167c9418",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-information-from-heterogeneous-sources-Biskup-Embley",
            "title": {
                "fragments": [],
                "text": "Extracting information from heterogeneous information sources using ontologically specified target views"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158090475"
                        ],
                        "name": "Y. S. Jiang",
                        "slug": "Y.-S.-Jiang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jiang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. S. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7457781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31aa4b4ddfa454bc0626cac4bf092c8d2fe2c852",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By \u201crecord\u201d we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted)."
            },
            "slug": "Record-boundary-discovery-in-Web-documents-Embley-Jiang",
            "title": {
                "fragments": [],
                "text": "Record-boundary discovery in Web documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes a heuristic approach to discovering record boundaries in Web documents that captures the structure of a document as a tree of nested HTML tags, and locates the subtree containing the records of interest using five independent heuristics and selects a consensus separator tag based on a combined heuristic."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171323607"
                        ],
                        "name": "Li Xu",
                        "slug": "Li-Xu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7231598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "335a139bde58fce6b855059a0497b9d9a95e0db0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Record extraction from data-rich, unstructured, multiplerecord Web documents works well [8], but only if the text for each record can be located and isolated. Although some multiple-record Web documents present records as contiguous, delineated chunks of text (which can thus be located and isolated [9]), many do not. When some values of textual records are factored out, are split unnaturally across boundaries, are joined unnaturally within boundaries, or are linked by off-page connectors, or when desired records are interspersed with records that are not of interest, it is difficult to automatically cull records and piece values together to form clean, delineated chunks of text that each represent a single record of interest. In this paper we attack this problem and propose an algorithm to find and rearrange (if necessary) records in an HTML document by attempting to maximize a record-recognition heuristic with respect to a given application ontology. Tests we conducted show that this technique properly locates and reconfigures records for all classified types of rearrangements both for artificial and for actual multiple-record Web documents."
            },
            "slug": "Record-Location-and-Reconfiguration-in-Unstructured-Embley-Xu",
            "title": {
                "fragments": [],
                "text": "Record Location and Reconfiguration in Unstructured Multiple-Record Web Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an algorithm to find and rearrange records in an HTML document by attempting to maximize a record-recognition heuristic with respect to a given application ontology and demonstrates that this technique properly locates and reconfigures records for all classified types of rearrangements."
            },
            "venue": {
                "fragments": [],
                "text": "WebDB"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742211"
                        ],
                        "name": "Stephen W. Liddle",
                        "slug": "Stephen-W.-Liddle",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Liddle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen W. Liddle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221440"
                        ],
                        "name": "S. H. Yau",
                        "slug": "S.-H.-Yau",
                        "structuredName": {
                            "firstName": "Sai",
                            "lastName": "Yau",
                            "middleNames": [
                                "Ho"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. H. Yau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17031984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd749bb74aa2d6ad336418669abf70c13911718d",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "An increasing amount of Web data is accessible only by filling out HTML forms to query an underlying data source. While this is most welcome from a user perspective (queries are easy and precise) and from a data management perspective (static pages need not be maintained; databases can be accessed directly), automated agents have greater difficulty accessing data behind forms. In this paper we present a method for automatically filling in forms to retrieve the associated dynamically generated pages. Using our approach automated agents can begin to systematically access portions of the \u201chidden Web.\u201d"
            },
            "slug": "On-the-Automatic-Extraction-of-Data-from-the-Hidden-Liddle-Yau",
            "title": {
                "fragments": [],
                "text": "On the Automatic Extraction of Data from the Hidden Web"
            },
            "venue": {
                "fragments": [],
                "text": "ER"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171323607"
                        ],
                        "name": "Li Xu",
                        "slug": "Li-Xu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10692226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ae678e0b51a8a7706b47225bfc62abf4e2ef1f4",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically recognizing which Web documents are \"of interest\" for some specified application is non-trivial. As a step toward solving this problem, we propose a technique for recognizing which multiplere-cord Web documents apply to an ontologically specified application. Given the values and kinds of values recognized by an ontological specification in an unstructuredWeb document, we apply three heuristics: (1) a density heuristic that measures the percent of the document that appears to apply to an application ontology, (2) an expected-value heuristic that compares the number and kind of values found in a document to the number and kind expected by the application ontology, and (3) a grouping heuristic that considers whether the values of the document appear to be grouped as application-ontology records. Then, based on machine-learned rules over these heuristic measurements, we determine whether a Web document is applicable for a given ontology. Our experimental results show that we have been able to achieve over 90% for both recall and precision, with an F-measure of about 95%."
            },
            "slug": "Recognizing-Ontology-Applicable-Multiple-Record-Web-Embley-Ng",
            "title": {
                "fragments": [],
                "text": "Recognizing Ontology-Applicable Multiple-Record Web Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A technique for recognizing which multiplere-cord Web documents apply to an ontologically specified application and, based on machine-learned rules over these heuristic measurements, determines whether a Web document is applicable for a given ontology."
            },
            "venue": {
                "fragments": [],
                "text": "ER"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030274"
                        ],
                        "name": "A. Doan",
                        "slug": "A.-Doan",
                        "structuredName": {
                            "firstName": "AnHai",
                            "lastName": "Doan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8997922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9011405b759b492b1132aea7b6165c9d1b0513e7",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy."
            },
            "slug": "Reconciling-schemas-of-disparate-data-sources:-a-Doan-Domingos",
            "title": {
                "fragments": [],
                "text": "Reconciling schemas of disparate data sources: a machine-learning approach"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "LSD is a system that employs and extends current machine-learning techniques to semi-automatically find semantic mappings between the source schemas and the mediated schema, and its architecture is extensible to additional learners that may exploit new kinds of information."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69484854"
                        ],
                        "name": "A. Rajaraman",
                        "slug": "A.-Rajaraman",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rajaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rajaraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193834"
                        ],
                        "name": "J. Ordille",
                        "slug": "J.-Ordille",
                        "structuredName": {
                            "firstName": "Joann",
                            "lastName": "Ordille",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ordille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 546155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2185d4535d9b3f82477897d2782a15705a53ea22",
            "isKey": false,
            "numCitedBy": 1437,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources"
            },
            "slug": "Querying-Heterogeneous-Information-Sources-Using-Halevy-Rajaraman",
            "title": {
                "fragments": [],
                "text": "Querying Heterogeneous Information Sources Using Source Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The Information Manifold is described, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW, and algorithms that use the source descriptions to prune effciently the set of information sources for a given query are described."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2224716"
                        ],
                        "name": "J. Madhavan",
                        "slug": "J.-Madhavan",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Madhavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Madhavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737944"
                        ],
                        "name": "P. Bernstein",
                        "slug": "P.-Bernstein",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747414"
                        ],
                        "name": "E. Rahm",
                        "slug": "E.-Rahm",
                        "structuredName": {
                            "firstName": "Erhard",
                            "lastName": "Rahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rahm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1456533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ff9bf4d58358fc008b059028a3e33919d12b335",
            "isKey": false,
            "numCitedBy": 1570,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Schema matching is a critical step in many applications, such as XML message mapping, data warehouse loading, and schema integration. In this paper, we investigate algorithms for generic schema matching, outside of any particular data model or application. We first present a taxonomy for past solutions, showing that a rich range of techniques is available. We then propose a new algorithm, Cupid, that discovers mappings between schema elements based on their names, data types, constraints, and schema structure, using a broader set of techniques than past approaches. Some of our innovations are the integrated use of linguistic and structural matching, context-dependent matching of shared types, and a bias toward leaf structure where much of the schema content resides. After describing our algorithm, we present experimental results that compare Cupid to two other schema matching systems."
            },
            "slug": "Generic-Schema-Matching-with-Cupid-Madhavan-Bernstein",
            "title": {
                "fragments": [],
                "text": "Generic Schema Matching with Cupid"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new algorithm, Cupid, that discovers mappings between schema elements based on their names, data types, constraints, and schema structure, using a broader set of techniques than past approaches."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742211"
                        ],
                        "name": "Stephen W. Liddle",
                        "slug": "Stephen-W.-Liddle",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Liddle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen W. Liddle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47856149"
                        ],
                        "name": "D. Scott",
                        "slug": "D.-Scott",
                        "structuredName": {
                            "firstName": "Del",
                            "lastName": "Scott",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Scott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221440"
                        ],
                        "name": "S. H. Yau",
                        "slug": "S.-H.-Yau",
                        "structuredName": {
                            "firstName": "Sai",
                            "lastName": "Yau",
                            "middleNames": [
                                "Ho"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. H. Yau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14512317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60b894a9ea5a25d89e2bc1bc1b8203482b063220",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A significant and ever-increasing amount of data is accessible only by filling out HTML forms to query an underlying Web data source. While this is most welcome from a user perspective (queries are relatively easy and precise) and from a data management perspective (static pages need not be maintained and databases can be accessed directly), automated agents must face the challenge of obtaining the data behind forms. In principle an agent can obtain all the data behind a form by multiple submissions of the form filled out in all possible ways, but efficiency concerns lead us to consider alternatives. We investigate these alternatives and show that we can estimate the amount of remaining data (if any) after a small number of submissions and that we can heuristically select a reasonably minimal number of submissions to maximize the coverage of the data. Experimental results show that these statistical predictions are appropriate and useful."
            },
            "slug": "Extracting-Data-behind-Web-Forms-Liddle-Embley",
            "title": {
                "fragments": [],
                "text": "Extracting Data behind Web Forms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work investigates alternatives to obtaining all the data behind a form by multiple submissions of the form filled out in all possible ways and shows that they can estimate the amount of remaining data after a small number of submissions and can heuristically select a reasonably minimal number of applications to maximize the coverage of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ER"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060211572"
                        ],
                        "name": "Kenneth M. Tubbs",
                        "slug": "Kenneth-M.-Tubbs",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Tubbs",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth M. Tubbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13970646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d75e6434f66062982e2eab16e00b1957d7a1d894",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Microfilm documents contain a wealth of information, but extracting and organizing this information by hand is slow, error-prone, and tedious. As an initial step toward automating access to this information, we describe in this paper an algorithmic process to automatically identify record patterns found in microfilm tables for pre-specified application domains. Our table-processing algorithm accepts an XML input file describing the individual cells of a table taken from a microfilm document, and finds for each record in the document the cells that together comprise the record. Two key features drive the algorithm: (1) geometric layout and (2) label matching with respect to a given domain-specific application ontology. The algorithm achieved an accuracy of 92% on our test corpus of genealogical microfilm tables."
            },
            "slug": "Recognizing-records-from-the-extracted-cells-of-Tubbs-Embley",
            "title": {
                "fragments": [],
                "text": "Recognizing records from the extracted cells of microfilm tables"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes an algorithmic process to automatically identify record patterns found in microfilm tables for pre-specified application domains, using an XML input file describing the individual cells of a table taken from a microfilm document."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49317498"
                        ],
                        "name": "S. Raghavan",
                        "slug": "S.-Raghavan",
                        "structuredName": {
                            "firstName": "Sriram",
                            "lastName": "Raghavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raghavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1493955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6c480ecfd35fd355eda7ca3fa0d9904ae7ed356",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Current-day crawlers retrieve content only from the publicly indexable Web, i.e., the set of Web pages reachable purely by following hypertext links, ignoring search forms and pages that require authorization or prior registration. In particular, they ignore the tremendous amount of high quality content \u201chidden\u201d behind search forms, in large searchable electronic databases. In this paper, we address the problem of designing a crawler capable of extracting content from this hidden Web. We introduce a generic operational model of a hidden Web crawler and describe how this model is realized in HiWE (Hidden Web Exposer), a prototype crawler built at Stanford. We introduce a new Layout-based Information Extraction Technique (LITE) and demonstrate its use in automatically extracting semantic information from search forms and response pages. We also present results from experiments conducted to test and validate our techniques."
            },
            "slug": "Crawling-the-Hidden-Web-Raghavan-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Crawling the Hidden Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generic operational model of a hidden Web crawler is introduced and how this model is realized in HiWE (Hidden Web Exposer), a prototype crawler built at Stanford is described."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9086294"
                        ],
                        "name": "S. Chawathe",
                        "slug": "S.-Chawathe",
                        "structuredName": {
                            "firstName": "Sudarshan",
                            "lastName": "Chawathe",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chawathe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144477659"
                        ],
                        "name": "J. Hammer",
                        "slug": "J.-Hammer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144185412"
                        ],
                        "name": "K. Ireland",
                        "slug": "K.-Ireland",
                        "structuredName": {
                            "firstName": "Kelly",
                            "lastName": "Ireland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ireland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786049"
                        ],
                        "name": "Y. Papakonstantinou",
                        "slug": "Y.-Papakonstantinou",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Papakonstantinou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Papakonstantinou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737896"
                        ],
                        "name": "J. Widom",
                        "slug": "J.-Widom",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Widom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Widom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2113876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14348170a14b4e2edca01521184cb2cd60b83200",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center. 1 Overview A common problem facing many organizations today is that of multiple, disparate information sources and repositories, including databases, object stores, knowledge bases, file systems, digital libraries, information retrieval systems, and electronic mail systems. Decision makers often need information from multiple sources, but are unable to get and fuse the required information in a timely fashion due to the diffculties of accessing the different systems, and due to the fact that the information obtained can be inconsistent and contradictory. Research sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Material Command, USAF, under Grant Number F33615-93-1-1339. The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the offcial policies or endorsements, either express or implied, of Wright Laboratory or the US Government. This work was also supported by the Reid and Polly Anderson Faculty Scholar Fund, the Center for Integrated Systems at Stanford University, and by Equipment Grants from Digital Equipment Corporation and IBM Corporation. The goal of the TSIMMIS 1 project is to provide tools for accessing, in an integrated fashion, multiple informati"
            },
            "slug": "The-TSIMMIS-Project:-Integration-of-Heterogeneous-Chawathe-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "The TSIMMIS Project: Integration of Heterogeneous Information Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An overview of the Tsimmis Project is given, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites."
            },
            "venue": {
                "fragments": [],
                "text": "IPSJ"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15075203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd1f9f7795b31493d98d9f260d37aad07550f6e",
            "isKey": false,
            "numCitedBy": 1157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach."
            },
            "slug": "RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences is developed, which confirms the feasibility of the approach on real-life data-intensive Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1053009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e936981f5a2d55bfec0143e9a15e23ad96436b",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories."
            },
            "slug": "Learning-Dictionaries-for-Information-Extraction-by-Riloff-Jones",
            "title": {
                "fragments": [],
                "text": "Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A multilevel bootstrapping algorithm is presented that generates both the semantic lexicon and extraction patterns simultaneously simultaneously and produces high-quality dictionaries for several semantic categories."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50df0dd6c8f9811e99f6c73b2f88330aeda2eb98",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "When applying text learning algorithms to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance. This paper presents bootstrapping as an alternative approach to learning from large sets of labeled data. Instead of a large quantity of labeled data, this paper advocates using a small amount of seed information and a large collection of easily-obtained unlabeled data. Bootstrapping initializes a learner with the seed information; it then iterates, applying the learner to calculate labels for the unlabeled data, and incorporating some of these labels into the training input for the learner. Two case studies of this approach are presented. Bootstrapping for information extraction provides 76% precision for a 250-word dictionary for extracting locations from web pages, when starting with just a few seed locations. Bootstrapping a text classifier from a few keywords per class and a class hierarchy provides accuracy of 66%, a level close to human agreement, when placing computer science research papers into a topic hierarchy. The success of these two examples argues for the strength of the general bootstrapping approach for text learning tasks."
            },
            "slug": "Bootstrapping-for-text-learning-tasks-Riloff",
            "title": {
                "fragments": [],
                "text": "Bootstrapping for text learning tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents bootstrapping as an alternative approach to learning from large sets of labeled data, using a small amount of seed information and a large collection of easily-obtained unlabeled data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695692"
                        ],
                        "name": "H. F. Korth",
                        "slug": "H.-F.-Korth",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Korth",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. F. Korth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688159"
                        ],
                        "name": "A. Silberschatz",
                        "slug": "A.-Silberschatz",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Silberschatz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Silberschatz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60911891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5edce377759894482464a133cb9ec6791709eb2",
            "isKey": false,
            "numCitedBy": 1767,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results."
            },
            "slug": "Database-System-Concepts-Korth-Silberschatz",
            "title": {
                "fragments": [],
                "text": "Database System Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This acclaimed revision of a classic database systems text provides the latest information combined with real-world examples to help readers master concepts in a technically complete yet easy-to-understand style."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1275701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4f36036c9747433e5ae9477c958cd6b212105a2",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-integration-using-logical-views-Ullman",
            "title": {
                "fragments": [],
                "text": "Information integration using logical views"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16106027,
            "fieldsOfStudy": [
                "Computer Science",
                "Philosophy"
            ],
            "id": "9e16dcd290d9ab780107270c666d71c00e569b22",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The principle that for every document analysis task there exists a mechanism for creating well-defined ground-truth is a widely held tenet. Past experience with standard datasets providing ground-truth for character recognition and page segmentation tasks supports this belief. In the process of attempting to evaluate several table recognition algorithms we have been developing, however, we have uncovered a number of serious hurdles connected with the ground-truthing of tables. This problem may, in fact, be much more difficult than it appears. We present a detailed analysis of why table ground-truthing is so hard, including the notions that there may exist more than one acceptable \"truth\" and/or incomplete or partial \"truths\"."
            },
            "slug": "Why-table-ground-truthing-is-hard-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Why table ground-truthing is hard"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a detailed analysis of why table ground-truthing is so hard, including the notions that there may exist more than one acceptable \"truth\" and/or incomplete or partial \"truths\"."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110417078"
                        ],
                        "name": "Ren\u00e9e J. Miller",
                        "slug": "Ren\u00e9e-J.-Miller",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9e J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704391"
                        ],
                        "name": "L. Haas",
                        "slug": "L.-Haas",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Haas",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Haas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49092460"
                        ],
                        "name": "Mauricio A. Hern\u00e1ndez",
                        "slug": "Mauricio-A.-Hern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Mauricio",
                            "lastName": "Hern\u00e1ndez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mauricio A. Hern\u00e1ndez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 40383321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0790bf8d16e0d5e0374566881dfdfb2f20faa88",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Schema-Mapping-as-Query-Discovery-Miller-Haas",
            "title": {
                "fragments": [],
                "text": "Schema Mapping as Query Discovery"
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Car [0:1] has Year [1:*]; 3. Car [0:1] has Make [1:*]; 4. Car [0:1] has Model [1:*]; 5. Car [0:1] has Mileage"
            },
            "venue": {
                "fragments": [],
                "text": "Car"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The development of a prototype knowledge-based table-processing system . Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "The development of a prototype knowledge-based table-processing system . Master's thesis"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Car [0:1] has Price PhoneNr [1:*] is for Car [0:1]; 9"
            },
            "venue": {
                "fragments": [],
                "text": "Car [0:1] has Price PhoneNr [1:*] is for Car [0:1]; 9"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing records from the extracted cells of genealogical microfilm tables. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Recognizing records from the extracted cells of genealogical microfilm tables. Master's thesis"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Home page for BYU Data Extraction Research Group"
            },
            "venue": {
                "fragments": [],
                "text": "Home page for BYU Data Extraction Research Group"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated table processing: An (opinionated) survey"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Third IAPR Workshop on Graphics Recognition,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The development of a prototype knowledge-based table-processing system"
            },
            "venue": {
                "fragments": [],
                "text": "Master\u2019s thesis,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting data behind Web forms. In Proceedings of the Joint Workshop on Conceptual Modeling Approaches for E-business: A Web Service Perspective"
            },
            "venue": {
                "fragments": [],
                "text": "(eCOMO"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing records from the extracted cells of genealogical microfilm"
            },
            "venue": {
                "fragments": [],
                "text": "tables. Master\u2019s thesis,"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Automating-the-extraction-of-data-from-HTML-tables-Embley-Tao/b777c808a71ed4bd8c9da4f1a2475110b15d23f7?sort=total-citations"
}