{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8416045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd1def5778f24c2c5a5f1c114846326e8f86123",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an important aspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Automatic-text-recognition-for-video-indexing-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Wu, Manmatha and Riseman [29] combine the search for vertical edges with a texture filter to detect text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Wu, Manmatha and Riseman [24] combine the search for vertical edges with a texture filter to detect text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The video indexing system introduced by Sato et al. [ 10 ] with super-imposed caption (artificial text) extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9520237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14ce174bddee5b6b2750cf14574773b537ac4d42",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The automatic extraction and recognition of news captions and annotations can be of great help locating topics of interest in digital news video libraries. To achieve this goal, we present a technique, called Video OCR (Optical Character Reader), which detects, extracts, and reads text areas in digital video data. In this paper, we address problems, describe the method by which Video OCR operates, and suggest applications for its use in digital news archives. To solve two problems of character recognition for videos, low-resolution characters and extremely complex backgrounds, we apply an interpolation filter, multi-frame integration and character extraction filters. Character segmentation is performed by a recognition-based segmentation method, and intermediate character recognition results are used to improve the segmentation. We also include a method for locating text areas using text-like properties and the use of a language-based postprocessing technique to increase word recognition rates. The overall recognition results are satisfactory for use in news indexing. Performing Video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR:-indexing-digital-news-libraries-by-of-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR: indexing digital news libraries by recognition of superimposed captions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "To solve two problems of character recognition for videos, low-resolution characters and extremely complex backgrounds, an interpolation filter, multi-frame integration and character extraction filters are applied and the overall recognition results are satisfactory for use in news indexing."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] perform a color space reduction followed by color segmentation and spatial regrouping to detect text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747647"
                        ],
                        "name": "S. Santini",
                        "slug": "S.-Santini",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Santini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Santini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722619"
                        ],
                        "name": "Amarnath Gupta",
                        "slug": "Amarnath-Gupta",
                        "structuredName": {
                            "firstName": "Amarnath",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amarnath Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2827898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7c4096ed697696a5f4fc8f3a6a750dc0cdecfe",
            "isKey": false,
            "numCitedBy": 6725,
            "numCiting": 410,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap."
            },
            "slug": "Content-Based-Image-Retrieval-at-the-End-of-the-Smeulders-Worring",
            "title": {
                "fragments": [],
                "text": "Content-Based Image Retrieval at the End of the Early Years"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap are discussed, as well as aspects of system engineering: databases, system architecture, and evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "See Smeulders et al. [2] and Law [ 3 ] for survey papers on this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27213561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495cbddac2680305bb54ce9c02e6ffa1f0940381",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This text introduces the basic concepts and techniques in VIR. In doing so, it develops a foundation for further research and study. Divided into two parts, the first part describes the fundamental principles. A chapter is devoted to each of the main features of VIR, such as colour, texture and shape-based search. There is coverage of search techniques for time-based image sequences or videos, and an overview of how to combine all the basic features described and integrate them into the search process. The second part looks at advanced topics such as multimedia query. This book is essential reading for researchers in VIR, and final-year undergraduate and postgraduate students on courses such as Multimedia Information Retrieval, Multimedia Databases, and others."
            },
            "slug": "Principles-of-Visual-Information-Retrieval-Lew",
            "title": {
                "fragments": [],
                "text": "Principles of Visual Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is essential reading for researchers in VIR, and final-year undergraduate and postgraduate students on courses such as Multimedia Information Retrieval, Multimedia Databases, and others."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Pattern Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37016602"
                        ],
                        "name": "Mika Rautiainen",
                        "slug": "Mika-Rautiainen",
                        "structuredName": {
                            "firstName": "Mika",
                            "lastName": "Rautiainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mika Rautiainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2844861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ed635a48334efbdd076b656ae05a61dacc98df0",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Our team from the University of Maryland and INSA de Lyon participated in the feature extraction evaluation with overlay text features and in the search evaluation with a query retrieval and browsing system. For search we developed a weighted query mechanism by integrating 1) text (OCR and speech recognition) content using full text and n-grams through the MG system, 2) color correlogram indexing of image and video shots reported last year in TREC, and 3) ranked versions of the extracted binary features. A command line version of the interface allows users to formulate simple queries, store them and use weighted combinations of the simple queries to generate compound queries. One novel component of our interactive approach is the ability for the users to formulate dynamic queries previously developed for database applications at Maryland. The interactive interface treats each video clip as visual object in a multi-dimensional space, and each \u201dfeature\u201d of that clip is mapped to one dimension. The user can visualize any two dimensions by placing any two features on the horizontal and vertical axis with additional dimensions visualized by adding attributes to each object."
            },
            "slug": "Video-Indexing-and-Retrieval-at-UMD-Wolf-Doermann",
            "title": {
                "fragments": [],
                "text": "Video Indexing and Retrieval at UMD"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An interactive interface that treats each video clip as visual object in a multi-dimensional space, and each \u201dfeature\u201d of that clip is mapped to one dimension and the ability for the users to formulate dynamic queries previously developed for database applications at Maryland is developed."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36058334"
                        ],
                        "name": "H. Kronenberg",
                        "slug": "H.-Kronenberg",
                        "structuredName": {
                            "firstName": "Heino",
                            "lastName": "Kronenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kronenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14868685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ba04958180cb158da0fe02a8599a7e301844456",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed. To reduce the amount of small variations in color, a clustering algorithm is applied in a preprocessing step. Two methods have been developed for extracting text hypotheses. One is based on a top-down analysis using successive splitting of image regions. The other is a bottom-up region growing algorithm. The results of both methods are combined to robustly distinguish between text and non-text elements. Text elements are binarized using automatically extracted information about text color. The binarized text regions can be used as input for a conventional OCR module. Results are shown for parts of book and journal covers of different complexity. The proposed method is not restricted to cover pages, but can be applied to the extraction of text from other types of color images as well."
            },
            "slug": "Identification-of-text-on-colored-book-and-journal-Sobottka-Bunke",
            "title": {
                "fragments": [],
                "text": "Identification of text on colored book and journal covers"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed and a clustering algorithm is applied in a preprocessing step to reduce the amount of small variations in color."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23073168,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "3ef36b0186bb73619d08d02661616ce7df218ecd",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors examine the problem of locating and extracting text from images on the World Wide Web. They describe a text detection algorithm which is based on color clustering and connected component analysis. The algorithm first quantizes the color space of the input image into a number of color classes using a parameter-free clustering procedure. It then identifies text-like connected components in each color class based on their shapes. Finally, a post-processing procedure aligns text-like components into text lines. Experimental results suggest this approach is promising despite the challenging nature of the input data."
            },
            "slug": "Extracting-text-from-WWW-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Extracting text from WWW images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A text detection algorithm which is based on color clustering and connected component analysis is described, which suggests this approach is promising despite the challenging nature of the input data."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Li and Doermann use the Haar wavelet for feature extraction [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808205"
                        ],
                        "name": "G. Ciocca",
                        "slug": "G.-Ciocca",
                        "structuredName": {
                            "firstName": "Gianluigi",
                            "lastName": "Ciocca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ciocca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001823"
                        ],
                        "name": "I. Gagliardi",
                        "slug": "I.-Gagliardi",
                        "structuredName": {
                            "firstName": "Isabella",
                            "lastName": "Gagliardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Gagliardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143940718"
                        ],
                        "name": "R. Schettini",
                        "slug": "R.-Schettini",
                        "structuredName": {
                            "firstName": "Raimondo",
                            "lastName": "Schettini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schettini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Systems mixing features from different domains (image and text) are an interesting alternative to mono-domain based features [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12215419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21521cfc2c7c4918e53a434d3fd78176d4d08900",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "tion retrieval engine of Quicklook 2 . Quicklook 2 allows the user to query image and multimedia databases with the aid of sample images, or an impromptu sketch and/or textual descriptions, and progressively refine the system\u2019s response by indicating the relevance, or non-relevance of the retrieved items. The major innovation of the system is its relevance feedback mechanism that performs a statistical analysis of both the image and textual feature distributions of the retrieved items the user has judged relevant, or not relevant to identify what features the user has taken into account (and to what extent) in formulating this judgement, and then weigh their influence in the overall evaluation of similarity, as well as in the formulation of a new, single query that better expresses the user\u2019s multimedia information needs. Another important contribution is the design and integration with the relevance feedback mechanism of an indexing scheme based on triangle inequality to improve retrieval efficiency. The performance of the system is illustrated with examples from various application domains and for different types of queries (target search as well as similarity search). ( 2001 Academic Press"
            },
            "slug": "Quicklook2:-An-Integrated-Multimedia-System-Ciocca-Gagliardi",
            "title": {
                "fragments": [],
                "text": "Quicklook2: An Integrated Multimedia System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The design and integration with the relevance feedback mechanism of an indexing scheme based on triangle inequality to improve retrieval efficiency and the performance of the system is illustrated with examples from various application domains and for different types of queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Lang. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10575879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f39ae0cd7c874758500bae542f87709ec03038f",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic content-based video indexing is an important research problem. One approach is to extract text appearing in video as an indication of a scene's semantic content. Most work so far has focused only on detecting the spatial extent of text instances in individual video frames. But text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is necessary to determine the temporal extent of text events by combining the results of text detection on individual frames, over time. This is a nontrivial problem because a text event may move, rotate, grow, shrink, or otherwise change throughout its lifetime. Such text effects are common in television programs and commercials to attract viewer attention, but have so far been ignored in the literature. We present a method for detecting and tracking moving, changing caption text events in MPEG-1 compressed video."
            },
            "slug": "Robust-detection-of-stylized-text-events-in-digital-Crandall-Kasturi",
            "title": {
                "fragments": [],
                "text": "Robust detection of stylized text events in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a method for detecting and tracking moving, changing caption text events in MPEG-1 compressed video, which has so far been ignored in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Li and Doermann use the Haar wavelet for feature extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38586525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3224ce96a5d8174a546dcfba0f2a202bed3ded16",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a video text detection system based on automated neural network training. Compared with previous work which detects only graphical text with fixed parameters, our system (1) provides a training mechanism so the parameters of the system can be adapted to changing environments, (2) can detect both graphical text and scene text located in complex backgrounds, (3) can detect text in any orientation and (4) can perform multilingual text detection. Experiments show the effectiveness of our system in various text detection tasks."
            },
            "slug": "A-video-text-detection-system-based-on-automated-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "A video text detection system based on automated training"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A video text detection system based on automated neural network training that can detect both graphical text and scene text located in complex backgrounds, can detect text in any orientation, and can perform multilingual text detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704694"
                        ],
                        "name": "J. Sauvola",
                        "slug": "J.-Sauvola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Sauvola",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sauvola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48819369"
                        ],
                        "name": "T. Sepp\u00e4nen",
                        "slug": "T.-Sepp\u00e4nen",
                        "structuredName": {
                            "firstName": "Tapio",
                            "lastName": "Sepp\u00e4nen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sepp\u00e4nen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141938"
                        ],
                        "name": "S. Haapakoski",
                        "slug": "S.-Haapakoski",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Haapakoski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haapakoski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[24] solve this problem by adding a hypothesis on the gray values of text and background pixels (text pixels have gray values near 0 and background pixels have gray values near 255), which results in the following formula for the threshold: T = m \u00b7 (1\u2212 k \u00b7 (1\u2212 s R )) (24)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2019s method [24] with parameters k = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206775223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ccc9d0c186afb682b852d342207f3a273ceb867",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type related degradations are addressed. The algorithm uses document characteristics to determine (surface) attributes, often used in document segmentation. Using characteristic analysis, two new algorithms are applied to determine a local threshold for each pixel. An algorithm based on soft decision control is used for thresholding the background and picture regions. An approach utilizing local mean and variance of gray values is applied to textual regions. Tests were performed with images including different types of document components and degradations. The results show that the method adapts and performs well in each case."
            },
            "slug": "Adaptive-document-binarization-Sauvola-Sepp\u00e4nen",
            "title": {
                "fragments": [],
                "text": "Adaptive document binarization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture, using document characteristics to determine (surface) attributes, often used in document segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Wu, Manmatha and Riseman [29] combine the search for vertical edges with a texture filter to detect text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Wu, Manmatha and Riseman [31] combine the search for vertical edges with a texture filter to detect text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, Wernike and Lienhart extract overall and directional edge strength as features and use them as input for a neural network classifier [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13609029,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "641483b9dcaa2bac13f95a3f2f6738140c170184",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed. The approach has been tested extensively on a large variety of video frame sizes such 352/spl times/240 up to 1920/spl times/1280 and a large representative set of video sequences such as home videos, newscasts, title sequences and commercials. 95% of the text bounding boxes in videos were localized correctly. 80% of all characters were segmented correctly, while 7.8% characters were damaged. 90% of the correctly segmented characters were recognized correctly by standard OCR software."
            },
            "slug": "On-the-segmentation-of-text-in-videos-Wernicke-Lienhart",
            "title": {
                "fragments": [],
                "text": "On the segmentation of text in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed that has been tested extensively on a large variety of video frame sizes and a large representative set of video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258400"
                        ],
                        "name": "\u00d8. Trier",
                        "slug": "\u00d8.-Trier",
                        "structuredName": {
                            "firstName": "\u00d8ivind",
                            "lastName": "Trier",
                            "middleNames": [
                                "Due"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d8. Trier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15780310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3a74fe4e79add9de4803a825b6eae013215dfe7",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance."
            },
            "slug": "Goal-Directed-Evaluation-of-Binarization-Methods-Trier-Jain",
            "title": {
                "fragments": [],
                "text": "Goal-Directed Evaluation of Binarization Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example, and defines the performance of the character recognition module as the objective measure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242803"
                        ],
                        "name": "L. Agnihotri",
                        "slug": "L.-Agnihotri",
                        "structuredName": {
                            "firstName": "Lalitha",
                            "lastName": "Agnihotri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Agnihotri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1667502629"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Natasa",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "A similar method using edge detection and edge clustering has been proposed by Agnihotri and Dimitrova [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60735577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3b2c6b7bfaf9ab0d44c7103585fa0c81f60f3b9",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information brings important semantic clues in video content analysis. We describe a method for detection and representation of text in video segments. The method consists of seven steps: channel separation, image enhancement, edge detection, edge filtering, character detection, text box detection, and text line detection. Our results show that this method can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "slug": "Text-detection-for-video-analysis-Agnihotri-Dimitrova",
            "title": {
                "fragments": [],
                "text": "Text detection for video analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes a method for detection and representation of text in video segments that can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Content-Based Access of Image and Video Libraries (CBAIVL'99)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59749567,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "03629f66fb817c8aeddef8f1a76552c9e6588cd2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Video technology has developed thus far as a technology of images, but little has been done to help us use those images effectively. We can buy a camera that \u201cknows\u201d how to focus itself properly or compensate for our inability to hold it steady without a tripod; but no camera knows \u201cwhere the action is\u201d during a football game or even a press conference. A camera shot can give us a clear image of the ball going through the goal posts, but only if we find the ball for it."
            },
            "slug": "Video-Indexing-and-Retrieval-Smoliar-Zhang",
            "title": {
                "fragments": [],
                "text": "Video Indexing and Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Video technology has developed thus far as a technology of images, but little has been done to help use those images effectively; but no camera knows \u201cwhere the action is\u201d during a football game or even a press conference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6107789,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6d90dca820b48d607b9a70386066c5d1c543dc23",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method based on statistical properties of local image pixels for focussing attention on regions of text in arbitrary scenes where the text plane is not necessarily fronto-parallel to the camera. This is particularly useful for desktop or wearable computing applications. The statistical measures are chosen to reveal characteristic properties of text. We combine a number of localised measures using a neural network to classify each pixel as text or non-text. We demonstrate our results on typical images."
            },
            "slug": "Combining-statistical-measures-to-find-image-text-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Combining statistical measures to find image text regions"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A method based on statistical properties of local image pixels for focussing attention on regions of text in arbitrary scenes where the text plane is not necessarily fronto-parallel to the camera is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783015"
                        ],
                        "name": "P. Sahoo",
                        "slug": "P.-Sahoo",
                        "structuredName": {
                            "firstName": "Prasanna",
                            "lastName": "Sahoo",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sahoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054982120"
                        ],
                        "name": "S. Soltani",
                        "slug": "S.-Soltani",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Soltani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soltani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873516"
                        ],
                        "name": "Andrew K. C. Wong",
                        "slug": "Andrew-K.-C.-Wong",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Wong",
                            "middleNames": [
                                "K.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew K. C. Wong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "surveys of existing techniques are [22] for the general thresholding case and the well known paper of Trier and Jain on binarization techniques for document images [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205113626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a050eb794e7cb65207d998371f8f0287e7ed53ab",
            "isKey": false,
            "numCitedBy": 2769,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-thresholding-techniques-Sahoo-Soltani",
            "title": {
                "fragments": [],
                "text": "A survey of thresholding techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6781817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7721138e41d82fedabca59c9a66e67d9b7053f3",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically locate captions in MPEG video. Caption text regions are segmented from the background using their distinguishing texture characteristics. This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain. Therefore, only a small amount of decoding is required. The proposed algorithm achieves about 4.0% false reject rate and less than 5.7% false positive rate on a variety of MPEG compressed video containing more than 42,000 frames."
            },
            "slug": "Automatic-caption-localization-in-compressed-video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain, so that only a small amount of decoding is required."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15034932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c8a2963be6088dd55959bfe8cd92916891fb66",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-based-text-location-in-color-images-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location in color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10699750"
                        ],
                        "name": "Xinbo Gao",
                        "slug": "Xinbo-Gao",
                        "structuredName": {
                            "firstName": "Xinbo",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinbo Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7137861"
                        ],
                        "name": "Jianzhuang Liu",
                        "slug": "Jianzhuang-Liu",
                        "structuredName": {
                            "firstName": "Jianzhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "use unsupervised learning to detect text [27]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15021030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1697bfbb2c701fba6032d63309a904d21b4f0d09",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier. Using a novel caption-transition detection scheme we locate both spatial and temporal positions of video captions with high precision and efficiency. Then employing several new character segmentation and binarization techniques, we improve the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions. As the first attempt on Chinese video-caption recognition, our experiment results are very encouraging."
            },
            "slug": "A-spatial-temporal-approach-for-video-caption-and-Tang-Gao",
            "title": {
                "fragments": [],
                "text": "A spatial-temporal approach for video caption detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier that improves the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145573734"
                        ],
                        "name": "F. Lebourgeois",
                        "slug": "F.-Lebourgeois",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lebourgeois"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "LeBourgeois [10] moves the binarization step after the clustering by calculating a measure of accumulated gradients instead of edges."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "LeBourgeois [11] moves the binarization step after the clustering by calculating a measure of accumulated gradients instead of edges."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "We slightly modified the algorithm of LeBourgeois [11], which detects the text with a measure of accumulated gradients: A(x, y) = \uf8ee\uf8f0 bS/2c \u2211"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 13
                            }
                        ],
                        "text": "In his work, LeBourgeois also proposes an OCR algorithm which uses statistics on the projections of the gray values to recognize the characters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 38
                            }
                        ],
                        "text": "We slightly modified the algorithm of LeBourgeois [10], which detects the text with a measure of accumulated gradients:\nA(x, y) =  bS/2c\u2211 i=\u2212bS/2c ( \u2202I \u2202x (x + i, y) )2 12 (1) The parameters of this filter are the implementation of the partial derivative and the size S of the accumulation window."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29939979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8754ab68589b8e893d6962eb92c56300ecb764",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera. The system works with minimum assumptions on font, text location, size, color and the background scene. The text blocks localization in complex scenes using a specific filter which enhances any text from the background without binarization. A special stage is designed to separate characters, even touched by using gray-level information. The authors also extract gray-level features which make the algorithm more reliable, in particular under poor printing conditions or bad contrast digitization."
            },
            "slug": "Robust-multifont-OCR-system-from-gray-level-images-Lebourgeois",
            "title": {
                "fragments": [],
                "text": "Robust multifont OCR system from gray level images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera, with minimum assumptions on font, text location, size, color and the background scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38612825"
                        ],
                        "name": "R. Kohler",
                        "slug": "R.-Kohler",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Kohler",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kohler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, it will be necessary to study the relation to Kholer\u2019s work [ 34 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123049968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e530dff4053fe79263ab92f022c2491590869aa",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-segmentation-system-based-on-thresholding-Kohler",
            "title": {
                "fragments": [],
                "text": "A segmentation system based on thresholding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780080"
                        ],
                        "name": "M. Naphade",
                        "slug": "M.-Naphade",
                        "structuredName": {
                            "firstName": "Milind",
                            "lastName": "Naphade",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Naphade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15460268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a83f9b10615851f9de699e818e0506efd72e6461",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes a probabilistic framework for semantic video indexing. The components of the framework are multijects and multinets. Multijects are probabilistic multimedia objects representing semantic features or concepts. A multinet is a probabilistic network of multijects which accounts for the interaction between concepts. The main contribution of the paper is the application of a graphical probabilistic framework to build the multinet. The multinet enhances the detection performance of individual multijects, provides a unified framework for integrating multiple modalities and supports inference of unobservable concepts based on their relation with observable concepts. We develop multijects for detecting sites (locations) in video and integrate the multijects using multinet in the form of a Bayesian network. Detection performance is significantly improved using the multinet."
            },
            "slug": "Semantic-video-indexing-using-a-probabilistic-Naphade-Huang",
            "title": {
                "fragments": [],
                "text": "Semantic video indexing using a probabilistic framework"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The main contribution of the paper is the application of a graphical probabilistic framework to build the multinet, which enhances the detection performance of individual multijects, provides a unified framework for integrating multiple modalities and supports inference of unobservable concepts based on their relation with observable concepts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809629"
                        ],
                        "name": "N. Otsu",
                        "slug": "N.-Otsu",
                        "structuredName": {
                            "firstName": "Nobuyuki",
                            "lastName": "Otsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Otsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15326934,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1d4816c612e38dac86f2149af667a5581686cdef",
            "isKey": false,
            "numCitedBy": 32883,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonparametric and unsupervised method ofautomatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zerothand the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method."
            },
            "slug": "A-threshold-selection-method-from-gray-level-Otsu",
            "title": {
                "fragments": [],
                "text": "A threshold selection method from gray level histograms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141915"
                        ],
                        "name": "W. Niblack",
                        "slug": "W.-Niblack",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Niblack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Niblack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "If we refer to Niblack [17], p. 45, then the contrast of the center pixel of a window of gray levels is defined as\nCL = |m\u2212 I|\ns (25)\nSince we suppose dark text on bright background, we do not consider points having a gray value which is higher than the local mean, therefore the absolute value can be eliminated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "- Niblack\u2019s method [17] with parameter k = \u22120."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Niblack\u2019s algorithm calculates a threshold surface by shifting a rectangular window across the image."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "The same problem can be observed for Niblack\u2019s method, which segments the characters very well, but also suffers from noise in the zones without text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "How can we define contrast? If we refer to Niblack [17], p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "The approaches considered as the best are Niblack\u2019s method [17] and Yanowitz-Bruckstein\u2019s method [30], where Niblack\u2019s is one of the simplest and fastest algorithms and Yanowitz-Bruckstein\u2019s one of the most complex and slowest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Niblack obtains a rate of 80% of corrected characters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "We therefore chose Niblack\u2019s method for our system, and finally derived a similar method based on a criterion maximizing local contrast."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "The results obtained by our method are better than Niblack\u2019s and Sauvola\u2019s."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 75
                            }
                        ],
                        "text": "The computational complexity of this new technique is slightly higher than Niblack\u2019s version, since one of the thresholding decision parameters is the maximum standard deviation of all windows of the image."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 2
                            }
                        ],
                        "text": "- Niblack\u2019s method [17] with parameter k = \u22120.2 (see section 2.6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 258
                            }
                        ],
                        "text": "We propose to normalize the different elements used in the equation which calculates the threshold T , i.e. formulate the binarization decision in terms of contrast instead of in terms of gray values, which is a natural way considering the motivation behind Niblack\u2019s technique4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 2
                            }
                        ],
                        "text": "- Niblack\u2019s method with parameter k = \u22120.2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "A simple thresholding criterion is to keep 4Niblack derived his method from algorithms to transform gray level histograms in order to increase the contrast in images, e.g. to display them.\nonly pixels which have a high local contrast compared to it\u2019s maximum value corrected by the contrast of the window centered on this pixel:\nI : CL > a(Cmax \u2212 CW ) (28)\nwhere a is a gain parameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 128
                            }
                        ],
                        "text": "Our solution solves this problem by combining Sauvola\u2019s robustness vis-a-vis background textures and the segmentation quality of Niblack\u2019s method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 73
                            }
                        ],
                        "text": "Sauvola\u2019s algorithm, which has been developed to overcome the problems of Niblack, obtains worse results."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60929037,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "43678765df1d0b4594f7a49298cf27d75e174787",
            "isKey": true,
            "numCitedBy": 1417,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-introduction-to-digital-image-processing-Niblack",
            "title": {
                "fragments": [],
                "text": "An introduction to digital image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796298"
                        ],
                        "name": "B. Kapralos",
                        "slug": "B.-Kapralos",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Kapralos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kapralos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "- Niblack\u2019s method [ 19 ] with parameter k = 0.2 (see section 2.6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "we refer to Niblack [ 19 ], p. 45, then the contrast of the center pixel of a window of gray levels is defined as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The approaches considered as the best are Niblack\u2019s method [ 19 ] and Yanowitz-Bruckstein\u2019s method [32], where Niblack\u2019s is one of the simplest and fastest algorithms and Yanowitz-Bruckstein\u2019s one of the most complex and slowest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14314155,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "7ccfc40123609c95fb4153c89fbca483336be02e",
            "isKey": true,
            "numCitedBy": 774,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "ELIC 629, Fall 2005 Bill Kapralos ELIC 629, Fall 2005, Bill Kapralos Fall 2005 Image Enhancement in the Spatial Domain: Histograms, Arithmetic/Logic Operators, Basics of Spatial Filtering, Smoothing Spatial Filters Bill Kapralos Monday, October 17 2005 Overview (1): Before We Begin Administrative details Review \u2192 some questions to consider Histogram Processing Introduction Examples Arithmetic/Logic Operator Enhancement Image subtraction Image averaging"
            },
            "slug": "I-An-Introduction-to-Digital-Image-Processing-Kapralos",
            "title": {
                "fragments": [],
                "text": "I An Introduction to Digital Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9129502,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87542f355982d1540941601a8e3cdf94440435ee",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A blind noise variance algorithm that recovers the variance of noise in two steps is proposed. The sample variances are computed for square cells tessellating the noise image. Several tessellations are applied with the size of the cells increasing fourfold for consecutive tessellations. The four smallest sample variance values are retained for each tessellation and combined through an outlier analysis into one estimate. The different tessellations thus yield a variance estimate sequence. The value of the noise variance is determined from this variance estimate sequence. The blind noise variance algorithm is applied to 500 noisy 256*256 images. In 98% of the cases, the relative estimation error was less than 0.2 with an average error of 0.06. Application of the algorithm to differently sized images is also discussed. >"
            },
            "slug": "A-Fast-Parallel-Algorithm-for-Blind-Estimation-of-Meer-Jolion",
            "title": {
                "fragments": [],
                "text": "A Fast Parallel Algorithm for Blind Estimation of Noise Variance"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A blind noise variance algorithm that recovers the variance of noise in two steps is proposed and application of the algorithm to differently sized images is also discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145793042"
                        ],
                        "name": "R. Wagner",
                        "slug": "R.-Wagner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wagner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298802"
                        ],
                        "name": "M. Fischer",
                        "slug": "M.-Fischer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fischer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13381535,
            "fieldsOfStudy": [
                "Mathematics",
                "Education",
                "Physics"
            ],
            "id": "455e1168304e0eb2909093d5ab9b5ec85cda5028",
            "isKey": false,
            "numCitedBy": 3190,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."
            },
            "slug": "The-String-to-String-Correction-Problem-Wagner-Fischer",
            "title": {
                "fragments": [],
                "text": "The String-to-String Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm is presented which solves the string-to-string correction problem in time proportional to the product of the lengths of the two strings."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48804780"
                        ],
                        "name": "James A. Herson",
                        "slug": "James-A.-Herson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Herson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Herson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61108969,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7cd8434022c068c5b72c71e5de0df49d3bddfd55",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous system for separating a liquid from a liquid-solid admixture comprises a feed conveyor means for transporting the admixture, an electrically-driven centrifugal separator and control means therefor, a discharge conveyor means for receiving and transporting away the separated solids, and a feed conveyor control means responsive to the rotational speed of the separator so that the feed conveyor is operable only when the separator is operating at a predetermined speed."
            },
            "slug": "RECOGNITION-OF-TEXT-IN-3-D-SCENES-Myers-Bolles",
            "title": {
                "fragments": [],
                "text": "RECOGNITION OF TEXT IN 3-D SCENES"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Continuous system for separating a liquid from a liquid-solid admixture comprises a feed conveyor means for transporting the admixture, an electrically-driven centrifugal separator and control means therefor, and a discharge conveyor for receiving and transporting away the separated solids."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043680"
                        ],
                        "name": "R. Landais",
                        "slug": "R.-Landais",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Landais",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Landais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073418410"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52193159"
                        ],
                        "name": "L. Vinet",
                        "slug": "L.-Vinet",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Vinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vinet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104004920"
                        ],
                        "name": "J.-M. Jolion",
                        "slug": "J.-M.-Jolion",
                        "structuredName": {
                            "firstName": "J.-M.",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.-M. Jolion"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 171435159,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "c7d5fa4bfc334ef89d9326043f8c7e866c761c72",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "De tous les vecteurs d\u2019informations qu\u2019il est possible d\u2019extraire automatiquement d\u2019un document audiovisuel en vue de l\u2019exploiter (le decrire, l\u2019archiver ou le classer dans un corpus thematique), le texte figure parmi les plus interessants d\u2019un point de vue descriptif. Nous proposons dans cet article une nouvelle approche de la problematique de la detection du texte artificiel et ceci sur un corpus constitue de six journaux televises de 20H. A la difference de la majorite des methodes deja proposees, nous essayons ici de construire des modeles statistiques de la \"physique\" du texte qui reposent sur des connaissances a priori des documents etudies : notre but est moins de parvenir a extraire tous les textes que de reduire le nombre de fausses alarmes du a une modelisation trop l\u00e2che d\u2019autant plus que \nce nombre, lorsqu\u2019il est trop eleve, empeche toute utilisation de l\u2019algorithme de detection a des fins documentaires. Nous presentons ici notre methode : construction d\u2019un outil de verite terrain efficace dont le modele de donnees est base sur le schema de description MPEG7 VideoText ; mise au point, a partir des donnees issues des verites terrains, d\u2019un modele spatio-temporel du texte de nature statistique ; parametrage d\u2019un algorithme de detection en fonction de ce modele et evaluation de l\u2019incidence de cet ajustement \n sur les resultats de l\u2019algorithme."
            },
            "slug": "Utilisation-de-connaissances-a-priori-pour-le-d'un-Landais-Wolf",
            "title": {
                "fragments": [],
                "text": "Utilisation de connaissances a priori pour le param\u00e9trage d'un algorithme de d\u00e9tection de textes dans les documents audiovisuels : application \u00e0 un corpus de journaux t\u00e9l\u00e9vis\u00e9s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34993677,
            "fieldsOfStudy": [],
            "id": "73bf7b2fdb26498c05896d99fee4b8f3608c3bd6",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143677016"
                        ],
                        "name": "K. Mohiuddin",
                        "slug": "K.-Mohiuddin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohiuddin",
                            "middleNames": [
                                "Moidin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mohiuddin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195867354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15dedaa657ff7cbb283aaf96d9be2f4c5dcea694",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "International-Conference-On-Document-Analysis-and-Mohiuddin",
            "title": {
                "fragments": [],
                "text": "International Conference On Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98801181"
                        ],
                        "name": "L. Gu",
                        "slug": "L.-Gu",
                        "structuredName": {
                            "firstName": "Lifang",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18235710,
            "fieldsOfStudy": [],
            "id": "63500f827e0afaac33725cc7c853db0006f24080",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-and-Extraction-in-MPEG-Video-Sequences-Gu",
            "title": {
                "fragments": [],
                "text": "Detection and Extraction in MPEG Video Sequences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TF1) #2: news casts (M6, Canal+) #3: cartoons and news casts (Arte) #4: news casts (France 3) Figure 10: Examples of our test database"
            },
            "venue": {
                "fragments": [],
                "text": "#1: commercials (France"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document Analysis and Recognition, pp 865\u2013869"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Christian Wolf received his Master's degree (the Austrian Dipl"
            },
            "venue": {
                "fragments": [],
                "text": "Christian Wolf received his Master's degree (the Austrian Dipl"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new method for image segmentation. Computer Vision, Graphics and Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location in color"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "He has studied robust statistics applied to clustering, multiresolution and pyramid techniques, graphs based representations . . . mainly applied to the image retrieval domain"
            },
            "venue": {
                "fragments": [],
                "text": "Professor Jolion is a member of IAPR and IEEE (M'90). (more details at"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "L'apport du texte pr\u00e9sent dans les vid\u00e9os aux pratiques documentairesdocumentaires\u00e0 l'institut national de l'audiovisuel (in french)"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ORASIS 2003: Journ\u00e9es Francophones des Jeunes Chercheurs en Vision par Ordinateur"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Survey of Threshold"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man and Cybernetics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "In a more recent paper, the same authors change the features to a 20\u00d710 map of color edges, which is fed directly to the neural network[15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Localizing and Segmenting Text in Images, Videos and Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Table 4: OCR Results for different binarization methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From 1994, he has been with INSA and the Lyon Research Center for Images and Intelligent Information Systems where he is currently Professor of computer science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Science from the National Institute for Applied Science (INSA) of"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Science from the National Institute for Applied Science (INSA) of"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Caption Localization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and Recognition, pp"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quicklook: An Integrated"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Localizing and Segmenting Text in Images , Videos and Web pages"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "A similar approach has been developed by Myers et al.[15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "A similar approach has been developed by Myers et al.[18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition of Text in 3D Scenes"
            },
            "venue": {
                "fragments": [],
                "text": "Fourth Symposium on Document Image Understanding Technology"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "See [25] and [12] for surveying papers on this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principles of Visual Information Retrieval , chapter 9"
            },
            "venue": {
                "fragments": [],
                "text": "Principles of Visual Information Retrieval , chapter 9"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 314
                            }
                        ],
                        "text": "5 The parameters of the system A joint project between our team and INA(5) is currently under way, whose goal is to investigate the role of text in video sequences from a documentalist\u2019s point of view and to develop the technology to steer detection techniques with a priori knowledge on the contents of the video [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "L\u2019apport du texte pr\u00e9sent dans les vid\u00e9os aux pratiques documentaires \u00e0 18  l\u2019institut national de l\u2019audiovisuel (in french)"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of ORASIS 2003: Journe\u0301es Francophones des Jeunes Chercheurs en Vision par Ordinateur (Submitted)"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Survey of Threshold Techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision , Graphics and Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A SpatialTemporal Approach for Video Caption Detection and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Textfinder: An automatic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Survey of Thresholding Techniques. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Graphics and Image Processing"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principles of Visual information Retrieval, Chapter 9 Quicklook: An Integrated Multimedia System"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Visual Languages and Computing"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "His research interests include image and video indexing and contents extraction from multimedia documents"
            },
            "venue": {
                "fragments": [],
                "text": "Christian WOLF received his Master's degree (the Austrian Dipl. Ing.) in computer science in 2000 at Vienna University of Technology, and is currently working on his PhD in the Lyon Research Center for Images and Intelligent Information Systems"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Extraction-and-recognition-of-artificial-text-in-Wolf-Jolion/07c4bf10fc43029bf4abcd22dd665eeb3673e218?sort=total-citations"
}