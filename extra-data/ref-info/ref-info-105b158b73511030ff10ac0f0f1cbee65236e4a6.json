{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3260570"
                        ],
                        "name": "Virginia E. Ogle",
                        "slug": "Virginia-E.-Ogle",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Ogle",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Virginia E. Ogle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145345023"
                        ],
                        "name": "M. Stonebraker",
                        "slug": "M.-Stonebraker",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stonebraker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stonebraker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Ogle and Stonebraker\u2019s Cypress system [3] uses information contained in hand-keyed database fields to supplement image content information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11195120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c17ee327e563536f8adaf214eb6d3bde33b73dd6",
            "isKey": false,
            "numCitedBy": 818,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >"
            },
            "slug": "Chabot:-Retrieval-from-a-Relational-Database-of-Ogle-Stonebraker",
            "title": {
                "fragments": [],
                "text": "Chabot: Retrieval from a Relational Database of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work presents an approach that integrates a relational database retrieval system with a color analysis technique, and shows how a coarse granularity is used for content analysis improves the ability to retrieve images efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "Srihari\u2019s Piction system [9] uses the captions of newspaper photographs containing human faces to help locate the faces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10324615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "171d580de976b3d4393b48b66271d1512420cc7b",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The interaction of textual and photographic information in an integrated text/image database environment is being explored. Specifically, our research group has developed an automatic indexing system for captioned pictures of people; the indexing information and other textual information is subsequently used in a content-based image retrieval system. Our approach presents an alternative to traditional face identification systems; it goes beyond a superficial combination of existing text-based and image-based approaches to information retrieval. By understanding the caption accompanying a picture, we can extract information that is useful both for retrieving the picture and for identifying the faces shown. In designing a pictorial database system, two major issues are (1) the amount and type of processing required when inserting new pictures into the database and (2) efficient retrieval schemes for query processing. Our research has focused on developing a computational model for understanding pictures based on accompanying descriptive text. Understanding a picture can be informally defined as the process of identifying relevant people and objects. Several current vision systems employ the idea of top-down control in picture understanding. We carry the notion of top-down control one step further, exploiting not only general context but also picture-specific context. >"
            },
            "slug": "Automatic-Indexing-and-Content-Based-Retrieval-of-Srihari",
            "title": {
                "fragments": [],
                "text": "Automatic Indexing and Content-Based Retrieval of Captioned Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The research group has developed an automatic indexing system for captioned pictures of people; the indexing information and other textual information is subsequently used in a content-based image retrieval system."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883679"
                        ],
                        "name": "J. Ashley",
                        "slug": "J.-Ashley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ashley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39311329"
                        ],
                        "name": "J. Hafner",
                        "slug": "J.-Hafner",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hafner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499047"
                        ],
                        "name": "Denis Lee",
                        "slug": "Denis-Lee",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867341"
                        ],
                        "name": "D. Petkovic",
                        "slug": "D.-Petkovic",
                        "structuredName": {
                            "firstName": "Dragutin",
                            "lastName": "Petkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028064"
                        ],
                        "name": "David Steele",
                        "slug": "David-Steele",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70341848"
                        ],
                        "name": "P. Yanker",
                        "slug": "P.-Yanker",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 110716,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "dc139f901c869f80b54b41f89d5b7f35c7dfa3c7",
            "isKey": false,
            "numCitedBy": 4258,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >"
            },
            "slug": "Query-by-Image-and-Video-Content:-The-QBIC-System-Flickner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content: The QBIC System"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The QBIC system is described and its query capabilities are demonstrated, which allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In Picard and Minka\u2019s Foureyes system [5][6] close interaction with a human user supplements information derived from the image content."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some of these categories may include advertisements, geographic maps, landscapes [9], city/country scenes [5][10], night scenes, sunsets, scenes with foliage, and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14297666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdcd8d93fc7a659b147f95842ca5e124b676f710",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The average person with a computer will soon have access to the world's collections of digital video and images. However, unlike text that can be alphabetized or numbers that can be ordered, image and video has no general language to aid in its organization. Tools that can \"see\" and \"understand\" the content of imagery are still in their infancy, but they are now at the point where they can provide substantial assistance to users in navigating through visual media. This paper describes new tools based on \"vision texture\" for modeling image and video. The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries. This paper surveys recent and present research in this fast-growing area."
            },
            "slug": "A-Society-of-Models-for-Video-and-Image-Libraries-Picard",
            "title": {
                "fragments": [],
                "text": "A Society of Models for Video and Image Libraries"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IBM Syst. J."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Srihari\u2019s theory of \u201cvisual semantics\u201d [ 12 ] provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17250007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a2179242a4c57415e5691bc7f0b5d4699ee6665",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This research explores the interaction of linguistic and photographic information in an integrated text/image database. By utilizing linguistic descriptions of a picture (speech and text input) coordinated with pointing references to the picture, we extract information useful in two aspects: image interpretation and image retrieval. In the image interpretation phase, objects and regions mentioned in the text are identified; the annotated image is stored in a database for future use. We incorporate techniques from our previous research on photo understanding using accompanying text: a system, PICTION, which identifies human faces in a newspaper photograph based on the caption. In the image retrieval phase, images matching natural language queries are presented to a user in a ranked order. This phase combines the output of (1) the image interpretation/annotation phase, (2) statistical text retrieval methods, and (3) image retrieval methods (e.g., color indexing). The system allows both point and click querying on a given image as well as intelligent querying across the entire text/image database."
            },
            "slug": "Multimedia-input-in-automated-image-annotation-and-Srihari",
            "title": {
                "fragments": [],
                "text": "Multimedia input in automated image annotation and content-based retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This research explores the interaction of linguistic and photographic information in an integrated text/image database by utilizing linguistic descriptions of a picture coordinated with pointing references to the picture to extract information useful in two aspects: image interpretation and image retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14646846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dccf7738dcfd578f023f4831fb5eca7e1449c753",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital library access is driven by features but features are often context dependent and noisy and their relevance for a query is not always obvious This paper describes an approach for utilizing many data dependent user dependent and task dependent features in a semi automated tool Instead of requiring universal similarity measures or manual selection of relevant features the approach provides a learning algorithm for selecting and combining groupings of the data where groupings can be induced by highly spe cialized and context dependent features The se lection process is guided by a rich example based interaction with the user The inherent com binatorics of using multiple features is reduced by a multistage grouping generation weighting and collection process The stages closest to the user are trained fastest and slowly propagate their adaptations back to earlier stages The weighting stage adapts the collection stage s search space across uses so that in later interactions good groupings are found given few examples from the user Described is an interactive time imple mentation of this architecture for semi automatic within image segmentation and across image la beling driven by concurrently active color mod els texture models or manually provided group ings Issues for digital libraries Digital libraries of images video and sound are a rich area for pattern recognition research They also introduce a host of new problems and requirements since the range of possible queries is immense and requires the utilization of many spe cialized features Also systems for retrieval browsing and annotation i e classifying regions often must perform with only a small number of examples from a user i e an insuf cient amount of training data by traditional requirements Thus the area is doubly exciting since it presents the eld of pattern recognition with new challenges while beckoning in new applications One important issue for digital libraries is nding good models and similarity measures for comparing database en tries A part of this di culty is that feature extraction and comparison methods are highly data dependent see Figure This work was supported in part by BT PLC Hewlett Packard Labs and NEC for an example with texture Similarity measures are also user and task dependent as demonstrated by Figure Un fortunately these dependencies are not at this point under stood well enough especially by the typical digital library user to permit careful selection of the optimal measure be forehand Note that the multi resolution simultaneous auto regressive MRSAR model of which fares poorly com pared to the shift invariant eigenvector EV model in the above two examples scores clearly above the EV model on the standard Brodatz database On the same test data but for a perceptually motivated similarity criteria based on periodicity directionality and randomness both the EV and MRSAR models are beat by a new Wold based model Attempts to use intuitive texture features like coarseness contrast and directionality are appropri ate in some cases but do not fully determine all the qualities people might use in judging similarity Thus an a priori opti mal context dependent selection among similarity measures either by human or computer seems unlikely Next the scope of queries that databases need to address is immense Current computational solutions attempt to of fer location of perceptual content nd round red objects and objective content nd pictures of people in Boston Desirable queries also extend to subjective content give me a scene of a romantic forest task speci c content I need something with open space to place text collaborative con tent show me pictures children like and more An swering such queries requires a variety of features or meta data to be attached to the data in a digital library some of which may not be computable directly from the data The implication for algorithms is that they cannot rely on one model or one small set of carefully picked features but will have to drink from a veritable feature hydrant from which only a few drops may be relevant for the query Finally there is a signi cant need for semi automated ver sus fully automated tools Human computer synergy can make ill de ned tasks manageable and has the power to over come many of the problems of current pattern recognition tools An important application of semi automated tools is to assist the population of a database viz the creation of metadata A crucial technical issue for such tools is the selec tion and combination of existing features which features are most useful for a given query or annotation how should they be combined and which combinations are useful for the sys tem to remember so that it gets smarter with increased use This last point is important since not only are the queries immensely variable but the amount of training data i e ex amples provided by a user of what they do and don t want available at any instant is usually limited Hence a tool should strive to improve its generalization ability"
            },
            "slug": "Interactive-Learning-Using-a-\"Society-of-Models\"-Minka-Picard",
            "title": {
                "fragments": [],
                "text": "Interactive Learning Using a \"Society of Models\""
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper describes an approach for utilizing many data dependent user dependent and task dependent features in a semi automated tool instead of requiring universal similarity measures or manual selection of relevant features the approach provides a learning algorithm for selecting and combining groupings of the data."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 1996"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081718"
                        ],
                        "name": "M. Stricker",
                        "slug": "M.-Stricker",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Stricker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stricker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2908099"
                        ],
                        "name": "A. Dimai",
                        "slug": "A.-Dimai",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Dimai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dimai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Horizon: A number of systems search images for the presence of a horizon [9] (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some of these categories may include advertisements, geographic maps, landscapes [9], city/country scenes [5][10], night scenes, sunsets, scenes with foliage, and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15041591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56b0814322903a5ff5f81404a3b213de2fe9d7e1",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "To improve the discrimination power of color indexing techniques we encode a minimal amount of spatial information in the index. We propose an approach that lies between uniformly tesselating the images with rectangular regions and relying on fully segmented images. For each image we define 5 partially overlapping, fuzzy regions. From each region in the image we extract the first three moments of the color distribution and store them in the index. The feature vectors in the index are relatively insensitive to small translations and small rotations of an image because they are extracted from fuzzy regions. To retrieve images we define a function which measures the similarity of two color feature vectors. Invariance of retrieval results with respect to the typical image rotations of 90 degrees around the center of the image is guaranteed because our feature similarity function exploits the spatial arrangement of the 5 image regions. We present experimental results using an image database which contains more than 11,000 color images. Our experiments demonstrate clearly that our weak encoding of spatial information significantly increases the discrimination power of the index compared to plain color indexing techniques."
            },
            "slug": "Color-indexing-with-weak-spatial-constraints-Stricker-Dimai",
            "title": {
                "fragments": [],
                "text": "Color indexing with weak spatial constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work proposes an approach that lies between uniformly tesselating the images with rectangular regions and relying on fully segmented images, and encoding a minimal amount of spatial information in the index to improve the discrimination power of color indexing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086150"
                        ],
                        "name": "C. Tomasi",
                        "slug": "C.-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tomasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Other researchers who have made contributions in the area of image similarity techniques for content-based indexing into image databases include Carlo Tomassi [6] and Rosalind Picard [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5853971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5849ded74651eecd20878351077b2bc8667f85b",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "ARPA Image Understanding Workshop 1996 As an alternative to texture segmentation for the description of the images, we propose a method for coalescing descriptors of adjacent image patches with similar textural content i n to tight clusters. We a c hieve this result by extending the notion of edge-preserving smoothing and anisotropic diiusion from gray-level and color images to vector-valued images that describe the textural content o f e a c h image patch. To this end, we rst compute raw texture descrip-tors, that is, vectors that describe local texture appearance. We then deene texture c ontrast as the maximum derivative of the texture descrip-tor vector, and we deene signiicant regions as those in which contrast is low. We propose an eecient implementation of edge-preserving smoothing and show that it is closely related to anisotropic diiusion. Experiments on texture mosaics and real images show that clear texture edges are found even where intensity edges are weak or poorly deened, and that signiicant texture regions yield tight clusters of texture de-scriptors."
            },
            "slug": "Coalescing-Texture-Descriptors-Tomasi",
            "title": {
                "fragments": [],
                "text": "Coalescing Texture Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experiments on texture mosaics and real images show that clear texture edges are found even where intensity edges are weak or poorly deened, and that signiicant texture regions yield tight clusters of texture de-scriptors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1. Faces: Face-finding algorithms have recently become fairly successful at f inding faces in grayscale photographs [ 7 ][8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are testing face finders written by Sung and Poggio [ 7 ] and Rowley, e t. al. [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": false,
            "numCitedBy": 2061,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Faces: Face-finding algorithms have recently become fairly successful at finding faces in grayscale photographs [7][8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2019s code is considerably more efficient, as has been reported in [8], and so is more suitable for our application."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 676887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6af749b2b813af20c2f26962249fafdccdc6a1e",
            "isKey": true,
            "numCitedBy": 477,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Human-Face-Detection-in-Visual-Scenes-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Human Face Detection in Visual Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that uses a bootstrap algorithm for training, which adds false detections into the training set as training progresses, and has better performance in terms of detection and false-positive rates than other state-of-the-art face detection systems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "The efficiency of the face finder is improved by searching for faces only in images determined to be photographs, and by detecting possible face locations using color cues [11] prior to pattern"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9478880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3d8f0e11b50550d29c000a6dd7a9fbeb3130607",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Many applications in human computer interaction (HCI) require tracking a human face. In this report, we address two important issues for tracking human faces in real-time: what to track and how to track. We present a stochastic model to characterize skin-colors of human faces. The information provided by the model is sufficient for tracking a human face in a various poses and views. The model can be adapted in real time for different people and different lighting conditions while a person is moving. We then present a model-based approach to implement a real-time face tracker. The system has achieved a rate of up to 30+ frames/second using an HP-9000 workstation with a framegrabber and a Canon VC-Cl camera. It can track a person's face while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed to track human faces. In addition to the skin-color model used to register the face, a motion model is used to estimate image motion and to predict search window; and a camera model is used to predict and to compensate for camera motion (panning, tilting, and zooming). The system can be applied to tele-conferencing and many human-computer interactive applications such as lip-reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand for gesture recognition."
            },
            "slug": "Tracking-Human-Faces-in-Real-Time,-Yang-Waibel",
            "title": {
                "fragments": [],
                "text": "Tracking Human Faces in Real-Time,"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A stochastic model to characterize skin-colors of human faces is presented and is sufficient for tracking a human face in a various poses and views and can be extended to other tracking problems such as tracking the human hand for gesture recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5], which searches for upright faces oriented towards the camera."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d76ef8e61395a6e9c32627f1f108772d084e2e9",
            "isKey": false,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113475673"
                        ],
                        "name": "Derek White",
                        "slug": "Derek-White",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145889709"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "For existing methods of similarity retrieval, search time increases exponentially with the dimensionality of the feature space(1) and logarithmically with the number of images in the database [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59888996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db92b14599d6230120c3cc54cf22ec1c3957d4cb",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-and-strategies-for-similarity-retrieval-White-Jain",
            "title": {
                "fragments": [],
                "text": "Algorithms and strategies for similarity retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791418"
                        ],
                        "name": "N. House",
                        "slug": "N.-House",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "House",
                            "middleNames": [
                                "A.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. House"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48399741"
                        ],
                        "name": "M. Butler",
                        "slug": "M.-Butler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Butler",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Butler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3260570"
                        ],
                        "name": "Virginia E. Ogle",
                        "slug": "Virginia-E.-Ogle",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Ogle",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Virginia E. Ogle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31625350"
                        ],
                        "name": "Lisa R. Schiff",
                        "slug": "Lisa-R.-Schiff",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Schiff",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lisa R. Schiff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2"
                    },
                    "intents": []
                }
            ],
            "corpusId": 21074087,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "94b640abf45cbef4c76e6507c2846fec2220968c",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "User-Centered-Iterative-Design-for-Digital-The-House-Butler",
            "title": {
                "fragments": [],
                "text": "User-Centered Iterative Design for Digital Libraries: The Cypress Experience"
            },
            "venue": {
                "fragments": [],
                "text": "D Lib Mag."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Example - based learning for viewbased human face detection . Technical Report A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "The author of this file has informed us that these restrictions were included since he believes that current robots are only interested in indexing textual information, and these directories contain no text [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Personal Communication. AcIS R&D Columbia University"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Use of Multimedia Input in Automated Image Annotation and Content-Based Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Presented at Conference on Storage and Retrieval Techniques for Image Databases, SPIE '95"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Faces: Face-finding algorithms have recently become fairly successful at f inding faces in grayscale photographs [7][8] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "\u2019s code is considerably more efficient, as has been re ported in [8], and so is more suitable for our application."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human Face De  tection in Visual Scenes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Srihari\u2019s theory of visual semantics [8] provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linguistic context in vision"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Workshop on Context-Based Vision,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "In Picard and Minka\u2019s Foureyes system [5][6] close interaction with a human user supplements information derived from the image content."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "Some of these ca tegories may include advertisements, geographic maps, landscapes [9], city/country scenes [5][10] , night scenes, sunsets, scenes with foliage, and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Society of Models for Video and Image Librarie  s. Media Laboratory Perceptual Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Yali Amit [1] describes how to use multiple decision trees to classify objects into certain categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing shapes from simple queries about geometry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Use of Multimedia Input in Automated Image Annotation and Content - Based Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Presented at Conference on Storage and Retrieval Techniques for Image Databases"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Srihari\u2019s theory of \u201cvisual semantics\u201d [12] provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Use of Multimedia Input in Automated Image Annotation and Content-Based Retrieval.  Presented at Conference on Storage and Retrieval Techniques for Image Databases"
            },
            "venue": {
                "fragments": [],
                "text": "SPIE \u201995,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Some of these categories may include advertisements, geographic maps, cartoons, graphs, landscapes, city/country scenes [4], night scenes, sunset s, scenes with foliage, and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 183
                            }
                        ],
                        "text": "Other researchers who have made contributions in the area of image similarity techniques for content-based indexing into image databases include Carlo Tomassi [6] and Rosalind Picard [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision texture for annotation.Journal of Multimedia Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Srihari\u2019s theory of \u201cvisual semantics\u201d [12] provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Use of Multimedia Input in Automated Image  Annotation and Content-Based Retrieval. Presented at Conference on Storage and Retrieval Techniques for Image Databases, SPIE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/WebSeer:-An-Image-Search-Engine-for-the-World-Wide-Frankel-Swain/105b158b73511030ff10ac0f0f1cbee65236e4a6?sort=total-citations"
}