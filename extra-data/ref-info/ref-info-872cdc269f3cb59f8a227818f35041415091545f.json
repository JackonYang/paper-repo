{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075413715"
                        ],
                        "name": "Zheng Zeng",
                        "slug": "Zheng-Zeng",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135018"
                        ],
                        "name": "R. Goodman",
                        "slug": "R.-Goodman",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 518523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bff3ea999978e8c9503b62510bba11c2a5f24e51",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a novel neural architecture for learning deterministic context-free grammars, or equivalently, deterministic pushdown automata. The unique feature of the proposed network is that it forms stable state representations during learning-previous work has shown that conventional analog recurrent networks can be inherently unstable in that they cannot retain their state memory for long input strings. The authors have previously introduced the discrete recurrent network architecture for learning finite-state automata. Here they extend this model to include a discrete external stack with discrete symbols. A composite error function is described to handle the different situations encountered in learning. The pseudo-gradient learning method (introduced in previous work) is in turn extended for the minimization of these error functions. Empirical trials validating the effectiveness of the pseudo-gradient learning method are presented, for networks both with and without an external stack. Experimental results show that the new networks are successful in learning some simple pushdown automata, though overfitting and non-convergent learning can also occur. Once learned, the internal representation of the network is provably stable; i.e., it classifies unseen strings of arbitrary length with 100% accuracy."
            },
            "slug": "Discrete-recurrent-neural-networks-for-grammatical-Zeng-Goodman",
            "title": {
                "fragments": [],
                "text": "Discrete recurrent neural networks for grammatical inference"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A novel neural architecture for learning deterministic context-free grammars, or equivalently, deterministic pushdown automata is described, and a composite error function is described to handle the different situations encountered in learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075413715"
                        ],
                        "name": "Zheng Zeng",
                        "slug": "Zheng-Zeng",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135018"
                        ],
                        "name": "R. Goodman",
                        "slug": "R.-Goodman",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 159635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3b51f94a5934fdda8f1e728f1b56c9bfeb1cb6",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that recurrent neural networks have the ability to learn finite state automata from examples. In particular, networks using second-order units have been successful at this task. In studying the performance and learning behavior of such networks we have found that the second-order network model attempts to form clusters in activation space as its internal representation of states. However, these learned states become unstable as longer and longer test input strings are presented to the network. In essence, the network forgets where the individual states are in activation space. In this paper we propose a new method to force such a network to learn stable states by introducing discretization into the network and using a pseudo-gradient learning rule to perform training. The essence of the learning rule is that in doing gradient descent, it makes use of the gradient of a sigmoid function as a heuristic hint in place of that of the hard-limiting function, while still using the discretized value in the feedback update path. The new structure uses isolated points in activation space instead of vague clusters as its internal representation of states. It is shown to have similar capabilities in learning finite state automata as the original network, but without the instability problem. The proposed pseudo-gradient learning rule may also be used as a basis for training other types of networks that have hard-limiting threshold activation functions."
            },
            "slug": "Learning-Finite-State-Machines-With-Self-Clustering-Zeng-Goodman",
            "title": {
                "fragments": [],
                "text": "Learning Finite State Machines With Self-Clustering Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a new method to force a recurrent neural network to learn stable states by introducing discretization into the network and using a pseudo-gradient learning rule to perform training, which has similar capabilities in learning finite state automata as the original network, but without the instability problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537431"
                        ],
                        "name": "Axel Cleeremans",
                        "slug": "Axel-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Cleeremans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403615640"
                        ],
                        "name": "D. Servan-Schreiber",
                        "slug": "D.-Servan-Schreiber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Servan-Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Servan-Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7741931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "slug": "Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber",
            "title": {
                "fragments": [],
                "text": "Finite State Automata and Simple Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A network architecture introduced by Elman (1988) for predicting successive elements of a sequence and shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740471"
                        ],
                        "name": "C. Omlin",
                        "slug": "C.-Omlin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Omlin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Omlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145200131"
                        ],
                        "name": "C. L. Giles",
                        "slug": "C.-L.-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107321237"
                        ],
                        "name": "C. B. Miller",
                        "slug": "C.-B.-Miller",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. B. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58167528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3688430ca8e55b32073d12b432e8980f0b50d659",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "It is pointed out that discrete recurrent neural networks can learn to classify long strings of a regular language correctly when trained on a small finite set of positive and negative example strings. Rules defining the learned grammar can be extracted from networks by applying clustering heuristics in the output space of recurrent state neurons. Empirical evidence that there exists a correlation between the generalization performance of recurrent neural networks for regular language recognition and the rules that can be extracted from a neural network is presented. A heuristic that makes it possible to extract good rules from trained networks is given, and the method is tested on networks that are trained to recognize a simple regular language.<<ETX>>"
            },
            "slug": "Heuristics-for-the-extraction-of-rules-from-neural-Omlin-Giles",
            "title": {
                "fragments": [],
                "text": "Heuristics for the extraction of rules from discrete-time recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Empirical evidence that there exists a correlation between the generalization performance of recurrent neural networks for regular language recognition and the rules that can be extracted from a neural network is presented and a heuristic that makes it possible to extract good rules from trained networks is given."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 42
                            }
                        ],
                        "text": "This greatly expands on our previous work (Giles et al. 1990; Liu et al. 1990), which considered only regular grammars of unusual state symmetries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 65
                            }
                        ],
                        "text": "To learn grammars, we use a secondorder recurrent neural network (Lee et al. 1986; Giles et al. 1990; Sun et al. 1990; Pollack 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 25151650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c19c8c1d6778a16f8b27beac4d9c6a55357580",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP, and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars."
            },
            "slug": "Higher-Order-Recurrent-Networks-and-Grammatical-Giles-Sun",
            "title": {
                "fragments": [],
                "text": "Higher Order Recurrent Networks and Grammatical Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars and can be interpreted as a neural net pushdown automata."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740471"
                        ],
                        "name": "C. Omlin",
                        "slug": "C.-Omlin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Omlin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Omlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62215117,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7126c9df4f43579b3ccd8c7d989d3a5d689ab706",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Recurrent neural networks readily process, learn and generate temporal sequences. In addition, they have been shown to have impressive computational power. Recurrent neural networks can be trained with symbolic string examples encoded as temporal sequences to behave like sequential finite slate recognizers. We discuss methods for extracting, inserting and refining symbolic grammatical rules for recurrent networks. This paper discusses various issues: how rules are inserted into recurrent networks, how they affect training and generalization, and how those rules can be checked and corrected. The capability of exchanging information between a symbolic representation (grammatical rules)and a connectionist representation (trained weights) has interesting implications. After partially known rules are inserted, recurrent networks can be trained to preserve inserted rules that were correct and to correct through training inserted rules that were \u2018incorrec\u2019\u2014rules inconsistent with the training data."
            },
            "slug": "Extraction,-Insertion-and-Refinement-of-Symbolic-in-Giles-Omlin",
            "title": {
                "fragments": [],
                "text": "Extraction, Insertion and Refinement of Symbolic Rules in Dynamically Driven Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper discusses various issues: how rules are inserted into recurrent networks, how they affect training and generalization, and how those rules can be checked and corrected."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 117
                            }
                        ],
                        "text": "Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4608586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ae1dd016502b853908038a6dfd97d967e2dd185",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedure uses a new representation for FSA's, based on the notion of equivalence between testa. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. The size of our representation of the FSA, and the running time of our procedure (in some case provably, in others conjecturally) is polynomial in the diversity and ln(1/\u03b5), where \u03b5 is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also present some evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC Micro Vax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)"
            },
            "slug": "Diversity-based-inference-of-finite-automata-Rivest-Schapire",
            "title": {
                "fragments": [],
                "text": "Diversity-based inference of finite automata"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments, based on the notion of equivalence between testa."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32480997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a64ca771a733d58dcbf8f7a3fe65a09310424bf8",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length."
            },
            "slug": "Induction-of-Finite-State-Languages-Using-Recurrent-Watrous-Kuhn",
            "title": {
                "fragments": [],
                "text": "Induction of Finite-State Languages Using Second-Order Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples to obtain solutions that correctly recognize strings of arbitrary length."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144342820"
                        ],
                        "name": "E. M. Gold",
                        "slug": "E.-M.-Gold",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Gold",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M. Gold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 132
                            }
                        ],
                        "text": "We give a brief introduction to formal grammars and grammatical inference; for a thorough introduction, we recommend, respectively, Harrison (1978) and Fu (1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 118
                            }
                        ],
                        "text": "strings of a language, is a hard problem, even for regular grammars; for a discussion of the levels of difficulty see Gold (1978) and Angluin and Smith (1983). Consequently, there have been many heuristic algorithms developed for grammatical inference, which either scale poorly with the number of states of the inferred automata or require additional information such as restrictions on the type of grammar or the use of queries (Angluin and Smith 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "strings of a language, is a hard problem, even for regular grammars; for a discussion of the levels of difficulty see Gold (1978) and Angluin and Smith (1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 90
                            }
                        ],
                        "text": "an excellent discussion of recurrent neural network models and references can be found in Hertz et al. (1991). To learn grammars, we use a secondorder recurrent neural network (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8943792,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9dfa951bec812bd7b8c905c587bca50b7883a10f",
            "isKey": true,
            "numCitedBy": 802,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Complexity-of-Automaton-Identification-from-Given-Gold",
            "title": {
                "fragments": [],
                "text": "Complexity of Automaton Identification from Given Data"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143696178"
                        ],
                        "name": "J. Bachrach",
                        "slug": "J.-Bachrach",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Bachrach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bachrach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Mozer and Bachrach (1990)  apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987).,These methods (Rivest and Shapire 1987;  Mozer and Bachrach 1990 ) require active exploration of the unknown environments, and produce very good finite state automata (FSA) models of those environments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7301965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20b335039270bb4bd11fb21dbd9b9b85936c3012",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a robot wandering around an unfamiliar environment, performing actions and observing the consequences. The robot's task is to construct an internal model of its environment, a model that will allow it to predict the effects of its actions and to determine what sequences of actions to take to reach particular goal states. Rivest and Schapire (1987a,b; Schapire 1988) have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of finite state environments. The heart of this algorithm is a clever representation of the environment called an update graph. We have developed a connectionist implementation of the update graph using a highly specialized network architecture. With backpropagation learning and a trivial exploration strategy choosing random actions the connectionist network can outperform the Rivest and Schapire algorithm on simple problems. Our approach has additional virtues, including the fact that the network can accommodate stochastic environments and that it suggests generalizations of the update graph representation that do not arise from a traditional, symbolic perspective."
            },
            "slug": "Discovering-the-Structure-of-a-Reactive-Environment-Mozer-Bachrach",
            "title": {
                "fragments": [],
                "text": "Discovering the Structure of a Reactive Environment by Exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A connectionist implementation of the update graph using a highly specialized network architecture that can outperform the Rivest and Schapire algorithm on simple problems and suggests generalizations of theupdate graph representation that do not arise from a traditional, symbolic perspective."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 357,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton. Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1232,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton. Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987). These methods (Rivest and Shapire 1987; Mozer and Bachrach 1990) require active exploration of the unknown environments, and produce very good finite state automata (FSA) models of those environments. We discuss a recurrent neural network solution to grammatical inference and show that second-order recurrent neural networks learn fairly well small regular grammars with an infinite number of strings. This greatly expands on our previous work (Giles et al. 1990; Liu et al. 1990), which considered only regular grammars of unusual state symmetries. Our approach is similar to that of Pollack (1990) and differs in the learning algorithm (the gradient computation is not truncated) and the emphasis on what is to be learned. In contrast to Pollack (1990), we emphasize that a recurrent network can be trained to exhibit fixed-point behavior and correctly classify long, previously unseen, strings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1399,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton. Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987). These methods (Rivest and Shapire 1987; Mozer and Bachrach 1990) require active exploration of the unknown environments, and produce very good finite state automata (FSA) models of those environments. We discuss a recurrent neural network solution to grammatical inference and show that second-order recurrent neural networks learn fairly well small regular grammars with an infinite number of strings. This greatly expands on our previous work (Giles et al. 1990; Liu et al. 1990), which considered only regular grammars of unusual state symmetries. Our approach is similar to that of Pollack (1990) and differs in the learning algorithm (the gradient computation is not truncated) and the emphasis on what is to be learned. In contrast to Pollack (1990), we emphasize that a recurrent network can be trained to exhibit fixed-point behavior and correctly classify long, previously unseen, strings. Watrous and Kuhn (1992) illustrate similar results using another complete gradient calculation method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 142
                            }
                        ],
                        "text": "How a complete-gradient calculation approach using second-order recurrent networks compares to other gradient-truncation, first-order methods (Cleeremans et al. 1989; Elman 1990) is another open question."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1077,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton. Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987). These methods (Rivest and Shapire 1987; Mozer and Bachrach 1990) require active exploration of the unknown environments, and produce very good finite state automata (FSA) models of those environments. We discuss a recurrent neural network solution to grammatical inference and show that second-order recurrent neural networks learn fairly well small regular grammars with an infinite number of strings. This greatly expands on our previous work (Giles et al. 1990; Liu et al. 1990), which considered only regular grammars of unusual state symmetries. Our approach is similar to that of Pollack (1990) and differs in the learning algorithm (the gradient computation is not truncated) and the emphasis on what is to be learned."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1658,
                                "start": 12
                            }
                        ],
                        "text": "(1989), and Elman (1990). The recurrent networks were trained by predicting the next symbol and using a truncation of the backward recurrence. Cleeremans et al. (1989) concluded that the hidden unit activations represented past histories and that clusters of these activations can represent the states of the generating automaton. Mozer and Bachrach (1990) apply a neural network approach with a second-order gating term to a query learning method (Rivest and Schapire 1987). These methods (Rivest and Shapire 1987; Mozer and Bachrach 1990) require active exploration of the unknown environments, and produce very good finite state automata (FSA) models of those environments. We discuss a recurrent neural network solution to grammatical inference and show that second-order recurrent neural networks learn fairly well small regular grammars with an infinite number of strings. This greatly expands on our previous work (Giles et al. 1990; Liu et al. 1990), which considered only regular grammars of unusual state symmetries. Our approach is similar to that of Pollack (1990) and differs in the learning algorithm (the gradient computation is not truncated) and the emphasis on what is to be learned. In contrast to Pollack (1990), we emphasize that a recurrent network can be trained to exhibit fixed-point behavior and correctly classify long, previously unseen, strings. Watrous and Kuhn (1992) illustrate similar results using another complete gradient calculation method. We also show that from different trained neural networks, a large equivalence class of FSA can be extracted. This is an important extension of the work of Cleeremans et al. (1989) where only the states of the FSA were extracted."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": true,
            "numCitedBy": 9863,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35116937"
                        ],
                        "name": "R. Moll",
                        "slug": "R.-Moll",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Moll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Moll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795076"
                        ],
                        "name": "M. Arbib",
                        "slug": "M.-Arbib",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Arbib",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Arbib"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712330"
                        ],
                        "name": "A. Kfoury",
                        "slug": "A.-Kfoury",
                        "structuredName": {
                            "firstName": "Assaf",
                            "lastName": "Kfoury",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kfoury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29304912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d278c42ab24e014da4f991944dc55c11484e0d3a",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This voume combines An Introduction to Formal Language Theory with issues in computational linguistics. The book begins with standard formal language material, including a discussion of regular, context-free, context sensitive, and arbitrary phrase stucture languages. This is followed by a discussion of the corresponding families of automata: finite-state, push-down, linear bounded and Turing machines. Important topics introduced along the way include closure properties, normal forms, nondeterminism, basic parsing algorithms, and the theory of computability and undecidability. Special emphasis is given to the role of algebraic techniques in formal language theory through a chapter devoted to the fixed point approach to the analysis of context-free languages. Advanced topics in parsing are also emphasized in an unusually clear and precise presentation. A unique feature of the book is the two chapter introduction to the formal theory of natural languages. Alternative schemes for representing natural language are discussed, in particular ATNs and GPSG. This book is part of the AKM Series in Theoretical Computer Science. \"A Basis for Theoretical Computer Science\", also in the series, should provide the necessary background for this volume intended to serve as a text for upper undergraduate and graduate level students."
            },
            "slug": "An-Introduction-to-Formal-Language-Theory-Moll-Arbib",
            "title": {
                "fragments": [],
                "text": "An Introduction to Formal Language Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This volume intended to serve as a text for upper undergraduate and graduate level students and special emphasis is given to the role of algebraic techniques in formal language theory through a chapter devoted to the fixed point approach to the analysis of context-free languages."
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3191120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e02786bad4cf8781950b5df615729a417b31d7",
            "isKey": false,
            "numCitedBy": 1261,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed. The network models considered are divided into two basic categories: static networks and dynamic networks. Unlike static networks, dynamic networks have memory. They fall into three groups: networks with feedforward dynamics, networks with output feedback, and networks with state feedback, which are emphasized in this work. Most of the networks discussed are trained using supervised learning.<<ETX>>"
            },
            "slug": "Progress-in-supervised-neural-networks-Hush-Horne",
            "title": {
                "fragments": [],
                "text": "Progress in supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072030878"
                        ],
                        "name": "K. Lindgren",
                        "slug": "K.-Lindgren",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Lindgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lindgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121806631"
                        ],
                        "name": "A. Nilsson",
                        "slug": "A.-Nilsson",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Nilsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nilsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2320024"
                        ],
                        "name": "M. Nordahl",
                        "slug": "M.-Nordahl",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Nordahl",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nordahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67020290"
                        ],
                        "name": "I. R\u00e5de",
                        "slug": "I.-R\u00e5de",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "R\u00e5de",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. R\u00e5de"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6683493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d08d2c3d51dc20ce6dfb0f513ff80e00b3ff0d28",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Regular language inference is studied using evolving recurrent neural networks that may change in size through mutations. The scaling of the learning time when information theoretic properties of the test problems are varied is also investigated.<<ETX>>"
            },
            "slug": "Regular-language-inference-using-evolving-neural-Lindgren-Nilsson",
            "title": {
                "fragments": [],
                "text": "Regular language inference using evolving neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Regular language inference is studied using evolving recurrent neural networks that may change in size through mutations and the scaling of the learning time when information theoretic properties of the test problems are varied is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "an excellent discussion of recurrent neural network models and references can be found in  Hertz et al. (1991) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": false,
            "numCitedBy": 6517,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706504"
                        ],
                        "name": "J. Hopcroft",
                        "slug": "J.-Hopcroft",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopcroft",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopcroft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31901407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a88a490d7ba9e383ecb16c4290083413a08258",
            "isKey": false,
            "numCitedBy": 13820,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Automata-Theory,-Languages-and-Hopcroft-Ullman",
            "title": {
                "fragments": [],
                "text": "Introduction to Automata Theory, Languages and Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Grammatical inference, the problem of inferring grammar(s) from sample strings of a language, is a hard problem, even for regular grammars; for a discussion of the levels of difficulty see Gold (1978) and  Angluin and Smith (1983) .,Consequently, there have been many heuristic algorithms developed for grammatical inference, which either scale poorly with the number of states of the inferred automata or require additional information such as restrictions on the type of grammar or the use of queries ( Angluin and Smith 1983 ).,Fu (1982) and  Angluin and Smith (1983)  and the recent, comprehensive summary by Miclet (1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3209224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a great deal of theoretical and experimental work in computer science on inductive inference systems, that is, systems that try to infer general rules from examples. However, a complete and applicable theory of such systems is still a distant goal. This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations. 154 references."
            },
            "slug": "Inductive-Inference:-Theory-and-Methods-Angluin-Smith",
            "title": {
                "fragments": [],
                "text": "Inductive Inference: Theory and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836317"
                        ],
                        "name": "O. Firschein",
                        "slug": "O.-Firschein",
                        "structuredName": {
                            "firstName": "Oscar",
                            "lastName": "Firschein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Firschein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 302
                            }
                        ],
                        "text": "Grammatical inference is concerned mainly with the procedures that can be used to infer the syntactic rules (or production rules) of an unknown grammar G based on a finite set of strings Z from L(G), the language generated by G, and possibly also on a finite set of strings from the complement of L(G) (Fu 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 601,
                                "start": 244
                            }
                        ],
                        "text": "The FSA extraction process includes the following steps: (1) clustering of FSA states, (2) constructing a transition diagram by connecting these states together with the alphabet labeled arcs, (3) putting these transitions together to make the full digraph-forming loops, and (4) reducing the digraph to a minimal representation. The hypothesis is that during training, the network begins to partition (or quantize) its state space into fairly well-separated, distinct regions or clusters, which represent corresponding states in some finite state automaton (see Fig. 1). See Cleeremans et al. (1989) for another clustering method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 152
                            }
                        ],
                        "text": "We give a brief introduction to formal grammars and grammatical inference; for a thorough introduction, we recommend, respectively, Harrison (1978) and Fu (1982). Briefly, a grammar G is a four tuple {N,T,P,S}, where N and T are sets of nonterminals and terminals (alphabet of the grammar), P a set of production rules, and S the start symbol."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 353,
                                "start": 75
                            }
                        ],
                        "text": "general, f and These partial where 9\u201dis the derivative of the discriminant function. In \u201d f - 1 can be replaced by any t and t - 1, respectively. derivative terms are calculated iteratively as the equation suggests, with one iteration per input symbol. This on-line learning rule is a secondorder form of the recurrent net of Williams and Zipser (1989). The initial"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6554647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86dbe878a56e5052a66c036996416a782b4da618",
            "isKey": true,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Syntactic-pattern-recognition-and-applications-Firschein",
            "title": {
                "fragments": [],
                "text": "Syntactic pattern recognition and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116687606"
                        ],
                        "name": "Y. C. Lee",
                        "slug": "Y.-C.-Lee",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30086297"
                        ],
                        "name": "G. Doolen",
                        "slug": "G.-Doolen",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Doolen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doolen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769857"
                        ],
                        "name": "H. H. Chen",
                        "slug": "H.-H.-Chen",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148800040"
                        ],
                        "name": "G. Sun",
                        "slug": "G.-Sun",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Sun",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152819463"
                        ],
                        "name": "T. Maxwell",
                        "slug": "T.-Maxwell",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Maxwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Maxwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152149"
                        ],
                        "name": "H. Y. Lee",
                        "slug": "H.-Y.-Lee",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Lee",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Y. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145200131"
                        ],
                        "name": "C. L. Giles",
                        "slug": "C.-L.-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53781918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40f3404ae1b397860c921eda79f94b0f5195f2fb",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-learning-using-higher-order-correlation-Lee-Doolen",
            "title": {
                "fragments": [],
                "text": "Machine learning using higher order correlation networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196008710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78053512af13466c569e5946acfc3953bbfc9d36",
            "isKey": false,
            "numCitedBy": 18023,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification-Hart-Duda",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ning fully recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comp"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for continually run - second - order recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammatical inference using second-order recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Neural Networks, IEEE"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computation: Finiteand InfiniteMachines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of finite-state automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fourth Annu. Cogn. Sci. Conf., p. 105."
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computation: Finiteand InfiniteMachines, Chap"
            },
            "venue": {
                "fragments": [],
                "text": "3.5. PrenticeHall, Englewood Cliffs, NJ."
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 65
                            }
                        ],
                        "text": "To learn grammars, we use a secondorder recurrent neural network (Lee et al. 1986; Giles et al. 1990; Sun et al. 1990; Pollack 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine learning using a higher order correlational network"
            },
            "venue": {
                "fragments": [],
                "text": "Physica D 22-DU-31, 276."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 65
                            }
                        ],
                        "text": "To learn grammars, we use a secondorder recurrent neural network (Lee et al. 1986; Giles et al. 1990; Sun et al. 1990; Pollack 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist pushdown automata that learn context-free grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammatical inference and neural network state machines"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of finite-state languages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-and-Extracting-Finite-State-Automata-with-Giles-Miller/872cdc269f3cb59f8a227818f35041415091545f?sort=total-citations"
}