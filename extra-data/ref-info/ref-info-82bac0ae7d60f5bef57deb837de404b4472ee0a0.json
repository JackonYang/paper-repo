{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2591506"
                        ],
                        "name": "Minoru Yokobayashi",
                        "slug": "Minoru-Yokobayashi",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Yokobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minoru Yokobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Compared to the methods [8] and [9], the performance of our system is less affected by the categorization of the test set, especially in the case of nonuniform lighting condition and serious distorsion, which is due to the good generalization ability of convolutional neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "Yokobayashi et al [8, 9] proposed two systems for character recognition in natural scene images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Figure 5 shows the results of our method compared to [8] and [9]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Here again our system outperforms the methods [8] and [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "53% which outperforms the methods [8] and [9]: it ranges from 67."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "As in their previous work [8], once the binary image is obtained, an improved GAT correlation method is applied for recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 140
                            }
                        ],
                        "text": "Given the wide range of variability contained in this database, and in order to compare our system to the recent works of Yokobayashi et al [8, 9], we consider the classification proposed in [8, 9] of 698 selected images from the above mentioned dataset, into seven groups according to the degree of image degradations and/or background complexity."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [8], the authors proposed a local binarization method applied to one of the Cyan/Magenta/Yellow color planes using the maximum breadth of histogram."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3198280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd568cc328d34c43af90e10f6b133c03994bee09",
            "isKey": true,
            "numCitedBy": 36,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new technique of segmentation and recognition of characters with a wide variety of image degradations and complex backgrounds in natural scenes. The key ideas are twofold. One is segmentation of character and background by local/adaptive binarization of one of Cyan/Magenta/Yellow (CMY) color planes with the maximum breadth of histogram. The other is affine-invariant grayscale character recognition using global affine transformation (GAT) correlation. In experiments, we use a total of 698 test images extracted from the public ICDAR 2003 robust OCR dataset containing images of single characters in natural scenes. In advance, we classify those images into seven groups according to the degree of image degradations and/or background complexity. On the other hand, we prepare a single-font set of 62 alphanumerics for templates. Experimental results show an average recognition rate of 70.3%, ranging from 95.5% for clear images to 24.3% for little-contrast images."
            },
            "slug": "Segmentation-and-recognition-of-characters-in-scene-Yokobayashi-Wakahara",
            "title": {
                "fragments": [],
                "text": "Segmentation and recognition of characters in scene images using selective binarization in color space and GAT correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new technique of segmentation and recognition of characters with a wide variety of image degradations and complex backgrounds in natural scenes using affine-invariant grayscale character recognition using global affine transformation (GAT) correlation is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9311673"
                        ],
                        "name": "S. Kopf",
                        "slug": "S.-Kopf",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Kopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276016"
                        ],
                        "name": "T. Haenselmann",
                        "slug": "T.-Haenselmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Haenselmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Haenselmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "CNNs are hierarchical multilayered neural networks that combine three architectural ideas to ensure some degree of shift, scale, and distortion invariance:\n\u2022 local receptive fields, to extract elementary features in the image\n\u2022 shared weights, to extract the same set of elementary features from the whole input image and to reduce the computational cost\n\u2022 spatial sub-sampling, to reduce the resolution of the extracted feature maps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "The training phase was performed using the classical back-propagation algorithm with momentum modified for being used in convolutional networks as described in [4] and it consists of the following steps:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The proposed neural architecture is based on convolutional neural network architecture (CNN) [2, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118964791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45a4c5a309abae017da667d0fd5690268456281",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Although OCR techniques work very reliably for high-resolution documents, the recognition of superimposed text in low-resolution images or videos with a complex background is still a challenge. Three major parts characterize our system for recognition of superimposed text in images and videos: localization of text regions, segmentation (binarization) of characters, and recognition. We use standard approaches to locate text regions and focus in this paper on the last two steps. Many approaches (e.g., projection profiles, k-mean clustering) do not work very well for separating characters with very small font sizes. We apply in a vertical direction a shortest-path algorithm to separate the characters in a text line. The recognition of characters is based on the curvature scale space (CSS) approach which smoothes the contour of a character with a Gaussian kernel and tracks its inflection points. A major drawback of the CSS method is its poor representation of convex segments: Convex objects cannot be represented at all due to missing inflection points. We have extended the CSS approach to generate feature points for concave and convex segments of a contour. This generic approach is not only applicable to text characters but to arbitrary objects as well. In the experimental results, we compare our approach against a pattern matching algorithm, two classification algorithms based on contour analysis, and a commercial OCR system. The overall recognition results are good enough even for the indexing of low resolution images and videos."
            },
            "slug": "Robust-Character-Recognition-in-Low-Resolution-and-Kopf-Haenselmann",
            "title": {
                "fragments": [],
                "text": "Robust Character Recognition in Low-Resolution Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper uses standard approaches to locate text regions, segmentation (binarization) of characters, and recognition of characters based on the curvature scale space approach to generate feature points for concave and convex segments of a contour."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31181234"
                        ],
                        "name": "T. B. Chen",
                        "slug": "T.-B.-Chen",
                        "structuredName": {
                            "firstName": "Teo",
                            "lastName": "Chen",
                            "middleNames": [
                                "Boon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. B. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153900878"
                        ],
                        "name": "D. Ghosh",
                        "slug": "D.-Ghosh",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66674724"
                        ],
                        "name": "S. Ranganath",
                        "slug": "S.-Ranganath",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ranganath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ranganath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "CNNs are hierarchical multilayered neural networks that combine three architectural ideas to ensure some degree of shift, scale, and distortion invariance:\n\u2022 local receptive fields, to extract elementary features in the image\n\u2022 shared weights, to extract the same set of elementary features from the whole input image and to reduce the computational cost\n\u2022 spatial sub-sampling, to reduce the resolution of the extracted feature maps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The proposed neural architecture is based on convolutional neural network architecture (CNN) [2, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32623257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf7edca25496335f180851111f446879d0b534d1",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and recognition of text from video is an important issue in automated content-based indexing of visual information in video archives. In this paper, we present a comprehensive system for extracting and recognizing artificial text from unconstrained, general-purpose videos. Exploiting the temporal feature of videos, an edge-detection-based text segmentation method is applied only on selective frames for extracting text from a video scene. Subsequently, a combination of techniques including multiple frame integration, gray-scale filtering, entropy-based thresholding and line adjacency graphs is used to enhance the detected text areas. Finally, character recognition is accomplished by using the character side profiles. Results obtained from experiments on uncompressed MPEG-1 video clips demonstrate the effectiveness of our proposed system."
            },
            "slug": "Video-text-extraction-and-recognition-Chen-Ghosh",
            "title": {
                "fragments": [],
                "text": "Video-text extraction and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A comprehensive system for extracting and recognizing artificial text from unconstrained, general-purpose videos is presented and character recognition is accomplished by using the character side profiles."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE Region 10 Conference TENCON 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2591506"
                        ],
                        "name": "Minoru Yokobayashi",
                        "slug": "Minoru-Yokobayashi",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Yokobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minoru Yokobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Compared to the methods [8] and [9], the performance of our system is less affected by the categorization of the test set, especially in the case of nonuniform lighting condition and serious distorsion, which is due to the good generalization ability of convolutional neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "Yokobayashi et al [8, 9] proposed two systems for character recognition in natural scene images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Figure 5 shows the results of our method compared to [8] and [9]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Here again our system outperforms the methods [8] and [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "53% which outperforms the methods [8] and [9]: it ranges from 67."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 140
                            }
                        ],
                        "text": "Given the wide range of variability contained in this database, and in order to compare our system to the recent works of Yokobayashi et al [8, 9], we consider the classification proposed in [8, 9] of 698 selected images from the above mentioned dataset, into seven groups according to the degree of image degradations and/or background complexity."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [9], the authors proposed a binarization method based on three steps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10678544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b8e87e9d7f1edebc8cb6992ed4eac17b4a8682c",
            "isKey": true,
            "numCitedBy": 31,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds. The key ideas are twofold. One is to automatically select one axis in the RGB color space that maximizes the between-class separability by a suitably chosen threshold for segmentation of character and background or binarization. The other is affine-invariant or distortion-tolerant grayscale character recognition using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images. In experiments, we use a total of 698 test images extracted from the public ICDAR 2003 robust OCR dataset containing a variety of single-character images in natural scenes. In advance, we classify those images into seven groups according to the degree of image degradations and/or background complexity. On the other hand, we only prepare a single-font set of 62 alphanumerics for templates. Experimental results show an average recognition rate of 81.4%, ranging from 94.5% for clear images to 39.3% for seriously distorted images"
            },
            "slug": "Binarization-and-Recognition-of-Degraded-Characters-Yokobayashi-Wakahara",
            "title": {
                "fragments": [],
                "text": "Binarization and Recognition of Degraded Characters Using a Maximum Separability Axis in Color Space and GAT Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35240,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1812064"
                        ],
                        "name": "M. Delakis",
                        "slug": "M.-Delakis",
                        "structuredName": {
                            "firstName": "Manolis",
                            "lastName": "Delakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Delakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2557058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68a859142ef42196e6a56305b8c6ac4cb2c9326e",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to /spl plusmn/20 degrees in image plane and turned up to /spl plusmn/60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns."
            },
            "slug": "Convolutional-face-finder:-a-neural-architecture-Garcia-Delakis",
            "title": {
                "fragments": [],
                "text": "Convolutional face finder: a neural architecture for fast and robust face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that an efficient face detection system does not require any costly local preprocessing before classification of image areas, and provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039509"
                        ],
                        "name": "Y. Kimura",
                        "slug": "Y.-Kimura",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Kimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309532"
                        ],
                        "name": "A. Tomono",
                        "slug": "A.-Tomono",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Tomono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tomono"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Once a binary image is obtained, an improved GAT correlation method [7] is applied for recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8493831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f3e0bd21ead365af8dd302ab7d5b1ab53e7d437",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a technique of gray-scale character recognition that offers both noise tolerance and affine-invariance. The key ideas are twofold. First is the use of normalized cross-correlation as a matching measure to realize noise tolerance. Second is the application of global affine transformation (GAT) to the input image so as to achieve affine-invariant correlation with the target image. In particular, optimal GAT is efficiently determined by the successive iteration method using topographic features of gray-scale images as matching constraints. We demonstrate the high matching ability of the proposed GAT correlation method using gray-scale images of numerals subjected to random Gaussian noise and a wide range of affine transformation. Moreover, extensive recognition experiments show that the achieved recognition rate of 94.3 percent against rotation within 30 degrees, scale change within 30 percent, and translation within 20 percent of the character width along with random Gaussian noise is sufficiently high compared to the 42.8 percent offered by simple correlation."
            },
            "slug": "Affine-Invariant-Recognition-of-Gray-Scale-Using-Wakahara-Kimura",
            "title": {
                "fragments": [],
                "text": "Affine-Invariant Recognition of Gray-Scale Characters Using Global Affine Transformation Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work demonstrates the high matching ability of the proposed GAT correlation method using gray-scale images of numerals subjected to random Gaussian noise and a wide range of affine transformation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809629"
                        ],
                        "name": "N. Otsu",
                        "slug": "N.-Otsu",
                        "structuredName": {
                            "firstName": "Nobuyuki",
                            "lastName": "Otsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Otsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Secondly, they calculate a maximum between-class separability by setting an optimal threshold according to the Otsu\u2019s binarization technique [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15326934,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1d4816c612e38dac86f2149af667a5581686cdef",
            "isKey": false,
            "numCitedBy": 32883,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonparametric and unsupervised method ofautomatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zerothand the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method."
            },
            "slug": "A-threshold-selection-method-from-gray-level-Otsu",
            "title": {
                "fragments": [],
                "text": "A threshold selection method from gray level histograms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 138
                            }
                        ],
                        "text": "Secondly, they calculate a maximum between-class separability by setting an optimal threshold according to the Otsu\u2019s binarization technique [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A threshold selection method from gray-level histogram . SMC-9"
            },
            "venue": {
                "fragments": [],
                "text": "A threshold selection method from gray-level histogram . SMC-9"
            },
            "year": 1979
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 7,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 9,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-Scene-Text-Recognition-using-a-Neural-Saidane/82bac0ae7d60f5bef57deb837de404b4472ee0a0?sort=total-citations"
}