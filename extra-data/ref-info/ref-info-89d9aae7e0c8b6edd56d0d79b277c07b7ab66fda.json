{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Tesseract was included in the 4th UNLV annual test [1] of OCR accuracy, as \u201cHP Labs OCR,\u201d but the code has changed a lot since then, including conversion to Unicode and retraining."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61651523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19b2308e9bbe77d2059706891a757bb90cb73049",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "It is argued that it is time for a major change of approach to optical character recognition (OCR) research. The traditional approach, focusing on the correct classification of isolated characters, has been exhausted. The demonstration of the superiority of a new classification method under operational conditions requires large experimental facilities and databases beyond the resources of most researchers. In any case, even perfect classification of individual characters is insufficient for the conversion of complex archival documents to a useful computer-readable form. Many practical OCR tasks require integrated treatment of entire documents and well-organized typographic and domain-specific knowledge. New OCR systems should take advantage of the typographic uniformity of paragraphs or other layout components. They should also exploit the unavoidable interaction with human operators to improve themselves without explicit 'training'. >"
            },
            "slug": "At-the-frontiers-of-OCR-Nagy",
            "title": {
                "fragments": [],
                "text": "At the frontiers of OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "It is argued that it is time for a major change of approach to optical character recognition (OCR) research, and new OCR systems should take advantage of the typographic uniformity of paragraphs or other layout components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110162837"
                        ],
                        "name": "Yihong Xu",
                        "slug": "Yihong-Xu",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Tesseract was included in the 4th UNLV annual test [1] of OCR accuracy, as \u201cHP Labs OCR,\u201d but the code has changed a lot since then, including conversion to Unicode and retraining."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206775316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96f46889b8e0a399a425855d27428e0826e6b146",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian method of isolating character bitmaps from paragraph-length samples of heavily degraded text images is demonstrated. The method requires a transcript of the text, but it is sufficiently robust to tolerate errors in transcripts obtained from multifont commercial OCR software. The resulting prototypes (labeled character images) are used to recognize additional text an the same document."
            },
            "slug": "Automatic-prototype-extraction-for-adaptive-OCR-Nagy-Xu",
            "title": {
                "fragments": [],
                "text": "Automatic prototype extraction for adaptive OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A Bayesian method of isolating character bitmaps from paragraph-length samples of heavily degraded text images is demonstrated and is sufficiently robust to tolerate errors in transcripts obtained from multifont commercial OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15307178"
                        ],
                        "name": "F. Jenkins",
                        "slug": "F.-Jenkins",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Jenkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jenkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "The engine was sent to UNLV for the 1995 Annual Test of OCR Accuracy[1], where it proved its worth against the commercial engines of the time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "After lying dormant for more than 10 years, Tesseract is now behind the leading commercial engines in terms of its accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Like a supernova, it appeared from nowhere for the 1995 UNLV Annual Test of OCR Accuracy [1], shone brightly with its results, and then vanished back under the same cloak of secrecy under which it had been developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17017303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fae039cc89b2cd453acb85d208e021907528b062",
            "isKey": true,
            "numCitedBy": 214,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "For four years, ISRI has conducted an annual test of optical character recognition (OCR) systems known as \u201cpage readers.\u201d These systems accept as input a bitmapped image of any document page, and attempt to identify the machine-printed characters on the page. In the annual test, we measure the accuracy of this process by comparing the text that is produced as output with the correct text. The goals of the test include:"
            },
            "slug": "The-Fourth-Annual-Test-of-OCR-Accuracy-Rice-Jenkins",
            "title": {
                "fragments": [],
                "text": "The Fourth Annual Test of OCR Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The annual test of optical character recognition systems known as \u201cpage readers\u201d accepts as input a bitmapped image of any document page, and attempts to identify the machine-printed characters on the page."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25217822"
                        ],
                        "name": "I. Marosi",
                        "slug": "I.-Marosi",
                        "structuredName": {
                            "firstName": "Istv\u00e1n",
                            "lastName": "Marosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Marosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "All four 300 DPI binary test sets that were used in the 1995 test are shown, along with the number of errors (Errs), the percent error rate (%Err) and the percent change relative to the 1995 results (%Chg) for both character errors and non-stopword errors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38585518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1765d58f8f551cd2ee5c06abf56108f5bbc14dae",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical Character Recognition is much more than character classification. An industrial OCR application combines algorithms studied in detail by different researchers in the area of image processing, pattern recognition, machine learning, language analysis, document understanding, data mining, and other, artificial intelligence domains. There is no single perfect algorithm for any of the OCR problems, so modern systems try to adapt themselves to the actual features of the image or document to be recognized. This paper describes the architecture of a modern OCR system with an emphasis on this adaptation process."
            },
            "slug": "Industrial-OCR-approaches:-architecture,-and-Marosi",
            "title": {
                "fragments": [],
                "text": "Industrial OCR approaches: architecture, algorithms, and adaptation techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The architecture of a modern OCR system is described with an emphasis on the adaptation process, where systems try to adapt themselves to the actual features of the image or document to be recognized."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70286642"
                        ],
                        "name": "Mindy Bokser",
                        "slug": "Mindy-Bokser",
                        "structuredName": {
                            "firstName": "Mindy",
                            "lastName": "Bokser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mindy Bokser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Each feature fetches, from a coarsely quantized 3-dimensional lookup table, a bit-vector of classes that it might match, and the bit-vectors are summed over all the features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "The final decision for a given segmentation is\nsimply the word with the lowest total distance rating, where each of the above categories is multiplied by a different constant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61670519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c7e86cee5a41818b5307eefc1e31b71233c21b2",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An optical character recognition (OCR) engine that is omnifont and reasonably robust on individual degraded characters is presented. The weakest link is its handling of characters which are difficult to segment. The engine is divided into four phases: segmentation, image recognition, ambiguity resolution, and document analysis. The features are zonal and reduce the image to a blurred, gray-level representation. The classifier is data-driven, trained offline, and model-free. Handcrafted features and decision trees tend to be brittle in the presence of noise. To satisfy the needs of full-text applications, the system captures the structure of the document so that, when viewed in a word processor or spreadsheet program, the formatting of the optically recognized document reflects that of the original document. To satisfy the needs of the forms market, a proofing and correction tool displays 'pop-up' images of uncertain characters. >"
            },
            "slug": "Omnidocument-technologies-Bokser",
            "title": {
                "fragments": [],
                "text": "Omnidocument technologies"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An optical character recognition (OCR) engine that is omnifont and reasonably robust on individual degraded characters is presented and captures the structure of the document so that the formatting of the optically recognized document reflects that of the original document."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "The line finding algorithm is designed so that a skewed page can be recognized without having to de-skew, thus saving loss of image quality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18198724,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "f19481afdf72f6f0cc19cf45812d09f5b7a9e809",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We offer a perspective on the performance of current OCR systems by illustrating and explaining actual OCR errors made by three commercial devices. After discussing briefly the character recognition abilities of humans and computers, we present illustrated examples of recognition errors. The top level of our taxonomy of the causes of errors consists of Imaging Defects, Similar Symbols, Punctuation, and Typography. The analysis of a series of 'snippets' from this perspective provides insight into the strengths and weaknesses of current systems, and perhaps a road map to future progress. The examples were drawn from the large-scale tests conducted by the authors at the Information Science Research Institute of the University of Nevada, Las Vegas. By way of conclusion, we point to possible approaches for improving the accuracy of today's systems. The talk is based on our eponymous monograph, recently published in The Kluwer International Series in Engineering and Computer Science, Kluwer Academic Publishers, 1999."
            },
            "slug": "Optical-character-recognition:-an-illustrated-guide-Nagy-Nartker",
            "title": {
                "fragments": [],
                "text": "Optical character recognition: an illustrated guide to the frontier"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A perspective on the performance of current OCR systems is offered by illustrating and explaining actual OCR errors made by three commercial devices, and possible approaches for improving the accuracy of today's systems are pointed to."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985360"
                        ],
                        "name": "Raymond W. Smith",
                        "slug": "Raymond-W.-Smith",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Smith",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond W. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Candidate chop points are found from concave vertices of a polygonal approximation [2] of the outline, and may have either another concave vertex opposite, or a line segment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "Each prototype character class is represented by a logical sum-of-product expression with each term called a configuration, so the distance calculation process keeps a record of the total similarity evidence of each feature in each configuration, as well as of each prototype."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Tesseract began as a PhD research project [2] in HP Labs, Bristol, and gained momentum as a possible software and/or hardware add-on for HP\u2019s line of flatbed scanners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56787271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7c6f0097af985a656a05a90693350c4799cdff",
            "isKey": true,
            "numCitedBy": 39,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Almost all the current commercial OCR machines employ matrix matching, resulting in high speed and accuracy, but a severely restrictive range of recognized fonts. Published algorithms conversely, concentrate on feature extraction for font independence, yet they have previously been too slow for commercial use. Current algorithms also fail to distinguish between text and non-text images. This thesis presents a new approach to the automatic extraction of text from multimedia printed documents. An edge detection algorithm, which is capable of extracting the outlines of text from a grey level image, is used to obtain a high level of discrimination between text and non-text. An additional benefit is that text of any colour can be read from almost any background, provided that the contrast is reasonable. The outlines are approximated by polygons using a fast two-stage algorithm. A feature extraction approach to font independent character recognition is described, which uses these outline polygons. It is shown that highly accurate and fast recognition can be achieved using a remarkably small number of carefully chosen features. The results show that after training on only seven quite similar fonts, the recognition algorithm provides greater than 95% accuracy on fonts different to the training set. A more complex edge extraction algorithm is also described. This is capable of extracting text and line graphics from an arbitrary page. Although not essential for character recognition, this algorithm is useful for the interpretation of engineering drawings. As a further contribution to this problem, a thinning algorithm is defined, which is non-iterative and uses the polygonal approximated outlines from the edge extractor."
            },
            "slug": "The-extraction-and-recognition-of-text-from-images-Smith",
            "title": {
                "fragments": [],
                "text": "The extraction and recognition of text from multimedia document images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that highly accurate and fast recognition can be achieved using a remarkably small number of carefully chosen features, and that after training on only seven quite similar fonts, the recognition algorithm provides greater than 95% accuracy on fonts different to the training set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153691390"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19998207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25ad5e46506b054e588cf63c6b327cfd7cae7b65",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An important part of any document recognition system is detection of skew in the image of a page. This paper presents a new, accurate and robust skew detection algorithm based on a method for finding rows of text in page images. Results of a comparison of the new algorithm against Baird's well-known algorithm on 400 pages show the new algorithm to be more accurate, robust and somewhat faster. In particular, the new algorithm only breaks down at skew angles in excess of 15 degrees, compared to the almost uniform distribution of breakdowns of Baird's algorithm."
            },
            "slug": "A-simple-and-efficient-skew-detection-algorithm-via-Smith",
            "title": {
                "fragments": [],
                "text": "A simple and efficient skew detection algorithm via text row accumulation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new, accurate and robust skew detection algorithm based on a method for finding rows of text in page images is presented, which is more accurate, robust and somewhat faster than Baird's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2297715"
                        ],
                        "name": "B. Blesser",
                        "slug": "B.-Blesser",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Blesser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Blesser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048116"
                        ],
                        "name": "T. Kuklinski",
                        "slug": "T.-Kuklinski",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Kuklinski",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kuklinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2935369"
                        ],
                        "name": "R. J. Shillman",
                        "slug": "R.-J.-Shillman",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Shillman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Shillman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "In the first step, a class pruner creates a shortlist of character classes that the unknown might match."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36327266,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d0798dc03701eb1a9df83c7372ae5b5d4162bfb4",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-tests-for-feature-selection-based-on-a-of-Blesser-Kuklinski",
            "title": {
                "fragments": [],
                "text": "Empirical tests for feature selection based on a psychological theory of character recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380483709"
                        ],
                        "name": "M. Medard",
                        "slug": "M.-Medard",
                        "structuredName": {
                            "firstName": "Muriel",
                            "lastName": "Medard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Medard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4153826,
            "fieldsOfStudy": [
                "Law"
            ],
            "id": "defd66114b63aae75e0af2bcff2e52ae8fd2c873",
            "isKey": false,
            "numCitedBy": 1752,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "WARNING NOTICE: The experiments described in these materials are potentially hazardous and require a high level ofsafety training, special facilities and equipment, and supervision by appropriate individuals. You bear the sole responsibility, liability, and risk for the implementation of such safety procedures and measures. MIT shall have no responsibility, liability, or risk for the content or implementation of any of the material presented. Legal Notices"
            },
            "slug": "Massachusetts-Institute-of-Technology-Medard",
            "title": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152878142"
                        ],
                        "name": "A. Leroy",
                        "slug": "A.-Leroy",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Leroy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leroy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 61563242,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "4fecc029c30800e5be60f2fbf22276c29e3c9f68",
            "isKey": false,
            "numCitedBy": 5806,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction. 2. Simple Regression. 3. Multiple Regression. 4. The Special Case of One-Dimensional Location. 5. Algorithms. 6. Outlier Diagnostics. 7. Related Statistical Techniques. References. Table of Data Sets. Index."
            },
            "slug": "Robust-Regression-and-Outlier-Detection-Rousseeuw-Leroy",
            "title": {
                "fragments": [],
                "text": "Robust regression and outlier detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents the results of a two-year study of the statistical treatment of outliers in the context of one-Dimensional Location and its applications to discrete-time reinforcement learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144951054"
                        ],
                        "name": "P. Schneider",
                        "slug": "P.-Schneider",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Schneider",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Schneider"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "The filtered blobs are more likely to fit a model of non-overlapping, parallel, but sloping lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61567691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0f33e9d2bd2631629a9a6508355383f41c8dd2",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-algorithm-for-automatically-fitting-digitized-Schneider",
            "title": {
                "fragments": [],
                "text": "An algorithm for automatically fitting digitized curves"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Once the filtered blobs have been assigned to lines, a least median of squares fit [4] is used to estimate the baselines, and the filtered-out blobs are fitted back into the appropriate lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust Regression and Outlier Detection, Wiley-IEEE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "In the first step, a class pruner creates a shortlist of character classes that the unknown might match."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character Recognition Based on Phenomenological Attributes: Theory and Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Character Recognition Based on Phenomenological Attributes: Theory and Methods"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[7-8] Though nicely independent of font and size, these features are not robust to the problems found in reallife images, as Bokser [9] describes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character Recognition Based on Phenomenological Attributes: Theory and Methods, PhD"
            },
            "venue": {
                "fragments": [],
                "text": "Thesis, Massachusetts Institute of Technology"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conf. on Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Conf. on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The final decision for a given segmentation is\nsimply the word with the lowest total distance rating, where each of the above categories is multiplied by a different constant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A 100-Font Classifier"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 1 st Int. Conf. on Document Analysis and Recognition"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[7-8] Though nicely independent of font and size, these features are not robust to the problems found in reallife images, as Bokser [9] describes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shillman, \u201cEmpirical Tests for Feature Selection Based on a Pscychological Theory of Character Recognition\u201d, Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Overview-of-the-Tesseract-OCR-Engine-Smith/89d9aae7e0c8b6edd56d0d79b277c07b7ab66fda?sort=total-citations"
}