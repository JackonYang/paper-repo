{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": ", sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "Many of these algorithms, such as [10, 9, 8, 6], compute a (approximately or exactly) sparse representation of the natural stimuli data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "(Filters learned by sparse coding [10, 11] also similarly give responses similar to V1 simple cells."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "Some hierarchical extensions of these models [15, 6, 16] are able to learn features that are more complex than simple oriented bars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1338750,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d086a103e7bff7a14e27685fb0dc88c3724c9e33",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-multi-layer-sparse-coding-network-learns-contour-Hoyer-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "A multi-layer sparse coding network learns contour coding from natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": ", sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "Many of these algorithms, such as [10, 9, 8, 6], compute a (approximately or exactly) sparse representation of the natural stimuli data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "Based on results from other methods (e.g., sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "A study by van Hateren and van der Schaaf [8] showed that the filters learned by independent components analysis (ICA) [9] on natural image data match very well with the classical receptive fields of V1 simple cells."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": true,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] proposed an algorithm based on learning individual layers of a hierarchical probabilistic graphical model from the bottom up."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 274
                            }
                        ],
                        "text": "In some cases, this iterative \u201cgreedy\u201d algorithm can further be shown to be optimizing a variational bound on the data likelihood, if each layer has at least as many units as the layer below (although in practice this is not necessary to arrive at a desirable solution; see [1] for a detailed discussion)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] showed that by repeatedly applying such a procedure, one can learn a multilayered deep belief network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] proposed an algorithm for learning deep belief networks, by treating each layer as a restricted Boltzmann machine (RBM) and greedily training the network one layer at a time from the bottom up [24, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": true,
            "numCitedBy": 13407,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145039030"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153379696"
                        ],
                        "name": "T. Hofmann",
                        "slug": "T.-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hofmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] proposed a similarly greedy algorithm, one based on autoencoders."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 196065172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c8a545f7166659e9e21c88fe234e0323855216",
            "isKey": false,
            "numCitedBy": 1216,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more ef cient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Sch\u00f6lkopf-Platt",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145992652"
                        ],
                        "name": "Michael U Gutmann",
                        "slug": "Michael-U-Gutmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gutmann",
                            "middleNames": [
                                "U"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael U Gutmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "For example, hierarchical sparse models of natural images have accounted for complex cell receptive fields [17], topography [18, 6], colinearity and contour coding [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18107769,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fbdfeeb93f51b21ea8568bd18481d11ea085dc3a",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "BackgroundIt has been shown that the classical receptive fields of simple and complex cells in the primary visual cortex emerge from the statistical properties of natural images by forcing the cell responses to be maximally sparse or independent. We investigate how to learn features beyond the primary visual cortex from the statistical properties of modelled complex-cell outputs. In previous work, we showed that a new model, non-negative sparse coding, led to the emergence of features which code for contours of a given spatial frequency band.ResultsWe applied ordinary independent component analysis to modelled outputs of complex cells that span different frequency bands. The analysis led to the emergence of features which pool spatially coherent across-frequency activity in the modelled primary visual cortex. Thus, the statistically optimal way of processing complex-cell outputs abandons separate frequency channels, while preserving and even enhancing orientation tuning and spatial localization. As a technical aside, we found that the non-negativity constraint is not necessary: ordinary independent component analysis produces essentially the same results as our previous work.ConclusionWe propose that the pooling that emerges allows the features to code for realistic low-level image features related to step edges. Further, the results prove the viability of statistical modelling of natural images as a framework that produces quantitative predictions of visual processing."
            },
            "slug": "Statistical-model-of-natural-stimuli-predicts-of-in-Hyv\u00e4rinen-Gutmann",
            "title": {
                "fragments": [],
                "text": "Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in V2"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The pooling that emerges allows the features to code for realistic low-level image features related to step edges in the modelled primary visual cortex to prove the viability of statistical modelling of natural images as a framework that produces quantitative predictions of visual processing."
            },
            "venue": {
                "fragments": [],
                "text": "BMC Neuroscience"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "Many of these algorithms, such as [10, 9, 8, 6], compute a (approximately or exactly) sparse representation of the natural stimuli data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 2
                            }
                        ],
                        "text": ", [5, 6, 2]) Indeed, some of these models are often viewed as first attempts to elucidate what learning algorithm (if any) the cortex may be using to model natural image statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "Some hierarchical extensions of these models [15, 6, 16] are able to learn features that are more complex than simple oriented bars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": ", sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "For example, hierarchical sparse models of natural images have accounted for complex cell receptive fields [17], topography [18, 6], colinearity and contour coding [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6699891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8d01934cb26064b253dbd0f1627519133c3df3e",
            "isKey": true,
            "numCitedBy": 120,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to natural data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous workin particular, gaussian scale mixture models and variants of independent components analysis."
            },
            "slug": "Topographic-Product-Models-Applied-to-Natural-Scene-Osindero-Welling",
            "title": {
                "fragments": [],
                "text": "Topographic Product Models Applied to Natural Scene Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An energy-based model is presented that uses a product of generalized Student-t distributions to capture the statistical structure in data sets to study the topographic organization of Gabor-like receptive fields that the model learns."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060013"
                        ],
                        "name": "Christopher S. Poultney",
                        "slug": "Christopher-S.-Poultney",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Poultney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher S. Poultney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 2
                            }
                        ],
                        "text": ", [5, 6, 2]) Indeed, some of these models are often viewed as first attempts to elucidate what learning algorithm (if any) the cortex may be using to model natural image statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": ", sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] developed an energy-based hierarchical algorithm, based on a sequence of sparsified autoencoders/decoders."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 819006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
            "isKey": false,
            "numCitedBy": 1182,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps."
            },
            "slug": "Efficient-Learning-of-Sparse-Representations-with-Ranzato-Poultney",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Sparse Representations with an Energy-Based Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A novel unsupervised method for learning sparse, overcomplete features using a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": ", sparse coding [10, 11], ICA [9], heavy-tailed models [6], and energy based models [2]), sparseness seems to play a key role in learning gabor-like filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "(Filters learned by sparse coding [10, 11] also similarly give responses similar to V1 simple cells."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "slug": "Efficient-sparse-coding-algorithms-Lee-Battle",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These algorithms are applied to natural images and it is demonstrated that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31461368"
                        ],
                        "name": "J. B. Levitt",
                        "slug": "J.-B.-Levitt",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Levitt",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. B. Levitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5512011"
                        ],
                        "name": "D. Kiper",
                        "slug": "D.-Kiper",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kiper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kiper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "One V2 study by [22] reported that the receptive fields in this area were similar to those in the neighboring areas V1 and V4."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 207547936,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e1eca2c4ed0bd3cab1950564c2b80ad0ec455ca4",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Visual area V2 of macaque monkey cerebral cortex is the largest of the extrastriate visual areas, yet surprisingly little is known of its neuronal properties. We have made a quantitative analysis of V2 receptive field properties. Our set of measurements was chosen to distinguish neuronal responses reflecting parvocellular (P) or magnocellular (M) inputs and to permit comparison with similar measurements made in other visual areas; we further describe the relationship of those properties to the laminar and cytochrome oxidase (CO) architecture of V2. 2. We recorded the activity of single units representing the central 5 degrees in all laminae and CO divisions of V2 in anesthetized, paralyzed macaque monkeys. We studied responses to geometric targets and to drifting sinusoidal gratings that varied in orientation, spatial frequency, drift rate, contrast, and color. 3. The orientation selectivity and spatial and temporal tuning of V2 neurons differed little from those in V1. As in V1, spatial and temporal tuning in V2 appeared separable, and we identified a population of simple cells (more common within the central 3 degrees) similar to those found in V1. Contrast sensitivity of V2 neurons was greater on average than in V1, perhaps reflecting the summation of inputs in V2's larger receptive fields. Many V2 neurons exhibited some degree of chromatic opponency, responding to isoluminant color variations, but these neurons differed from V1 in the linearity with which they summate cone signals. 4. In agreement with others, we found that neurons with selective responses to color, size, and motion did seem to cluster in different CO compartments. However, this segregation of qualitatively different response selectivities was not absolute, and response properties also seemed to depend on laminar position within each compartment. As others also have noted, we found that CO stripe widths in the macaque (unlike in the squirrel monkey) did not consistently appear different. We relied on the segregation of qualitatively distinct cell types, and in some cases the pattern of Cat-301 staining as well, to distinguish CO stripes when the staining pattern of CO alone was ambiguous. Although all cell types were found in all CO compartments and laminae, unoriented cells were more prominent in layers 2-4 of \"thin\" stripes, direction-selective cells in layers 3B/4 of \"thick\" stripes, color-selective cells in the upper layers of thin and pale stripes, and end-stopped cells mainly outside of layer 4 in thin stripes.(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Receptive-fields-and-functional-architecture-of-V2.-Levitt-Kiper",
            "title": {
                "fragments": [],
                "text": "Receptive fields and functional architecture of macaque V2."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A quantitative analysis of V2 receptive field properties found that neurons with selective responses to color, size, and motion did seem to cluster in different CO compartments, however, this segregation of qualitatively different response selectivities was not absolute, and response properties also seemed to depend on laminar position within each compartment."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115493"
                        ],
                        "name": "K. Bao",
                        "slug": "K.-Bao",
                        "structuredName": {
                            "firstName": "Kejie",
                            "lastName": "Bao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 2
                            }
                        ],
                        "text": ", [5, 6, 2]) Indeed, some of these models are often viewed as first attempts to elucidate what learning algorithm (if any) the cortex may be using to model natural image statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14133766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ffc193177ee33a92f3acf3f8e607a2c861461df",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables. The learning procedure uses a variational approximation for the posterior distribution over the hidden variables. Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data. The parameters of the MRF are learned by using the mean field version of contrastive divergence [1]. We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images. We discuss the extension to multiple MRF\u2019s linked into in a chain graph by directed connections."
            },
            "slug": "Learning-Causally-Linked-Markov-Random-Fields-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "Learning Causally Linked Markov Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A generative model that contains a hidden Markov Random Field which has directed connections to the observable variables and a hybrid model that simultaneously learns parts of objects and their inter-relationships from intensity images is described."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "Many of these algorithms, such as [10, 9, 8, 6], compute a (approximately or exactly) sparse representation of the natural stimuli data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "A study by van Hateren and van der Schaaf [8] showed that the filters learned by independent components analysis (ICA) [9] on natural image data match very well with the classical receptive fields of V1 simple cells."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "Our work takes inspiration from the work of van Hateren and van der Schaaf, and represents a study that is done in a similar spirit, only extending the comparisons to a deeper area in the cortical hierarchy, namely visual area V2."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46717066"
                        ],
                        "name": "J. Hegd\u00e9",
                        "slug": "J.-Hegd\u00e9",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Hegd\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hegd\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7549580"
                        ],
                        "name": "D. V. Van Essen",
                        "slug": "D.-V.-Van-Essen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Van Essen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. V. Van Essen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Another study by Hegde and Van Essen [23] studied the responses of a population of V2 neurons to complex contour and grating stimuli."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 100
                            }
                        ],
                        "text": "Our second experiment represents a comparison to a subset of the results described in Hegde and van Essen [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "We generated a stimulus set comprising some [23]\u2019s complex shaped stimuli: angles, single bars, tri-stars (three line segments that meet at a point), and arcs/circles, and measured the response of the second layer of our sparse RBM model to these stimuli."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "This is also consistent with the results described in [23]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "2 Complex shaped model V2 neurons Our second experiment represents a comparison to a subset of the results described in Hegde and van Essen [23]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14987632,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "560b91cd4e2a5b1f0f371c8ee10b1eefa94fe3e9",
            "isKey": true,
            "numCitedBy": 359,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "To explore the role of visual area V2 in shape analysis, we studied the responses of neurons in area V2 of the alert macaque using a set of 128 grating and geometric line stimuli that varied in their shape characteristics and geometric complexity. Simple stimuli included oriented bars and sinusoidal gratings; complex stimuli included angles, arcs, circles, and intersecting lines, plus hyperbolic and polar gratings. We found that most V2 cells responded well to at least some of the complex stimuli, and in many V2 cells the most effective complex stimulus elicited a significantly larger response than the most effective bar or sinusoid. Approximately one-third of the V2 cells showed significant differential responsiveness to various complex shape characteristics, and many were also selective for the orientation, size, and/or spatial frequency of the preferred shape. These results indicate that V2 cells explicitly represent complex shape information and suggest specific types of higher order visual information that V2 cells extract from visual scenes."
            },
            "slug": "Selectivity-for-Complex-Shapes-in-Primate-Visual-V2-Hegd\u00e9-Essen",
            "title": {
                "fragments": [],
                "text": "Selectivity for Complex Shapes in Primate Visual Area V2"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results indicate that V2 cells explicitly represent complex shape information and suggest specific types of higher order visual information that V1 cells extract from visual scenes."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Other models, such as [20], have also been shown to give V1 complex cell-like properties."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12366835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5127759530ce213f488af2859190697770f557f3",
            "isKey": false,
            "numCitedBy": 1188,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decor-related features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously."
            },
            "slug": "Slow-Feature-Analysis:-Unsupervised-Learning-of-Wiskott-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Slow Feature Analysis: Unsupervised Learning of Invariances"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal that is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decor-related features, which are ordered by their degree of invariance."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108813944"
                        ],
                        "name": "Minami Ito",
                        "slug": "Minami-Ito",
                        "structuredName": {
                            "firstName": "Minami",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minami Ito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40579588"
                        ],
                        "name": "H. Komatsu",
                        "slug": "H.-Komatsu",
                        "structuredName": {
                            "firstName": "Hidehiko",
                            "lastName": "Komatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Komatsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 197
                            }
                        ],
                        "text": "More interestingly, the second layer captures a variety of both colinear (\u201ccontour\u201d) features as well as corners and junctions, that in a quantitative comparison to measurements of V2 taken by Ito & Komatsu, appeared to give responses that were similar along several dimensions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "We generated a stimulus set consisting of the same set of angles (pairs of edges) as [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Specifically, we take Ito & Komatsu [7]\u2019s characterization of V2 in terms of its responses to a large class of angled bar stimuli, and quantitatively measure the degree to which the deep belief network algorithm generates similar responses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Figure 1: (Images from [7]; courtesy of Ito and Komatsu) Left: Visualization of angle profiles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 292
                            }
                        ],
                        "text": "But in our experiments, even in these cases we found that repeating this process to build a two layer deep belief net (see Section 4.3) did not encode a significant number of corners/angles, unlike one trained using the sparse RBM; therefore, it showed significantly worse match to the Ito & Komatsu statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "The analysis methods adopted in [7] are very useful in characterizing the response properties, and we use these methods to evaluate our own model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Following a protocol similar to [7], we also eliminate from consideration the model neurons that do not respond strongly to corners and edges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "The 36*36 grid of stimuli follows [7], in which the orientation of two lines are varied to form different angles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "In one of these studies, Ito and Komatsu [7] investigated how V2 neurons responded to angular stimuli."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "Therefore, an important conclusion made from [7] was that a V2 neuron\u2019s response to an angle stimulus is highly dependent on its responses to each individual line component of the angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 150
                            }
                        ],
                        "text": "They found several V2 neurons responding maximally for angles, and the distribution of peak angles for these neurons is consistent with that found by [7]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "1 Method: Ito-Komatsu paper protocol We now describe the procedure we used to compare our model with the experimental data in [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 238
                            }
                        ],
                        "text": "For example, the fraction of model V2 neurons that respond strongly to a pair of edges near right angles (formally, have peak angle in the range 60-120 degrees) was 2% for the regular RBM, whereas it was 17% for the sparse RBM (and Ito & Komatsu reported 22%)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12116094,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "940e19622b547b5e47bf8bd341e22fbd039b5ede",
            "isKey": true,
            "numCitedBy": 294,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Angles and junctions embedded within contours are important features to represent the shape of objects. To study the neuronal basis to extract these features, we conducted extracellular recordings while two macaque monkeys performed a fixation task. Angle stimuli were the combination of two straight half-lines larger than the size of the classical receptive fields (CRFs). Each line was drawn from the center to outside the CRFs in 1 of 12 directions, so that the stimuli passed through the CRFs and formed angles at the center of the CRFs. Of 114 neurons recorded from the superficial layer of area V2, 91 neurons showed selective responses to these angle stimuli. Of these, 41 neurons (36.0%) showed selective responses to wide angles between 60\u00b0 and 150\u00b0 that were distinct from responses to straight lines or sharp angles (30\u00b0). Responses were highly selective to a particular angle in approximately one-fourth of neurons. When we tested the selectivity of the same neurons to individual half-lines, the preferred direction was more or less consistent with one or two components of the optimal angle stimuli. These results suggest that the selectivity of the neurons depends on both the combination of two components and the responses to individual components. Angle-selective V2 neurons are unlikely to be specific angle detectors, because the magnitude of their responses to the optimal angle was indistinguishable from that to the optimal half-lines. We suggest that the extraction of information of angles embedded within contour stimuli may start in area V2."
            },
            "slug": "Representation-of-Angles-Embedded-within-Contour-in-Ito-Komatsu",
            "title": {
                "fragments": [],
                "text": "Representation of Angles Embedded within Contour Stimuli in Area V2 of Macaque Monkeys"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is suggested that the extraction of information of angles embedded within contour stimuli may start in area V2, and Angle-selective V2 neurons are unlikely to be specific angle detectors, because the magnitude of their responses to the optimal angle was indistinguishable from that from the optimal half-lines."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638490"
                        ],
                        "name": "Dirk B. Walther",
                        "slug": "Dirk-B.-Walther",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Walther",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk B. Walther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6286101,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b73f2d7b58bfc555d8037b3fdb673c4cec1aecf0",
            "isKey": false,
            "numCitedBy": 1260,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-attention-to-salient-proto-objects-Walther-Koch",
            "title": {
                "fragments": [],
                "text": "Modeling attention to salient proto-objects"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39409429"
                        ],
                        "name": "R. L. Valois",
                        "slug": "R.-L.-Valois",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Valois",
                            "middleNames": [
                                "L.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Valois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2439030"
                        ],
                        "name": "E. Yund",
                        "slug": "E.-Yund",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Yund",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Yund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48334085"
                        ],
                        "name": "N. Hepler",
                        "slug": "N.-Hepler",
                        "structuredName": {
                            "firstName": "Norva",
                            "lastName": "Hepler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hepler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "The selectivity of neurons for oriented bar stimuli in cortical area V1 has been well documented [12, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33506510,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b55a81730b89cfdc8bfc240ac301fbf803cd25ac",
            "isKey": false,
            "numCitedBy": 838,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-orientation-and-direction-selectivity-of-cells-Valois-Yund",
            "title": {
                "fragments": [],
                "text": "The orientation and direction selectivity of cells in macaque visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": false,
            "numCitedBy": 973,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The only free parameters were p (which is chosen based on rough estimates of the average firing rate of neurons in V1 and V2 when exposed to natural scenes [26]), and \u03c3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207594808,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "278f4708b70f6a277208fb3a36dd51dfc42b7809",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 163,
            "paperAbstract": {
                "fragments": [],
                "text": "A wide variety of papers have reviewed what is known about the function of primary visual cortex. In this review, rather than stating what is known, we attempt to estimate how much is still unknown about V1 function. In particular, we identify five problems with the current view of V1 that stem largely from experimental and theoretical biases, in addition to the contributions of nonlinearities in the cortex that are not well understood. Our purpose is to open the door to new theories, a number of which we describe, along with some proposals for testing them."
            },
            "slug": "How-Close-Are-We-to-Understanding-V1-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "How Close Are We to Understanding V1?"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This review attempts to estimate how much is still unknown about V1 function, and identifies five problems with the current view of V1 that stem largely from experimental and theoretical biases, in addition to the contributions of nonlinearities in the cortex that are not well understood."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "For example, hierarchical sparse models of natural images have accounted for complex cell receptive fields [17], topography [18, 6], colinearity and contour coding [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7296718,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f02f3eccc1cf74e435721d09e4834aff6c1d12ed",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such independent feature subspaces then indicate the values of invariant features."
            },
            "slug": "Emergence-of-Phase-and-Shift-Invariant-Features-by-Hyv\u00e4rinen-Hoyer",
            "title": {
                "fragments": [],
                "text": "Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells, by maximizing the independence between norms of projections on linear subspaces."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "(12)In preliminary experiments, we also found that when these ideas are applied to self-taught learning [26] (in which one may use unlabeled data to identify features that are then useful for some supervised learning task), using a two-layer sparse RBM usually results in significantly better features for object recognition than using only a one-layer network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633389"
                        ],
                        "name": "Yan Karklin",
                        "slug": "Yan-Karklin",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Karklin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Karklin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "Some hierarchical extensions of these models [15, 6, 16] are able to learn features that are more complex than simple oriented bars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 490453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68f58d5b4b4797955b5965f10d424764bd6ee839",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Capturing statistical regularities in complex, high-dimensional data is an important problem in machine learning and signal processing. Models such as principal component analysis (PCA) and independent component analysis (ICA) make few assumptions about the structure in the data and have good scaling properties, but they are limited to representing linear statistical regularities and assume that the distribution of the data is stationary. For many natural, complex signals, the latent variables often exhibit residual dependencies as well as nonstationary statistics. Here we present a hierarchical Bayesian model that is able to capture higher-order nonlinear structure and represent nonstationary data distributions. The model is a generalization of ICA in which the basis function coefficients are no longer assumed to be independent; instead, the dependencies in their magnitudes are captured by a set of density components. Each density component describes a common pattern of deviation from the marginal density of the pattern ensemble; in different combinations, they can describe nonstationary distributions. Adapting the model to image or audio data yields a nonlinear, distributed code for higher-order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "slug": "A-Hierarchical-Bayesian-Model-for-Learning-in-Karklin-Lewicki",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hierarchical Bayesian model is presented that is able to capture higher-order nonlinear structure and represent nonstationary data distributions and Adapting the model to image or audio data yields a nonlinear, distributed code for higher- order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "Fortunately, the contrastive divergence learning algorithm gives an efficient approximation to the gradient of the loglikelihood [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4567,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355118"
                        ],
                        "name": "G. Boynton",
                        "slug": "G.-Boynton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Boynton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Boynton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46717066"
                        ],
                        "name": "J. Hegd\u00e9",
                        "slug": "J.-Hegd\u00e9",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Hegd\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hegd\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "It is uncertain what type of stimuli cause V2 neurons to respond optimally [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "While the former responses suggest a simple linear computation of V1 neural responses, the latter responses suggest a nonlinear computation [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28379537,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1cbaf4592efd15829455d6051d3dd85378ad83ce",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Visual-Cortex:-The-Continuing-Puzzle-of-Area-V2-Boynton-Hegd\u00e9",
            "title": {
                "fragments": [],
                "text": "Visual Cortex: The Continuing Puzzle of Area V2"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "The selectivity of neurons for oriented bar stimuli in cortical area V1 has been well documented [12, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7136759,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c5f5311fa1f34159ab3a0a1d58da51cd0340a640",
            "isKey": false,
            "numCitedBy": 6319,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded."
            },
            "slug": "Receptive-fields-and-functional-architecture-of-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields and functional architecture of monkey striate cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light, with response properties very similar to those previously described in the cat."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 197
                            }
                        ],
                        "text": "[1] proposed an algorithm for learning deep belief networks, by treating each layer as a restricted Boltzmann machine (RBM) and greedily training the network one layer at a time from the bottom up [24, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "It is also straightforward to formulate a sparse RBM with binary-valued visible units; for example, we can write the energy function as E(v,h) = \u22121/\u03c3(2)( P i civi + P j bjhj + P i,j viwijhj) (see also [24])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14638,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "For example, hierarchical sparse models of natural images have accounted for complex cell receptive fields [17], topography [18, 6], colinearity and contour coding [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1585328,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c07182933e7d8f308292300a63c4b95864d8ff5",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "In ordinary independent component analysis, the components are assumed to be completely independent, and they do not necessarily have any meaningful order relationships. In practice, however, the estimated independent components are often not at all independent. We propose that this residual dependence structure could be used to define a topo-graphic order for the components. In particular, a distance between two components could be defined using their higher-order correlations, and this distance could be used to create a topographic representation. Thus, we obtain a linear decomposition into approximately independent components, where the dependence of two components is approximated by the proximity of the components in the topographic representation."
            },
            "slug": "Topographic-Independent-Component-Analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Topographic Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A linear decomposition is obtained into approximately independent components, where the dependence of two components is approximated by the proximity of the components in the topographic representation."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732625"
                        ],
                        "name": "S. Zenios",
                        "slug": "S.-Zenios",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Zenios",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zenios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "updating the parameters by taking a gradient descent step followed by a projection into the constraint set satisfying Equation 5 (see [24])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115480745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d044b2ece3579ad1d6452f725524694a078b4cc",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword Preface Glossary of Symbols 1. Introduction Part I Theory 2. Generalized Distances and Generalized Projections 3. Proximal Minimization with D-Functions Part II Algorithms 4. Penalty Methods, Barrier Methods and Augmented Lagrangians 5. Iterative Methods for Convex Feasibility Problems 6. Iterative Algorithms for Linearly Constrained Optimization Problems 7. Model Decomposition Algorithms 8. Decompositions in Interior Point Algorithms Part III Applications 9. Matrix Estimation Problems 10. Image Reconsturction from Projections 11. The Inverse Problem in Radiation Therapy Treatment Planning 12. Multicommodity Network Flow Problems 13. Planning Under Uncertainty 14. Decompositions for Parallel Computing 15. Numerical Investigations"
            },
            "slug": "Parallel-Optimization:-Theory,-Algorithms,-and-Censor-Zenios",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization: Theory, Algorithms, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field . How close are we to understanding V 1 ?"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "However, to our knowledge no serious attempt has been made to directly relate, such as through quantitative comparisons, the computations of these deep learning algorithms to areas deeper in the cortical hierarchy, such as to visual areas V2, V4, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The coding of sensory messages. Current Problems in Animal Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "The coding of sensory messages. Current Problems in Animal Behavior"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Movshon . Receptive fields and functional architecture of macaque v 2"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Neurophysiology"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "These results are consistent with the \u201cefficient coding hypothesis\u201d which posits that the goal of early visual processing is to encode visual information as efficiently as possible [14]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The coding of sensory messages"
            },
            "venue": {
                "fragments": [],
                "text": "Current Problems in Animal Behavior,"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inki . Topographic independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 23,
            "methodology": 6,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Sparse-deep-belief-net-model-for-visual-area-V2-Lee-Ekanadham/202cbbf671743aefd380d2f23987bd46b9caaf97?sort=total-citations"
}