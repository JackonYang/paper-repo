{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10043879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0999dc17b35c0d893974f03d98293f71f27698b",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach."
            },
            "slug": "Probabilistic-Independence-Networks-for-Hidden-Smyth-Heckerman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Independence Networks for Hidden Markov Probability Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the well-known forward-backward and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs and the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14500325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dc4d6d7d55f9f0f1de53bb7f6816502f8f38892",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address particular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary computations are done by exact decimation procedures from statistical mechanics."
            },
            "slug": "Boltzmann-Chains-and-Hidden-Markov-Models-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Boltzmann Chains and Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A statistical mechanical framework for the modeling of discrete time series is proposed, and maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights, which motivates new architectures that address particular shortcomings of HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "ConclusionIn this paper we have examined the problem of learning for a class of generalizedhidden Markov models with distributed state representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2798755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "slug": "Hidden-Markov-Model}-Induction-by-Bayesian-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Model} Induction by Bayesian Model Merging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is compared with the Baum-Welch method of estimating fixed-size models, and it is found that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906655"
                        ],
                        "name": "T. Hunkapiller",
                        "slug": "T.-Hunkapiller",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hunkapiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hunkapiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35016244"
                        ],
                        "name": "M. A. McClure",
                        "slug": "M.-A.-McClure",
                        "structuredName": {
                            "firstName": "Marcella",
                            "lastName": "McClure",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. McClure"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 44553028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a68a57a90611629ee21f4cb3b8a2ddadbb6b41b",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov model (HMM) techniques are used to model families of biological sequences. A smooth and convergent algorithm is introduced to iteratively adapt the transition and emission parameters of the models from the examples in a given family. The HMM approach is applied to three protein families: globins, immunoglobulins, and kinases. In all cases, the models derived capture the important statistical characteristics of the family and can be used for a number of tasks, including multiple alignments, motif detection, and classification. For K sequences of average length N, this approach yields an effective multiple-alignment algorithm which requires O(KN2) operations, linear in the number of sequences."
            },
            "slug": "Hidden-Markov-models-of-biological-primary-sequence-Baldi-Chauvin",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models of biological primary sequence information."
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A smooth and convergent algorithm is introduced to iteratively adapt the transition and emission parameters of the models from the examples in a given family, yielding an effective multiple-alignment algorithm which requires O(KN2) operations, linear in the number of sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 51
                            }
                        ],
                        "text": "We explorethis generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997).6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11100657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dab928d7caa4fb8ea321003c550e144138504b6",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales."
            },
            "slug": "Hidden-Markov-Decision-Trees-Jordan-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A time series model that can be viewed as a decision tree with Markov temporal structure is studied and a Viterbi-like assumption is made to pick out a single most likely state sequence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2160404,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5d28fc1a4027d23cc9e4ad8555361d48940e9be8",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both."
            },
            "slug": "Hidden-Markov-models-in-computational-biology.-to-Krogh-Brown",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models in computational biology. Applications to protein modeling."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18231498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd7d7767129ed180db39d38be28c1ae389481d2f",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the hard k-means clustering algorithm and the soft mixture model, each of which assumes that a single hidden event generates each data point, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. We employ an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model. A crucial issue is the mixing function for combining beliefs from different cluster centers in order to generate data predictions whose errors are minimized both during recognition and learning. The mixing function constitutes a prior assumption about underlying structural regularities of the data domain; we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer alternative forms of the nonlinearity for two types of data domain. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations in several experimental data sets."
            },
            "slug": "A-Multiple-Cause-Mixture-Model-for-Unsupervised-Saund",
            "title": {
                "fragments": [],
                "text": "A Multiple Cause Mixture Model for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data, which employs an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model and demonstrates its ability to discover coherent multiple causal representations in several experimental data sets."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Williams and Hinton (1991) rst formulated the problem of learning in HMMswith distributed state representations and proposed a solution based on determinis-tic Boltzmann learning.1 The approach presented in this paper is similar to Williamsand Hinton's in that it can also be viewed from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "On the other hand, an HMM with adistributed state representation could achieve the same task with 30 binary state\n2 Z. GHAHRAMANI AND M.I. JORDANvariables (Williams & Hinton, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "Due to the combinatorial natureof the hidden state representation, this exact algorithm is intractable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 284
                            }
                        ],
                        "text": "Finally, we use the structured approximation to model Bach'schorales and show that factorial HMMs can capture statistical structure in this data set which anunconstrained HMM cannot.Keywords: Hidden Markov models, time series, EM algorithm, graphical models, Bayesian net-works, mean eld theory1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117771711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4725bea4175f9a3912188aca3c4f25c66469fea1",
            "isKey": true,
            "numCitedBy": 49,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mean-field-networks-that-learn-to-discriminate-Williams-Hinton",
            "title": {
                "fragments": [],
                "text": "Mean field networks that learn to discriminate temporally distorted strings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 212
                            }
                        ],
                        "text": "Factorial Hidden Markov ModelsZOUBIN GHAHRAMANI zoubin@cs.toronto.eduDepartment of Computer Science, University of Toronto, Toronto, ON M5S 3H5, CanadaMICHAEL I. JORDAN jordan@psyche.mit.eduDepartment of Brain & Cognitive Sciences, Massachusetts Institute of Technology,Cambridge, MA 02139, USAEditor: Padhraic SmythAbstract."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 190
                            }
                        ],
                        "text": "\u2026GHAHRAMANI zoubin@cs.toronto.eduDepartment of Computer Science, University of Toronto, Toronto, ON M5S 3H5, CanadaMICHAEL I. JORDAN jordan@psyche.mit.eduDepartment of Brain & Cognitive Sciences, Massachusetts Institute of Technology,Cambridge, MA 02139, USAEditor: Padhraic SmythAbstract."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17743203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df682aa90fbbbf665a8b273a57ca87d6cea9ff99",
            "isKey": false,
            "numCitedBy": 1561,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of hidden Markov models for speech recognition has become predominant in the last several years, as evidenced by the number of published papers and talks at major speech conferences. The reasons this method has become so popular are the inherent statistical (mathematically precise) framework; the ease and availability of training algorithms for cstimating the parameters of the models from finite training sets of speech data; the flexibility of the resulting recognition system in which one can easily change the size, type, or architecture of the models to suit particular words, sounds, and so forth; and the ease of implementation of the overall recognition system. In this expository article, we address the role of statistical methods in this powerful technology as applied to speech recognition and discuss a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations."
            },
            "slug": "Hidden-Markov-Models-for-Speech-Recognition-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The role of statistical methods in this powerful technology as applied to speech recognition is addressed and a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40359484"
                        ],
                        "name": "K. Kanazawa",
                        "slug": "K.-Kanazawa",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Kanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "One approach to dealing with this issue isto utilize stochastic sampling methods (Kanazawa et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 421074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "135d19fe9c3836d5ba5f6af7620e4d25f2fed710",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods bf choice for very large networks. Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly. In essence, the simulation trials diverge further and further from reality as the process is observed over time. In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality. The first algorithm, \"evidence reversal\" (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables. The second algorithm, called \"survival of the fittest\" sampling (SOF), \"repopulates\" the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial. We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods. The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation."
            },
            "slug": "Stochastic-simulation-algorithms-for-dynamic-Kanazawa-Koller",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation algorithms for dynamic probabilistic networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality are presented and the benefits of combining the ER and SOF methods are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13863771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa426fc1d6bed08eb8f0d44d5e574dcbcd3f4678",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Compliant control is a standard method for performing fine manipulation tasks, like grasping and assembly, but it requires estimation of the state of contact between the robot arm and the objects involved. Here we present a method to learn a model of the movement from measured data. The method requires little or no prior knowledge and the resulting model explicitly estimates the state of contact. The current state of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition probabilities between states are modeled as parametrized functions of the measurement We show that their parameters can be estimated from measurements concurrently with the estimation of the parameters of the movement in each state of contact. The learning algorithm is a variant of the EM procedure. The E step is computed exactly; solving the M step exactly would require solving a set of coupled nonlinear algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood."
            },
            "slug": "Learning-Fine-Motion-by-Markov-Mixtures-of-Experts-Meil\u0103-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning Fine Motion by Markov Mixtures of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A method to learn a model of the movement from measured data that requires little or no prior knowledge and the resulting model explicitly estimates the state of contact."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 203
                            }
                        ],
                        "text": "Assuming that all probabilitiesare bounded away from zero, this Markov chain is guaranteed to converge to the\n8 Z. GHAHRAMANI AND M.I. JORDANposterior probabilities of the states given the observations (Geman & Geman, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "Although there are manypossible sampling schemes (for a review see Neal, 1993), here we present one of thesimplest|Gibbs sampling (Geman & Geman, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18709,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 193
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Jordan, M. I., & Jacobs, R. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "This constraint canbe relaxed by introducing couplings between the hidden state variables (cf. Saul &Jordan, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 60
                            }
                        ],
                        "text": "We explore this generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997). 6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 198
                            }
                        ],
                        "text": "This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right (Zemel, 1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "30 Z. GHAHRAMANI AND M.I. JORDANJordan, M. I., Ghahramani, Z., & Saul, L. K. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 28
                            }
                        ],
                        "text": "Smyth, P., Heckerman, D., & Jordan, M. I. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "Meila, M., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "We explorethis generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997).6."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Saul, L., & Jordan, M. I. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 149
                            }
                        ],
                        "text": "Another approach,which provides the basis for algorithms described in the current paper, is to makeuse of variational methods (cf. Saul, Jaakkola, & Jordan, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In M. Mozer,M. Jordan, & T. Petsche (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Saul, L., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 231
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "Saul, L. K., & Jordan, M. I. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "Ft.Lauderdale, FL.Saul, L., Jaakkola, T., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8523597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0182096896504acf759110091a6bca3ca75e828",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real world learning problems are best characterized by an interaction of multiple independent causes or factors. Discovering such causal structure from the data is the focus of this paper. Based on Zemel and Hinton's cooperative vector quantizer (CVQ) architecture, an unsupervised learning algorithm is derived from the Expectation-Maximization (EM) framework. Due to the combinatorial nature of the data generation process, the exact E-step is computationally intractable. Two alternative methods for computing the E-step are proposed: Gibbs sampling and mean-field approximation, and some promising empirical results are presented."
            },
            "slug": "Factorial-Learning-and-the-EM-Algorithm-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Factorial Learning and the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Based on Zemel and Hinton's cooperative vector quantizer (CVQ) architecture, an unsupervised learning algorithm is derived from the Expectation-Maximization (EM) framework, and some promising empirical results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 40
                            }
                        ],
                        "text": "A natural structure to consider is one in which each state variableevolves according to its own dynamics, and is a priori uncoupled from the otherstate variables"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60866167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fa47fc237a0781b4bf1c84fedb728d20db26a1",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, we consider learning algorithms for neural networks which are based on fitting a mixture probability density to a set of data. \nWe begin with an unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms. Rather than updating only the parameters of the \"winner\" on each case, the parameters of all competitors are updated in proportion to their relative responsibility for the case. Use of such a \"soft\" competitive algorithm is shown to give better performance than the more traditional algorithms, with little additional cost. \nWe then consider a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task. A soft competitive mechanism is used to determine how much an expert learns on a case, based on how well the expert performs relative to the other expert networks. At the same time, a separate gating network learns to weight the output of each expert according to a prediction of its relative performance based on the input to the system. Experiments on a number of tasks illustrate that this architecture is capable of uncovering interesting task decompositions and of generalizing better than a single network with small training sets. \nFinally, we consider learning algorithms in which we assume that the actual output of the network should fall into one of a small number of classes or clusters. The objective of learning is to make the variance of these classes as small as possible. In the classical decision-directed algorithm, we decide that an output belongs to the class it is closest to and minimize the squared distance between the output and the center (mean) of this closest class. In the \"soft\" version of this algorithm, we minimize the squared distance between the actual output and a weighted average of the means of all of the classes. The weighting factors are the relative probability that the output belongs to each class. This idea may also be used to model the weights of a network, to produce networks which generalize better from small training sets."
            },
            "slug": "Soft-competitive-adaptation:-neural-network-based-Nowlan",
            "title": {
                "fragments": [],
                "text": "Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms and a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task are considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "The approach of exploiting such tractablesubstructures was rst suggested in the machine learning literature by Saul andJordan (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3396,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31781461"
                        ],
                        "name": "M. Tanner",
                        "slug": "M.-Tanner",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Tanner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725639"
                        ],
                        "name": "W. Wong",
                        "slug": "W.-Wong",
                        "structuredName": {
                            "firstName": "Wing",
                            "lastName": "Wong",
                            "middleNames": [
                                "Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026of the learning problem, in which the parameters are also consid-ered hidden random variables, can be handled by Gibbs sampling by replacing the \\M step\"with sampling from the conditional distribution of the parameters given the other hiddenvariables (for example, see Tanner and Wong, 1987).5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122088924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a44241bf4d932fc09bc683f211360c43f02fd106",
            "isKey": false,
            "numCitedBy": 4041,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea ..."
            },
            "slug": "The-calculation-of-posterior-distributions-by-data-Tanner-Wong",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions by data augmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution of parameters of interest."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 193
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "Another approach,which provides the basis for algorithms described in the current paper, is to makeuse of variational methods (cf. Saul, Jaakkola, & Jordan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15116562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9a54374aec5c92296c7b24436f08934643829ae",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory."
            },
            "slug": "Exploiting-Tractable-Substructures-in-Intractable-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Exploiting Tractable Substructures in Intractable Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A refined mean field approximation for inference and learning in probabilistic neural networks is developed, and it is shown how to incorporate weak higher order interactions into a first-order hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 14
                            }
                        ],
                        "text": "Where necessary, details of derivationsare provided in the appendixes.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2445072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f",
            "isKey": false,
            "numCitedBy": 958,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes."
            },
            "slug": "Autoencoders,-Minimum-Description-Length-and-Free-Hinton-Zemel",
            "title": {
                "fragments": [],
                "text": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 135
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models which are conditioned on inputs have been recently presented in the literature (Cacciatore and Nowlan, 1994; Bengio and Frasconi, 1995; Meila and Jordan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29085cdffb3277c1c8fd10ac09e0d89452c8db83",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation."
            },
            "slug": "An-Input-Output-HMM-Architecture-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "An Input Output HMM Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A recurrent architecture having a modular structure that has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 197
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 198
                            }
                        ],
                        "text": "This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right (Zemel, 1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117036852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b2bffdf5b62305bec4c0f1ea7e3c1ba66fccb5",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental problem in learning and reasoning about a set of information is finding the right representation. The primary goal of an unsupervised learning procedure is to optimize the quality of a system's internal representation. In this thesis, we present a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle. The MDL principle states that the best model is one that minimizes the summed description length of the model and the data with respect to the model. Applying this approach to the unsupervised learning problem makes explicit a key trade off between the accuracy of a representation (i.e., how concise a description of the input may be generated from it) and its succinctness (i.e., how compactly the representation itself can be described). \nViewing existing unsupervised learning procedures in terms of the framework exposes their implicit assumptions about the type of structure assumed to underlie the data. While these existing algorithms typically minimize the data description using a fixed length representation, we use the framework to derive a class of objective functions for training self-supervised neural networks, where the goal is to minimize the description length of the representation simultaneously with that of the data. Formulating a description of the representation forces assumptions about the structure of the data to be made explicit, which in turn leads to a particular network configuration as well as an objective function that can be used to optimize the network parameters. We describe three new learning algorithms derived in this manner from the MDL framework. Each algorithm embodies a different scheme for describing the internal representation, and is therefore suited to a range of datasets based on the structure underlying the data. Simulations demonstrate the applicability of these algorithms on some simple computational vision tasks."
            },
            "slug": "A-minimum-description-length-framework-for-learning-Zemel",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis presents a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle, and describes three new learning algorithms derived in this manner from the MDL framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708077"
                        ],
                        "name": "S. Eddy",
                        "slug": "S.-Eddy",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Eddy",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eddy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784862,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2e49816e154db2c7e81f5bb5d8acc01a24626b00",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hidden-Markov-models.-Eddy",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "Current opinion in structural biology"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143942430"
                        ],
                        "name": "D. Conklin",
                        "slug": "D.-Conklin",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Conklin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conklin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 131
                            }
                        ],
                        "text": "Bach's Chorales, obtained from the UCI Repository for Machine Learning Databases (Merz & Murphy, 1996) and originally discussed in Conklin and Witten (1995). Each event in the sequence was represented by six attributes, described in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 588511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afd6eaa5dc1faf4103383b397b1ee18ba0a25ee4",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short\u2010term and long\u2010term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are used for analysis and prediction, and we conjecture that highly predictive theories will also generate original, acceptable, works. Although the quality of the works generated is hard to quantify objectively, the predictive power of models can be measured by the notion of entropy, or unpredictability. Highly predictive theories will produce low\u2010entropy estimates of a musical language. The methods developed are applied to the Bach chorale melodies. Multiple\u2010viewpoint systems are learned from a sample of 95 chorales, estimates of entropy are produced, and a predictive theory is used to..."
            },
            "slug": "Multiple-viewpoint-systems-for-music-prediction-Conklin-Witten",
            "title": {
                "fragments": [],
                "text": "Multiple viewpoint systems for music prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Examination of the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena, concludes that highly predictive theories will produce low\u2010entropy estimates of a musical language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40359484"
                        ],
                        "name": "K. Kanazawa",
                        "slug": "K.-Kanazawa",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Kanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanazawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 61
                            }
                        ],
                        "text": "For related work on inference in distributed state HMMs, see Dean and Kanazawa (1989).2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 61
                            }
                        ],
                        "text": "For related work on inference in distributed state HMMs, see Dean and Kanazawa (1989). 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57798167,
            "fieldsOfStudy": [
                "Philosophy",
                "Computer Science"
            ],
            "id": "959d2b9268294248b47dacfb15009699f0f2e80b",
            "isKey": false,
            "numCitedBy": 1178,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Reasoning about change requires predicting how long a proposition, having become true, will continue to be so. Lacking perfect knowledge, an agent may be constrained to believe that a proposition persists indefinitely simply because there is no way for the agent to infer a contravening proposition with certainty. In this paper, we describe a model of causal reasoning that accounts for knowledge concerning cause\u2010and\u2010effect relationships and knowledge concerning the tendency for propositions to persist or not as a function of time passing. Our model has a natural encoding in the form of a network representation for probabilistic models. We consider the computational properties of our model by reviewing recent advances in computing the consequences of models encoded in this network representation. Finally, we discuss how our probabilistic model addresses certain classical problems in temporal reasoning (e. g., the frame and qualification problems)."
            },
            "slug": "A-model-for-reasoning-about-persistence-and-Dean-Kanazawa",
            "title": {
                "fragments": [],
                "text": "A model for reasoning about persistence and causation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of causal reasoning that accounts for knowledge concerning cause\u2010and\u2010effect relationships and knowledge concerning the tendency for propositions to persist or not as a function of time passing is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 21
                            }
                        ],
                        "text": "Appendix AThe M stepThe M step equations for each parameter are obtained by setting the derivativesof Q with respect to that parameters to zero."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 107
                            }
                        ],
                        "text": "For log-linearmodels, the M step can be solved using an inner loop of iteratively reweightedleast-squares (McCullagh & Nelder, 1989).5.4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7218290,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b461250c014b460e7c97b6138a3ee811f198f43",
            "isKey": true,
            "numCitedBy": 11587,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the \u008e rst book on generalized linear models written by authors not mostly associated with the biological sciences. Subtitled \u201cWith Applications in Engineering and the Sciences,\u201d this book\u2019s authors all specialize primarily in engineering statistics. The \u008e rst author has produced several recent editions of Walpole, Myers, and Myers (1998), the last reported by Ziegel (1999). The second author has had several editions of Montgomery and Runger (1999), recently reported by Ziegel (2002). All of the authors are renowned experts in modeling. The \u008e rst two authors collaborated on a seminal volume in applied modeling (Myers and Montgomery 2002), which had its recent revised edition reported by Ziegel (2002). The last two authors collaborated on the most recent edition of a book on regression analysis (Montgomery, Peck, and Vining (2001), reported by Gray (2002), and the \u008e rst author has had multiple editions of his own regression analysis book (Myers 1990), the latest of which was reported by Ziegel (1991). A comparable book with similar objectives and a more speci\u008e c focus on logistic regression, Hosmer and Lemeshow (2000), reported by Conklin (2002), presumed a background in regression analysis and began with generalized linear models. The Preface here (p. xi) indicates an identical requirement but nonetheless begins with 100 pages of material on linear and nonlinear regression. Most of this will probably be a review for the readers of the book. Chapter 2, \u201cLinear Regression Model,\u201d begins with 50 pages of familiar material on estimation, inference, and diagnostic checking for multiple regression. The approach is very traditional, including the use of formal hypothesis tests. In industrial settings, use of p values as part of a risk-weighted decision is generally more appropriate. The pedagologic approach includes formulas and demonstrations for computations, although computing by Minitab is eventually illustrated. Less-familiar material on maximum likelihood estimation, scaled residuals, and weighted least squares provides more speci\u008e c background for subsequent estimation methods for generalized linear models. This review is not meant to be disparaging. The authors have packed a wealth of useful nuggets for any practitioner in this chapter. It is thoroughly enjoyable to read. Chapter 3, \u201cNonlinear Regression Models,\u201d is arguably less of a review, because regression analysis courses often give short shrift to nonlinear models. The chapter begins with a great example on the pitfalls of linearizing a nonlinear model for parameter estimation. It continues with the effective balancing of explicit statements concerning the theoretical basis for computations versus the application and demonstration of their use. The details of maximum likelihood estimation are again provided, and weighted and generalized regression estimation are discussed. Chapter 4 is titled \u201cLogistic and Poisson Regression Models.\u201d Logistic regression provides the basic model for generalized linear models. The prior development for weighted regression is used to motivate maximum likelihood estimation for the parameters in the logistic model. The algebraic details are provided. As in the development for linear models, some of the details are pushed into an appendix. In addition to connecting to the foregoing material on regression on several occasions, the authors link their development forward to their following chapter on the entire family of generalized linear models. They discuss score functions, the variance-covariance matrix, Wald inference, likelihood inference, deviance, and overdispersion. Careful explanations are given for the values provided in standard computer software, here PROC LOGISTIC in SAS. The value in having the book begin with familiar regression concepts is clearly realized when the analogies are drawn between overdispersion and nonhomogenous variance, or analysis of deviance and analysis of variance. The authors rely on the similarity of Poisson regression methods to logistic regression methods and mostly present illustrations for Poisson regression. These use PROC GENMOD in SAS. The book does not give any of the SAS code that produces the results. Two of the examples illustrate designed experiments and modeling. They include discussion of subset selection and adjustment for overdispersion. The mathematic level of the presentation is elevated in Chapter 5, \u201cThe Family of Generalized Linear Models.\u201d First, the authors unify the two preceding chapters under the exponential distribution. The material on the formal structure for generalized linear models (GLMs), likelihood equations, quasilikelihood, the gamma distribution family, and power functions as links is some of the most advanced material in the book. Most of the computational details are relegated to appendixes. A discussion of residuals returns one to a more practical perspective, and two long examples on gamma distribution applications provide excellent guidance on how to put this material into practice. One example is a contrast to the use of linear regression with a log transformation of the response, and the other is a comparison to the use of a different link function in the previous chapter. Chapter 6 considers generalized estimating equations (GEEs) for longitudinal and analogous studies. The \u008e rst half of the chapter presents the methodology, and the second half demonstrates its application through \u008e ve different examples. The basis for the general situation is \u008e rst established using the case with a normal distribution for the response and an identity link. The importance of the correlation structure is explained, the iterative estimation procedure is shown, and estimation for the scale parameters and the standard errors of the coef\u008e cients is discussed. The procedures are then generalized for the exponential family of distributions and quasi-likelihood estimation. Two of the examples are standard repeated-measures illustrations from biostatistical applications, but the last three illustrations are all interesting reworkings of industrial applications. The GEE computations in PROC GENMOD are applied to account for correlations that occur with multiple measurements on the subjects or restrictions to randomizations. The examples show that accounting for correlation structure can result in different conclusions. Chapter 7, \u201cFurther Advances and Applications in GLM,\u201d discusses several additional topics. These are experimental designs for GLMs, asymptotic results, analysis of screening experiments, data transformation, modeling for both a process mean and variance, and generalized additive models. The material on experimental designs is more discursive than prescriptive and as a result is also somewhat theoretical. Similar comments apply for the discussion on the quality of the asymptotic results, which wallows a little too much in reports on various simulation studies. The examples on screening and data transformations experiments are again reworkings of analyses of familiar industrial examples and another obvious motivation for the enthusiasm that the authors have developed for using the GLM toolkit. One can hope that subsequent editions will similarly contain new examples that will have caused the authors to expand the material on generalized additive models and other topics in this chapter. Designating myself to review a book that I know I will love to read is one of the rewards of being editor. I read both of the editions of McCullagh and Nelder (1989), which was reviewed by Schuenemeyer (1992). That book was not fun to read. The obvious enthusiasm of Myers, Montgomery, and Vining and their reliance on their many examples as a major focus of their pedagogy make Generalized Linear Models a joy to read. Every statistician working in any area of applied science should buy it and experience the excitement of these new approaches to familiar activities."
            },
            "slug": "Generalized-Linear-Models-Ziegel",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This is the \u008e rst book on generalized linear models written by authors not mostly associated with the biological sciences, and it is thoroughly enjoyable to read."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 129
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations are a particularclass of probabilistic graphical model (Pearl, 1988; Lauritzen & Spiegelhalter, 1988),which represent probability distributions as graphs in which the nodes correspondto random variables and the links represent conditional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 152
                            }
                        ],
                        "text": "This fact can best be shown by making reference to standard algorithms for prob-\nFACTORIAL HIDDEN MARKOV MODELS 7abilistic inference in graphical models (Lauritzen & Spiegelhalter, 1988), althoughit can also be derived readily from direct application of Bayes rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58792451,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0a3767909649cf31d32e087693d93171af28ebe0",
            "isKey": false,
            "numCitedBy": 4303,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-computations-with-probabilities-on-graphical-Lauritzen-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Local computations with probabilities on graphical structures and their application to expert systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27820793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c9aa374f3f5e1338a5cd9bbbd0bddea212788e",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorial learning, finding a statistically independent representation of a sensory imagea factorial codeis applied here to solve multilayer supervised learning problems that have traditionally required backpropagation. This lends support to Barlow's argument for factorial sensory processing, by demonstrating how it can solve actual pattern recognition problems. Two techniques for supervised factorial learning are explored, one of which gives a novel distributed solution requiring only positive examples. Also, a new nonlinear technique for factorial learning is introduced that uses neural networks based on almost reversible cellular automata. Due to the special functional connectivity of these networkswhich resemble some biological microcircuitslearning requires only simple local algorithms. Also, supervised factorial learning is shown to be a viable alternative to backpropagation. One significant advantage is the existence of a measure for the performance of intermediate learning stages."
            },
            "slug": "Supervised-Factorial-Learning-Redlich",
            "title": {
                "fragments": [],
                "text": "Supervised Factorial Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work lends support to Barlow's argument for factorial sensory processing, by demonstrating how it can solve actual pattern recognition problems, and two techniques for supervised factorial learning are explored, one of which gives a novel distributed solution requiring only positive examples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2359255"
                        ],
                        "name": "T. Cacciatore",
                        "slug": "T.-Cacciatore",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Cacciatore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cacciatore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 135
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models which are conditioned on inputs have been recently presented in the literature (Cacciatore and Nowlan, 1994; Bengio and Frasconi, 1995; Meila and Jordan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 134
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17567961,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7077a8802ac5f65991bf1086b07d0d6f17e9d894",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior."
            },
            "slug": "Mixtures-of-Controllers-for-Jump-Linear-and-Plants-Cacciatore-Nowlan",
            "title": {
                "fragments": [],
                "text": "Mixtures of Controllers for Jump Linear and Non-Linear Plants"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior is described, based on a Markov process model, and a recurrent network for gating a set of linear or non-linear controllers is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61247712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9abff70b0acf9c6bb74d85f3141d76bef2039a5",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic expert system provides a graphical representation of a joint probability distribution which can be used to simplify and localize calculations. Jensenet al. (1990) introduced a \u2018flow-propagation\u2019 algorithm for calculating marginal and conditional distributions in such a system. This paper analyses that algorithm in detail, and shows how it can be modified to perform other tasks, including maximization of the joint density and simultaneous \u2018fast retraction\u2019 of evidence entered on several variables."
            },
            "slug": "Applications-of-a-general-propagation-algorithm-for-Dawid",
            "title": {
                "fragments": [],
                "text": "Applications of a general propagation algorithm for probabilistic expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper analyses a \u2018flow-propagation\u2019 algorithm for calculating marginal and conditional distributions in a probabilistic expert system in detail, and shows how it can be modified to perform other tasks, including maximization of the joint density and simultaneous 'fast retraction' of evidence entered on several variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 39
                            }
                        ],
                        "text": "Adi erent approach comes from Saul and Jordan (1995), who derived a set of rulesfor computing the gradients required for learning in HMMs with distributed statespaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7424318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79433b5feacd9e8feeafa629dae5a85f362fef",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-Networks-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The utility of a mean field theory for sigmoid belief networks based on ideas from statistical mechanics is demonstrated on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "This can be achieved viathe Viterbi (1967) algorithm, a form of dynamic programming that is very closelyrelated to the forward{backward algorithm and also has analogues in the graphicalmodel literature (Dawid, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144023189"
                        ],
                        "name": "I. Morgenstern",
                        "slug": "I.-Morgenstern",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Morgenstern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Morgenstern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144469417"
                        ],
                        "name": "K. Binder",
                        "slug": "K.-Binder",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Binder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Binder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122412403,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "45ab86a5190c4cae8b3d174366fe4918bee6b0c0",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "By a recursive method numerically exact free energies are calculated for square $L\\ifmmode\\times\\else\\texttimes\\fi{}L$ Ising lattices, with $6l~Ll~18$, for several kinds of frozen-in bond disorder: (i) bonds $\\ifmmode\\pm\\else\\textpm\\fi{}J$ with various concentrations of negative bonds; (ii) bonds distributed according to a Gaussian distribution. Ground states of these systems are identified, the response to \"ordering fields\" is studied, and the correlation function ${\u3008{S}_{0}{S}_{R}\u3009}_{T}^{2}$ is calculated as a function of temperature for various distances $R$ in the lattice. This correlation is found to decay strongly (presumably exponentially) with increasing $R$ even at temperatures distinctly below the apparent freezing temperature ${T}_{f}$ of previous Monte Carlo simulations; this \"freezing transition\" is hence unambiguously identified as a nonequilibrium effect. However, the correlation length is found to become long ranged at low temperatures, and it is suggested that a phase transition still occurs at $T=0$; while in the Gaussian model the spin-glass order parameter $q(T=0)=1$, it is found that $q\\ensuremath{\\equiv}0$ in the $\\ifmmode\\pm\\else\\textpm\\fi{}J$ model where rather a power-law decay of correlations ${\u3008{S}_{0}{S}_{R}\u3009}_{T=0}^{2}$ occurs. Performing Monte Carlo simulations for precisely the same systems, the cooling times necessary to reach the true ground states of the system are identified, as well as the simulation times necessary to reach thermal equilibrium for the correlation functions. These times are found to increase so strongly with $L$ that for systems of macroscopic size the correct thermal equilibrium is probably irrelevant for experimental purposes. Rather a statistical mechanics based on the many long-lived metastable states would be required."
            },
            "slug": "Magnetic-correlations-in-two-dimensional-Morgenstern-Binder",
            "title": {
                "fragments": [],
                "text": "Magnetic correlations in two-dimensional spin-glasses"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42793,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 69
                            }
                        ],
                        "text": "This is analogous to the fully-connectedBoltzmannmachinewithN units (Hinton & Sejnowski,1986), in which every binary unit is coupled to every other unit usingO(N2) parameters, ratherthan the O(2N) parameters required to specify the complete probability table."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 26
                            }
                        ],
                        "text": "As in Boltzmann machines (Hinton & Sejnowski, 1986), botha clamped and an unclamped phase are therefore required for learning, where thegoal of the unclamped phase is to compute the derivative of the partition functionwith respect to the parameters (Neal, 1992).5.3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": true,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 11
                            }
                        ],
                        "text": "For example, a speech signal generated by the superpo-sition of multiple simultaneous speakers can be potentially modeled with such anarchitecture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 568745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d62bcde418144411068d5b09952090962fbc05f6",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output."
            },
            "slug": "Unsupervised-Learning-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning and Data Mining"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Choice of approximationThe theory of the EM algorithm as presented in Dempster et al. (1977) assumesthe use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 70
                            }
                        ],
                        "text": "Since t is a function of all thestate variables, the probability of a setting of one of the state variables will dependon the setting of the other state variables.3 This dependency e ectively couples allof the hidden state variables for the purposes of calculating posterior probabilitiesand makes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145052965"
                        ],
                        "name": "G. Parisi",
                        "slug": "G.-Parisi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48215291"
                        ],
                        "name": "R. Shankar",
                        "slug": "R.-Shankar",
                        "structuredName": {
                            "firstName": "Ramamurti",
                            "lastName": "Shankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shankar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the HMM notation of Rabiner and Juang (1986), \u3008S t \u3009 corresponds to \u03b3t, the vector of state occupation probabilities, \u3008S t\u22121S (m)\u2032 t \u3009 corresponds to \u03bet, the K \u00d7K matrix of state occupation probabilities at two consecutive time steps, and \u3008S t S (n)\u2032 t \u3009 has no analogue when there is only a single underlying Markov model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119396873,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "cf2af6e45d25f62bc52bb1ecae86a2c3d0aae72b",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Classical equilibrium statistical mechanics magnetic systems the Ising model the low-temperature and high-temperature expansions the Landau-Ginsberg model near the transition the renormalization group perturbative evaluation of the critical exponents near four dimensions on spontaneous symmetry breaking other models the transfer matrix path integrals for quantum mechanics semiclassical methods relativistic quantum field theory particle-field duality time dependent correlations the approach to equilibrium the stochastic approach to equilibrium the stochastic approach computer simulation."
            },
            "slug": "Statistical-Field-Theory-Parisi-Shankar",
            "title": {
                "fragments": [],
                "text": "Statistical Field Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Machine Learning, ?, 1{31 (1997)c 1997 Kluwer Academic Publishers, Boston."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 71
                            }
                        ],
                        "text": "Bach's Chorales (obtained from the UCI Repository for Machine Learning (Murphy and Aha, 1992); originally discussed in Conklin and Witten, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 57
                            }
                        ],
                        "text": "S. Bach's Chorales, obtained from the UCI Repository for Machine LearningDatabases (Merz & Murphy, 1996) and originally discussed in Conklin and Wit-ten (1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": true,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Whilethe Jensen, Lauritzen and Olesen (1990) algorithm can still be used to propagateinformation exactly in chain graphs, such undirected links cause the normalizationconstant of the probability distribution|the partition function|to depend on thecoupling parameters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 156
                            }
                        ],
                        "text": "Consider thecomputations that are required for calculating posterior probabilities for the fac-torial HMM shown in Figure 1 (b) within the framework of the Lauritzen andSpiegelhalter algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Lauritzen, S. L., & Spiegelhalter, D. J. (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 150
                            }
                        ],
                        "text": "This problem can be solved e cientlyvia the forward{backward algorithm (Rabiner & Juang, 1986), which can be shownto be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm forprobability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "Although exact probabilitypropagation algorithms exist for general graphical models (Jensen, Lauritzen, &Olesen, 1990), these algorithms are intractable for densely-connected models suchas the ones we consider in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations are a particularclass of probabilistic graphical model (Pearl, 1988; Lauritzen & Spiegelhalter, 1988),which represent probability distributions as graphs in which the nodes correspondto random variables and the links represent conditional independence relations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 152
                            }
                        ],
                        "text": "This fact can best be shown by making reference to standard algorithms for prob-\nFACTORIAL HIDDEN MARKOV MODELS 7abilistic inference in graphical models (Lauritzen & Spiegelhalter, 1988), althoughit can also be derived readily from direct application of Bayes rule."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 581,
                                "start": 576
                            }
                        ],
                        "text": "Similarly, to obtain the backward recursions we de ne t = P (fY gTt+1jS(1)t ; : : :S(M)t ; ) (M)t 1 = P (fY gTt jS(1)t ; : : :S(M)t ; )... (1)t 1 = P (fY gTt jS(1)t ; S(2)t 1 : : :S(M)t 1 ; ) (0)t 1 = P (fY gTt jS(1)t 1; S(2)t 1 : : : S(M)t 1 ; ) = t 1;from which we obtain (M)t 1 = P (YtjS(1)t ; : : : ; S(M)t ; ) t (B.3) (m 1)t 1 = XS(m)t P (S(m)t jS(m)t 1) (m)t 1 : (B.4)The posterior probability of the state at time t is obtained by multiplying t and t and normalizing: t = P (StjfY gT1 ; ) = t tPSt t t : (B.5)This algorithm can be shown to be equivalent to the Jensen, Lauritzen and Ole-sen algorithm for probability propagation in graphical models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9365056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72247e4e34de0dd2d0428522ded24b49fb1632be",
            "isKey": true,
            "numCitedBy": 1772,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Complexity-in-Statistical-Inquiry-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific Series in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726068"
                        ],
                        "name": "F. Radermacher",
                        "slug": "F.-Radermacher",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Radermacher",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Radermacher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 111
                            }
                        ],
                        "text": "This can be deter-mined by applying the semantics of directed graphs, in particular the d-separationcriterion (Pearl, 1988), to the graphical model in Figure 1 (b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 116
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations are a particularclass of probabilistic graphical model (Pearl, 1988; Lauritzen & Spiegelhalter, 1988),which represent probability distributions as graphs in which the nodes correspondto random variables and the links represent conditional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "Standard methods from belief networks suggest approximating such large matrices with \\noisy-OR\" (Pearl, 1988) or \\sigmoid\" (Neal, 1992) models of interaction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "Standard methods from graphical models suggest approximatingsuch large matrices with \\noisy-OR\" (Pearl, 1988) or \\sigmoid\" (Neal, 1992)modelsof interaction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 121
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations can be considered a special case of Bayesian belief networks (Pearl, 1988; Lauritzen and Spiegelhalter, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9998243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "808eaed9facd1379e4d5efd49ed9e634edca6e35",
            "isKey": true,
            "numCitedBy": 104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Reasoning-in-Intelligent-Systems:-of-Radermacher",
            "title": {
                "fragments": [],
                "text": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Judea Pearl)"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 107
                            }
                        ],
                        "text": "For log-linearmodels, the M step can be solved using an inner loop of iteratively reweightedleast-squares (McCullagh & Nelder, 1989).5.4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized l i n e ar models"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized l i n e ar models"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 100
                            }
                        ],
                        "text": "The relation between hidden Markov models and graphical models has recently been reviewed in Smyth, Heckerman and Jordan (1997). Although exact probability propagation algorithms exist for general graphical models (Jensen, Lauritzen, & Olesen, 1990), these algorithms are intractable for densely-connected models such as the ones we consider in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Smyth, P., Heckerman, D., & Jordan, M. I. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 92
                            }
                        ],
                        "text": "The relation between hidden Markov models and graphical models has recentlybeen reviewed in Smyth, Heckerman and Jordan (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "Structure learning is atopic of current research in both the graphical model and machine learning commu-nities (e.g. Heckerman, 1995; Stolcke & Omohundro, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A tutorial on learning Bayesian networks. (Technical Report MSR-TR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 197
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 132
                            }
                        ],
                        "text": "The goal of the learner is to communicate the data e ciently to a receiver, thereby producing a compact representation for the data (Zemel, 1993; Hinton and Zemel, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 210
                            }
                        ],
                        "text": "The models considered in this paper are also closely related to recent work in unsupervised learning which has focused on the problem of discovering multiple independent causes, or factors, underlying the data (Barlow, 1989; Redlich, 1993; Zemel, 1993; Saund, 1995; Hinton et al., 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 157
                            }
                        ],
                        "text": "The application of MDL principles to the problem of learning a factorial code has resulted in a learning architecture called cooperative vector quantization (CVQ; Zemel, 1993; Hinton and Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 193
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Jordan, M. I., & Jacobs, R. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "This constraint canbe relaxed by introducing couplings between the hidden state variables (cf. Saul &Jordan, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 198
                            }
                        ],
                        "text": "This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right (Zemel, 1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "30 Z. GHAHRAMANI AND M.I. JORDANJordan, M. I., Ghahramani, Z., & Saul, L. K. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 28
                            }
                        ],
                        "text": "Smyth, P., Heckerman, D., & Jordan, M. I. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "Meila, M., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "We explorethis generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997).6."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Saul, L., & Jordan, M. I. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 60
                            }
                        ],
                        "text": "We explore this generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 149
                            }
                        ],
                        "text": "Another approach,which provides the basis for algorithms described in the current paper, is to makeuse of variational methods (cf. Saul, Jaakkola, & Jordan, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In M. Mozer,M. Jordan, & T. Petsche (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Saul, L., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 231
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "Saul, L. K., & Jordan, M. I. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "Ft.Lauderdale, FL.Saul, L., Jaakkola, T., & Jordan, M. I. (1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factorial learning and the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126123958,
            "fieldsOfStudy": [],
            "id": "358fe49d4eab63e89ce9ca21de4f35ea27d078e4",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026of the learning problem, in which the parameters are also consid-ered hidden random variables, can be handled by Gibbs sampling by replacing the \\M step\"with sampling from the conditional distribution of the parameters given the other hiddenvariables (for example, see Tanner and Wong, 1987).5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124151931,
            "fieldsOfStudy": [],
            "id": "0abe8ce4e7cc7afc40ff1e5309b4a9162169226d",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Calculation of Posterior Distributions by Data Augmentation: Rejoinder"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102329511"
                        ],
                        "name": "George W. Soules",
                        "slug": "George-W.-Soules",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Soules",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George W. Soules"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063108982"
                        ],
                        "name": "Norman Weiss",
                        "slug": "Norman-Weiss",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norman Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122568650,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4",
            "isKey": false,
            "numCitedBy": 4551,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Maximization-Technique-Occurring-in-the-Analysis-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2810659"
                        ],
                        "name": "K. Cios",
                        "slug": "K.-Cios",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Cios",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cios"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145895848"
                        ],
                        "name": "M. E. Shields",
                        "slug": "M.-E.-Shields",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Shields",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Shields"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 231
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 78
                            }
                        ],
                        "text": "First, for the sizeof state space we wished to explore, the exact algorithms were prohibitively slow."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120130418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13c1b1f6dcad68d92b15ce87333c2cc6593c9263",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-neural-information-processing-systems-7-Cios-Shields",
            "title": {
                "fragments": [],
                "text": "Advances in neural information processing systems 7"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 69371331,
            "fieldsOfStudy": [],
            "id": "bc1c64b80c6bb204ffe2467dab0f87afec40896a",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "An alternative view given by Neal and Hinton (1993) describes EM in terms of thenegative free energy, F , which is a function of the parameters, , the observations,Y , and a posterior probability distribution over the hidden variables, Q(S):F (Q; ) = EQ flogP (Y; Sj )g EQ flogQ(S)g ;where EQ\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62562212,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "418cc44768ff9d0ed8cf4cef79869f90ab672f7b",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-view-of-the-EM-algorithm-that-justifies-and-Neal",
            "title": {
                "fragments": [],
                "text": "A new view of the EM algorithm that justifies incremental and other variants"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Choice of approximationThe theory of the EM algorithm as presented in Dempster et al. (1977) assumesthe use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1664,
                                "start": 71
                            }
                        ],
                        "text": "Choice of approximation The theory of the EM algorithm as presented in Dempster et al. (1977) assumes the use of an exact E step. For models in which the exact E step is intractable, one must instead use an approximation like those we have just described. The choice among these approximations must take into account several theoretical and practical issues. Monte Carlo approximations based on Markov chains, such as Gibbs sampling, o er the theoretical assurance that the sampling procedure will converge to the correct posterior distribution in the limit. Although this means that one can come arbitrarily close to the exact E step, in practice convergence can be slow (especially for multimodal distributions) and it is often very di cult to determine how close one is to convergence. However, when sampling is used for the E step of EM, there is a time tradeo between the number of samples used and the number of EM iterations. It seems wasteful to wait until convergence early on in learning, when the posterior distribution from which samples are drawn is far from the posterior given the optimal parameters. In practice we have found that even approximate E steps using very few Gibbs samples (e.g. around ten samples of each hidden variable) tend to increase the true likelihood. Variational approximations o er the theoretical assurance that a lower bound on the likelihood is being maximized. Both the minimization of the KL divergence in the E step and the parameter update in the M step are guaranteed not to decrease this lower bound, and therefore convergence can be de ned in terms of the bound. An alternative view given by Neal and Hinton (1993) describes EM in terms of the negative free energy, F , which is a function of the parameters, , the observations, Y , and a posterior probability distribution over the hidden variables, Q(S): F (Q; ) = EQ flogP (Y; Sj )g EQ flogQ(S)g ; where EQ denotes expectation over S using the distribution Q(S)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 71
                            }
                        ],
                        "text": "Choice of approximation The theory of the EM algorithm as presented in Dempster et al. (1977) assumes the use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60618317,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ecb37a4e32d6faef4ac99b45d9ab9b2d92693985",
            "isKey": false,
            "numCitedBy": 1169,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Max-imum-Likelihood-from-Incomplete-Data-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Max-imum Likelihood from Incomplete Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60256361,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b073e08b683640aebeed4e589efaeb9c90cc8482",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-models-:-proceedings-of-the-1990-Touretzky",
            "title": {
                "fragments": [],
                "text": "Connectionist models : proceedings of the 1990 summer school"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144745619"
                        ],
                        "name": "S. German",
                        "slug": "S.-German",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "German",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. German"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121982077"
                        ],
                        "name": "D. German",
                        "slug": "D.-German",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "German",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. German"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 203
                            }
                        ],
                        "text": "Assuming that all probabilitiesare bounded away from zero, this Markov chain is guaranteed to converge to the\n8 Z. GHAHRAMANI AND M.I. JORDANposterior probabilities of the states given the observations (Geman & Geman, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "Although there are manypossible sampling schemes (for a review see Neal, 1993), here we present one of thesimplest|Gibbs sampling (Geman & Geman, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59916588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1f9fcf2ccc313a5018e536e76e75d1f7992937b",
            "isKey": false,
            "numCitedBy": 2215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-relaxation,-Gibbs-distributions,-and-the-German-German",
            "title": {
                "fragments": [],
                "text": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for SigmoidBelief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Arti cial Intelligence Research"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "One approach to dealing with this issue isto utilize stochastic sampling methods (Kanazawa et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 82
                            }
                        ],
                        "text": "One approach to dealing with this issue is to utilize stochastic sampling methods (Kanazawa et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation algorithms for dynamic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For related work on inference in distributed state HMMs"
            },
            "venue": {
                "fragments": [],
                "text": "For related work on inference in distributed state HMMs"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributionsby data augmentation ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American StatisticalAssociation"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 51
                            }
                        ],
                        "text": "We explorethis generalization of factorial HMMs in Jordan, Ghahramani, and Saul (1997).6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov decision trees Advances in neural information processing systems 9"
            },
            "venue": {
                "fragments": [],
                "text": "Hidden Markov decision trees Advances in neural information processing systems 9"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 134
                            }
                        ],
                        "text": "Structure learning is atopic of current research in both the graphical model and machine learning commu-nities (e.g. Heckerman, 1995; Stolcke & Omohundro, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction by Bayesian model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 129
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations are a particularclass of probabilistic graphical model (Pearl, 1988; Lauritzen & Spiegelhalter, 1988),which represent probability distributions as graphs in which the nodes correspondto random variables and the links represent conditional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 152
                            }
                        ],
                        "text": "This fact can best be shown by making reference to standard algorithms for prob-\nFACTORIAL HIDDEN MARKOV MODELS 7abilistic inference in graphical models (Lauritzen & Spiegelhalter, 1988), althoughit can also be derived readily from direct application of Bayes rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 121
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations can be considered a special case of Bayesian belief networks (Pearl, 1988; Lauritzen and Spiegelhalter, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local computations with probabil"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 109
                            }
                        ],
                        "text": "This procedure, which can be seen as a particular form of Gibbs sampling, is also known as data augmentation (Tanner and Wong, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026of the learning problem, in which the parameters are also consid-ered hidden random variables, can be handled by Gibbs sampling by replacing the \\M step\"with sampling from the conditional distribution of the parameters given the other hiddenvariables (for example, see Tanner and Wong, 1987).5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 129
                            }
                        ],
                        "text": "Hidden Markov models with distributed state representations are a particularclass of probabilistic graphical model (Pearl, 1988; Lauritzen & Spiegelhalter, 1988),which represent probability distributions as graphs in which the nodes correspondto random variables and the links represent conditional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 152
                            }
                        ],
                        "text": "This fact can best be shown by making reference to standard algorithms for prob-\nFACTORIAL HIDDEN MARKOV MODELS 7abilistic inference in graphical models (Lauritzen & Spiegelhalter, 1988), althoughit can also be derived readily from direct application of Bayes rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local computationswith probabilities on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Choice of approximationThe theory of the EM algorithm as presented in Dempster et al. (1977) assumesthe use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete datavia the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "J . Royal Statistical Society Series B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks and the bias/variance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "One approach to dealing with this issue isto utilize stochastic sampling methods (Kanazawa et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "One approach to dealing with this issue isto utilize stochastic sampling methods (Kanazawa et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "An alternative view given by Neal and Hinton (1993) describes EM in terms of thenegative free energy, F , which is a function of the parameters, , the observations,Y , and a posterior probability distribution over the hidden variables, Q(S):F (Q; ) = EQ flogP (Y; Sj )g EQ flogQ(S)g ;where EQ\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new view of the EM algorithm that justi es incremental"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks and the bias / variancedilemma"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 250
                            }
                        ],
                        "text": "This problem can be solved e cientlyvia the forward{backward algorithm (Rabiner & Juang, 1986), which can be shownto be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm forprobability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 252
                            }
                        ],
                        "text": "This problem can be solved e ciently via the forward{backward algorithm (Rabiner & Juang, 1986), which can be shown to be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm for probability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic independence networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks and thebias / variance dilemma"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 106
                            }
                        ],
                        "text": "Structure learning is a topic of current research in both belief network and machine learning communities (e.g. Pearl, 1988; Stolcke and Omohundro, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 134
                            }
                        ],
                        "text": "Structure learning is atopic of current research in both the graphical model and machine learning commu-nities (e.g. Heckerman, 1995; Stolcke & Omohundro, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction by Bayesian"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphical models by local computations"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Statistical Quarterly"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction by Bayesianmodelmerging"
            },
            "venue": {
                "fragments": [],
                "text": "Advancesin Neural Information Processing Systems"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for Boltzmannmachines"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning ne motion by M a r k ov mixtures of experts"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems 8"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "This can be achieved viathe Viterbi (1967) algorithm, a form of dynamic programming that is very closelyrelated to the forward{backward algorithm and also has analogues in the graphicalmodel literature (Dawid, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting tractable substructures in Intractable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "Williams and Hinton (1991) rst formulated the problem of learning in HMMswith distributed state representations and proposed a solution based on determinis-tic Boltzmann learning.1 The approach presented in this paper is similar to Williamsand Hinton's in that it can also be viewed from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 167
                            }
                        ],
                        "text": "On the other hand, an HMM with adistributed state representation could achieve the same task with 30 binary state\n2 Z. GHAHRAMANI AND M.I. JORDANvariables (Williams & Hinton, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mean eld networks that learn to discriminate temporallydistorted strings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 176
                            }
                        ],
                        "text": "All algorithms appeared to be very sensitive to this randomseed, suggesting that di erent runs on each training set found di erent local maximaor plateaus of the likelihood (Figure 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 134
                            }
                        ],
                        "text": "Structure learning is atopic of current research in both the graphical model and machine learning commu-nities (e.g. Heckerman, 1995; Stolcke & Omohundro, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction by B a yesian model merging"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems 5"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Choice of approximationThe theory of the EM algorithm as presented in Dempster et al. (1977) assumesthe use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 99
                            }
                        ],
                        "text": "The parameters of a factorial HMM can be estimated via the Expectation Maximization (EM) algorithm (Dempster et al., 1977), which in the case of HMMs is known as the Baum{Welch algorithm (Baum et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In Section 4 wedescribe empirical results comparing exact and approximate algorithms for learningon the basis of time complexity and model quality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of nite mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical analysis of nite mixture distributions"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 301
                            }
                        ],
                        "text": "Given a sequence of input vectors fXtg, the probabilistic model for an input-conditioned factorial HMM is P (fSt; YtgjfXtg) = M Y m=1P (S(m) 1 jX1)P (Y1jS(m) 1 ;X1) T Y t=2 M Y m=1P (S(m) t jS(m) t 1 ;Xt)P (YtjS(m) t ;Xt): (19) 1This is analogous to the fully-connected Boltzmann machine with N units (Ackley et al., 1985), in which every binary unit is coupled to every other unit using O(N2) parameters, rather than the O(2N ) parameters required to specify the complete probability table."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 27
                            }
                        ],
                        "text": "Like in Boltzmann machines (Ackley et al., 1985), both a clamped and an unclamped phase are therefore required for learning, where the goal of the unclamped phase is to compute the derivative of the partition function with respect to the parameters (Neal, 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for Boltzmann"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026of the learning problem, in which the parameters are also consid-ered hidden random variables, can be handled by Gibbs sampling by replacing the \\M step\"with sampling from the conditional distribution of the parameters given the other hiddenvariables (for example, see Tanner and Wong, 1987).5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions by d a t a augmentation (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Statistical Association"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In speech, neural networks are generally used to model P(StjYt); this probability is converted to the observation probabilities needed in the HMM via Bayes rule"
            },
            "venue": {
                "fragments": [],
                "text": "In speech, neural networks are generally used to model P(StjYt); this probability is converted to the observation probabilities needed in the HMM via Bayes rule"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026of the learning problem, in which the parameters are also consid-ered hidden random variables, can be handled by Gibbs sampling by replacing the \\M step\"with sampling from the conditional distribution of the parameters given the other hiddenvariables (for example, see Tanner and Wong, 1987).5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions by dataaugmentation ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Statistical Association"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": ", 1977), which in the case of HMMs is known as the Baum{Welch algorithm (Baum et al., 1970)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 142
                            }
                        ],
                        "text": "This problem can be solved e cientlyvia the forward{backward algorithm (Rabiner & Juang, 1986), which can be shownto be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm forprobability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 142
                            }
                        ],
                        "text": "This problem can be solved e cientlyvia the forward{backward algorithm (Rabiner & Juang, 1986), which can be shownto be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm forprobability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": "The need for distributed state representations in HMMs can be motivated in twoways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphical models by local computations"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Statistical Quarterly"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 142
                            }
                        ],
                        "text": "This problem can be solved e cientlyvia the forward{backward algorithm (Rabiner & Juang, 1986), which can be shownto be a special case of the Jensen, Lauritzen, and Olesen (1990) algorithm forprobability propagation in more general graphical models (Smyth et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphicalmodels by local computations"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Statistical Quarterly"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 160
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An input{outputHMM architecture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 135
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models which are conditioned on inputs have been recently presented in the literature (Cacciatore and Nowlan, 1994; Bengio and Frasconi, 1995; Meila and Jordan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning ne motion by Markov mixtures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 132
                            }
                        ],
                        "text": "The goal of the learner is to communicate the data e ciently to a receiver, thereby producing a compact representation for the data (Zemel, 1993; Hinton and Zemel, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 157
                            }
                        ],
                        "text": "The application of MDL principles to the problem of learning a factorial code has resulted in a learning architecture called cooperative vector quantization (CVQ; Zemel, 1993; Hinton and Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Autoencoders, minimum description"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique occurring"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphical models by local computations.Computational"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Williams and Hinton (1991) rst formulated the problem of learning in HMMswith distributed state representations and proposed a solution based on determinis-tic Boltzmann learning.1 The approach presented in this paper is similar to Williamsand Hinton's in that it can also be viewed from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 114
                            }
                        ],
                        "text": "On the other hand an HMM with a distributed state representation could achieve the same task with 30 binary units (Williams and Hinton, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "On the other hand, an HMM with adistributed state representation could achieve the same task with 30 binary state\n2 Z. GHAHRAMANI AND M.I. JORDANvariables (Williams & Hinton, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mean eld networks that learn to discrimi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 197
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 2
                            }
                        ],
                        "text": "Therefore, more structured variationalapproximations can be obtained by using more structured variational distributionsQ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised l e arning"
            },
            "venue": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised l e arning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "This static mixture model, withoutinclusion of the time index and the Markov dynamics, is a factorial parameterizationof the standard mixture of Gaussians model that has interest in its own right (Zemel,1993; Hinton & Zemel, 1994; Ghahramani, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Autoencoders , minimumdescription length , and Helmholtzfree energy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 195
                            }
                        ],
                        "text": "(8) This completely factorized approximation is often used in statistical physics, where it provides the basis for simple yet powerful mean field approximations to statistical mechanical systems (Parisi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 276
                            }
                        ],
                        "text": "\u2026Q:Q(S(m)t j (m)t ) = KYk=1 (m)t;k S(m)t;k where S(m)t;k 2 f0; 1g; KXk=1S(m)t;k = 1: (8)This completely factorized approximation is often used in statistical physics, whereit provides the basis for simple yet powerful mean eld approximations to statisticalmechanical systems (Parisi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1988).Statistical field theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Choice of approximationThe theory of the EM algorithm as presented in Dempster et al. (1977) assumesthe use of an exact E step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via theEM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society Series B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Several algorithms for learning in hidden Markov models that are conditioned oninputs have been recently presented in the literature (Cacciatore & Nowlan, 1994;Bengio & Frasconi, 1995; Meila & Jordan, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning ne motion by Markov mixtures ofexperts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 444,
                                "start": 430
                            }
                        ],
                        "text": "The elements of the vector (m) t therefore de ne the state occupation probabilities for the multinomial variable S(m) t under the distribution Q: Q(S(m) t j (m) t ) = K Y k=1 (m) t;k S(m) t;k where S(m) t;k 2 f0; 1g; K Xk=1S(m) t;k = 1: (8) This completely factorized approximation is often used in statistical physics, where it provides the basis for simple yet powerful mean eld approximations to statistical mechanical systems (Parisi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 31
                            }
                        ],
                        "text": "A di erent approach comes from Saul and Jordan (1995), who derived a set of rules for computing the gradients required for learning in HMMs with distributed state spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 276
                            }
                        ],
                        "text": "\u2026Q:Q(S(m)t j (m)t ) = KYk=1 (m)t;k S(m)t;k where S(m)t;k 2 f0; 1g; KXk=1S(m)t;k = 1: (8)This completely factorized approximation is often used in statistical physics, whereit provides the basis for simple yet powerful mean eld approximations to statisticalmechanical systems (Parisi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical eld theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 35,
            "methodology": 49
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 106,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Factorial-Hidden-Markov-Models-Ghahramani-Jordan/78e510627d3f28601e370212bf063bbfa539ebed?sort=total-citations"
}