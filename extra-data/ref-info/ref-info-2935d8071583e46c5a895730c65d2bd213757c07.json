{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144315453"
                        ],
                        "name": "Mingtao Pei",
                        "slug": "Mingtao-Pei",
                        "structuredName": {
                            "firstName": "Mingtao",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingtao Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7415267"
                        ],
                        "name": "Yunde Jia",
                        "slug": "Yunde-Jia",
                        "structuredName": {
                            "firstName": "Yunde",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunde Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "It has been employed in computer vision to model objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "We perform temporal parsing following the approach proposed in [9], which is based on the Earley parser [54]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "AOGs has been employed in computer vision to model the hierarchical decompositions of objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "\u2022 A temporal And-Or graph (T-AOG) [9] models the temporal decompositions of events to sub-events and atomic actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "A temporal And-Or graph (T-AOG) [9] models the hierarchical decompositions from events to sub-events and then to atomic actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "It is also guided by a spatial-temporal-causal And-Or graph (S/T/C-AOG) [6], [7], [9], [10], which models the compositional structures of objects, scenes and events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 258
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1526327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "054e8684578eb6f85cabcfb31ada42a9b7ec8fd6",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an event parsing algorithm based on Stochastic Context Sensitive Grammar (SCSG) for understanding events, inferring the goal of agents, and predicting their plausible intended actions. The SCSG represents the hierarchical compositions of events and the temporal relations between the sub-events. The alphabets of the SCSG are atomic actions which are defined by the poses of agents and their interactions with objects in the scene. The temporal relations are used to distinguish events with similar structures, interpolate missing portions of events, and are learned from the training data. In comparison with existing methods, our paper makes the following contributions. i) We define atomic actions by a set of relations based on the fluents of agents and their interactions with objects in the scene. ii) Our algorithm handles events insertion and multi-agent events, keeps all possible interpretations of the video to preserve the ambiguities, and achieves the globally optimal parsing solution in a Bayesian framework; iii) The algorithm infers the goal of the agents and predicts their intents by a top-down process; iv) The algorithm improves the detection of atomic actions by event contexts. We show satisfactory results of event recognition and atomic action detection on the data set we captured which contains 12 event categories in both indoor and outdoor videos."
            },
            "slug": "Parsing-video-events-with-goal-inference-and-intent-Pei-Jia",
            "title": {
                "fragments": [],
                "text": "Parsing video events with goal inference and intent prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An event parsing algorithm based on Stochastic Context Sensitive Grammar for understanding events, inferring the goal of agents, and predicting their plausible intended actions achieves the globally optimal parsing solution in a Bayesian framework."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110901865"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[65] to generate text descriptions from our joint parse graphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6023198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05e074abddd3fe987b9bebd46f6cf4bf8465c37e",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding. The proposed I2T framework follows three steps: 1) input images (or video frames) are decomposed into their constituent visual patterns by an image parsing engine, in a spirit similar to parsing sentences in natural language; 2) the image parsing results are converted into semantic representation in the form of Web ontology language (OWL), which enables seamless integration with general knowledge bases; and 3) a text generation engine converts the results from previous steps into semantically meaningful, human readable, and query-able text reports. The centerpiece of the I2T framework is an and-or graph (AoG) visual knowledge representation, which provides a graphical representation serving as prior knowledge for representing diverse visual patterns and provides top-down hypotheses during the image parsing. The AoG embodies vocabularies of visual elements including primitives, parts, objects, scenes as well as a stochastic image grammar that specifies syntactic relations (i.e., compositional) and semantic relations (e.g., categorical, spatial, temporal, and functional) between these visual elements. Therefore, the AoG is a unified model of both categorical and symbolic representations of visual knowledge. The proposed I2T framework has two objectives. First, we use semiautomatic method to parse images from the Internet in order to build an AoG for visual knowledge representation. Our goal is to make the parsing process more and more automatic using the learned AoG model. Second, we use automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications. In the case studies at the end of this paper, we demonstrate two automatic I2T systems: a maritime and urban scene video surveillance system and a real-time automatic driving scene understanding system."
            },
            "slug": "I2T:-Image-Parsing-to-Text-Description-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "I2T: Image Parsing to Text Description"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding and uses automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1752880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca9200d9da958b7f90eab901b2f30c04f1e0e9c",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches\u2014generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns\u2014generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu and Zhu, 2002. IEEE Trans. PAMI, 24(5):657\u2013673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions."
            },
            "slug": "Image-Parsing:-Unifying-Segmentation,-Detection,-Tu-Chen",
            "title": {
                "fragments": [],
                "text": "Image Parsing: Unifying Segmentation, Detection, and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian framework for parsing images into their constituent visual patterns that optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807123"
                        ],
                        "name": "Zhangzhang Si",
                        "slug": "Zhangzhang-Si",
                        "structuredName": {
                            "firstName": "Zhangzhang",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangzhang Si"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144315453"
                        ],
                        "name": "Mingtao Pei",
                        "slug": "Mingtao-Pei",
                        "structuredName": {
                            "firstName": "Mingtao",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingtao Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "These energy terms can be either manually specified by domain experts or learned from data [52], [53], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1090654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f710fb18b85aa2a9b4a73f6d0a81746b50d8c0ff",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of automatically learning event AND-OR grammar from videos of a certain environment, e.g. an office where students conduct daily activities. We propose to learn the event grammar under the information projection and minimum description length principles in a coherent probabilistic framework, without manual supervision about what events happen and when they happen. Firstly a predefined set of unary and binary relations are detected for each video frame: e.g. agent's position, pose and interaction with environment. Then their co-occurrences are clustered into a dictionary of simple and transient atomic actions. Recursively these actions are grouped into longer and complexer events, resulting in a stochastic event grammar. By modeling time constraints of successive events, the learned grammar becomes context-sensitive. We introduce a new dataset of surveillance-style video in office, and present a prototype system for video analysis integrating bottom-up detection, grammatical learning and parsing. On this dataset, the learning algorithm is able to automatically discover important events and construct a stochastic grammar, which can be used to accurately parse newly observed video. The learned grammar can be used as a prior to improve the noisy bottom-up detection of atomic actions. It can also be used to infer semantics of the scene. In general, the event grammar is an efficient way for common knowledge acquisition from video."
            },
            "slug": "Unsupervised-learning-of-event-AND-OR-grammar-and-Si-Pei",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of event AND-OR grammar and semantics from video"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work proposes to learn the event grammar under the information projection and minimum description length principles in a coherent probabilistic framework, without manual supervision about what events happen and when they happen, and presents a prototype system for video analysis integrating bottom-up detection, grammatical learning and parsing."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40374464"
                        ],
                        "name": "G. Iyengar",
                        "slug": "G.-Iyengar",
                        "structuredName": {
                            "firstName": "Giridharan",
                            "lastName": "Iyengar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Iyengar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238630"
                        ],
                        "name": "P. Ircing",
                        "slug": "P.-Ircing",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Ircing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ircing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561225"
                        ],
                        "name": "D. Klakow",
                        "slug": "D.-Klakow",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "Klakow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klakow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053721883"
                        ],
                        "name": "M. R. Krause",
                        "slug": "M.-R.-Krause",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Krause",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1956222"
                        ],
                        "name": "H. Nock",
                        "slug": "H.-Nock",
                        "structuredName": {
                            "firstName": "Harriet",
                            "lastName": "Nock",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35522796"
                        ],
                        "name": "D. Petkova",
                        "slug": "D.-Petkova",
                        "structuredName": {
                            "firstName": "Desislava",
                            "lastName": "Petkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227927"
                        ],
                        "name": "Brock Pytlik",
                        "slug": "Brock-Pytlik",
                        "structuredName": {
                            "firstName": "Brock",
                            "lastName": "Pytlik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brock Pytlik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2763470"
                        ],
                        "name": "Paola Virga",
                        "slug": "Paola-Virga",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Virga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Virga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] modeled videos and text jointly for the purpose of multimedia information retrieval; Yang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3833221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d55de6dcdfde200b9a975578ffe8cb5c056e2c76",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a novel approach for jointly modeling the text and the visual components of multimedia documents for the purpose of information retrieval(IR). We propose a novel framework where individual components are developed to model different relationships between documents and queries and then combined into a joint retrieval framework. In the state-of-the-art systems, a late combination between two independent systems, one analyzing just the text part of such documents, and the other analyzing the visual part without leveraging any knowledge acquired in the text processing, is the norm. Such systems rarely exceed the performance of any single modality (i.e. text or video) in information retrieval tasks. Our experiments indicate that allowing a rich interaction between the modalities results in significant improvement in performance over any single modality. We demonstrate these results using the TRECVID03 corpus, which comprises 120 hours of broadcast news videos. Our results demonstrate over 14 % improvement in IR performance over the best reported text-only baseline and ranks amongst the best results reported on this corpus."
            },
            "slug": "Joint-visual-text-modeling-for-automatic-retrieval-Iyengar-Sahin",
            "title": {
                "fragments": [],
                "text": "Joint visual-text modeling for automatic retrieval of multimedia documents"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A novel framework where individual components are developed to model different relationships between documents and queries and then combined into a joint retrieval framework is proposed, which demonstrates over 14 % improvement in IR performance over the best reported text-only baseline and ranks amongst the best results reported on this corpus."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48858384"
                        ],
                        "name": "William Brendel",
                        "slug": "William-Brendel",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Brendel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Brendel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841336"
                        ],
                        "name": "Alan Fern",
                        "slug": "Alan-Fern",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Fern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Fern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 357
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8097011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bda2bb50f3b1803b09bf54f0206c2db0161eccc1",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is about detecting and segmenting interrelated events which occur in challenging videos with motion blur, occlusions, dynamic backgrounds, and missing observations. We argue that holistic reasoning about time intervals of events, and their temporal constraints is critical in such domains to overcome the noise inherent to low-level video representations. For this purpose, our first contribution is the formulation of probabilistic event logic (PEL) for representing temporal constraints among events. A PEL knowledge base consists of confidence-weighted formulas from a temporal event logic, and specifies a joint distribution over the occurrence time intervals of all events. Our second contribution is a MAP inference algorithm for PEL that addresses the scalability issue of reasoning about an enormous number of time intervals and their constraints in a typical video. Specifically, our algorithm leverages the spanning-interval data structure for compactly representing and manipulating entire sets of time intervals without enumerating them. Our experiments on interpreting basketball videos show that PEL inference is able to jointly detect events and identify their time intervals, based on noisy input from primitive-event detectors."
            },
            "slug": "Probabilistic-event-logic-for-interval-based-event-Brendel-Fern",
            "title": {
                "fragments": [],
                "text": "Probabilistic event logic for interval-based event recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper argues that holistic reasoning about time intervals of events, and their temporal constraints is critical in such domains to overcome the noise inherent to low-level video representations and proposes a MAP inference algorithm for PEL that addresses the scalability issue of reasoning about an enormous number of time intervals and their constraints in a typical video."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005568"
                        ],
                        "name": "Asaad Hakeem",
                        "slug": "Asaad-Hakeem",
                        "structuredName": {
                            "firstName": "Asaad",
                            "lastName": "Hakeem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asaad Hakeem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2805147"
                        ],
                        "name": "O. Javed",
                        "slug": "O.-Javed",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Javed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Javed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085175"
                        ],
                        "name": "N. Haering",
                        "slug": "N.-Haering",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Haering",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Haering"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The Earley parsing algorithm [54], [58] is used to parse the dependencies using the attribute grammar, which employs topdown dynamic programming to search for a valid set of production rules that generate the dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16586089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ee034ad25fe2f0b739fc7fb342dfde6e4fc8cbf",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in computer vision and artificial intelligence algorithms have allowed automatic extraction of metadata from video. This metadata can be represented by using the RDF/OWL ontology which can encode scene objects and their relationships in an unambiguous and well-formed manner. The encoded data can be queried using SPARQL. However, SPARQL has a steep learning curve and cannot be directly utilized by a general user for video content search. In this paper, we propose a method to bridge this gap by automatically translating user provided natural language query into an ontology-based SPARQL query for semantic video search. The proposed method consists of three major steps. First, semantically labeled training corpus of natural language query sentences is used for learning the Semantic Stochastic Context Free Grammar (SSCFG). Second, given a user provided natural language query sentence, we use the Earley-Stolcke parsing algorithm to determine the maximum likelihood semantic parsing of the query sentence. This parsing infers the semantic meaning for each word in the query sentence from which the SPARQL query is constructed. Third, the SPARQL query is executed to retrieve relevant video segments from the RDF-OWL video content database. The method is evaluated by running natural language queries on surveillance videos from maritime and land-based domains, though the framework itself is general and extensible to search videos from other domains."
            },
            "slug": "Semantic-video-search-using-natural-language-Hakeem-Lee",
            "title": {
                "fragments": [],
                "text": "Semantic video search using natural language queries"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a method to bridge the gap by automatically translating user provided natural language query into an ontology-based SPARQL query for semantic video search, which is evaluated by running natural language queries on surveillance videos from maritime and land-based domains."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46317361"
                        ],
                        "name": "Yibiao Zhao",
                        "slug": "Yibiao-Zhao",
                        "structuredName": {
                            "firstName": "Yibiao",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yibiao Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "It has been employed in computer vision to model objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "We first perform spatial parsing on each video frame following the approach proposed in [7] called hierarchical cluster sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "AOGs has been employed in computer vision to model the hierarchical decompositions of objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "(b) An example spatial parse graph from [7], which is a realization of the S-AOG by making selections at Or-nodes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "A spatial And-Or graph (S-AOG) [6], [7] models the spatial decompositions of objects and scenes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "It is also guided by a spatial-temporal-causal And-Or graph (S/T/C-AOG) [6], [7], [9], [10], which models the compositional structures of objects, scenes and events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "\u2022 A spatial And-Or graph (S-AOG) [6], [7] models the spatial decompositions of objects and scenes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34350854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d71414ef4ad577555fc6b3fdad26d6376503bf0c",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classifiers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative \"+\" relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive \"-\" relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efficient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene configurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to find the most probable configuration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree."
            },
            "slug": "Image-Parsing-with-Stochastic-Scene-Grammar-Zhao-Zhu",
            "title": {
                "fragments": [],
                "text": "Image Parsing with Stochastic Scene Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An efficient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene configurations, and achieves richer structures in the parse tree."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005568"
                        ],
                        "name": "Asaad Hakeem",
                        "slug": "Asaad-Hakeem",
                        "structuredName": {
                            "firstName": "Asaad",
                            "lastName": "Hakeem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asaad Hakeem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774867"
                        ],
                        "name": "Yaser Sheikh",
                        "slug": "Yaser-Sheikh",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Sheikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaser Sheikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "oral parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and \ufb02uent changes [33], [34], [10]. Our work also involves semantic parsing of text descriptions, which is related to a variety of semantic parsing approaches studied in the natural language processing community, for examp"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 960755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c37223eb7280047960239c47e16aad1dc42b3e9",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A representational gap exists between low-level measurements (segmentation, object classification, tracking) and high-level understanding of video sequences. In this paper, we propose a novel representation of events in videos to bridge this gap, based on the CASE representation of natural languages. The proposed representation has three significant contributions over existing frameworks. First, we recognize the importance of causal and temporal relationships between subevents and extend CASE to allow the representation of temporal structure and causality between sub-events. Second, in order to capture both multi-agent and multithreaded events, we introduce a hierarchical CASE representation of events in terms of sub-events and case-lists. Last, for purposes of implementation we present the concept of a temporal event-tree, and pose the problem of event detection as subtree pattern matching. By extending CASE, a natural language representation, for the representation of events, the proposed work allows a plausible means of interface between users and the computer. We show two important applications of the proposed event representation for the automated annotation of standard meeting video sequences, and for event detection in extended videos of railroad crossings."
            },
            "slug": "CASEE:-A-Hierarchical-Event-Representation-for-the-Hakeem-Sheikh",
            "title": {
                "fragments": [],
                "text": "CASEE: A Hierarchical Event Representation for the Analysis of Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper proposes a novel representation of events in videos to bridge the representational gap between low-level measurements and high-level understanding of video sequences, based on the CASE representation of natural languages."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 982,
                                "start": 967
                            }
                        ],
                        "text": "For joint processing of image and text, Paek et al. [15] performed scene classification with a TF-IDF based approach applied to both images and the accompanying text; Barnard et al. [2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al. [4] simultaneously modeled the image elements, text annotations and image class labels using a probabilistic topic model; Liu et al. [18] presented an image retrieval system by leveraging large-scale web image and their associated textual descriptions; Jia et al. [19] proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata [20] learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "[19] proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata [20] learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 679163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "194e9a6f02fd5f39226dc9848213479fec5f1821",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 154,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the task of automatically generating captions for images, which is important for many image-related applications. Examples include video and image retrieval as well as the development of tools that aid visually impaired individuals to access pictorial information. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned and colocated with thematically related documents. Our model learns to create captions from a database of news articles, the pictures embedded in them, and their captions, and consists of two stages. Content selection identifies what the image and accompanying article are about, whereas surface realization determines how to verbalize the chosen content. We approximate content selection with a probabilistic image annotation model that suggests keywords for an image. The model postulates that images and their textual descriptions are generated by a shared set of latent variables (topics) and is trained on a weakly labeled dataset (which treats the captions and associated news articles as image labels). Inspired by recent work in summarization, we propose extractive and abstractive surface realization models. Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description. Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
            },
            "slug": "Automatic-Caption-Generation-for-News-Images-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "Automatic Caption Generation for News Images"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description, as well as superior to handwritten captions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [1], [2], [3], [4]), in this work we propose to represent the joint interpretation of video and text in the form of a parse graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] applied a joint model of video/image and text for both automatic image annotation and retrieval; Iyengar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3829888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba4e1089e2c5a1c12e9f6c2686e9c8d1870c718e",
            "isKey": false,
            "numCitedBy": 912,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Retrieving images in response to textual queries requires some knowledge of the semantics of the picture. Here, we show how we can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model. The model assumes that a training set of images or videos along with keyword annotations is provided. Multiple keywords are provided for an image and the specific correspondence between a keyword and an image is not provided. Each image is partitioned into a set of rectangular regions and a real-valued feature vector is computed over these regions. The relevance model is a joint probability distribution of the word annotations and the image feature vectors and is computed using the training set. The word probabilities are estimated using a multiple Bernoulli model and the image feature probabilities using a non-parametric kernel density estimate. The model is then used to annotate images in a test set. We show experiments on both images from a standard Corel data set and a set of video key frames from NIST's video tree. Comparative experiments show that the model performs better than a model based on estimating word probabilities using the popular multinomial distribution. The results also show that our model significantly outperforms previously reported results on the task of image and video annotation."
            },
            "slug": "Multiple-Bernoulli-relevance-models-for-image-and-Feng-Manmatha",
            "title": {
                "fragments": [],
                "text": "Multiple Bernoulli relevance models for image and video annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work shows how it can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model, which significantly outperforms previously reported results on the task of image and video annotation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145194969"
                        ],
                        "name": "Changsheng Xu",
                        "slug": "Changsheng-Xu",
                        "structuredName": {
                            "firstName": "Changsheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40382978"
                        ],
                        "name": "Yifan Zhang",
                        "slug": "Yifan-Zhang",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145377023"
                        ],
                        "name": "Guangyu Zhu",
                        "slug": "Guangyu-Zhu",
                        "structuredName": {
                            "firstName": "Guangyu",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangyu Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459057"
                        ],
                        "name": "Y. Rui",
                        "slug": "Y.-Rui",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Rui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694235"
                        ],
                        "name": "Hanqing Lu",
                        "slug": "Hanqing-Lu",
                        "structuredName": {
                            "firstName": "Hanqing",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanqing Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] performed sports video semantic event detection based on joint analysis of videos and webcast texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18172204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90fbd777cf57096e9292601dfe0dbab30198d40f",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Sports video semantic event detection is essential for sports video summarization and retrieval. Extensive research efforts have been devoted to this area in recent years. However, the existing sports video event detection approaches heavily rely on either video content itself, which face the difficulty of high-level semantic information extraction from video content using computer vision and image processing techniques, or manually generated video ontology, which is domain specific and difficult to be automatically aligned with the video content. In this paper, we present a novel approach for sports video semantic event detection based on analysis and alignment of Webcast text and broadcast video. Webcast text is a text broadcast channel for sports game which is co-produced with the broadcast video and is easily obtained from the Web. We first analyze Webcast text to cluster and detect text events in an unsupervised way using probabilistic latent semantic analysis (pLSA). Based on the detected text event and video structure analysis, we employ a conditional random field model (CRFM) to align text event and video event by detecting event moment and event boundary in the video. Incorporation of Webcast text into sports video analysis significantly facilitates sports video semantic event detection. We conducted experiments on 33 hours of soccer and basketball games for Webcast analysis, broadcast video analysis and text/video semantic alignment. The results are encouraging and compared with the manually labeled ground truth."
            },
            "slug": "Using-Webcast-Text-for-Semantic-Event-Detection-in-Xu-Zhang",
            "title": {
                "fragments": [],
                "text": "Using Webcast Text for Semantic Event Detection in Broadcast Sports Video"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a novel approach for sports video semantic event detection based on analysis and alignment of Webcast text and broadcast video, and employs a conditional random field model (CRFM) to align text event and video event."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": ", [1], [2], [3], [4]), in this work we propose to represent the joint interpretation of video and text in the form of a parse graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 868535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
            },
            "slug": "Matching-Words-and-Pictures-Barnard-Sahin",
            "title": {
                "fragments": [],
                "text": "Matching Words and Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text, is presented, and a number of models for the joint distribution of image regions and words are developed, including several which explicitly learn the correspondence between regions and Words."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075292388"
                        ],
                        "name": "P. Liang",
                        "slug": "P.-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 281
                            }
                        ],
                        "text": "Our work also involves semantic parsing of text descriptions, which is related to a variety of semantic parsing approaches studied in the natural language processing community, for example, a supervised approach [35], an unsupervised approach [36], and a dependency based approach [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 340852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c",
            "isKey": false,
            "numCitedBy": 568,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical forxm and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.Our goal is to instead learn a semantic parser from question\u2013answer pairs, where the logical form is modeled as a latent variable. We develop a new semantic formalism, dependency-based compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The model parameters are estimated using a simple procedure that alternates between beam search and numerical optimization. On two standard semantic parsing benchmarks, we show that our system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms."
            },
            "slug": "Learning-Dependency-Based-Compositional-Semantics-Liang-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning Dependency-Based Compositional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new semantic formalism, dependency-based compositional semantics (DCS) is developed and a log-linear distribution over DCS logical forms is defined and it is shown that the system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824057"
                        ],
                        "name": "Florent Monay",
                        "slug": "Florent-Monay",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Monay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florent Monay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403029865"
                        ],
                        "name": "D. G\u00e1tica-P\u00e9rez",
                        "slug": "D.-G\u00e1tica-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "G\u00e1tica-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00e1tica-P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 458,
                                "start": 436
                            }
                        ],
                        "text": "For joint processing of image and text, Paek et al. [15] performed scene classification with a TF-IDF based approach applied to both images and the accompanying text; Barnard et al. [2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al. [4] simultaneously modeled the image elements, text annotations and image class labels using a probabilistic topic model; Liu et al. [18] presented an image retrieval system by leveraging large-scale web image and their associated textual descriptions; Jia et al. [19] proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata [20] learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": ", [1], [2], [3], [4]), in this work we propose to represent the joint interpretation of video and text in the form of a parse graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 277
                            }
                        ],
                        "text": "[2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15084283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c79b3086598da24bb26f7da043741666b03d9b9",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "To go beyond the query-by-example paradigm in image retrieval, there is a need for semantic indexing of large image collections for intuitive text-based image search. Different models have been proposed to learn the dependencies between the visual content of an image set and the associated text captions, then allowing for the automatic creation of semantic indexes for unannotated images. The task, however, remains unsolved. In this paper, we present three alternatives to learn a probabilistic latent semantic analysis (PLSA) model for annotated images and evaluate their respective performance for automatic image indexing. Under the PLSA assumptions, an image is modeled as a mixture of latent aspects that generates both image features and text captions, and we investigate three ways to learn the mixture of aspects. We also propose a more discriminative image representation than the traditional Blob histogram, concatenating quantized local color information and quantized local texture descriptors. The first learning procedure of a PLSA model for annotated images is a standard expectation-maximization (EM) algorithm, which implicitly assumes that the visual and the textual modalities can be treated equivalently. The other two models are based on an asymmetric PLSA learning, allowing to constrain the definition of the latent space on the visual or on the textual modality. We demonstrate that the textual modality is more appropriate to learn a semantically meaningful latent space, which translates into improved annotation performance. A comparison of our learning algorithms with respect to recent methods on a standard data set is presented, and a detailed evaluation of the performance shows the validity of our framework."
            },
            "slug": "Modeling-Semantic-Aspects-for-Cross-Media-Image-Monay-G\u00e1tica-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "Modeling Semantic Aspects for Cross-Media Image Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents three alternatives to learn a probabilistic latent semantic analysis (PLSA) model for annotated images and evaluates their respective performance for automatic image indexing, and proposes a more discriminative image representation than the traditional Blob histogram."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064563493"
                        ],
                        "name": "Feng Han",
                        "slug": "Feng-Han",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2084421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79f2caf6d8133052ab6aaedde9da5fb1d5e305e3",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple attribute graph grammar as a generative representation for made-made scenes, such as buildings, hallways, kitchens, and living rooms, and studies an effective top-down/bottom-up inference algorithm for parsing images in the process of maximizing a Bayesian posterior probability or equivalently minimizing a description length (MDL). Given an input image, the inference algorithm computes (or constructs) a parse graph, which includes a parse tree for the hierarchical decomposition and a number of spatial constraints. In the inference algorithm, the bottom-up step detects an excessive number of rectangles as weighted candidates, which are sorted in certain order and activate top-down predictions of occluded or missing components through the grammar rules. In the experiment, we show that the grammar and top-down inference can largely improve the performance of bottom-up detection."
            },
            "slug": "Bottom-Up/Top-Down-Image-Parsing-with-Attribute-Han-Zhu",
            "title": {
                "fragments": [],
                "text": "Bottom-Up/Top-Down Image Parsing with Attribute Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A simple attribute graph grammar as a generative representation for made-made scenes, such as buildings, hallways, kitchens, and living rooms, and an effective top-down/bottom-up inference algorithm for parsing images in the process of maximizing a Bayesian posterior probability or equivalently minimizing a description length are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318588"
                        ],
                        "name": "Jake Porway",
                        "slug": "Jake-Porway",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Porway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jake Porway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2565606"
                        ],
                        "name": "Qiongchen Wang",
                        "slug": "Qiongchen-Wang",
                        "structuredName": {
                            "firstName": "Qiongchen",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiongchen Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8801326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f63887e053157e168ed86f779bc31b67d0ae727",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a hierarchical and contextual model for aerial image understanding. Our model organizes objects (cars, roofs, roads, trees, parking lots) in aerial scenes into hierarchical groups whose appearances and configurations are determined by statistical constraints (e.g. relative position, relative scale, etc.). Our hierarchy is a non-recursive grammar for objects in aerial images comprised of layers of nodes that can each decompose into a number of different configurations. This allows us to generate and recognize a vast number of scenes with relatively few rules. We present a minimax entropy framework for learning the statistical constraints between objects and show that this learned context allows us to rule out unlikely scene configurations and hallucinate undetected objects during inference. A\u00a0similar algorithm was proposed for texture synthesis (Zhu et\u00a0al. in Int. J. Comput. Vis. 2:107\u2013126, 1998) but didn\u2019t incorporate hierarchical information. We use a range of different bottom-up detectors (AdaBoost, TextonBoost, Compositional Boosting (Freund and Schapire in J. Comput. Syst. Sci. 55, 1997; Shotton et\u00a0al. in Proceedings of the European Conference on Computer Vision, pp.\u00a01\u201315, 2006; Wu et\u00a0al. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138, 2007)) to propose locations of objects in new aerial images and employ a cluster sampling algorithm (C4 (Porway and Zhu, 2009)) to choose the subset of detections that best explains the image according to our learned prior model. The C4 algorithm can quickly and efficiently switch between alternate competing sub-solutions, for example whether an image patch is better explained by a parking lot with cars or by a building with vents. We also show that our model can predict the locations of objects our detectors missed. We conclude by presenting parsed aerial images and experimental results showing that our cluster sampling and top-down prediction algorithms use the learned contextual cues from our model to improve detection results over traditional bottom-up detectors alone."
            },
            "slug": "A-Hierarchical-and-Contextual-Model-for-Aerial-Porway-Wang",
            "title": {
                "fragments": [],
                "text": "A Hierarchical and Contextual Model for Aerial Image Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A hierarchical and contextual model for aerial image understanding that organizes objects in aerial scenes into hierarchical groups whose appearances and configurations are determined by statistical constraints, and a minimax entropy framework for learning the statistical constraints between objects."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63473035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2dac26f6dacf5679a155f138b44a51081400f25",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Managing large and growing collections of information is a central goal of modern computer science. Data repositories of texts, images, sounds, and genetic information have become widely accessible, thus necessitating good methods of retrieval, organization, and exploration. In this thesis, we describe a suite of probabilistic models of information collections for which the above problems can be cast as statistical queries. \nWe use directed graphical models as a flexible, modular framework for describing appropriate modeling assumptions about the data. Fast approximate posterior inference algorithms based on variational methods free us from having to specify tractable models, and further allow us to take the Bayesian perspective, even in the face of large datasets. \nWith this framework in hand, we describe latent Dirichlet allocation (LDA), a graphical model particularly suited to analyzing text collections. LDA posits a finite index of hidden topics which describe the underlying documents. New documents are situated into the collection via approximate posterior inference of their associated index terms. Extensions to LDA can index a set of images, or multimedia collections of interrelated text and images. \nFinally, we describe nonparametric Bayesian methods for relaxing the assumption of a fixed number of topics, and develop models based on the natural assumption that the size of the index can grow with the collection. This idea is extended to trees, and to models which represent the hidden structure and content of a topic hierarchy that underlies a collection."
            },
            "slug": "Probabilistic-models-of-text-and-images-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Probabilistic models of text and images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A suite of probabilistic models of information collections for which the above problems can be cast as statistical queries are described, and directed graphical models are used as a flexible, modular framework for describing appropriate modeling assumptions about the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48805574"
                        ],
                        "name": "Zhang Zhang",
                        "slug": "Zhang-Zhang",
                        "structuredName": {
                            "firstName": "Zhang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143874948"
                        ],
                        "name": "T. Tan",
                        "slug": "T.-Tan",
                        "structuredName": {
                            "firstName": "Tieniu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887871"
                        ],
                        "name": "Kaiqi Huang",
                        "slug": "Kaiqi-Huang",
                        "structuredName": {
                            "firstName": "Kaiqi",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiqi Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 252
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206764843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91b45e9f29771654316b80635cf67623e8388ca2",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "For a grammar-based approach to the recognition of visual events, there are two major limitations that prevent it from real application. One is that the event rules are predefined by domain experts, which means huge manual cost. The other is that the commonly used grammar can only handle sequential relations between subevents, which is inadequate to recognize more complex events involving parallel subevents. To solve these problems, we propose an extended grammar approach to modeling and recognizing complex visual events. First, motion trajectories as original features are transformed into a set of basic motion patterns of a single moving object, namely, primitives (terminals) in the grammar system. Then, a Minimum Description Length (MDL) based rule induction algorithm is performed to discover the hidden temporal structures in primitive stream, where Stochastic Context-Free Grammar (SCFG) is extended by Allen's temporal logic to model the complex temporal relations between subevents. Finally, a Multithread Parsing (MTP) algorithm is adopted to recognize interesting complex events in a given primitive stream, where a Viterbi-like error recovery strategy is also proposed to handle large-scale errors, e.g., insertion and deletion errors. Extensive experiments, including gymnastic exercises, traffic light events, and multi-agent interactions, have been executed to validate the effectiveness of the proposed approach."
            },
            "slug": "An-Extended-Grammar-System-for-Learning-and-Complex-Zhang-Tan",
            "title": {
                "fragments": [],
                "text": "An Extended Grammar System for Learning and Recognizing Complex Visual Events"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An extended grammar approach to modeling and recognizing complex visual events and a Viterbi-like error recovery strategy is proposed to handle large-scale errors, e.g., insertion and deletion errors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3187096"
                        ],
                        "name": "Jerry R. Hobbs",
                        "slug": "Jerry-R.-Hobbs",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Hobbs",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry R. Hobbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403856055"
                        ],
                        "name": "Rutu Mulkar-Mehta",
                        "slug": "Rutu-Mulkar-Mehta",
                        "structuredName": {
                            "firstName": "Rutu",
                            "lastName": "Mulkar-Mehta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rutu Mulkar-Mehta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "For joint parsing of video and text, Hobbs and MulkarMehta [38] described a preliminary framework that interprets the combination of video and text from news broadcasts, which is the work most closely related to ours."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10880863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46393d7c5b59713945a51f50125e56d61951645e",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe preliminary efforts to use an abductive theoremprover, called Mini-Tacitus, to interpret the combination of video and audio streams of news broadcasts, aiming at a coherent interpretation of the two together. As a side effect, this process identifies entities and events that occur in both streams, across the modalities. Because vision research are not yet quite up to the task of analyzing the video into the required meaningful predicates, we begin with a hand-encoding of the video stream into a logical form; similarly, we begin with a transcript of the audio stream. We translate the transcript into logical form. We then use knowledge of the structure of events and of the environment to infer the most plausible and coherent interpretation of the sentences, the discourse, and the video, in combination."
            },
            "slug": "Using-Abduction-for-Video-Text-Coreference-Hobbs-Mulkar-Mehta",
            "title": {
                "fragments": [],
                "text": "Using Abduction for Video-Text Coreference"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Preliminary efforts to use an abductive theoremprover, called Mini-Tacitus, to interpret the combination of video and audio streams of news broadcasts, aiming at a coherent interpretation of the two together."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862871"
                        ],
                        "name": "M. Salzmann",
                        "slug": "M.-Salzmann",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Salzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Salzmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata [20] learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1256390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85cb25e88d3b0548a26e7a70b6953e500d27eb9a",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Many applications involve multiple-modalities such as text and images that describe the problem of interest. In order to leverage the information present in all the modalities, one must model the relationships between them. While some techniques have been proposed to tackle this problem, they either are restricted to words describing visual objects only, or require full correspondences between the different modalities. As a consequence, they are unable to tackle more realistic scenarios where a narrative text is only loosely related to an image, and where only a few image-text pairs are available. In this paper, we propose a model that addresses both these challenges. Our model can be seen as a Markov random field of topic models, which connects the documents based on their similarity. As a consequence, the topics learned with our model are shared across connected documents, thus encoding the relations between different modalities. We demonstrate the effectiveness of our model for image retrieval from a loosely related text."
            },
            "slug": "Learning-cross-modality-similarity-for-multinomial-Jia-Salzmann",
            "title": {
                "fragments": [],
                "text": "Learning cross-modality similarity for multinomial data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The model can be seen as a Markov random field of topic models, which connects the documents based on their similarity, and the topics learned with the model are shared across connected documents, thus encoding the relations between different modalities."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144411536"
                        ],
                        "name": "Y. Jin",
                        "slug": "Y.-Jin",
                        "structuredName": {
                            "firstName": "Ya",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10695459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d48f8e1739ab85bb409b5a8813787534cabafc",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "It is widely conjectured that the excellent ROC performance of biological vision systems is due in large part to the exploitation of context at each of many levels in a part/whole hierarchy. We propose a mathematical framework (a \"composition machine\") for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, and we build a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport. The demonstration system detects and correctly reads more than 98% of the plates, with a negligible rate of false detection. Unlike a formal grammar, the architecture of a composition machine does not exclude the sharing of sub-parts among multiple entities, and does not limit interpretations to single trees (e.g. a scene can have multiple license plates, or no plates at all). In this sense, the architecture is more like a general Bayesian network than a formal grammar. On the other hand, unlike a Bayesian network, the distribution is non-Markovian, and therefore more like a probabilistic context-sensitive grammar. The conceptualization and construction of a composition machine is facilitated by its formulation as the result of a series of non-Markovian perturbations of a \"Markov backbone.\""
            },
            "slug": "Context-and-Hierarchy-in-a-Probabilistic-Image-Jin-Geman",
            "title": {
                "fragments": [],
                "text": "Context and Hierarchy in a Probabilistic Image Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A mathematical framework for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, is proposed, and a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport is built."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2942555"
                        ],
                        "name": "Sebastian Gorga",
                        "slug": "Sebastian-Gorga",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Gorga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Gorga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713258"
                        ],
                        "name": "K. Otsuka",
                        "slug": "K.-Otsuka",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Otsuka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Otsuka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 234
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2220385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d89b21faf6c0a32b167591e9875b677ccd9c3678",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a probabilistic framework, which incorporates automatic image-based gaze detection, for inferring the structure of multiparty face-to-face conversations. This framework aims to infer conversation regimes and gaze patterns from the nonverbal behaviors of meeting participants, which are captured from image and audio streams with cameras and microphones. The conversation regime corresponds to a global conversational pattern such as monologue and dialogue, and the gaze pattern indicates \"who is looking at whom\". Input nonverbal behaviors include presence/absence of utterances, head directions, and discrete head-centered eye-gaze directions. In contrast to conventional meeting analysis methods that focus only on the participant's head pose as a surrogate of visual focus of attention, this paper newly incorporates vision-based gaze detection combined with head pose tracking into a probabilistic conversation model based on dynamic Bayesian network. Our gaze detector is able to differentiate 3 to 5 different eye gaze directions, e.g. left, straight and right. Experiments on four-person conversations confirm the power of the proposed framework in identifying conversation structure and in estimating gaze patterns with higher accuracy then previous models."
            },
            "slug": "Conversation-scene-analysis-based-on-dynamic-and-Gorga-Otsuka",
            "title": {
                "fragments": [],
                "text": "Conversation scene analysis based on dynamic Bayesian network and image-based gaze detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experiments on four-person conversations confirm the power of the proposed framework in identifying conversation structure and in estimating gaze patterns with higher accuracy then previous models."
            },
            "venue": {
                "fragments": [],
                "text": "ICMI-MLMI '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146154794"
                        ],
                        "name": "Jun Yang",
                        "slug": "Jun-Yang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47909531"
                        ],
                        "name": "Yan Liu",
                        "slug": "Yan-Liu",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] addressed the problem of video classification by joint modeling of videos and transcript texts; Xu et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6709166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e92eb0571953efd19c4b6e3ffb89ad9ab6c5efc",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Accurate and efficient video classification demands the fusion of multimodal information and the use of intermediate representations. Combining the two ideas into the same framework, we propose a probabilistic approach for video classification using intermediate semantic representations derived from the multi-modal features. Based on a class of bipartite undirected graphical models named harmonium, our approach represents video data as latent semantic topics derived by jointly modeling the transcript keywords and color-histogram features, and perform classification using these latent topics under a unified framework. We show satisfactory classification performance of our approach on a benchmark dataset, and some interesting insights of the data provided by this approach."
            },
            "slug": "Harmonium-Models-for-Semantic-Video-Representation-Yang-Liu",
            "title": {
                "fragments": [],
                "text": "Harmonium Models for Semantic Video Representation and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a probabilistic approach for video classification using intermediate semantic representations derived from the multi-modal features based on a class of bipartite undirected graphical models named harmonium."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766489"
                        ],
                        "name": "M. Ryoo",
                        "slug": "M.-Ryoo",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ryoo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ryoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 240
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14039104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6e1a68340951785c7aa3d3ba7a99d64a997a2ef",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a general methodology for automated recognition of complex human activities. The methodology uses a context-free grammar (CFG) based representation scheme to represent composite actions and interactions. The CFG-based representation enables us to formally define complex human activities based on simple actions or movements. Human activities are classified into three categories: atomic action, composite action, and interaction. Our system is not only able to represent complex human activities formally, but also able to recognize represented actions and interactions with high accuracy. Image sequences are processed to extract poses and gestures. Based on gestures, the system detects actions and interactions occurring in a sequence of image frames. Our results show that the system is able to represent composite actions and interactions naturally. The system was tested to represent and recognize eight types of interactions: approach, depart, point, shake-hands, hug, punch, kick, and push. The experiments show that the system can recognize sequences of represented composite actions and interactions with a high recognition rate."
            },
            "slug": "Recognition-of-Composite-Human-Activities-through-Ryoo-Aggarwal",
            "title": {
                "fragments": [],
                "text": "Recognition of Composite Human Activities through Context-Free Grammar Based Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The system was tested to represent and recognize eight types of interactions: approach, depart, point, shake-hands, hug, punch, kick, and push, and the results show that the system is able to represent composite actions and interactions naturally."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108881999"
                        ],
                        "name": "Chong Wang",
                        "slug": "Chong-Wang",
                        "structuredName": {
                            "firstName": "Chong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": ", [1], [2], [3], [4]), in this work we propose to represent the joint interpretation of video and text in the form of a parse graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4] simultaneously modeled the image elements, text annotations and image class labels using a probabilistic topic model; Liu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14362511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fdf31d5ebdd293b3027e6555e256a936ff5515a",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Image classification and annotation are important problems in computer vision, but rarely considered together. Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words \u201croad,\u201d \u201ccar,\u201d and \u201ctraffic\u201d than words \u201cfish,\u201d \u201cboat,\u201d and \u201cscuba.\u201d In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance."
            },
            "slug": "Simultaneous-image-classification-and-annotation-Wang-Blei",
            "title": {
                "fragments": [],
                "text": "Simultaneous image classification and annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new probabilistic model for jointly modeling the image, its class label, and its annotations is developed, which derives an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759772"
                        ],
                        "name": "Hoifung Poon",
                        "slug": "Hoifung-Poon",
                        "structuredName": {
                            "firstName": "Hoifung",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoifung Poon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "Our work also involves semantic parsing of text descriptions, which is related to a variety of semantic parsing approaches studied in the natural language processing community, for example, a supervised approach [35], an unsupervised approach [36], and a dependency based approach [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5337047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df733a3274c55414629c5e90debef629631fcaa",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task."
            },
            "slug": "Unsupervised-Semantic-Parsing-Poon",
            "title": {
                "fragments": [],
                "text": "Unsupervised Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work presents the first unsupervised approach to the problem of learning a semantic parser, using Markov logic, and substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784228"
                        ],
                        "name": "J. Finkel",
                        "slug": "J.-Finkel",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Finkel",
                            "middleNames": [
                                "Rose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Finkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050250"
                        ],
                        "name": "Trond Grenager",
                        "slug": "Trond-Grenager",
                        "structuredName": {
                            "firstName": "Trond",
                            "lastName": "Grenager",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trond Grenager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10977241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f410ab5c8b12b34b38421241366ee456bbebab9",
            "isKey": false,
            "numCitedBy": 3246,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks."
            },
            "slug": "Incorporating-Non-local-Information-into-Extraction-Finkel-Grenager",
            "title": {
                "fragments": [],
                "text": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4599641"
                        ],
                        "name": "Mohamed R. Amer",
                        "slug": "Mohamed-R.-Amer",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Amer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed R. Amer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48811777"
                        ],
                        "name": "Dan Xie",
                        "slug": "Dan-Xie",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2504013"
                        ],
                        "name": "Mingtian Zhao",
                        "slug": "Mingtian-Zhao",
                        "structuredName": {
                            "firstName": "Mingtian",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingtian Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 263
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7337379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7257ba406dbe79e1c17a630ac97717ac7e0ccb",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses a new problem, that of multiscale activity recognition. Our goal is to detect and localize a wide range of activities, including individual actions and group activities, which may simultaneously co-occur in high-resolution video. The video resolution allows for digital zoom-in (or zoom-out) for examining fine details (or coarser scales), as needed for recognition. The key challenge is how to avoid running a multitude of detectors at all spatiotemporal scales, and yet arrive at a holistically consistent video interpretation. To this end, we use a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. The AND-OR graph allows a principled formulation of efficient, cost-sensitive inference via an explore-exploit strategy. Our inference optimally schedules the following computational processes: 1) direct application of activity detectors --- called \u03b1 process; 2) bottom-up inference based on detecting activity parts --- called \u03b2 process; and 3) top-down inference based on detecting activity context --- called \u03b3 process. The scheduling iteratively maximizes the log-posteriors of the resulting parse graphs. For evaluation, we have compiled and benchmarked a new dataset of high-resolution videos of group and individual activities co-occurring in a courtyard of the UCLA campus."
            },
            "slug": "Cost-Sensitive-Top-Down/Bottom-Up-Inference-for-Amer-Xie",
            "title": {
                "fragments": [],
                "text": "Cost-Sensitive Top-Down/Bottom-Up Inference for Multiscale Activity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A three-layered AND-OR graph is used to jointly model group activities, individual actions, and participating objects and optimally schedules the following computational processes: direct application of activity detectors, bottom-up inference based on detecting activity parts, and top-down inference based upon detecting activity context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144470068"
                        ],
                        "name": "Yiming Liu",
                        "slug": "Yiming-Liu",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38188040"
                        ],
                        "name": "Dong Xu",
                        "slug": "Dong-Xu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807998"
                        ],
                        "name": "I. Tsang",
                        "slug": "I.-Tsang",
                        "structuredName": {
                            "firstName": "Ivor",
                            "lastName": "Tsang",
                            "middleNames": [
                                "Wai-Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tsang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, the query-answering-based evaluation mimics the natural human-computer interaction via questions of who, what, when, where and why, which goes beyond the conventional evaluation frame-\n6 \u252c\nworks such as those based on classification or detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4964720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d387cf919280239d46dd2d2b7aa09a6d3fea8cd5",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "The rapid popularization of digital cameras and mobile phone cameras has led to an explosive growth of personal photo collections by consumers. In this paper, we present a real-time textual query-based personal photo retrieval system by leveraging millions of Web images and their associated rich textual descriptions (captions, categories, etc.). After a user provides a textual query (e.g., \"water\u201d), our system exploits the inverted file to automatically find the positive Web images that are related to the textual query \"water\u201d as well as the negative Web images that are irrelevant to the textual query. Based on these automatically retrieved relevant and irrelevant Web images, we employ three simple but effective classification methods, k-Nearest Neighbor (kNN), decision stumps, and linear SVM, to rank personal photos. To further improve the photo retrieval performance, we propose two relevance feedback methods via cross-domain learning, which effectively utilize both the Web images and personal images. In particular, our proposed cross-domain learning methods can learn robust classifiers with only a very limited amount of labeled personal photos from the user by leveraging the prelearned linear SVM classifiers in real time. We further propose an incremental cross-domain learning method in order to significantly accelerate the relevance feedback process on large consumer photo databases. Extensive experiments on two consumer photo data sets demonstrate the effectiveness and efficiency of our system, which is also inherently not limited by any predefined lexicon."
            },
            "slug": "Textual-Query-of-Personal-Photos-Facilitated-by-Web-Liu-Xu",
            "title": {
                "fragments": [],
                "text": "Textual Query of Personal Photos Facilitated by Large-Scale Web Data"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper presents a real-time textual query-based personal photo retrieval system by leveraging millions of Web images and their associated rich textual descriptions (captions, categories, etc.) to improve the photo retrieval performance and proposes two relevance feedback methods via cross-domain learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46317361"
                        ],
                        "name": "Yibiao Zhao",
                        "slug": "Yibiao-Zhao",
                        "structuredName": {
                            "firstName": "Yibiao",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yibiao Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "It has been employed in computer vision to model objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "AOGs has been employed in computer vision to model the hierarchical decompositions of objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 517215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e812a949c9a4ac3ea9327805f102eb0f837a5d63",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Indoor functional objects exhibit large view and appearance variations, thus are difficult to be recognized by the traditional appearance-based classification paradigm. In this paper, we present an algorithm to parse indoor images based on two observations: i) The functionality is the most essential property to define an indoor object, e.g. \"a chair to sit on\", ii) The geometry (3D shape) of an object is designed to serve its function. We formulate the nature of the object function into a stochastic grammar model. This model characterizes a joint distribution over the function-geometry-appearance (FGA) hierarchy. The hierarchical structure includes a scene category, functional groups, functional objects, functional parts and 3D geometric shapes. We use a simulated annealing MCMC algorithm to find the maximum a posteriori (MAP) solution, i.e. a parse tree. We design four data-driven steps to accelerate the search in the FGA space: i) group the line segments into 3D primitive shapes, ii) assign functional labels to these 3D primitive shapes, iii) fill in missing objects/parts according to the functional labels, and iv) synthesize 2D segmentation maps and verify the current parse tree by the Metropolis-Hastings acceptance probability. The experimental results on several challenging indoor datasets demonstrate the proposed approach not only significantly widens the scope of indoor scene parsing algorithm from the segmentation and the 3D recovery to the functional object recognition, but also yields improved overall performance."
            },
            "slug": "Scene-Parsing-by-Integrating-Function,-Geometry-and-Zhao-Zhu",
            "title": {
                "fragments": [],
                "text": "Scene Parsing by Integrating Function, Geometry and Appearance Models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The proposed approach not only significantly widens the scope of indoor scene parsing algorithm from the segmentation and the 3D recovery to the functional object recognition, but also yields improved overall performance."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807123"
                        ],
                        "name": "Zhangzhang Si",
                        "slug": "Zhangzhang-Si",
                        "structuredName": {
                            "firstName": "Zhangzhang",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangzhang Si"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6063389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f8f64382681fb92da07bac2d7b5bdd136c6f1fe",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a framework for unsupervised learning of a hierarchical reconfigurable image template - the AND-OR Template (AOT) for visual objects. The AOT includes: 1) hierarchical composition as \"AND\" nodes, 2) deformation and articulation of parts as geometric \"OR\" nodes, and 3) multiple ways of composition as structural \"OR\" nodes. The terminal nodes are hybrid image templates (HIT) [17] that are fully generative to the pixels. We show that both the structures and parameters of the AOT model can be learned in an unsupervised way from images using an information projection principle. The learning algorithm consists of two steps: 1) a recursive block pursuit procedure to learn the hierarchical dictionary of primitives, parts, and objects, and 2) a graph compression procedure to minimize model structure for better generalizability. We investigate the factors that influence how well the learning algorithm can identify the underlying AOT. And we propose a number of ways to evaluate the performance of the learned AOTs through both synthesized examples and real-world images. Our model advances the state of the art for object detection by improving the accuracy of template matching."
            },
            "slug": "Learning-AND-OR-Templates-for-Object-Recognition-Si-Zhu",
            "title": {
                "fragments": [],
                "text": "Learning AND-OR Templates for Object Recognition and Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows that both the structures and parameters of the AOT model can be learned in an unsupervised way from images using an information projection principle, and proposes a number of ways to evaluate the performance of the learned AOTs through both synthesized examples and real-world images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46479604"
                        ],
                        "name": "Amit Goyal",
                        "slug": "Amit-Goyal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682965"
                        ],
                        "name": "Xufeng Han",
                        "slug": "Xufeng-Han",
                        "structuredName": {
                            "firstName": "Xufeng",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xufeng Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40614240"
                        ],
                        "name": "Alyssa C. Mensch",
                        "slug": "Alyssa-C.-Mensch",
                        "structuredName": {
                            "firstName": "Alyssa",
                            "lastName": "Mensch",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alyssa C. Mensch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49501003"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714215"
                        ],
                        "name": "K. Stratos",
                        "slug": "K.-Stratos",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Stratos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stratos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721910"
                        ],
                        "name": "Kota Yamaguchi",
                        "slug": "Kota-Yamaguchi",
                        "structuredName": {
                            "firstName": "Kota",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kota Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "Currently, we set the values of all the constants in the definition heuristically, but it is possible to optimize them by learning from annotated training data, as in earlier work.(30)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3570838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d5e1395e1ace37d9d5b7ce6854d518e7f128e79",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "When people describe a scene, they often include information that is not visually apparent; sometimes based on background knowledge, sometimes to tell a story. We aim to separate visual text---descriptions of what is being seen---from non-visual text in natural images and their descriptions. To do so, we first concretely define what it means to be visual, annotate visual text and then develop algorithms to automatically classify noun phrases as visual or non-visual. We find that using text alone, we are able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance. Finally, we show that we can reliably mine visual nouns and adjectives from large corpora and that we can use these effectively in the classification task."
            },
            "slug": "Detecting-Visual-Text-Dodge-Goyal",
            "title": {
                "fragments": [],
                "text": "Detecting Visual Text"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work concretely defines what it means to be visual, annotate visual text and develops algorithms to automatically classify noun phrases as visual or non-visual, and finds that using text alone, it is able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36642579"
                        ],
                        "name": "Seungyup Paek",
                        "slug": "Seungyup-Paek",
                        "structuredName": {
                            "firstName": "Seungyup",
                            "lastName": "Paek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungyup Paek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37365764"
                        ],
                        "name": "C. Sable",
                        "slug": "C.-Sable",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Sable",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sable"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064865585"
                        ],
                        "name": "A. Jaimes",
                        "slug": "A.-Jaimes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Jaimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jaimes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118504"
                        ],
                        "name": "Barry Schiffman",
                        "slug": "Barry-Schiffman",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Schiffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barry Schiffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[15] performed scene classification with a TF-IDF based approach applied to both images and the accompanying text; Barnard et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17818432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62fa002224aaad7a94334da61de0435be65749ff",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Annotating photographs automatically with content descriptions facilitates organization, storage, and search over visual information. We present an integrated approach for scene classi cation that combines image-based and text-based approaches. On the text side, we use the text accompanying an image in a novel TF*IDF vector-based approach to classi cation. On the image side, we present a novel OF*IIF (object frequency) vector-based approach to classi cation. Objects are de ned by clustering of segmented regions of training images. The image based OF*IIF approach is synergistic with the text based TF*IDF approach. By integrating the TF*IDF approach and the OF*IIF approach, we achieved a classi cation accuracy of 86%. This is an improvement of approximately 12% over existing image classi ers, an improvement of approximately 3% over the TF*IDF image classi er based on textual information, and an improvement of approximately 4% over the OF*IIF image classi er based on visual information."
            },
            "slug": "Integration-of-Visual-and-Text-Based-Approaches-for-McKeown-Hatzivassiloglou",
            "title": {
                "fragments": [],
                "text": "Integration of Visual and Text-Based Approaches for the Content Labeling and Classification of Photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An integrated approach for scene classi cation that combines image-based and text-based approaches and uses the text accompanying an image in a novel TF*IDF vector-based approach to classiCation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR 1999"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "h a functional description as the input, the sentence realizer generates the sentence by determining a grammatical form and performing word substitution. A head-driven phrase structure grammar (HPSG) [67] of English syntax is used to generate text sentences. Table 3 shows some examples of the generated text. By comparing the input text and the generated text, it can be seen that when the input text is"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14500645,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3cee61e41731adb3356233aed2a9b33c2280683b",
            "isKey": false,
            "numCitedBy": 4233,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This book presents the most complete exposition of the theory of head-driven phrase structure grammar (HPSG), introduced in the authors' \"Information-Based Syntax and Semantics.\" HPSG provides an integration of key ideas from the various disciplines of cognitive science, drawing on results from diverse approaches to syntactic theory, situation semantics, data type theory, and knowledge representation. The result is a conception of grammar as a set of declarative and order-independent constraints, a conception well suited to modelling human language processing. This self-contained volume demonstrates the applicability of the HPSG approach to a wide range of empirical problems, including a number which have occupied center-stage within syntactic theory for well over twenty years: the control of \"understood\" subjects, long-distance dependencies conventionally treated in terms of \"wh\"-movement, and syntactic constraints on the relationship between various kinds of pronouns and their antecedents. The authors make clear how their approach compares with and improves upon approaches undertaken in other frameworks, including in particular the government-binding theory of Noam Chomsky."
            },
            "slug": "Head-driven-phrase-structure-grammar-Sag-Pollard",
            "title": {
                "fragments": [],
                "text": "Head-driven phrase structure grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This book presents the most complete exposition of the theory of head-driven phrase structure grammar, introduced in the authors' \"Information-Based Syntax and Semantics,\" and demonstrates the applicability of the HPSG approach to a wide range of empirical problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "Our work also involves semantic parsing of text descriptions, which is related to a variety of semantic parsing approaches studied in the natural language processing community, for example, a supervised approach [35], an unsupervised approach [36], and a dependency based approach [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10737514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18da6a6fdd698660a7f41fe0533d05a4e6443183",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic parsing is the task of mapping a natural language sentence into a complete, formal meaning representation. Over the past decade, we have developed a number of machine learning methods for inducing semantic parsers by training on a corpus of sentences paired with their meaning representations in a specified formal language. We have demonstrated these methods on the automated construction of natural-language interfaces to databases and robot command languages. This paper reviews our prior work on this topic and discusses directions for future research."
            },
            "slug": "Learning-for-Semantic-Parsing-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning for Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper reviews prior work on semantic parsing, and discusses directions for future research on the automated construction of natural-language interfaces to databases and robot command languages."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5934579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f602c33b1762d57223c9f9656579f9d1dc2e30a",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories. Inspired by the principles of efficient indexing (bottom-up,), robust matching (top-down,), and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent features, we learn the statistically most significant compositions (parts composed of parts), which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally adjusting to the visual data. Lower layers are learned in a category-independent way to obtain complex, yet sharable visual building blocks, which is a crucial step towards a scalable representation. Higher layers of the hierarchy, on the other hand, are constructed by using specific categories, achieving a category representation with a small number of highly generalizable parts that gained their structural flexibility through composition within the hierarchy. Built in this way, new categories can be efficiently and continuously added to the system by adding a small number of parts only in the higher layers. The approach is demonstrated on a large collection of images and a variety of object categories. Detection results confirm the effectiveness and robustness of the learned parts."
            },
            "slug": "Towards-Scalable-Representations-of-Object-Learning-Fidler-Leonardis",
            "title": {
                "fragments": [],
                "text": "Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories by learning a hierarchy of spatially flexible compositions in an unsupervised, statistics-driven manner."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70730705"
                        ],
                        "name": "Marie-Catherine de Marnee",
                        "slug": "Marie-Catherine-de-Marnee",
                        "structuredName": {
                            "firstName": "Marie-Catherine",
                            "lastName": "Marnee",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marie-Catherine de Marnee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "een the words in the sentence. It generates a dependency graph in which each node is a word and each directed edge represents a basic grammatical relation between the words. The Stanford Dependencies [57] include more than 50 types of grammatical relations such as subject, modi\ufb01er, complement, direct and direct object. For example, the following dependency identi\ufb01es Tom as the subject of the buying ev"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61960986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ad0a038a0bd241561462a005742193a7c623478",
            "isKey": false,
            "numCitedBy": 738,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that can easily be understood and effectively used by people without linguistic expertise who want to extract textual relations. In particular, rather than the phrase structure representations that have long dominated in the computational linguistic community, it represents all sentence relationships uniformly as typed dependency relations. That is, as triples of a relation between pairs of words, such as \u201cthe subject of distributes is Bell.\u201d Our experience is that this simple, uniform representation is quite accessible to non-linguists thinking about tasks involving information extraction from text and is quite effective in relation extraction applications. Here is an example sentence:"
            },
            "slug": "Stanford-typed-dependencies-manual-Marnee-Manning",
            "title": {
                "fragments": [],
                "text": "Stanford typed dependencies manual"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that can easily be understood and effectively used by people without linguistic expertise who want to extract textual relations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49854448"
                        ],
                        "name": "A. Fire",
                        "slug": "A.-Fire",
                        "structuredName": {
                            "firstName": "Amy",
                            "lastName": "Fire",
                            "middleNames": [
                                "Sue"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7217896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0feccf5158dcb092b6278eaeb4c02003ea525e8",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Using Causal Induction in Humans to Learn and Infer Causality from Video Amy Fire (amy.fire@ucla.edu) Song-Chun Zhu (sczhu@stat.ucla.edu) Center for Vision, Cognition, Learning, and Art University of California, Los Angeles Los Angeles, CA 90095 USA Abstract For both human and machine learners, it is a challenge to make high-level sense of observations by identifying causes, effects, and their connections. Once these connections are learned, the knowledge can be used to infer causes and effects where visual data might be partially hidden or ambiguous. In this paper, we present a Bayesian grammar model for human-perceived causal relationships that is learnable from video. Two exper- iments investigate high-level causal induction from low-level visual cues. In the first experiment, we show that a computer can apply known heuristics used for causal induction by hu- mans to learn perceptual causal relationships. In the second experiment, we show that our learned model can represent hu- mans\u2019 performance in reasoning about hidden effects in video, even when the computer initially misdetects those effects. Keywords: Perceptual causality; causal induction; statistical models. Introduction A man approaches a closed door. He reaches out to grasp the handle and then stands there. Is it locked? Does he not have the key? He knocks and waits, but the door remains closed. Is there no one on the other side to open it? Watching these events unfold, humans can readily answer these questions based on their causal knowledge. One way humans can learn causal relationships is through daily ob- servation by internally measuring co-occurrence of events (Griffiths & Tenenbaum, 2005). Research suggests that humans use a few heuristics to determine whether a co- occurrence is causal, including: \u2022 whether the temporal lag between cause and effect is short, and the cause precedes the effect (Carey, 2009) and \u2022 whether agent actions are responsible for causes (Saxe, Tenenbaum, & Carey, 2005). However, learning from daily observation is limited: many actions and effects are hidden. Our prior knowledge about causal relationships between actions and effects allows us to fill in information about the events in the scene. Some current models represent knowledge with Bayesian networks, e.g., (Griffiths & Tenenbaum, 2005). These mod- els, however, are disjoint from the low-level visual data that people observe. Instead, models are built using high-level an- notations. In reality, agents build knowledge by observing low-level visual data, and models need to be able to deal with uncertainty in observation. Although Bayesian networks are commonly used to repre- sent causality (Pearl, 2009), grammar models have the ex- pressive power to represent a greater breadth of possibili- ties than a single instance of a Bayesian network (Griffiths & Tenenbaum, 2007). Grammar models allow for multiple configurations and high-level structures, making them more suitable for applications grounded on visual cues; Bayesian networks lack the representative power needed for this. Grammar models are represented graphically in the And- Or Graph (AOG). In the AOG, Or-nodes represent the mul- tiple alternatives, and And-nodes represent hierarchical de- compositions. The AOG naturally lends itself to represent causation where multiple alternative causes can lead to an ef- fect, and each cause is composed of conditions necessary for the effect. In this paper, we introduce a grammar model for repre- senting causal relationships between actions and object-status changes, the Causal And-Or Graph (C-AOG). We describe methods for learning the model by using co-occurrence to identify potential causal relationships between events and ap- plying the heuristics listed above to those potential relation- ships. In two experiments, we investigate how the model matches human perceptions of causality. Experiment 1 uses input typical of computer vision detection systems to investi- gate learning the C-AOG and human perceptions of causality. Experiment 2 demonstrates that the C-AOG models human judgments on imputing hidden variables from video. A Grammar Model for Causality In this section, we introduce the Causal And-Or Graph for causal reasoning, which ties agent actions to fluents. Fluents and Actions Specifically defining those object statuses that vary over time, the term fluents comes from the commonsense-reasoning lit- erature (Mueller, 2006). Relevant here are two kinds of flu- ents that intentional agents can change: object fluents (e.g., a light can be on or off) and fluents of the mind (e.g., an agent can be thirsty or not thirsty). The values of these fluents change as a result of agent ac- tions and also trigger rational agents to take action. A lack of change-inducing action (also known as the inertial action) causes the fluent to maintain its value; for example, a door that is closed will remain closed until some action changes it. In this work, fluents are modeled discriminatively. Actions (A i ) are modeled using the Temporal And-Or Graph (T-AOG), a grammar model for actions (Pei, Jia, & Zhu, 2011). In the T-AOG, And-nodes group the necessary ways for an action to be performed that allow detection of the action (e.g., object/agent spatial relations, agent poses, scene contexts, and temporal relationships), and Or-nodes provide"
            },
            "slug": "Using-Causal-Induction-in-Humans-to-Learn-and-Infer-Fire-Zhu",
            "title": {
                "fragments": [],
                "text": "Using Causal Induction in Humans to Learn and Infer Causality from Video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Bayesian grammar model for human-perceived causal relationships that is learnable from video and methods for learning the model are described by using co-occurrence to identify potential causal relationships between events and ap- plying the heuristics listed above."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2999991"
                        ],
                        "name": "R. Thiagarajan",
                        "slug": "R.-Thiagarajan",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Thiagarajan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Thiagarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144476768"
                        ],
                        "name": "G. Manjunath",
                        "slug": "G.-Manjunath",
                        "structuredName": {
                            "firstName": "Geetha",
                            "lastName": "Manjunath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Manjunath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697771"
                        ],
                        "name": "M. Stumptner",
                        "slug": "M.-Stumptner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Stumptner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stumptner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": ", [59], [60], [61], can be used to measure the semantic distance given the taxonomy in an ontology."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14200953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ba062820157d2c0acf6f1da85e6d9da32c9e239",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Determining semantic similarity of two sets of words that describe two entities is an important problem in web mining (search and recommendation systems), targeted advertisement and domains that need semantic content matching. Traditional Information Retrieval approaches, even when extended to include semantics by performing the similarity comparison on concepts instead of words/terms, may not always determine the right matches when there is no direct overlap in the exact concepts that represent the semantics. As the entity descriptions are treated as self-contained units, the relationships that are not explicit in the entity descriptions are usually ignored. We extend this notion of semantic similarity to consider inherent relationships between concepts using ontologies. We propose simple metrics for computing semantic similarity using spreading activation networks with multiple mechanisms for activation (set based spreading and graph based spreading) and concept matching (using bipartite graphs). We evaluate these metrics in the context of matching two user profiles to determine overlapping interests between users. Our similarity computation results show an improvement in accuracy over other approaches, when compared with human-computed similarity. Although the techniques presented here are used to compute similarity between two user profiles, these are applicable to any content matching scenario. External Posting Date: July 6, 2008 [Fulltext] Approved for External Publication Internal Posting Date: July 6, 2008 [Fulltext] Submitted to ISWC 08, the International Semantic Web Conference (ISWC), 2008, Karlsruhe, Germany \u00a9 Copyright 2008 Hewlett-Packard Development Company, L.P. Computing Semantic Similarity Using Ontologies Rajesh Thiagarajan, Geetha Manjunath, and Markus Stumptner 1 Advanced Computing Research Centre, University of South Australia {cisrkt|mst}@cs.unisa.edu.au 2 Hewlett-Packard Labs, Bangalore, India geetha.manjunath@hp.com Abstract. Determining semantic similarity of two sets of words that Determining semantic similarity of two sets of words that describe two entities is an important problem in web mining (search and recommendation systems), targeted advertisement and domains that need semantic content matching. Traditional Information Retrieval approaches, even when extended to include semantics by performing the similarity comparison on concepts instead of words/terms, may not always determine the right matches when there is no direct overlap in the exact concepts that represent the semantics. As the entity descriptions are treated as self-contained units, the relationships that are not explicit in the entity descriptions are usually ignored. We extend this notion of semantic similarity to consider inherent relationships between concepts using ontologies. We propose simple metrics for computing semantic similarity using spreading activation networks with multiple mechanisms for activation (set based spreading and graph based spreading) and concept matching (using bipartite graphs). We evaluate these metrics in the context of matching two user profiles to determine overlapping interests between users. Our similarity computation results show an improvement in accuracy over other approaches, when compared with human-computed similarity. Although the techniques presented here are used to compute similarity between two user profiles, these are applicable to any content matching scenario."
            },
            "slug": "Computing-Semantic-Similarity-Using-Ontologies-Thiagarajan-Manjunath",
            "title": {
                "fragments": [],
                "text": "Computing Semantic Similarity Using Ontologies"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Simple metrics for computing semantic similarity using spreading activation networks with multiple mechanisms for activation (set based spreading and graph based spreading) and concept matching (using bipartite graphs) are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060131247"
                        ],
                        "name": "N. Oliver",
                        "slug": "N.-Oliver",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Oliver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Oliver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 222
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5391819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51d09ba221f10bc14ae7b31a1a91d155deeca1d5",
            "isKey": false,
            "numCitedBy": 1216,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present algorithms for coupling and training hidden Markov models (HMMs) to model interacting processes, and demonstrate their superiority to conventional HMMs in a vision task classifying two-handed actions. HMMs are perhaps the most successful framework in perceptual computing for modeling and classifying dynamic behaviors, popular because they offer dynamic time warping, a training algorithm and a clear Bayesian semantics. However the Markovian framework makes strong restrictive assumptions about the system generating the signal-that it is a single process having a small number of states and an extremely limited state memory. The single-process model is often inappropriate for vision (and speech) applications, resulting in low ceilings on model performance. Coupled HMMs provide an efficient way to resolve many of these problems, and offer superior training speeds, model likelihoods, and robustness to initial conditions."
            },
            "slug": "Coupled-hidden-Markov-models-for-complex-action-Brand-Oliver",
            "title": {
                "fragments": [],
                "text": "Coupled hidden Markov models for complex action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Algorithms for coupling and training hidden Markov models (HMMs) to model interacting processes, and demonstrate their superiority to conventional HMMs in a vision task classifying two-handed actions are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23736435"
                        ],
                        "name": "Seong-Wook Joo",
                        "slug": "Seong-Wook-Joo",
                        "structuredName": {
                            "firstName": "Seong-Wook",
                            "lastName": "Joo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Wook Joo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 246
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15130454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "641b971a940f9ec9ad1fc9d0fb8d4cb5f3a8cd19",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for representing and recognizing visual events using attribute grammars. In contrast to conventional grammars, attribute grammars are capable of describing features that are not easily represented by finite symbols. Our approach handles multiple concurrent events involving multiple entities by associating unique object identification labels with multiple event threads. Probabilistic parsing and probabilistic conditions on the attributes are used to achieve a robust recognition system. We demonstrate the effectiveness of our method for the task of recognizing vehicle casing in parking lots and events occurring in an airport tarmac."
            },
            "slug": "Recognition-of-Multi-Object-Events-Using-Attribute-Joo-Chellappa",
            "title": {
                "fragments": [],
                "text": "Recognition of Multi-Object Events Using Attribute Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The effectiveness of the method for representing and recognizing visual events using attribute grammars for the task of recognizing vehicle casing in parking lots and events occurring in an airport tarmac is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2006 International Conference on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3338131"
                        ],
                        "name": "P. Buitelaar",
                        "slug": "P.-Buitelaar",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Buitelaar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Buitelaar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748977"
                        ],
                        "name": "P. Cimiano",
                        "slug": "P.-Cimiano",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Cimiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cimiano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "\u2022 Although spatial, temporal and causal AOGs and their parsing techniques have been separately investigated before, our work is the first to propose the joint S/T/C-AOG which crosses the spatial, temporal and causal dimensions and enables the propagation of information between them during parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18638602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96133e6f5d80c89d3ea31673dc9e467f63fd7366",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume brings together a collection of extended versions of selected papers from two workshops on ontology learning, knowledge acquisition and related topics that were organized in the context of the European Conference on Artificial Intelligence (ECAI) 2004 and the International Conference on Knowledge Engineering and Management (EKAW) 2004. The volume presents current research in ontology learning, addressing three perspectives: methodologies that have been proposed to automatically extract information from texts and to give a structured organization to such knowledge, including approaches based on machine learning techniques; evaluation methods for ontology learning, aiming at defining procedures and metrics for a quantitative evaluation of the ontology learning task; and finally application scenarios that make ontology learning a challenging area in the context of real applications such as bio-informatics. According to the three perspectives mentioned above, the book is divided into three sections, each including a selection of papers addressing respectively the methods, the applications and the evaluation of ontology learning approaches. However, all selected papers pay considerably attention to the evaluation perspective, as this was a central topic of the ECAI 2004 workshop out of which most of the papers in this volume originate."
            },
            "slug": "Ontology-Learning-from-Text:-An-Overview-Buitelaar-Cimiano",
            "title": {
                "fragments": [],
                "text": "Ontology Learning from Text: An Overview"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This volume brings together a collection of extended versions of selected papers from two workshops on ontology learning, knowledge acquisition and related topics that were organized in the context of the European Conference on Artificial Intelligence (ECAI) 2004 and the International Conference on Knowledge Engineering and Management (EKAW) 2004."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144954740"
                        ],
                        "name": "J. Earley",
                        "slug": "J.-Earley",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Earley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Earley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "The Earley parsing algorithm [54], [58] is used to parse the dependencies using the attribute grammar, which employs topdown dynamic programming to search for a valid set of production rules that generate the dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "We perform temporal parsing following the approach proposed in [9], which is based on the Earley parser [54]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b",
            "isKey": false,
            "numCitedBy": 1339,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(<italic>k</italic>) algorithm and the familiar top-down algorithm. It has a time bound proportional to <italic>n</italic><supscrpt>3</supscrpt> (where <italic>n</italic> is the length of the string being parsed) in general; it has an <italic>n</italic><supscrpt>2</supscrpt> bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "slug": "An-efficient-context-free-parsing-algorithm-Earley",
            "title": {
                "fragments": [],
                "text": "An efficient context-free parsing algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described and appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784914"
                        ],
                        "name": "A. Lavie",
                        "slug": "A.-Lavie",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Lavie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "We also evaluated the generated text based on the joint parse graph against the merged text descriptions from human subjects, using quantitative metrics such as BLEU [68] and METEOR [69] which were developed for evaluating machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7164502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7",
            "isKey": false,
            "numCitedBy": 2986,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."
            },
            "slug": "METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Banerjee-Lavie",
            "title": {
                "fragments": [],
                "text": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "METEOR is described, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations and can be easily extended to include more advanced matching strategies."
            },
            "venue": {
                "fragments": [],
                "text": "IEEvaluation@ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32072466"
                        ],
                        "name": "T. Gruber",
                        "slug": "T.-Gruber",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Gruber",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gruber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "An ontology is a \u201cformal, explicit specification of a shared conceptualisation\u201d [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15709015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5120f65919f77859a974fcc1ad08f72b2918b8ec",
            "isKey": false,
            "numCitedBy": 13142,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse\u2014definitions of classes, relations, functions, and other objects\u2014is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms."
            },
            "slug": "A-translation-approach-to-portable-ontology-Gruber",
            "title": {
                "fragments": [],
                "text": "A translation approach to portable ontology specifications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a mechanism for defining ontologies that are portable over representation systems, basing Ontolingua itself on an ontology of domain-independent, representational idioms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748977"
                        ],
                        "name": "P. Cimiano",
                        "slug": "P.-Cimiano",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Cimiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cimiano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "\u2022 Although spatial, temporal and causal AOGs and their parsing techniques have been separately investigated before, our work is the first to propose the joint S/T/C-AOG which crosses the spatial, temporal and causal dimensions and enables the propagation of information between them during parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28114065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "996e591bbeab149a8f624a89048c7a10be8322d6",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last decade, ontologies have received much attention within computer science and related disciplines, most often as the semantic web. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications discusses ontologies for the semantic web, as well as knowledge management, information retrieval, text clustering and classification, as well as natural language processing. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications is structured for research scientists and practitioners in industry. This book is also suitable for graduate-level students in computer science."
            },
            "slug": "Ontology-learning-and-population-from-text-and-Cimiano",
            "title": {
                "fragments": [],
                "text": "Ontology learning and population from text - algorithms, evaluation and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Ontology Learning and Population from Text: Algorithms, Evaluation and Applications discusses ontologies for the semantic web, aswell as knowledge management, information retrieval, text clustering and classification, as well as natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49824581"
                        ],
                        "name": "P. Natarajan",
                        "slug": "P.-Natarajan",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Natarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Natarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 228
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1925530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "120ee12b651a892ab021978b3fcb7256b936ea71",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing human activity from a stream of sensory observations is important for a number of applications such as surveillance and human-computer interaction. Hidden Markov Models (HMMs) have been proposed as suitable tools for modeling the variations in the observations for the same action and for discriminating among different actions. HMMs have come in wide use for this task but the standard form suffers from several limitations. These include unrealistic models for the duration of a sub-event and not encoding interactions among multiple agents directly. Semi- Markov models and coupled HMMs have been proposed in previous work to handle these issues. We combine these two concepts into a coupled Hidden semi-Markov Model (CHSMM). CHSMMs pose huge computational complexity challenges. We present efficient algorithms for learning and decoding in such structures and demonstrate their utility by experiments with synthetic and real data."
            },
            "slug": "Coupled-Hidden-Semi-Markov-Models-for-Activity-Natarajan-Nevatia",
            "title": {
                "fragments": [],
                "text": "Coupled Hidden Semi Markov Models for Activity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A coupled Hidden semi-Markov Model (CHSMM) is presented, which presents efficient algorithms for learning and decoding in such structures and demonstrates their utility by experiments with synthetic and real data."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Workshop on Motion and Video Computing (WMVC'07)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "An AOG is an extension of a constituency grammar used in natural language parsing [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "An AOG is an extension of a constituency grammar used in natural language parsing [5] to represent hierarchical compositions of objects, scenes and events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "A parse graph is a labeled directed graph that can be seen as an extension of the constituency-based parse tree used in natural language syntactic parsing [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": true,
            "numCitedBy": 7801,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 305
                            }
                        ],
                        "text": "For joint processing of image and text, Paek et al. [15] performed scene classification with a TF-IDF based approach applied to both images and the accompanying text; Barnard et al. [2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al. [4] simultaneously modeled the image elements, text annotations and image class labels using a probabilistic topic model; Liu et al. [18] presented an image retrieval system by leveraging large-scale web image and their associated textual descriptions; Jia et al. [19] proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata [20] learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "[2] and Blei [16] discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth [17] demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez [3] applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8348240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e397800e8601631a8e01f210e5665b731fd7ebfc",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate a method for identifying images containing categories of animals. The images we classify depict animals in a wide range of aspects, configurations and appearances. In addition, the images typically portray multiple species that differ in appearance (e.g. ukari\u2019s, vervet monkeys, spider monkeys, rhesus monkeys, etc.). Our method is accurate despite this variation and relies on four simple cues: text, color, shape and texture. Visual cues are evaluated by a voting method that compares local image phenomena with a number of visual exemplars for the category. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required involves identifying which clusters of exemplars refer to which sense of a term (for example, \"monkey\" can refer to an animal or a bandmember). Because our method is applied to web pages with free text, the word cue is extremely noisy. We show unequivocal evidence that visual information improves performance for our task. Our method allows us to produce large, accurate and challenging visual datasets mostly automatically."
            },
            "slug": "Animals-on-the-Web-Berg-Forsyth",
            "title": {
                "fragments": [],
                "text": "Animals on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work demonstrates a method for identifying images containing categories of animals using a clustering method applied to text on web pages and shows unequivocal evidence that visual information improves performance for this task."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34796035"
                        ],
                        "name": "R. Poli",
                        "slug": "R.-Poli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Poli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145445555"
                        ],
                        "name": "Michael Healy",
                        "slug": "Michael-Healy",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Healy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Healy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737301"
                        ],
                        "name": "A. Kameas",
                        "slug": "A.-Kameas",
                        "structuredName": {
                            "firstName": "Achilles",
                            "lastName": "Kameas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kameas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63340131,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "3f8b9c29445cc4b785feecf060b5db3e8203bdfc",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Ontology was once understood to be the philosophical inquiry into the structure of reality: the analysis and categorization of what there is. Recently, however, a field called ontology has become part of the rapidly growing research industry in information technology. The two fields have more in common than just their name.Theory and Applications of Ontology is a two-volume anthology that aims to further an informed discussion about the relationship between ontology in philosophy and ontology in information technology. It fills an important lacuna in cutting-edge research on ontology in both fields, supplying stage-setting overview articles on history and method, presenting directions of current research in either field, and highlighting areas of productive interdisciplinary contact.Theory and Applications of Ontology: Computer Applications presents ontology in ways that philosophers are not likely to find elsewhere. The volume offers an overview of current research in ontology, distinguishing basic conceptual issues, domain applications, general frameworks, and mathematical formalisms. It introduces the reader to current research on frameworks and applications in information technology in ways that are sure to invite reflection and constructive responses from ontologists in philosophy."
            },
            "slug": "Theory-and-Applications-of-Ontology:-Computer-Poli-Healy",
            "title": {
                "fragments": [],
                "text": "Theory and Applications of Ontology: Computer Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Theory and Applications of Ontology: Computer Applications presents ontology in ways that philosophers are not likely to find elsewhere, and introduces the reader to current research on frameworks and applications in information technology in Ways that are sure to invite reflection and constructive responses from ontologists in philosophy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Our work of joint semantic parsing also goes beyond the traditional bag-of-words approaches to joint processing of video and text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13889,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390051006"
                        ],
                        "name": "I. Langkilde-Geary",
                        "slug": "I.-Langkilde-Geary",
                        "structuredName": {
                            "firstName": "Irene",
                            "lastName": "Langkilde-Geary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Langkilde-Geary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "During this process, the information from the parse graph is converted to a functional description, which specifies the sentence elements, such as the event, subjects, and objects, and their functional properties.(32) Using a functional description as input, the sentence realizer generates the sentence by determining a grammatical form and performing word substitution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2680971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0886bd3d1b4fd46928a295a36b5230c4352f699b",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities."
            },
            "slug": "Generation-that-Exploits-Corpus-Based-Statistical-Langkilde-Geary-Knight",
            "title": {
                "fragments": [],
                "text": "Generation that Exploits Corpus-Based Statistical Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Novel aspects of a new natural language generator called Nitrogen are described, which has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3246706"
                        ],
                        "name": "Catia Pesquita",
                        "slug": "Catia-Pesquita",
                        "structuredName": {
                            "firstName": "Catia",
                            "lastName": "Pesquita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catia Pesquita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143886603"
                        ],
                        "name": "Daniel Faria",
                        "slug": "Daniel-Faria",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Faria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Faria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689110"
                        ],
                        "name": "A. Falc\u00e3o",
                        "slug": "A.-Falc\u00e3o",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Falc\u00e3o",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Falc\u00e3o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145651178"
                        ],
                        "name": "P. Lord",
                        "slug": "P.-Lord",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Lord",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721340"
                        ],
                        "name": "F. Couto",
                        "slug": "F.-Couto",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Couto",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Couto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": ", [59], [60], [61], can be used to measure the semantic distance given the taxonomy in an ontology."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14535046,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "079b4a023b07df979f20d543df60004dcca0630c",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, ontologies have become a mainstream topic in biomedical research. When biological entities are described using a common schema, such as an ontology, they can be compared by means of their annotations. This type of comparison is called semantic similarity, since it assesses the degree of relatedness between two entities by the similarity in meaning of their annotations. The application of semantic similarity to biomedical ontologies is recent; nevertheless, several studies have been published in the last few years describing and evaluating diverse approaches. Semantic similarity has become a valuable tool for validating the results drawn from biomedical studies such as gene clustering, gene expression data analysis, prediction and validation of molecular interactions, and disease gene prioritization. We review semantic similarity measures applied to biomedical ontologies and propose their classification according to the strategies they employ: node-based versus edge-based and pairwise versus groupwise. We also present comparative assessment studies and discuss the implications of their results. We survey the existing implementations of semantic similarity measures, and we describe examples of applications to biomedical research. This will clarify how biomedical researchers can benefit from semantic similarity measures and help them choose the approach most suitable for their studies. Biomedical ontologies are evolving toward increased coverage, formality, and integration, and their use for annotation is increasingly becoming a focus of both effort by biomedical experts and application of automated annotation procedures to create corpora of higher quality and completeness than are currently available. Given that semantic similarity measures are directly dependent on these evolutions, we can expect to see them gaining more relevance and even becoming as essential as sequence similarity is today in biomedical research."
            },
            "slug": "Semantic-Similarity-in-Biomedical-Ontologies-Pesquita-Faria",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity in Biomedical Ontologies"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work reviews semantic similarity measures applied to biomedical ontologies and proposes their classification according to the strategies they employ: node-based versus edge-based and pairwise versus groupwise."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107904041"
                        ],
                        "name": "Changsong Liu",
                        "slug": "Changsong-Liu",
                        "structuredName": {
                            "firstName": "Changsong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055079585"
                        ],
                        "name": "Rui Fang",
                        "slug": "Rui-Fang",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707259"
                        ],
                        "name": "J. Chai",
                        "slug": "J.-Chai",
                        "structuredName": {
                            "firstName": "Joyce",
                            "lastName": "Chai",
                            "middleNames": [
                                "Yue"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[39] is also related to our work, but they focused on spatial relations between objects while we study spatialtemporal-causal parsing of objects, scenes and events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1247193,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d8fab0c039acd2a054b82d443e5fa02634ac9eb1",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 222,
            "paperAbstract": {
                "fragments": [],
                "text": "To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction."
            },
            "slug": "Towards-Mediating-Shared-Perceptual-Basis-in-Liu-Fang",
            "title": {
                "fragments": [],
                "text": "Towards Mediating Shared Perceptual Basis in Situated Dialogue"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A graph-based representation to capture linguistic discourse and visual discourse, and inexact graph matching to ground references is developed and shown to provide a potential solution to mediate shared perceptual basis for referential grounding in situated interaction."
            },
            "venue": {
                "fragments": [],
                "text": "SIGDIAL Conference"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70053552"
                        ],
                        "name": "\u00d3scar Corcho",
                        "slug": "\u00d3scar-Corcho",
                        "structuredName": {
                            "firstName": "\u00d3scar",
                            "lastName": "Corcho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d3scar Corcho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398981516"
                        ],
                        "name": "M. Fern\u00e1ndez-L\u00f3pez",
                        "slug": "M.-Fern\u00e1ndez-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Mariano",
                            "lastName": "Fern\u00e1ndez-L\u00f3pez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fern\u00e1ndez-L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398348796"
                        ],
                        "name": "A. G\u00f3mez-P\u00e9rez",
                        "slug": "A.-G\u00f3mez-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Asunci\u00f3n",
                            "lastName": "G\u00f3mez-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. G\u00f3mez-P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073059260"
                        ],
                        "name": "\u00d3scar Vicente",
                        "slug": "\u00d3scar-Vicente",
                        "structuredName": {
                            "firstName": "\u00d3scar",
                            "lastName": "Vicente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d3scar Vicente"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1244,
                                "start": 1240
                            }
                        ],
                        "text": "For joint processing of images and text, Seungyup Paek and his colleagues performed scene classification with a TF-IDF based approach applied to both images and the accompanying text;(5) Kobus Barnard and his colleagues(6) and David Blei(7) discussed a series of probabilistic models to capture the dependencies between image elements and words; Tamara Berg and David Forsyth demonstrated a method for identifying images containing categories of animals based on word and visual cues;(8) Florent Monay and Daniel GaticaPerez applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing;(9) Chong Wang and his colleagues simultaneously modeled image elements, text annotations, and image class labels using a probabilistic topic model;(10) Yiming Liu and his colleagues presented an image retrieval system by leveraging large-scale Web images and their associated textual descriptions;(11) Yangqing Jia and his colleagues proposed a probabilistic model that encodes relations between loosely related images and text descriptions;(12) and Yansong Feng and Mirella Lapata learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation.(13) In contrast, our work aims at deep semantic parsing of video and text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17549360,
            "fieldsOfStudy": [
                "Computer Science",
                "Philosophy"
            ],
            "id": "843e7d2446f3fca715d620f5b59d116178acdca9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present WebODE as a scalable, integrated workbench for ontological engineering that eases the modelling of ontologies, the reasoning with ontologies and the exchange of ontologies with other ontology tools and ontology-based applications. We will first describe the WebODE's knowledge model. We will then describe its extensible architecture, focusing on the set of independent ontology development functionalities that are integrated in this framework, such as the Ontology Editor, the Axiom Builder, the OKBC-based inference engine, and the documentation and interoperability services."
            },
            "slug": "WebODE:-An-Integrated-Workbench-for-Ontology-and-Corcho-Fern\u00e1ndez-L\u00f3pez",
            "title": {
                "fragments": [],
                "text": "WebODE: An Integrated Workbench for Ontology Representation, Reasoning, and Exchange"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work presents WebODE as a scalable, integrated workbench for ontological engineering that eases the modelling of ontology, the reasoning with ontologies and the exchange of ontologies with other ontology tools and ontology-based applications."
            },
            "venue": {
                "fragments": [],
                "text": "EKAW"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8626264"
                        ],
                        "name": "J. Gennari",
                        "slug": "J.-Gennari",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gennari",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gennari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680938"
                        ],
                        "name": "M. Musen",
                        "slug": "M.-Musen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Musen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Musen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93553483"
                        ],
                        "name": "R. Fergerson",
                        "slug": "R.-Fergerson",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Fergerson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873828"
                        ],
                        "name": "W. Grosso",
                        "slug": "W.-Grosso",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Grosso",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grosso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887609"
                        ],
                        "name": "Monica Crub\u00e9zy",
                        "slug": "Monica-Crub\u00e9zy",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Crub\u00e9zy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monica Crub\u00e9zy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144447059"
                        ],
                        "name": "H. Eriksson",
                        "slug": "H.-Eriksson",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "Eriksson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Eriksson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791431"
                        ],
                        "name": "Natasha Noy",
                        "slug": "Natasha-Noy",
                        "structuredName": {
                            "firstName": "Natasha",
                            "lastName": "Noy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natasha Noy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702645"
                        ],
                        "name": "S. Tu",
                        "slug": "S.-Tu",
                        "structuredName": {
                            "firstName": "Samson",
                            "lastName": "Tu",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7077574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e964849b0f8334c0905d6ea8cac6beefbbd8939",
            "isKey": false,
            "numCitedBy": 1339,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-evolution-of-Prot\u00e9g\u00e9:-an-environment-for-Gennari-Musen",
            "title": {
                "fragments": [],
                "text": "The evolution of Prot\u00e9g\u00e9: an environment for knowledge-based systems development"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Hum. Comput. Stud."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2021747"
                        ],
                        "name": "Wei-Nchih Lee",
                        "slug": "Wei-Nchih-Lee",
                        "structuredName": {
                            "firstName": "Wei-Nchih",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Nchih Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143665076"
                        ],
                        "name": "N. Shah",
                        "slug": "N.-Shah",
                        "structuredName": {
                            "firstName": "Nigam",
                            "lastName": "Shah",
                            "middleNames": [
                                "Haresh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3248468"
                        ],
                        "name": "Karanjot Sundlass",
                        "slug": "Karanjot-Sundlass",
                        "structuredName": {
                            "firstName": "Karanjot",
                            "lastName": "Sundlass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karanjot Sundlass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680938"
                        ],
                        "name": "M. Musen",
                        "slug": "M.-Musen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Musen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Musen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [59], [60], [61], can be used to measure the semantic distance given the taxonomy in an ontology."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10858795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6909b0782f21ca9d10e1b35f6c9d2b5450cf7e41",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic-similarity measures quantify concept similarities in a given ontology. Potential applications for these measures include search, data mining, and knowledge discovery in database or decision-support systems that utilize ontologies. To date, there have not been comparisons of the different semantic-similarity approaches on a single ontology. Such a comparison can offer insight on the validity of different approaches. We compared 3 approaches to semantic similarity-metrics (which rely on expert opinion, ontologies only, and information content) with 4 metrics applied to SNOMED-CT. We found that there was poor agreement among those metrics based on information content with the ontology only metric. The metric based only on the ontology structure correlated most with expert opinion. Our results suggest that metrics based on the ontology only may be preferable to information-content-based metrics, and point to the need for more research on validating the different approaches."
            },
            "slug": "Comparison-of-Ontology-based-Semantic-Similarity-Lee-Shah",
            "title": {
                "fragments": [],
                "text": "Comparison of Ontology-based Semantic-Similarity Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work compared 3 approaches to semantic similarity-metrics (which rely on expert opinion, ontologies only, and information content) with 4 metrics applied to SNOMED-CT and found that there was poor agreement among those metrics based on information content with the ontology only metric."
            },
            "venue": {
                "fragments": [],
                "text": "AMIA"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806905"
                        ],
                        "name": "A. Maedche",
                        "slug": "A.-Maedche",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Maedche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maedche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752093"
                        ],
                        "name": "Steffen Staab",
                        "slug": "Steffen-Staab",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Staab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Staab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "\u2022 Although spatial, temporal and causal AOGs and their parsing techniques have been separately investigated before, our work is the first to propose the joint S/T/C-AOG which crosses the spatial, temporal and causal dimensions and enables the propagation of information between them during parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1411149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48e752c719d33ff55b3b3bec3538727f8ce69399",
            "isKey": false,
            "numCitedBy": 2204,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation."
            },
            "slug": "Ontology-Learning-for-the-Semantic-Web-Maedche-Staab",
            "title": {
                "fragments": [],
                "text": "Ontology Learning for the Semantic Web"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools and encompasses ontology import, extraction, pruning, refinement and evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "We also evaluated the generated text based on the joint parse graph against the merged text descriptions from human subjects, using quantitative metrics such as BLEU [68] and METEOR [69] which were developed for evaluating machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16617,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53142908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3524cdf7cf8344e7eb74886f71fcbb5c6732c337",
            "isKey": false,
            "numCitedBy": 26734,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
            },
            "slug": "Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modern Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883814"
                        ],
                        "name": "E. Davis",
                        "slug": "E.-Davis",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "\u2022 Fluent, which is used in the AI community to refer to an attribute of a physical object that can change over time [49], e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 945039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "615d8a1f198fb508c84276d372909f554e375445",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Commonsense-Reasoning-Davis",
            "title": {
                "fragments": [],
                "text": "Commonsense Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152829259"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811427"
                        ],
                        "name": "Yong Yu",
                        "slug": "Yong-Yu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115404503"
                        ],
                        "name": "Jing Lu",
                        "slug": "Jing-Lu",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146250807"
                        ],
                        "name": "Chenxi Lin",
                        "slug": "Chenxi-Lin",
                        "structuredName": {
                            "firstName": "Chenxi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenxi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40341553"
                        ],
                        "name": "Kewei Tu",
                        "slug": "Kewei-Tu",
                        "structuredName": {
                            "firstName": "Kewei",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kewei Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112542812"
                        ],
                        "name": "MingChuan Guo",
                        "slug": "MingChuan-Guo",
                        "structuredName": {
                            "firstName": "MingChuan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MingChuan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109109074"
                        ],
                        "name": "Zhuo Zhang",
                        "slug": "Zhuo-Zhang",
                        "structuredName": {
                            "firstName": "Zhuo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052157756"
                        ],
                        "name": "G. Xie",
                        "slug": "G.-Xie",
                        "structuredName": {
                            "firstName": "Guo",
                            "lastName": "Xie",
                            "middleNames": [
                                "Tong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145640647"
                        ],
                        "name": "Z. Su",
                        "slug": "Z.-Su",
                        "structuredName": {
                            "firstName": "Zhong",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144455843"
                        ],
                        "name": "Yue Pan",
                        "slug": "Yue-Pan",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Pan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": ", [41], [42], [43]), adapted from an existing ontology (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9911841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75401f5eca4080d924dbe0c361f96570ed77c17e",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Orient is a project to develop an ontology engineering tool that integrates into existing industry tooling environments - the Eclipse platform and the WebSphere Studio developing tools family. This paper describes how two important issues are addressed during the project, namely tool integration and scalability. We show how Orient morphs into the Eclipse platform and achieves UI and data level integration with the Eclipse platform and other modelling tools. We also describe how we implemented a scalable RDF(S) storage, query, manipulation and inference mechanism on top of a relational database. In particular, we report the empirical performance of our RDF(S) closure inference algorithm on a DB2 database."
            },
            "slug": "ORIENT:-Integrate-Ontology-Engineering-into-Tooling-Zhang-Yu",
            "title": {
                "fragments": [],
                "text": "ORIENT: Integrate Ontology Engineering into Industry Tooling Environment"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown how Orient morphs into the Eclipse platform and achieves UI and data level integration with theclipse platform and other modelling tools, and how a scalable RDF(S) storage, query, manipulation and inference mechanism on top of a relational database is implemented."
            },
            "venue": {
                "fragments": [],
                "text": "SEMWEB"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112157045"
                        ],
                        "name": "Meng Meng",
                        "slug": "Meng-Meng",
                        "structuredName": {
                            "firstName": "Meng",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meng Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2504013"
                        ],
                        "name": "Mingtian Zhao",
                        "slug": "Mingtian-Zhao",
                        "structuredName": {
                            "firstName": "Mingtian",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingtian Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11677270,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "46e38ef00035b2e376bd6c6c8e5261570695250b",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The creation of art from photos aims at raising aesthetic perception of human viewers while preserving important contents in the source photos. In this process, two divergent factors are usually manipulated and balanced: likeness/fidelity and aesthetic depictions, since in many situations perfecting one of them compromises the other. Based on this two-factor model, we present a method for rendering artistic paper-cut from human portrait by extracting likeness cues and reproducing them with aesthetic practices. We segment the input photo for the regions of face, hair, and clothes, and extract information from each region: shapes and shadings for facial components, hair flow and shading for hairlines, and shadings and depths for clothes. Then we render the paper-cut by expressing these likeness cues in artistic formats with additive elements, such as hallucinated curves and decorative patterns. Using paper-cut images rendered with different levels of likeness and aesthetic factors, we have also conducted human perceptual experiments to study whether and how these factors affect visual perceptions, and how to manipulate rendering configurations accordingly for desired results. On the aspect of likeness, we observed that human subjects are able to achieve comparable face learning and recognition performances on paper-cut images and their corresponding photos, and their performances improve when more likeness cues are preserved in rendering. On the aspect of aesthetics, subjects usually favor images with moderate levels of decorative curves and patterns which do not submerge the likeness cues. These experimental results verify our likeness-aesthetics model and the rendering algorithm."
            },
            "slug": "Artistic-Papercut-of-Human-Portraits-:-Rendering-Meng-Zhao",
            "title": {
                "fragments": [],
                "text": "Artistic Papercut of Human Portraits : Rendering and Perceptual Studies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "We adopt the open world assumption [50] which states that a parse graph may not be a complete interpretation of the scene and any statement not included or implied by the parse graph can be either true or false."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46211846,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "bae3eda9605700b14237f4d04652ab6759c68eef",
            "isKey": false,
            "numCitedBy": 1862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach"
            },
            "slug": "Artificial-intelligence-a-modern-approach,-2nd-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial intelligence - a modern approach, 2nd Edition"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "Artificial IntelligenceArtificial intelligence: A Modern Approach 2Nd Ed, eBook, Global Edition."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in artificial intelligence"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "It has been employed in computer vision to model objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "AOGs has been employed in computer vision to model the hierarchical decompositions of objects [6], scenes [7], [8] and events [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "A spatial And-Or graph (S-AOG) [6], [7] models the spatial decompositions of objects and scenes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "It is also guided by a spatial-temporal-causal And-Or graph (S/T/C-AOG) [6], [7], [9], [10], which models the compositional structures of objects, scenes and events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "\u2022 A spatial And-Or graph (S-AOG) [6], [7] models the spatial decompositions of objects and scenes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "In particular, spatial parsing identifies the spatial compositional structures of objects and scenes [21], [6], [22], [23], [24], [25], [7], [8]; temporal parsing recognizes the temporal compositional structures of events [26], [27], [28], [29], [30], [31], [9], [32]; and causal parsing detects the causal relations between events and fluent changes [33], [34], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic grammar of images"
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends. Comput. Graph. Vis., vol. 2, no. 4, pp. 259\u2013362, 2006."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70631054"
                        ],
                        "name": "E. P. hommeaux",
                        "slug": "E.-P.-hommeaux",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "hommeaux",
                            "middleNames": [
                                "Prud"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. P. hommeaux"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61857080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2eac2cd4dadd45129b9ea21a0a9c453731c7c7a4",
            "isKey": false,
            "numCitedBy": 3128,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SPARQL-query-language-for-RDF-hommeaux",
            "title": {
                "fragments": [],
                "text": "SPARQL query language for RDF"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Apache jena"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": ", [44], [45]), or automatically or semi-automatically learned from data (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cyc"
            },
            "venue": {
                "fragments": [],
                "text": "Theory and Applications of Ontology: Computer Applications. Springer, 2010, pp. 259\u2013278."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English (fluent), Spanish (beginning) Image Processing Photoshop"
            },
            "venue": {
                "fragments": [],
                "text": "Skills Language Mandarin Lightroom. Programming C"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Honours and Awards Fellowship"
            },
            "venue": {
                "fragments": [],
                "text": "Exceptional Student (top 5%), Beijing Institute of Technology, 2006 National Schalarship (top 5%)"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RDF/XML syntax specification (revised), \" http://www.w3.org/ TR/REC-rdf-syntax"
            },
            "venue": {
                "fragments": [],
                "text": "RDF/XML syntax specification (revised), \" http://www.w3.org/ TR/REC-rdf-syntax"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The stanford parser: A statistical parser"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPARQL query language for RDF, \" http://www.w3.org/TR/ rdf-sparql-query"
            },
            "venue": {
                "fragments": [],
                "text": "SPARQL query language for RDF, \" http://www.w3.org/TR/ rdf-sparql-query"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detecting visual text , \u201d in NAACL , 2012 . [ 63 ] \u201c SPARQL query language for RDF"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "These energy terms can be either manually specified by domain experts or learned from data [52], [53], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and-or templates for object modeling and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans on Pattern Analysis and Machine Intelligence, 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "During this process, the information from the parse graph is converted to a functional description [66], which specifies elements of a sentence such as the event, subjects, objects, and their functional properties."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generation that exploits corpusbased statistical knowledge"
            },
            "venue": {
                "fragments": [],
                "text": "ACL-COLING, 1998."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RDF/XML syntax specification (revised)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": ") An AOG is an extension of a constituency grammar used in natural language parsing.(5) Similar to a constituency grammar, an AOG represents possible hierarchical compositions of a set of entities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": ") A parse graph is a labeled directed graph that can be seen as an extension of the constituency-based parse tree used in natural language syntactic parsing.(5) Computer vision researchers have used it to model objects,(6) scenes,(7,8) and events."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sch\u20ac  utze, Foundations of Statistical Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 43,
            "methodology": 23
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 79,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Joint-Video-and-Text-Parsing-for-Understanding-and-Tu-Meng/2935d8071583e46c5a895730c65d2bd213757c07?sort=total-citations"
}