{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length."
            },
            "slug": "Statistical-Decision-Tree-Models-for-Parsing-Magerman",
            "title": {
                "fragments": [],
                "text": "Statistical Decision-Tree Models for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SPATTER is described, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002335"
                        ],
                        "name": "Jan Hajic",
                        "slug": "Jan-Hajic",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Hajic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Hajic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324070"
                        ],
                        "name": "C. Tillmann",
                        "slug": "C.-Tillmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Tillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 153
                            }
                        ],
                        "text": "Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1269169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cb912b4a208b217c45d57e28fc0f59599f92330",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text."
            },
            "slug": "A-Statistical-Parser-for-Czech-Collins-Hajic",
            "title": {
                "fragments": [],
                "text": "A Statistical Parser for Czech"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 67
                            }
                        ],
                        "text": "We take as our baseline parser the statistical model of Model 1 of Collins (1997). The model is a historybased, generative model, in which the probability for a parse tree is found by expanding each node in the tree in turn into its child nodes, and multiplying the probabilities for each action in the derivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "pair relations, also called lexical bigrams (Collins, 1996), are reminiscent of dependency grammars such"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9880507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5e619f2c5f4220438b1357e596db5b1578398d",
            "isKey": false,
            "numCitedBy": 643,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance."
            },
            "slug": "Statistical-Parsing-with-a-Context-Free-Grammar-and-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with a Context-Free Grammar and Word Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence that outperforms previous schemes is described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 124
                            }
                        ],
                        "text": "Models 2 and 3 of Collins (1997) add some slightly more elaborate features to the probability model, as do the additions of Charniak (2000) to the model of Charniak (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "The parsing models of Charniak (2000) and Collins (2000) add more complex features to the parsing model that we use as our baseline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "While this does not re ect the state-of-the-art performance on the WSJ task achieved by the more the complex models of Charniak (2000) and Collins (2000), we regard it as a reasonable baseline for the investigation of corpus e ects on statistical parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58",
            "isKey": false,
            "numCitedBy": 1855,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."
            },
            "slug": "A-Maximum-Entropy-Inspired-Parser-Charniak",
            "title": {
                "fragments": [],
                "text": "A Maximum-Entropy-Inspired Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less and 89.5% when trained and tested on the previously established sections of the Wall Street Journal treebank is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 8
                            }
                        ],
                        "text": "as Me l cuk (1988) and the link grammar of Sleator and Temperley (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 8
                            }
                        ],
                        "text": "as Me l cuk (1988) and the link grammar of Sleator and Temperley (1993). In Collins' Model 1, the word pair statistics occur in the distribution"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726601"
                        ],
                        "name": "R. Hwa",
                        "slug": "R.-Hwa",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Hwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hwa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7117045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3eb3a37a72087c96937a0e21f736c6e661a1d6de",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is prohibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses. They account for only 20% of all constituents. For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases."
            },
            "slug": "Supervised-Grammar-Induction-using-Training-Data-Hwa",
            "title": {
                "fragments": [],
                "text": "Supervised Grammar Induction using Training Data with Limited Constituent Information"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses, and an adaptation strategy is proposed, which produces grammars that parse almost as well as Grammars induced from fully labeled corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c846ee00c6132d70429cc279e8577f63ed05e4",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "slug": "A-Linear-Observed-Time-Statistical-Parser-Based-on-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A statistical parser for natural language that obtains a parsing accuracy that surpasses the best previously published results on the Wall St. Journal domain, and it is shown that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144056359"
                        ],
                        "name": "D. Biber",
                        "slug": "D.-Biber",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Biber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Biber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7525315,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "79a0e0d47f80d9cfb3b50bf6aa6922a0026b93aa",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation."
            },
            "slug": "Using-Register-Diversified-Corpora-for-General-Biber",
            "title": {
                "fragments": [],
                "text": "Using Register-Diversified Corpora for General Language Studies"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47594353"
                        ],
                        "name": "Douglas Roland",
                        "slug": "Douglas-Roland",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Roland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Roland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2110314,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7b8308c7022ccc3861df5e26008eef91b6081949",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies."
            },
            "slug": "How-Verb-Subcategorization-Frequencies-are-Affected-Roland-Jurafsky",
            "title": {
                "fragments": [],
                "text": "How Verb Subcategorization Frequencies Are Affected By Corpus Choice"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is concluded that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335864"
                        ],
                        "name": "D. Temperley",
                        "slug": "D.-Temperley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Temperley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Temperley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5118729,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "5752b8dcec5856b7ad6289bbe1177acce535fba4",
            "isKey": false,
            "numCitedBy": 1030,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a new formal grammatical system called a link grammar. A sequence of words is in the language of a link grammar if there is a way to draw links between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. We have encoded English grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. The formalism is lexical and makes no explicit use of constituents and categories. The breadth of English phenomena that our system handles is quite large. A number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. Our program is written in C, and the entire system may be obtained via anonymous ftp. Several other researchers have begun to use link grammars in their own research."
            },
            "slug": "Parsing-English-with-a-Link-Grammar-Sleator-Temperley",
            "title": {
                "fragments": [],
                "text": "Parsing English with a Link Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work has encoded English grammar into a new formal grammatical system called a link grammar, and written a program (based on new algorithms) for efficiently parsing with this very complex grammar."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47594353"
                        ],
                        "name": "Douglas Roland",
                        "slug": "Douglas-Roland",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Roland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Roland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2882851"
                        ],
                        "name": "L. Menn",
                        "slug": "L.-Menn",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Menn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Menn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2944537"
                        ],
                        "name": "S. Gahl",
                        "slug": "S.-Gahl",
                        "structuredName": {
                            "firstName": "Susanne",
                            "lastName": "Gahl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48233254"
                        ],
                        "name": "Elizabeth Elder",
                        "slug": "Elizabeth-Elder",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Elder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Elder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46674386"
                        ],
                        "name": "C. Riddoch",
                        "slug": "C.-Riddoch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Riddoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Riddoch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Roland et al. (2000) nd that subcategorization frequencies for certain verbs vary signi cantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2289528,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "41cab281d4b89ce8108c247d07e22d8dc20894ae",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the differences in verb subcategorization frequencies across several corpora in an effort to obtain stable cross corpus subcategorization probabilities for use in norming psychological experiments. For the 64 single sense verbs we looked at, subcategorization preferences were remarkably stable between British and American corpora, and between balanced corpora and financial news corpora. Of the verbs that did show differences, these differences were generally found between the balanced corpora and the financial news data. We show that all or nearly all of these shifts in subcategorization are realised via (often subtle) word sense differences. This is an interesting observation in itself, and also suggests that stable cross corpus subcategorization frequencies may be found when verb sense is adequately controlled."
            },
            "slug": "Verb-Subcategorization-Frequency-Differences-News-Roland-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Verb Subcategorization Frequency Differences between Business- News and Balanced Corpora: The Role of Verb Sense"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is shown that all or nearly all of these shifts in subcategorization are realised via (often subtle) word sense differences, and suggests that stable cross corpus subc categorization frequencies may be found when verb sense is adequately controlled."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544946"
                        ],
                        "name": "K. Seymore",
                        "slug": "K.-Seymore",
                        "structuredName": {
                            "firstName": "Kristie",
                            "lastName": "Seymore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Seymore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "Ronald Rosenfeld",
                        "slug": "Ronald-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 192
                            }
                        ],
                        "text": "Significant effort has gone into developing techniques for pruning statistical language models for speech recognition, and we borrow from this work, using the weighted difference technique of Seymore and Rosenfeld (1996). This technique applies to any statistical model which estimates probabilities by backing off, that is, using probabilities from a less specific distribution when no data are available are available for the full distribution, as the following equations show for the general case:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9379111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone."
            },
            "slug": "Scalable-backoff-language-models-Seymore-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Scalable backoff language models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased and shows that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexityand word error rate performance than excluding trigram and bigram based on counts alone."
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 12
                            }
                        ],
                        "text": "As shown by Stolcke (1998), this criterion is an approximation of the relative entropy between the original and pruned distributions, but does not take into account the e ect of changing the backo weight on other events' probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8150809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29053eab305c2b585bcfbb713243b05646e7d62d",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance."
            },
            "slug": "Entropy-based-Pruning-of-Backoff-Language-Models-Stolcke",
            "title": {
                "fragments": [],
                "text": "Entropy-based Pruning of Backoff Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models and shown that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1993044131"
                        ],
                        "name": "\u674e\u5e7c\u5347",
                        "slug": "\u674e\u5e7c\u5347",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u674e\u5e7c\u5347",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u674e\u5e7c\u5347"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6622887"
                        ],
                        "name": "F. G. J. Hayhoe",
                        "slug": "F.-G.-J.-Hayhoe",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Hayhoe",
                            "middleNames": [
                                "G.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. G. J. Hayhoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 222242179,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "0e9a7789ec7cfc34a1bb953530a0e57bd0e8d018",
            "isKey": false,
            "numCitedBy": 40985,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The following is a fundamental reading list for doctoral candidates to use as a guide in preparing for their comprehensive examination in the field of Modernism. A student is expected to have read widely in the field; to be thoroughly familiar with the major writers; and to read widely in the journal literature. The following reading list is suggestive rather than definitive, a list for the student and Committee on Studies to begin with. The list has four sections: \u2022 Poetry \u2022 Drama \u2022 Fiction \u2022 Secondary Sources"
            },
            "slug": "Ph-\u674e\u5e7c\u5347-Hayhoe",
            "title": {
                "fragments": [],
                "text": "Ph"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405337742"
                        ],
                        "name": "I. Mel'cuk",
                        "slug": "I.-Mel'cuk",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Mel'cuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mel'cuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203672231,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1215415ac4e5abb82d7596538bc81e6247d4f020",
            "isKey": false,
            "numCitedBy": 1326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Syntax:-Theory-and-Practice-Mel'cuk",
            "title": {
                "fragments": [],
                "text": "Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "M\u00e9 l\u02d8 cuk. 1988. Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": "M\u00e9 l\u02d8 cuk. 1988. Dependency Syntax: Theory and Practice"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dependency language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Summer Workshop Final Report"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 189
                            }
                        ],
                        "text": "Signi cant e ort has gone into developing techniques for pruning statistical language models for speech recognition, and we borrow from this work, using the weighted di erence technique of Seymore and Rosenfeld (1996). This technique applies to any statistical model which estimates probabilities by backing o , that is, using probabilities from a less speci c distribution when no data are available are available for the full distribution, as the following equations show for the general case:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable backo language models"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP-96, volume 1, pages 232{235, Philadelphia."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 0
                            }
                        ],
                        "text": "Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language. Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998). Roland et al. (2000) nd that subcategorization frequencies for certain verbs vary signi cantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using register-diversi ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 189
                            }
                        ],
                        "text": "Signi cant e ort has gone into developing techniques for pruning statistical language models for speech recognition, and we borrow from this work, using the weighted di erence technique of Seymore and Rosenfeld (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "(The head word of a parent is the same as the head word of its head child.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable backoo language models"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP-96"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 12
                            }
                        ],
                        "text": "As shown by Stolcke (1998), this criterion is an approximation of the relative entropy between the original and pruned distributions, but does not take into account the e ect of changing the backo weight on other events' probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Entropy-based pruning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 124
                            }
                        ],
                        "text": "Models 2 and 3 of Collins (1997) add some slightly more elaborate features to the probability model, as do the additions of Charniak (2000) to the model of Charniak (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "The parsing models of Charniak (2000) and Collins (2000) add more complex features to the parsing model that we use as our baseline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "While this does not re ect the state-of-the-art performance on the WSJ task achieved by the more the complex models of Charniak (2000) and Collins (2000), we regard it as a reasonable baseline for the investigation of corpus e ects on statistical parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum-entropyinspired parser"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1st Annual Meeting of the North American Chapter of the ACL NAACL"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 158
                            }
                        ],
                        "text": "Further details of the model, including the distance features used and special handling of punctuation, conjunctions, and base noun phrases, are described in Collins (1999). The fundamental features of used in the probability distributions are the lexical heads and head tags of each constituent, the co-occurrences of parent nodes and their head children, and the cooccurrences of child nodes with their head siblings and parents."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical parser for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 92
                            }
                        ],
                        "text": "Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dependency language modeling. Summer Workshop Final Report"
            },
            "venue": {
                "fragments": [],
                "text": "Dependency language modeling. Summer Workshop Final Report"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Me l cuk. 1988. Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": "Me l cuk. 1988. Dependency Syntax: Theory and Practice"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Corpus-Variation-and-Parser-Performance-Gildea/ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50?sort=total-citations"
}