{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 185
                            }
                        ],
                        "text": "There are 16 sports actions: high-jump, long-jump, triple-jump, pole-vault, basketball lay-up, bowling, tennis-\n7 Note that here we use the same dataset as (Liu et al, 2009), whereas in (Wang et al, 2011) we used a different version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "A preliminary version of this article has appeared in (Wang et al, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "The resulting trajectories are more robust, in particular in the presence of fast irregular motions (Sand and Teller, 2008; Brox and Malik, 2010; Wang et al, 2011), see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13537104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3afbb0e64fcb70496b44b30b76fac9456cc51e34",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports."
            },
            "slug": "Action-recognition-by-dense-trajectories-Wang-Kl\u00e4ser",
            "title": {
                "fragments": [],
                "text": "Action recognition by dense trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a novel descriptor based on motion boundary histograms, which is robust to camera motion and consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930329"
                        ],
                        "name": "P. Matikainen",
                        "slug": "P.-Matikainen",
                        "structuredName": {
                            "firstName": "Pyry",
                            "lastName": "Matikainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Matikainen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": "Some recent methods (Matikainen et al, 2009; Messing et al, 2009; Sun et al, 2009, 2010) show good results for action recognition by leveraging the motion information of trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 169
                            }
                        ],
                        "text": "\u2026trajectories\nTo quantify the improvement obtained with our dense trajectories, we compare to three baseline trajectories in our experimental results: KLT trajectories (Matikainen et al, 2009; Messing et al, 2009; Sun et al, 2010), SIFT trajectories (Sun et al, 2009, 2010), as well as dense cuboids."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 119
                            }
                        ],
                        "text": "To obtain feature trajectories, either tracking techniques based on the KLT tracker (Lucas and Kanade, 1981) are used (Matikainen et al, 2009; Messing et al, 2009), or SIFT descriptors between consecutive frames are matched (Sun et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6634725,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "b155538a9079e4d8476e11afe09a7ebf5543fd50",
            "isKey": true,
            "numCitedBy": 272,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The defining feature of video compared to still images is motion, and as such the selection of good motion features for action recognition is crucial, especially for bag of words techniques that rely heavily on their features. Existing motion techniques either assume that a difficult problem like background/foreground segmentation has already been solved (contour/silhouette based techniques) or are computationally expensive and prone to noise (optical flow). We present a technique for motion based on quantized trajectory snippets of tracked features. These quantized snippets, or trajectons, rely only on simple feature tracking and are computationally efficient. We demonstrate that within a bag of words framework trajectons can match state of the art results, slightly outperforming histogram of optical flow features on the Hollywood Actions dataset. Additionally, we present qualitative results in a video search task on a custom dataset of challenging YouTube videos."
            },
            "slug": "Trajectons:-Action-recognition-through-the-motion-Matikainen-Hebert",
            "title": {
                "fragments": [],
                "text": "Trajectons: Action recognition through the motion analysis of tracked features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that within a bag of words framework trajectons can match state of the art results, slightly outperforming histogram of optical flow features on the Hollywood Actions dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144706338"
                        ],
                        "name": "Ju Sun",
                        "slug": "Ju-Sun",
                        "structuredName": {
                            "firstName": "Ju",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ju Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145353089"
                        ],
                        "name": "Yadong Mu",
                        "slug": "Yadong-Mu",
                        "structuredName": {
                            "firstName": "Yadong",
                            "lastName": "Mu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yadong Mu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6835136"
                        ],
                        "name": "L. Cheong",
                        "slug": "L.-Cheong",
                        "structuredName": {
                            "firstName": "Loong",
                            "lastName": "Cheong",
                            "middleNames": [
                                "Fah"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cheong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Sun et al (2010) combined both approaches and added random trajectories in low density regions of both trackers in order to increase density."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "Table 5 compares the performance of our trajectory shape\ndescriptor with Sun et al (2010) and Messing et al (2009) on the KTH dataset15."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 98
                            }
                        ],
                        "text": "In the experimental section, we compare to KLT and SIFT trajectories as well as to the results of Sun et al (2010) and Messing et al (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 214
                            }
                        ],
                        "text": "\u2026trajectories\nTo quantify the improvement obtained with our dense trajectories, we compare to three baseline trajectories in our experimental results: KLT trajectories (Matikainen et al, 2009; Messing et al, 2009; Sun et al, 2010), SIFT trajectories (Sun et al, 2009, 2010), as well as dense cuboids."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 66
                            }
                        ],
                        "text": "Some recent methods (Matikainen et al, 2009; Messing et al, 2009; Sun et al, 2009, 2010) show good results for action recognition by leveraging the motion information of trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1484181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "270fa209a25e30ccbdb76fa534e844eaa126f9ea",
            "isKey": true,
            "numCitedBy": 64,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Current research on visual action/activity analysis has mostly exploited appearance-based static feature descriptions, plus statistics of short-range motion fields. The deliberate ignorance of dense, long-duration motion trajectories as features is largely due to the lack of mature mechanism for efficient extraction and quantitative representation of visual trajectories. In this paper, we propose a novel scheme for extraction and representation of dense, long-duration trajectories from video sequences, and demonstrate its ability to handle video sequences containing occlusions, camera motions, and nonrigid deformations. Moreover, we test the scheme on the KTH action recognition dataset [1], and show its promise as a scheme for general purpose long-duration motion description in realistic video sequences."
            },
            "slug": "Activity-recognition-using-dense-long-duration-Sun-Mu",
            "title": {
                "fragments": [],
                "text": "Activity recognition using dense long-duration trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel scheme for extraction and representation of dense, long-duration trajectories from video sequences, and its ability to handle video sequences containing occlusions, camera motions, and nonrigid deformations is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2439921"
                        ],
                        "name": "A. Hervieu",
                        "slug": "A.-Hervieu",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Hervieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hervieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716733"
                        ],
                        "name": "P. Bouthemy",
                        "slug": "P.-Bouthemy",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Bouthemy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bouthemy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781214"
                        ],
                        "name": "J. Cadre",
                        "slug": "J.-Cadre",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Cadre",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cadre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9015959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab9b02bcbf61fb228e15f289dc5687a25c89c6b5",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "This work is dedicated to a statistical trajectory-based approach addressing two issues related to dynamic video content understanding: recognition of events and detection of unexpected events. Appropriate local differential features combining curvature and motion magnitude are defined and robustly computed on the motion trajectories in the image sequence. These features are invariant to image translation, in-the-plane rotation and spatial scaling. The temporal causality of the features is then captured by hidden Markov models dedicated to trajectory description, whose states are properly quantized values. The similarity between trajectories is expressed by exploiting this quantization-based HMM framework. Moreover statistical techniques have been developed for parameter estimations. Evaluations of the method have been conducted on several data sets including real trajectories obtained from sport videos, especially Formula One and ski TV program. The novel method compares favorably with other methods including feature histogram comparisons, HMM/GMM modeling and SVM classification."
            },
            "slug": "A-Statistical-Video-Content-Recognition-Method-on-Hervieu-Bouthemy",
            "title": {
                "fragments": [],
                "text": "A Statistical Video Content Recognition Method Using Invariant Features on Object Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This work is dedicated to a statistical trajectory-based approach addressing two issues related to dynamic video content understanding: recognition of events and detection of unexpected events."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125709"
                        ],
                        "name": "Xinxiao Wu",
                        "slug": "Xinxiao-Wu",
                        "structuredName": {
                            "firstName": "Xinxiao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinxiao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38188040"
                        ],
                        "name": "Dong Xu",
                        "slug": "Dong-Xu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055900"
                        ],
                        "name": "Lixin Duan",
                        "slug": "Lixin-Duan",
                        "structuredName": {
                            "firstName": "Lixin",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lixin Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "\u201cCombined+STP\u201d reports 93.6% and significantly outperforms the state of the art (Wu et al, 2011b) by 5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Wu et al (2011a) decompose Lagrangian particle trajectories into camera-induced and objectinduced components for videos acquired by a moving camera."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4922513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c4c6f59aa6a001dcc6ce87232fe065a2e869e5f",
            "isKey": true,
            "numCitedBy": 242,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We first propose a new spatio-temporal context distribution feature of interest points for human action recognition. Each action video is expressed as a set of relative XYT coordinates between pairwise interest points in a local region. We learn a global GMM (referred to as Universal Background Model, UBM) using the relative coordinate features from all the training videos, and then represent each video as the normalized parameters of a video-specific GMM adapted from the global GMM. In order to capture the spatio-temporal relationships at different levels, multiple GMMs are utilized to describe the context distributions of interest points over multi-scale local regions. To describe the appearance information of an action video, we also propose to use GMM to characterize the distribution of local appearance features from the cuboids centered around the interest points. Accordingly, an action video can be represented by two types of distribution features: 1) multiple GMM distributions of spatio-temporal context; 2) GMM distribution of local video appearance. To effectively fuse these two types of heterogeneous and complementary distribution features, we additionally propose a new learning algorithm, called Multiple Kernel Learning with Augmented Features (AFMKL), to learn an adapted classifier based on multiple kernels and the pre-learned classifiers of other action classes. Extensive experiments on KTH, multi-view IXMAS and complex UCF sports datasets demonstrate that our method generally achieves higher recognition accuracy than other state-of-the-art methods."
            },
            "slug": "Action-recognition-using-context-and-appearance-Wu-Xu",
            "title": {
                "fragments": [],
                "text": "Action recognition using context and appearance distribution features"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new spatio-temporal context distribution feature of interest points for human action recognition, and a new learning algorithm, called Multiple Kernel Learning with Augmented Features (AFMKL), to learn an adapted classifier based on multiple kernels and the pre-learned classifiers of other action classes."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112537744"
                        ],
                        "name": "Shandong Wu",
                        "slug": "Shandong-Wu",
                        "structuredName": {
                            "firstName": "Shandong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shandong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2405613"
                        ],
                        "name": "Omar Oreifej",
                        "slug": "Omar-Oreifej",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Oreifej",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omar Oreifej"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "\u201cCombined+STP\u201d reports 93.6% and significantly outperforms the state of the art (Wu et al, 2011b) by 5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Wu et al (2011a) decompose Lagrangian particle trajectories into camera-induced and objectinduced components for videos acquired by a moving camera."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15444833,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "ac02a6c37a7913194496a914461caedd49febdc9",
            "isKey": true,
            "numCitedBy": 178,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of human actions in a video acquired by a moving camera typically requires standard preprocessing steps such as motion compensation, moving object detection and object tracking. The errors from the motion compensation step propagate to the object detection stage, resulting in miss-detections, which further complicates the tracking stage, resulting in cluttered and incorrect tracks. Therefore, action recognition from a moving camera is considered very challenging. In this paper, we propose a novel approach which does not follow the standard steps, and accordingly avoids the aforementioned difficulties. Our approach is based on Lagrangian particle trajectories which are a set of dense trajectories obtained by advecting optical flow over time, thus capturing the ensemble motions of a scene. This is done in frames of unaligned video, and no object detection is required. In order to handle the moving camera, we propose a novel approach based on low rank optimization, where we decompose the trajectories into their camera-induced and object-induced components. Having obtained the relevant object motion trajectories, we compute a compact set of chaotic invariant features which captures the characteristics of the trajectories. Consequently, a SVM is employed to learn and recognize the human actions using the computed motion features. We performed intensive experiments on multiple benchmark datasets and two new aerial datasets called ARG and APHill, and obtained promising results."
            },
            "slug": "Action-recognition-in-videos-acquired-by-a-moving-Wu-Oreifej",
            "title": {
                "fragments": [],
                "text": "Action recognition in videos acquired by a moving camera using motion decomposition of Lagrangian particle trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach which does not follow the standard steps, and accordingly avoids the aforementioned difficulties is proposed, based on Lagrangian particle trajectories which are a set of dense trajectories obtained by advecting optical flow over time, thus capturing the ensemble motions of a scene."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153055503"
                        ],
                        "name": "G. Piriou",
                        "slug": "G.-Piriou",
                        "structuredName": {
                            "firstName": "Gwena\u00eblle",
                            "lastName": "Piriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716733"
                        ],
                        "name": "P. Bouthemy",
                        "slug": "P.-Bouthemy",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Bouthemy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bouthemy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40040335"
                        ],
                        "name": "Jian-Feng Yao",
                        "slug": "Jian-Feng-Yao",
                        "structuredName": {
                            "firstName": "Jian-Feng",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Feng Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "To take into account camera motion, Piriou et al (2006) define global probabilistic motion models for both the dominant image motion (assumed to be due to camera motion) and the residual image motion (related to scene motion) to recognize dynamic video content."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10944826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "551f132d9c5df83db3683ffcfb6975802dfed086",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The exploitation of video data requires methods able to extract high-level information from the images. Video summarization, video retrieval, or video surveillance are examples of applications. In this paper, we tackle the challenging problem of recognizing dynamic video contents from low-level motion features. We adopt a statistical approach involving modeling, (supervised) learning, and classification issues. Because of the diversity of video content (even for a given class of events), we have to design appropriate models of visual motion and learn them from videos. We have defined original parsimonious global probabilistic motion models, both for the dominant image motion (assumed to be due to the camera motion) and the residual image motion (related to scene motion). Motion measurements include affine motion models to capture the camera motion and low-level local motion features to account for scene motion. Motion learning and recognition are solved using maximum likelihood criteria. To validate the interest of the proposed motion modeling and recognition framework, we report dynamic content recognition results on sports videos"
            },
            "slug": "Recognition-of-Dynamic-Video-Contents-With-Global-Piriou-Bouthemy",
            "title": {
                "fragments": [],
                "text": "Recognition of Dynamic Video Contents With Global Probabilistic Models of Visual Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper tackles the challenging problem of recognizing dynamic video contents from low-level motion features with a statistical approach involving modeling, (supervised) learning, and classification issues and reports dynamic content recognition results on sports videos."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30507218"
                        ],
                        "name": "Wang-Chou Lu",
                        "slug": "Wang-Chou-Lu",
                        "structuredName": {
                            "firstName": "Wang-Chou",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang-Chou Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108898473"
                        ],
                        "name": "Y. Wang",
                        "slug": "Y.-Wang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720473"
                        ],
                        "name": "Chu-Song Chen",
                        "slug": "Chu-Song-Chen",
                        "structuredName": {
                            "firstName": "Chu-Song",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chu-Song Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 34
                            }
                        ],
                        "text": "A similar approach is proposed by Lu et al (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9371227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22c7c57cee7173156eab0e38c22eac0a1b48f524",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We proposes an unsupervised method to address videoobject extraction (VOE) in uncontrolled videos, i.e. videoscaptured by low-resolution and freely moving cameras. Weadvocate the use of dense optical-flow trajectories (DOTs),which are obtained by propagating the optical flow informationat the pixel level. Therefore, no interest point extractionis required in our framework. To integrate colorand and shape information of moving objects, we groupthe DOTs at the super-pixel level to extract co-motion regions,and use the associated pyramid histogram of orientedgradients (PHOG) descriptors to extract objects of interestacross video frames. Our approach for VOE is easy to implement,and the use of DOTs for both motion segmentationand object tracking is more robust than existing trajectorybasedmethods. Experiments on several video sequencesexhibit the feasibility of our proposed VOE framework."
            },
            "slug": "Learning-Dense-Optical-Flow-Trajectory-Patterns-for-Lu-Wang",
            "title": {
                "fragments": [],
                "text": "Learning Dense Optical-Flow Trajectory Patterns for Video Object Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The proposed unsupervised method to address videoobject extraction (VOE) in uncontrolled videos, i.e. videos captured by low-resolution and freely moving cameras, advocates the use of dense optical-flow trajectories (DOTs), which are obtained by propagating the optical flow information at the pixel level."
            },
            "venue": {
                "fragments": [],
                "text": "2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1871915"
                        ],
                        "name": "H. Uemura",
                        "slug": "H.-Uemura",
                        "structuredName": {
                            "firstName": "Hirofumi",
                            "lastName": "Uemura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Uemura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101851"
                        ],
                        "name": "S. Ishikawa",
                        "slug": "S.-Ishikawa",
                        "structuredName": {
                            "firstName": "Seiji",
                            "lastName": "Ishikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ishikawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 93
                            }
                        ],
                        "text": "Compared to video stabilization (Ikizler-Cinbis and Sclaroff, 2010) and motion compensation (Uemura et al, 2008), this is a simpler way to discount for camera motion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Uemura et al (2008) segment feature tracks to separate motion characterizing actions from the dominant camera motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16744939,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6fae85e53ed84c91d5bec49b32b4ebb6c9d99f2d",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses an approach to human action recognition via local feature tracking and robust estimation of background motion. The main contribution is a robust feature extraction algorithm based on KLT tracker and SIFT as well as a method for estimating dominant planes in the scene. Multiple interest point detectors are used to provide large number of features for every frame. The motion vectors for the features are estimated using optical flow and SIFT based matching. The features are combined with image segmentation to estimate dominant homographies, and then separated into static and moving ones regardless the camera motion. The action recognition approach can handle camera motion, zoom, human appearance variations, background clutter and occlusion. The motion compensation shows very good accuracy on a number of test sequences. The recognition system is extensively compared to state-of-the art action recognition methods and the results are improved."
            },
            "slug": "Feature-Tracking-and-Motion-Compensation-for-Action-Uemura-Ishikawa",
            "title": {
                "fragments": [],
                "text": "Feature Tracking and Motion Compensation for Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An approach to human action recognition via local feature tracking and robust estimation of background motion through a robust feature extraction algorithm based on KLT tracker and SIFT as well as a method for estimating dominant planes in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404610182"
                        ],
                        "name": "Orit Kliper-Gross",
                        "slug": "Orit-Kliper-Gross",
                        "structuredName": {
                            "firstName": "Orit",
                            "lastName": "Kliper-Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Orit Kliper-Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2916582"
                        ],
                        "name": "Yaron Gurovich",
                        "slug": "Yaron-Gurovich",
                        "structuredName": {
                            "firstName": "Yaron",
                            "lastName": "Gurovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaron Gurovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756099"
                        ],
                        "name": "Tal Hassner",
                        "slug": "Tal-Hassner",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Hassner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Hassner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48519520"
                        ],
                        "name": "L. Wolf",
                        "slug": "L.-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Kliper-Gross et al (2012) reported 72.7% by designing descriptors to capture local changes in motion directions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9754357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "993a2c02a5a3263b3047202e3d86aa9a0dd6ebfe",
            "isKey": true,
            "numCitedBy": 201,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Action Recognition in videos is an active research field that is fueled by an acute need, spanning several application domains. Still, existing systems fall short of the applications' needs in real-world scenarios, where the quality of the video is less than optimal and the viewpoint is uncontrolled and often not static. In this paper, we consider the key elements of motion encoding and focus on capturing local changes in motion directions. In addition, we decouple image edges from motion edges using a suppression mechanism, and compensate for global camera motion by using an especially fitted registration scheme. Combined with a standard bag-of-words technique, our methods achieves state-of-the-art performance in the most recent and challenging benchmarks."
            },
            "slug": "Motion-Interchange-Patterns-for-Action-Recognition-Kliper-Gross-Gurovich",
            "title": {
                "fragments": [],
                "text": "Motion Interchange Patterns for Action Recognition in Unconstrained Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper considers the key elements of motion encoding and focuses on capturing local changes in motion directions, and decouple image edges from motion edges using a suppression mechanism, and compensate for global camera motion by using an especially fitted registration scheme."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35033378"
                        ],
                        "name": "M. M. Ullah",
                        "slug": "M.-M.-Ullah",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Ullah",
                            "middleNames": [
                                "Muneeb"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M. Ullah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2483430"
                        ],
                        "name": "S. N. Parizi",
                        "slug": "S.-N.-Parizi",
                        "structuredName": {
                            "firstName": "Sobhan",
                            "lastName": "Parizi",
                            "middleNames": [
                                "Naderi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. N. Parizi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18559869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97982cbe8b006b24d9a39fb02c24ef171db88876",
            "isKey": true,
            "numCitedBy": 133,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features have recently shown promising results within Bag-of-Features (BoF) approach to action recognition in video. Pure local features and descriptors, however, provide only limited discriminative power implying ambiguity among features and sub-optimal classification performance. In this work, we propose to disambiguate local space-time features and to improve action recognition by integrating additional nonlocal cues with BoF representation. For this purpose, we decompose video into region classes and augment local features with corresponding region-class labels. In particular, we investigate unsupervised and supervised video segmentation using (i) motion-based foreground segmentation, (ii) person detection, (iii) static action detection and (iv) object detection. While such segmentation methods might be imperfect, they provide complementary region-level information to local features. We demonstrate how this information can be integrated with BoF representations in a kernel-combination framework. We evaluate our method on the recent and challenging Hollywood-2 action dataset and demonstrate significant improvements."
            },
            "slug": "Improving-bag-of-features-action-recognition-with-Ullah-Parizi",
            "title": {
                "fragments": [],
                "text": "Improving bag-of-features action recognition with non-local cues"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work decompose video into region classes and augment local features with corresponding region-class labels and demonstrates how this information can be integrated with BoF representations in a kernel-combination framework."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123446103"
                        ],
                        "name": "Hilde Kuehne",
                        "slug": "Hilde-Kuehne",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Kuehne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilde Kuehne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676982"
                        ],
                        "name": "Hueihan Jhuang",
                        "slug": "Hueihan-Jhuang",
                        "structuredName": {
                            "firstName": "Hueihan",
                            "lastName": "Jhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hueihan Jhuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930964"
                        ],
                        "name": "Est\u00edbaliz Garrote",
                        "slug": "Est\u00edbaliz-Garrote",
                        "structuredName": {
                            "firstName": "Est\u00edbaliz",
                            "lastName": "Garrote",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Est\u00edbaliz Garrote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 22
                            }
                        ],
                        "text": "The HMDB51 dataset14 (Kuehne et al, 2011) is collected from a variety of sources ranging from digitized movies to YouTube videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 239
                            }
                        ],
                        "text": "Datasets original stabilized\nTrajectory 28.0% 34.0%\nHOG 27.9% 30.1%\nHOF 31.5% 39.5%\nMBH 43.2% 44.9%\nCombined 46.6% 50.8%\nTable 3 Descriptor comparison on HMDB51 for the original videos and their stabilized version provided by the authors (Kuehne et al, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 63
                            }
                        ],
                        "text": "We follow the original protocol using three train-test splits (Kuehne et al, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206769852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b3b8848a311c501e704c45c6d50430ab7068956",
            "isKey": true,
            "numCitedBy": 2547,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion."
            },
            "slug": "HMDB:-A-large-video-database-for-human-motion-Kuehne-Jhuang",
            "title": {
                "fragments": [],
                "text": "HMDB: A large video database for human motion recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper uses the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube, to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800425"
                        ],
                        "name": "Jingen Liu",
                        "slug": "Jingen-Liu",
                        "structuredName": {
                            "firstName": "Jingen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor. Willems et al (2008) generalizes the image SURF descriptor (Bay et al, 2006) to the video domain by computing weighted sums of uniformly sampled responses of spatio-temporal Haar wavelets. Yeffet and Wolf (2009) propose Local Trinary Patterns for videos as extension of Local Binary Patterns (LBP) (Ojala et al, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "The YouTube dataset6 (Liu et al, 2009) contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor. Willems et al (2008) generalizes the image SURF descriptor (Bay et al, 2006) to the video domain by computing weighted sums of uniformly sampled responses of spatio-temporal Haar wavelets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 155
                            }
                        ],
                        "text": "There are 16 sports actions: high-jump, long-jump, triple-jump, pole-vault, basketball lay-up, bowling, tennis-\n7 Note that here we use the same dataset as (Liu et al, 2009), whereas in (Wang et al, 2011) we used a different version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "We follow the original setup (Liu et al, 2009), using Leave-One-Out Cross-Validation for a pre-defined set\n5 http://www.nada.kth.se/cvap/actions/ 6 http://www.cs.ucf.edu/\u02dcliujg/YouTube_\nAction_dataset.html\nof 25 groups."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206597309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aca29d7bbbf54078f842c8ca1d75d8d8c68191d2",
            "isKey": true,
            "numCitedBy": 967,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a systematic framework for recognizing realistic actions from videos \u201cin the wild\u201d. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization."
            },
            "slug": "Recognizing-realistic-actions-from-videos-\u201cin-the-Liu-Luo",
            "title": {
                "fragments": [],
                "text": "Recognizing realistic actions from videos \u201cin the wild\u201d"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents a systematic framework for recognizing realistic actions from videos \u201cin the wild\u201d, and uses motion statistics to acquire stable motion features and clean static features, and PageRank is used to mine the most informative static features."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770205"
                        ],
                        "name": "Adriana Kovashka",
                        "slug": "Adriana-Kovashka",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Kovashka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adriana Kovashka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 55
                            }
                        ],
                        "text": "1% which is around 2% better than the state of the art (Kovashka and Grauman, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 95
                            }
                        ],
                        "text": "On UCF Sports \u201ccombined+STP\u201d achieves 89.1% which is around 2% better than the state of the art (Kovashka and Grauman, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 966135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698305079aad248aa23bbc87d12a9452f6fda579",
            "isKey": true,
            "numCitedBy": 553,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work shows how to use local spatio-temporal features to learn models of realistic human actions from video. However, existing methods typically rely on a predefined spatial binning of the local descriptors to impose spatial information beyond a pure \u201cbag-of-words\u201d model, and thus may fail to capture the most informative space-time relationships. We propose to learn the shapes of space-time feature neighborhoods that are most discriminative for a given action category. Given a set of training videos, our method first extracts local motion and appearance features, quantizes them to a visual vocabulary, and then forms candidate neighborhoods consisting of the words associated with nearby points and their orientation with respect to the central interest point. Rather than dictate a particular scaling of the spatial and temporal dimensions to determine which points are near, we show how to learn the class-specific distance functions that form the most informative configurations. Descriptors for these variable-sized neighborhoods are then recursively mapped to higher-level vocabularies, producing a hierarchy of space-time configurations at successively broader scales. Our approach yields state-of-theart performance on the UCF Sports and KTH datasets."
            },
            "slug": "Learning-a-hierarchy-of-discriminative-space-time-Kovashka-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning a hierarchy of discriminative space-time neighborhood features for human action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to learn the shapes of space-time feature neighborhoods that are most discriminative for a given action category by extracting local motion and appearance features, quantizing them to a visual vocabulary, and forming candidate neighborhoods that form the most informative configurations."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35033378"
                        ],
                        "name": "M. M. Ullah",
                        "slug": "M.-M.-Ullah",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Ullah",
                            "middleNames": [
                                "Muneeb"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M. Ullah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 158
                            }
                        ],
                        "text": "More specifically, about 100 interest points are detected in each frame and are added to the tracker \u2013 this is somewhat denser than space-time interest points (Wang et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 131
                            }
                        ],
                        "text": "This is also consistent with dense sampling at regular positions where more features in general improve the results up to a point (Wang et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 13
                            }
                        ],
                        "text": "As shown by (Wang et al, 2009), HOG (histograms of oriented gradients) and HOF (histograms of optical flow) descriptors (Laptev et al, 2008) yield excellent results on a variety of datasets in comparison with other state-of-the-art descriptors for action recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 70
                            }
                        ],
                        "text": "The same is observed for action recognition in a recent evaluation by Wang et al (2009), where dense sampling at regular positions in space and time outperforms state-of-the-art spatio-temporal interest point de-\ntectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 72
                            }
                        ],
                        "text": "Their descriptors show state-of-the-art results in a recent evaluation (Wang et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "Note that the flipped version of the tested sequence is removed from the training set as in (Wang et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6367640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a39e6968580762ac5ae3cd064e86e1849f3efb7f",
            "isKey": false,
            "numCitedBy": 1452,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations."
            },
            "slug": "Evaluation-of-Local-Spatio-temporal-Features-for-Wang-Ullah",
            "title": {
                "fragments": [],
                "text": "Evaluation of Local Spatio-temporal Features for Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that regular sampling of space-time features consistently outperforms all testedspace-time interest point detectors for human actions in realistic settings and is a consistent ranking for the majority of methods over different datasets."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving bag-offeatures action recognition with non-local cues. In British machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712738"
                        ],
                        "name": "C. Sch\u00fcldt",
                        "slug": "C.-Sch\u00fcldt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Sch\u00fcldt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sch\u00fcldt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "To describe spatio-temporal points, Schu\u0308ldt et al (2004) use higher order derivatives (local jets)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026from higher order derivatives (local jets), gradient information, optical flow, and brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 25
                            }
                        ],
                        "text": "As in the initial paper (Schu\u0308ldt et al, 2004), we train and evaluate a multi-class classifier and report average accuracy over all classes."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 18
                            }
                        ],
                        "text": "The KTH dataset5 (Schu\u0308ldt et al, 2004) consists of six human action classes: walking, jogging, running, boxing, waving and clapping."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8777811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b480f6a3750b4cebaf1db205692c8321d45926a2",
            "isKey": true,
            "numCitedBy": 3080,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition."
            },
            "slug": "Recognizing-human-actions:-a-local-SVM-approach-Sch\u00fcldt-Laptev",
            "title": {
                "fragments": [],
                "text": "Recognizing human actions: a local SVM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition and presents the presented results of action recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119821"
                        ],
                        "name": "Daniel Weinland",
                        "slug": "Daniel-Weinland",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weinland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Weinland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898850"
                        ],
                        "name": "R\u00e9mi Ronfard",
                        "slug": "R\u00e9mi-Ronfard",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Ronfard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Ronfard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719388"
                        ],
                        "name": "Edmond Boyer",
                        "slug": "Edmond-Boyer",
                        "structuredName": {
                            "firstName": "Edmond",
                            "lastName": "Boyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edmond Boyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Weinland et al (2006) reported 93.33% using motion history volumes of 3D human body models for training and testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6087524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f81eb3ea2d96629b220993151625419f79cf656a",
            "isKey": true,
            "numCitedBy": 918,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Free-viewpoint-action-recognition-using-motion-Weinland-Ronfard",
            "title": {
                "fragments": [],
                "text": "Free viewpoint action recognition using motion history volumes"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2948848"
                        ],
                        "name": "S. Sadanand",
                        "slug": "S.-Sadanand",
                        "structuredName": {
                            "firstName": "Sreemanananth",
                            "lastName": "Sadanand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sadanand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 66
                            }
                        ],
                        "text": "On the recent HMDB51 dataset, we outperform the state of the art (Sadanand and Corso, 2012) by 16% using only \u201cMBH\u201d."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9208396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d90cb88d89408daf4a0fe5ac341a6b9db747a556",
            "isKey": false,
            "numCitedBy": 764,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Activity recognition in video is dominated by low- and mid-level features, and while demonstrably capable, by nature, these features carry little semantic meaning. Inspired by the recent object bank approach to image representation, we present Action Bank, a new high-level representation of video. Action bank is comprised of many individual action detectors sampled broadly in semantic space as well as viewpoint space. Our representation is constructed to be semantically rich and even when paired with simple linear SVM classifiers is capable of highly discriminative performance. We have tested action bank on four major activity recognition benchmarks. In all cases, our performance is better than the state of the art, namely 98.2% on KTH (better by 3.3%), 95.0% on UCF Sports (better by 3.7%), 57.9% on UCF50 (baseline is 47.9%), and 26.9% on HMDB51 (baseline is 23.2%). Furthermore, when we analyze the classifiers, we find strong transfer of semantics from the constituent action detectors to the bank classifier."
            },
            "slug": "Action-bank:-A-high-level-representation-of-in-Sadanand-Corso",
            "title": {
                "fragments": [],
                "text": "Action bank: A high-level representation of activity in video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Inspired by the recent object bank approach to image representation, Action Bank is presented, a new high-level representation of video comprised of many individual action detectors sampled broadly in semantic space as well as viewpoint space that is capable of highly discriminative performance."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731402"
                        ],
                        "name": "Shu-Fai Wong",
                        "slug": "Shu-Fai-Wong",
                        "structuredName": {
                            "firstName": "Shu-Fai",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu-Fai Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Wong and Cipolla (2007) have added global informa-\n1 http://lear.inrialpes.fr/software\ntion to the interest point detection by applying non-negative matrix factorization (NNMF) on the entire video sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11061616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "033795f6cf5f16c062ff0197f9f52aa2d8adb91b",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Local spatiotemporal features or interest points provide compact but descriptive representations for efficient video analysis and motion recognition. Current local feature extraction approaches involve either local filtering or entropy computation which ignore global information (e.g. large blobs of moving pixels) in video inputs. This paper presents a novel extraction method which utilises global information from each video input so that moving parts such as a moving hand can be identified and are used to select relevant interest points for a condensed representation. The proposed method involves obtaining a small set of subspace images, which can synthesise frames in the video input from their corresponding coefficient vectors, and then detecting interest points from the subspaces and the coefficient vectors. Experimental results indicate that the proposed method can yield a sparser set of interest points for motion recognition than existing methods."
            },
            "slug": "Extracting-Spatiotemporal-Interest-Points-using-Wong-Cipolla",
            "title": {
                "fragments": [],
                "text": "Extracting Spatiotemporal Interest Points using Global Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel extraction method which utilises global information from each video input so that moving parts such as a moving hand can be identified and are used to select relevant interest points for a condensed representation."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47018987"
                        ],
                        "name": "P. Sand",
                        "slug": "P.-Sand",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720894"
                        ],
                        "name": "S. Teller",
                        "slug": "S.-Teller",
                        "structuredName": {
                            "firstName": "Seth",
                            "lastName": "Teller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Sand and Teller (2008) investigate long range motion estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 101
                            }
                        ],
                        "text": "The resulting trajectories are more robust, in particular in the presence of fast irregular motions (Sand and Teller, 2008; Brox and Malik, 2010; Wang et al, 2011), see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7697706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43fe15d04f220b6d537d35a8412903acb4700f6",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new approach to motion estimation in video. We represent video motion using a set of particles. Each particle is an image point sample with a long-duration trajectory and other properties. To optimize particle trajectories we measure appearance consistency along the particle trajectories and distortion between the particles. The resulting motion representation is useful for a variety of applications and cannot be directly obtained using existing methods such as optical flow or feature tracking. We demonstrate the algorithm on challenging real-world videos that include complex scene geometry, multiple types of occlusion, regions with low texture, and non-rigid deformations."
            },
            "slug": "Particle-Video:-Long-Range-Motion-Estimation-Using-Sand-Teller",
            "title": {
                "fragments": [],
                "text": "Particle Video: Long-Range Motion Estimation Using Point Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A new approach to motion estimation in video using a set of particles that is useful for a variety of applications and cannot be directly obtained using existing methods such as optical flow or feature tracking."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34316743"
                        ],
                        "name": "Junsong Yuan",
                        "slug": "Junsong-Yuan",
                        "structuredName": {
                            "firstName": "Junsong",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junsong Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691128"
                        ],
                        "name": "Zicheng Liu",
                        "slug": "Zicheng-Liu",
                        "structuredName": {
                            "firstName": "Zicheng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zicheng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50118130"
                        ],
                        "name": "Ying Wu",
                        "slug": "Ying-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3728773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b97a5850d257e068f5c0b5f6eb303920a81d1cd",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Actions are spatiotemporal patterns. Similar to the sliding window-based object detection, action detection finds the reoccurrences of such spatiotemporal patterns through pattern matching, by handling cluttered and dynamic backgrounds and other types of action variations. We address two critical issues in pattern matching-based action detection: 1) the intrapattern variations in actions, and 2) the computational efficiency in performing action pattern search in cluttered scenes. First, we propose a discriminative pattern matching criterion for action classification, called naive Bayes mutual information maximization (NBMIM). Each action is characterized by a collection of spatiotemporal invariant features and we match it with an action class by measuring the mutual information between them. Based on this matching criterion, action detection is to localize a subvolume in the volumetric video space that has the maximum mutual information toward a specific action class. A novel spatiotemporal branch-and-bound (STBB) search algorithm is designed to efficiently find the optimal solution. Our proposed action detection method does not rely on the results of human detection, tracking, or background subtraction. It can handle action variations such as performing speed and style variations as well as scale changes well. It is also insensitive to dynamic and cluttered backgrounds and even to partial occlusions. The cross-data set experiments on action detection, including KTH, CMU action data sets, and another new MSR action data set, demonstrate the effectiveness and efficiency of the proposed multiclass multiple-instance action detection method."
            },
            "slug": "Discriminative-Video-Pattern-Search-for-Efficient-Yuan-Liu",
            "title": {
                "fragments": [],
                "text": "Discriminative Video Pattern Search for Efficient Action Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a discriminative pattern matching criterion for action classification, called naive Bayes mutual information maximization (NBMIM), and proposes a novel spatiotemporal branch-and-bound (STBB) search algorithm to efficiently find the optimal solution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 117
                            }
                        ],
                        "text": "This is presumably due to the fact that the descriptor is based on spatial histograms, which are not well localized (Brox and Malik, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 122
                            }
                        ],
                        "text": "In this section, we compare to a state-of-the-art optical flow algorithm, the large displacement optical flow (LDOF) from Brox and Malik (2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4129821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffd97ed7408fa75b86d4c8e4280f8d730ffa2cf0",
            "isKey": false,
            "numCitedBy": 1304,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied."
            },
            "slug": "Large-Displacement-Optical-Flow:-Descriptor-in-Brox-Malik",
            "title": {
                "fragments": [],
                "text": "Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A way to approach the problem of dense optical flow estimation by integrating rich descriptors into the variational optical flow setting, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving bag-offeatures action recognition with non-local cues. In British machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 288
                            }
                        ],
                        "text": "Feature descriptors range from higher order derivatives (local jets), gradient information, optical flow, and brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 186
                            }
                        ],
                        "text": "\u2026brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "Previous local descriptors (Dolla\u0301r et al, 2005; Kla\u0308ser et al, 2008; Laptev et al, 2008; Scovanner et al, 2007; Willems et al, 2008) are usually computed in a 3D video volume around interest points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5607238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719",
            "isKey": false,
            "numCitedBy": 1876,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we present a novel local descriptor for video sequences. The proposed descriptor is based on histograms of oriented 3D spatio-temporal gradients. Our contribution is four-fold. (i) To compute 3D gradients for arbitrary scales, we develop a memory-efficient algorithm based on integral videos. (ii) We propose a generic 3D orientation quantization which is based on regular polyhedrons. (iii) We perform an in-depth evaluation of all descriptor parameters and optimize them for action recognition. (iv) We apply our descriptor to various action datasets (KTH, Weizmann, Hollywood) and show that we outperform the state-of-the-art."
            },
            "slug": "A-Spatio-Temporal-Descriptor-Based-on-3D-Gradients-Kl\u00e4ser-Marszalek",
            "title": {
                "fragments": [],
                "text": "A Spatio-Temporal Descriptor Based on 3D-Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work presents a novel local descriptor for video sequences based on histograms of oriented 3D spatio-temporal gradients based on regular polyhedrons which outperform the state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48858384"
                        ],
                        "name": "William Brendel",
                        "slug": "William-Brendel",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Brendel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Brendel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 62
                            }
                        ],
                        "text": "On YouTube, we significantly outperform the state of the art (Brendel and Todorovic, 2010) by over 8%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9453015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e07da41678f4b3cc1d149caf117db2c7b9f62ff7",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an exemplar-based approach to detecting and localizing human actions, such as running, cycling, and swinging, in realistic videos with dynamic backgrounds. We show that such activities can be compactly represented as time series of a few snapshots of human-body parts in their most discriminative postures, relative to other activity classes. This enables our approach to efficiently store multiple diverse exemplars per activity class, and quickly retrieve exemplars that best match the query by aligning their short time-series representations. Given a set of example videos of all activity classes, we extract multiscale regions from all their frames, and then learn a sparse dictionary of most discriminative regions. The Viterbi algorithm is then used to track detections of the learned codewords across frames of each video, resulting in their compact time-series representations. Dictionary learning is cast within the largemargin framework, wherein we study the effects of l1 and l2 regularization on the sparseness of the resulting dictionaries. Our experiments demonstrate robustness and scalability of our approach on challenging YouTube videos."
            },
            "slug": "Activities-as-Time-Series-of-Human-Postures-Brendel-Todorovic",
            "title": {
                "fragments": [],
                "text": "Activities as Time Series of Human Postures"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents an exemplar-based approach to detecting and localizing human actions, such as running, cycling, and swinging, in realistic videos with dynamic backgrounds, and shows that such activities can be compactly represented as time series of a few snapshots of human-body parts in their most discriminative postures, relative to other activity classes."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799820"
                        ],
                        "name": "Adrien Gaidon",
                        "slug": "Adrien-Gaidon",
                        "structuredName": {
                            "firstName": "Adrien",
                            "lastName": "Gaidon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrien Gaidon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753355"
                        ],
                        "name": "Z. Harchaoui",
                        "slug": "Z.-Harchaoui",
                        "structuredName": {
                            "firstName": "Za\u00efd",
                            "lastName": "Harchaoui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harchaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Gaidon et al (2012) achieved 82.7% by clustering dense trajectories and modeling the relationship between the clusters via a tree structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2664004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd580aba9d264512d7fb256d42f8fc87dd6df16",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of recognizing complex activities, such as pole vaulting, which are characterized by the composition of a large and variable number of different spatio-temporal parts. We represent a video as a hierarchy of mid-level motion components. This hierarchy is a data-driven decomposition specific to each video. We introduce a divisive clustering algorithm that can efficiently extract a hierarchy over a large number of local trajectories. We use this structure to represent a video as an unordered binary tree. This tree is modeled by nested histograms of local motion features. We provide an efficient positive definite kernel that computes the structural and visual similarity of two tree decompositions by relying on models of their edges. Contrary to most approaches based on action decompositions, we propose to use the full hierarchical action structure instead of selecting a small fixed number of parts. We present experimental results on two recent challenging benchmarks that focus on complex activities and show that our kernel on per-video hierarchies allows to efficiently discriminate between complex activities sharing common action parts. Our approach improves over the state of the art, including unstructured activity models, baselines using other motion decomposition algorithms, graph matching, and latent models explicitly selecting a fixed number of parts."
            },
            "slug": "Recognizing-activities-with-cluster-trees-of-Gaidon-Harchaoui",
            "title": {
                "fragments": [],
                "text": "Recognizing activities with cluster-trees of tracklets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a divisive clustering algorithm that can efficiently extract a hierarchy over a large number of local trajectories and provides an efficient positive definite kernel that computes the structural and visual similarity of two tree decompositions by relying on models of their edges."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789372"
                        ],
                        "name": "N. Sundaram",
                        "slug": "N.-Sundaram",
                        "structuredName": {
                            "firstName": "Narayanan",
                            "lastName": "Sundaram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sundaram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 88
                            }
                        ],
                        "text": "As the median filter\nis more robust to outliers than bilinear interpolation (as used by Sundaram et al (2010)), it improves trajectories for points at motion boundaries that would otherwise be smoothed out (c.f., Figure 4)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13438488,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "d3ef059816bcf2d2b519ac36935c61a5a5e81e9b",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Dense and accurate motion tracking is an important requirement for many video feature extraction algorithms. In this paper we provide a method for computing point trajectories based on a fast parallel implementation of a recent optical flow algorithm that tolerates fast motion. The parallel implementation of large displacement optical flow runs about 78\u00d7 faster than the serial C++ version. This makes it practical to use in a variety of applications, among them point tracking. In the course of obtaining the fast implementation, we also proved that the fixed point matrix obtained in the optical flow technique is positive semi-definite. We compare the point tracking to the most commonly used motion tracker - the KLT tracker - on a number of sequences with ground truth motion. Our resulting technique tracks up to three orders of magnitude more points and is 46% more accurate than the KLT tracker. It also provides a tracking density of 48% and has an occlusion error of 3% compared to a density of 0.1% and occlusion error of 8% for the KLT tracker. Compared to the Particle Video tracker, we achieve 66% better accuracy while retaining the ability to handle large displacements while running an order of magnitude faster."
            },
            "slug": "Dense-Point-Trajectories-by-GPU-Accelerated-Large-Sundaram-Brox",
            "title": {
                "fragments": [],
                "text": "Dense Point Trajectories by GPU-Accelerated Large Displacement Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper provides a method for computing point trajectories based on a fast parallel implementation of a recent optical flow algorithm that tolerates fast motion and proves that the fixed point matrix obtained in the optical flow technique is positive semi-definite."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109675919"
                        ],
                        "name": "Chih-Wei Chen",
                        "slug": "Chih-Wei-Chen",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 63
                            }
                        ],
                        "text": "We report mean average precision over all classes (mAP) as in (Niebles et al, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "The Olympic Sports dataset11 (Niebles et al, 2010) consists of athletes practicing different sports, which are collected from YouTube and annotated using Amazon Mechanical Turk."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Niebles et al (2010) obtain 72.1% on Olympic Sports with an approach which models the temporal structure of a human action."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14779543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "994a7b903b937f8b177c035db86852091fd26aa7",
            "isKey": true,
            "numCitedBy": 745,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent research in human activity recognition has focused on the problem of recognizing simple repetitive (walking, running, waving) and punctual actions (sitting up, opening a door, hugging). However, many interesting human activities are characterized by a complex temporal composition of simple actions. Automatic recognition of such complex actions can benefit from a good understanding of the temporal structures. We present in this paper a framework for modeling motion by exploiting the temporal structure of the human activities. In our framework, we represent activities as temporal compositions of motion segments. We train a discriminative model that encodes a temporal decomposition of video sequences, and appearance models for each motion segment. In recognition, a query video is matched to the model according to the learned appearances and motion segment decomposition. Classification is made based on the quality of matching between the motion segment classifiers and the temporal segments in the query sequence. To validate our approach, we introduce a new dataset of complex Olympic Sports activities. We show that our algorithm performs better than other state of the art methods."
            },
            "slug": "Modeling-Temporal-Structure-of-Decomposable-Motion-Niebles-Chen",
            "title": {
                "fragments": [],
                "text": "Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for modeling motion by exploiting the temporal structure of the human activities, which represents activities as temporal compositions of motion segments, and shows that the algorithm performs better than other state of the art methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving bag-offeatures action recognition with non-local cues. In British machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66971399"
                        ],
                        "name": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys",
                        "slug": "Svenska-S\u00e4llskapet-f\u00f6r-Automatiserad-Bildanalys",
                        "structuredName": {
                            "firstName": "Svenska",
                            "lastName": "Bildanalys",
                            "middleNames": [
                                "S\u00e4llskapet",
                                "f\u00f6r",
                                "Automatiserad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58872812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257a1fdf27448aa24a89e23ba8e58b5cb0db074e",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-...-Scandinavian-Conference-on-Bildanalys",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ... Scandinavian Conference on Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119821"
                        ],
                        "name": "Daniel Weinland",
                        "slug": "Daniel-Weinland",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weinland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Weinland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719388"
                        ],
                        "name": "Edmond Boyer",
                        "slug": "Edmond-Boyer",
                        "structuredName": {
                            "firstName": "Edmond",
                            "lastName": "Boyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edmond Boyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898850"
                        ],
                        "name": "R\u00e9mi Ronfard",
                        "slug": "R\u00e9mi-Ronfard",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Ronfard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Ronfard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "We apply Leave-One-Actor-Out Cross-Validation as recommended in the original paper (Weinland et al, 2007), and use samples from all five views for training and testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 9
                            }
                        ],
                        "text": "IXMAS10 (Weinland et al, 2007) is a dataset recorded by cameras from five different viewpoints, as shown in the fifth row of Figure 8."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1407568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "deb259adece9095000a79f86aabfb303580be1cd",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of learning compact, view-independent, realistic 3D models of human actions recorded with multiple cameras, for the purpose of recognizing those same actions from a single or few cameras, without prior knowledge about the relative orientations between the cameras and the subjects. To this aim, we propose a new framework where we model actions using three dimensional occupancy grids, built from multiple viewpoints, in an exemplar-based HMM. The novelty is, that a 3D reconstruction is not required during the recognition phase, instead learned 3D exemplars are used to produce 2D image information that is compared to the observations. Parameters that describe image projections are added as latent variables in the recognition process. In addition, the temporal Markov dependency applied to view parameters allows them to evolve during recognition as with a smoothly moving camera. The effectiveness of the framework is demonstrated with experiments on real datasets and with challenging recognition scenarios."
            },
            "slug": "Action-Recognition-from-Arbitrary-Views-using-3D-Weinland-Boyer",
            "title": {
                "fragments": [],
                "text": "Action Recognition from Arbitrary Views using 3D Exemplars"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new framework is proposed where actions are model actions using three dimensional occupancy grids, built from multiple viewpoints, in an exemplar-based HMM, where a 3D reconstruction is not required during the recognition phase, instead learned 3D exemplars are used to produce 2D image information that is compared to the observations."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969896"
                        ],
                        "name": "Nadeem Anjum",
                        "slug": "Nadeem-Anjum",
                        "structuredName": {
                            "firstName": "Nadeem",
                            "lastName": "Anjum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadeem Anjum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145440152"
                        ],
                        "name": "A. Cavallaro",
                        "slug": "A.-Cavallaro",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Cavallaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cavallaro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 84
                            }
                        ],
                        "text": "Mean shift is applied to cluster object trajectories based on multiple features in (Anjum and Cavallaro, 2008), and clusters with less trajectories are considered as rare events. Similarly, Jung et al (2008) design a framework for event detection in video"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "Mean shift is applied to cluster object trajectories based on multiple features in (Anjum and Cavallaro, 2008), and clusters with less trajectories are considered as rare events."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2792320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d53526e14a62c06ba9669bcfa7a7204b8ead539",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel multifeature video object trajectory clustering algorithm that estimates common patterns of behaviors and isolates outliers. The proposed algorithm is based on four main steps, namely the extraction of a set of representative trajectory features, non-parametric clustering, cluster merging and information fusion for the identification of normal and rare object motion patterns. First we transform the trajectories into a set of feature spaces on which mean-shift identifies the modes and the corresponding clusters. Furthermore, a merging procedure is devised to refine these results by combining similar adjacent clusters. The final common patterns are estimated by fusing the clustering results across all feature spaces. Clusters corresponding to reoccurring trajectories are considered as normal, whereas sparse trajectories are associated to abnormal and rare events. The performance of the proposed algorithm is evaluated on standard data-sets and compared with state-of-the-art techniques. Experimental results show that the proposed approach outperforms state-of-the-art algorithms both in terms of accuracy and robustness in discovering common patterns in video as well as in recognizing outliers."
            },
            "slug": "Multifeature-Object-Trajectory-Clustering-for-Video-Anjum-Cavallaro",
            "title": {
                "fragments": [],
                "text": "Multifeature Object Trajectory Clustering for Video Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results show that the proposed approach outperforms state-of-the-art algorithms both in terms of accuracy and robustness in discovering common patterns in video as well as in recognizing outliers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2405888"
                        ],
                        "name": "Lahav Yeffet",
                        "slug": "Lahav-Yeffet",
                        "structuredName": {
                            "firstName": "Lahav",
                            "lastName": "Yeffet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lahav Yeffet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 403,
                                "start": 380
                            }
                        ],
                        "text": "Feature descriptors range from higher order derivatives (local jets), gradient information, optical flow, and brightness information (Doll\u00e1r et al, 2005; Laptev et al, 2008; Sch\u00fcldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kl\u00e4ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 357
                            }
                        ],
                        "text": "Feature descriptors range from higher order derivatives (local jets), gradient information, optical flow, and brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 272
                            }
                        ],
                        "text": "\u2026brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Yeffet and Wolf (2009) propose Local Trinary Patterns for videos as extension of Local Binary Patterns (LBP) (Ojala et al, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17740922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68080fa24fef0f6eaa3dfd6f63978de01bc251bf",
            "isKey": true,
            "numCitedBy": 363,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel action recognition method which is based on combining the effective description properties of Local Binary Patterns with the appearance invariance and adaptability of patch matching based methods. The resulting method is extremely efficient, and thus is suitable for real-time uses of simultaneous recovery of human action of several lengths and starting points. Tested on all publicity available datasets in the literature known to us, our system repeatedly achieves state of the art performance. Lastly, we present a new benchmark that focuses on uncut motion recognition in broadcast sports video."
            },
            "slug": "Local-Trinary-Patterns-for-human-action-recognition-Yeffet-Wolf",
            "title": {
                "fragments": [],
                "text": "Local Trinary Patterns for human action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel action recognition method which is based on combining the effective description properties of Local Binary Patterns with the appearance invariance and adaptability of patch matching based methods is presented, which is suitable for real-time uses of simultaneous recovery of human action of several lengths and starting points."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144639556"
                        ],
                        "name": "Graham W. Taylor",
                        "slug": "Graham-W.-Taylor",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Taylor",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham W. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16347832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d476b96be73fccc61f2076befbf5a468caa4180",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent \"flow fields\" which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets."
            },
            "slug": "Convolutional-Learning-of-Spatio-temporal-Features-Taylor-Fergus",
            "title": {
                "fragments": [],
                "text": "Convolutional Learning of Spatio-temporal Features"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A model that learns latent representations of image sequences from pairs of successive images is introduced, allowing it to scale to realistic image sizes whilst using a compact parametrization."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048032"
                        ],
                        "name": "P. Scovanner",
                        "slug": "P.-Scovanner",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Scovanner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Scovanner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38245610"
                        ],
                        "name": "Saad Ali",
                        "slug": "Saad-Ali",
                        "structuredName": {
                            "firstName": "Saad",
                            "lastName": "Ali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saad Ali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 155
                            }
                        ],
                        "text": "\u2026brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D (Kla\u0308ser et al, 2008), extended SURF (Willems et al, 2008), and Local Trinary Patterns (Yeffet and Wolf, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 88
                            }
                        ],
                        "text": "Previous local descriptors (Dolla\u0301r et al, 2005; Kla\u0308ser et al, 2008; Laptev et al, 2008; Scovanner et al, 2007; Willems et al, 2008) are usually computed in a 3D video volume around interest points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1087061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe1b412ce7a4a36664734c4cad97b939b6ea6015",
            "isKey": false,
            "numCitedBy": 1660,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a 3-dimensional (3D) SIFT descriptor for video or 3D imagery such as MRI data. We also show how this new descriptor is able to better represent the 3D nature of video data in the application of action recognition. This paper will show how 3D SIFT is able to outperform previously used description methods in an elegant and efficient manner. We use a bag of words approach to represent videos, and present a method to discover relationships between spatio-temporal words in order to better describe the video data."
            },
            "slug": "A-3-dimensional-sift-descriptor-and-its-application-Scovanner-Ali",
            "title": {
                "fragments": [],
                "text": "A 3-dimensional sift descriptor and its application to action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper uses a bag of words approach to represent videos, and presents a method to discover relationships between spatio-temporal words in order to better describe the video data."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 5
                            }
                        ],
                        "text": "HOG (Dalal and Triggs, 2005) focuses on static appearance information, whereas HOF captures the local motion information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving bag-offeatures action recognition with non-local cues. In British machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66971399"
                        ],
                        "name": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys",
                        "slug": "Svenska-S\u00e4llskapet-f\u00f6r-Automatiserad-Bildanalys",
                        "structuredName": {
                            "firstName": "Svenska",
                            "lastName": "Bildanalys",
                            "middleNames": [
                                "S\u00e4llskapet",
                                "f\u00f6r",
                                "Automatiserad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58872812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257a1fdf27448aa24a89e23ba8e58b5cb0db074e",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-...-Scandinavian-Conference-on-Bildanalys",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ... Scandinavian Conference on Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059735197"
                        ],
                        "name": "Andrew Gilbert",
                        "slug": "Andrew-Gilbert",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gilbert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144275801"
                        ],
                        "name": "J. Illingworth",
                        "slug": "J.-Illingworth",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Illingworth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Illingworth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9874020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e696d7d9ef575804e1e53c7f050a4e32a2fe64c",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The field of Action Recognition has seen a large increase in activity in recent years. Much of the progress has been through incorporating ideas from single-frame object recognition and adapting them for temporal-based action recognition. Inspired by the success of interest points in the 2D spatial domain, their 3D (space-time) counterparts typically form the basic components used to describe actions, and in action recognition the features used are often engineered to fire sparsely. This is to ensure that the problem is tractable; however, this can sacrifice recognition accuracy as it cannot be assumed that the optimum features in terms of class discrimination are obtained from this approach. In contrast, we propose to initially use an overcomplete set of simple 2D corners in both space and time. These are grouped spatially and temporally using a hierarchical process, with an increasing search area. At each stage of the hierarchy, the most distinctive and descriptive features are learned efficiently through data mining. This allows large amounts of data to be searched for frequently reoccurring patterns of features. At each level of the hierarchy, the mined compound features become more complex, discriminative, and sparse. This results in fast, accurate recognition with real-time performance on high-resolution video. As the compound features are constructed and selected based upon their ability to discriminate, their speed and accuracy increase at each level of the hierarchy. The approach is tested on four state-of-the-art data sets, the popular KTH data set to provide a comparison with other state-of-the-art approaches, the Multi-KTH data set to illustrate performance at simultaneous multiaction classification, despite no explicit localization information provided during training. Finally, the recent Hollywood and Hollywood2 data sets provide challenging complex actions taken from commercial movie sequences. For all four data sets, the proposed hierarchical approach outperforms all other methods reported thus far in the literature and can achieve real-time operation."
            },
            "slug": "Action-Recognition-Using-Mined-Hierarchical-Gilbert-Illingworth",
            "title": {
                "fragments": [],
                "text": "Action Recognition Using Mined Hierarchical Compound Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The proposed hierarchical approach outperforms all other methods reported thus far in the literature and can achieve real-time operation on high-resolution video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398643531"
                        ],
                        "name": "Nazli Ikizler-Cinbis",
                        "slug": "Nazli-Ikizler-Cinbis",
                        "structuredName": {
                            "firstName": "Nazli",
                            "lastName": "Ikizler-Cinbis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nazli Ikizler-Cinbis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Ikizler-Cinbis and Sclaroff (2010) apply video stabilization by motion compensation to remove camera motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 33
                            }
                        ],
                        "text": "Compared to video stabilization (Ikizler-Cinbis and Sclaroff, 2010) and motion compensation (Uemura et al, 2008), this is a simpler way to discount for camera motion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Johnson and Hogg (1996) propose to track humans and model the distribution of trajectories in order to identify atypical events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 0
                            }
                        ],
                        "text": "Ikizler-Cinbis and Sclaroff (2010) apply video stabilization by motion compensation to remove camera motion. Recently, Wu et al (2011a) decompose Lagrangian particle trajectories into camera-induced and objectinduced components for videos acquired by a moving camera."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9645996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7de6028a3b6c07a5544b48e132862923d9c01bd",
            "isKey": true,
            "numCitedBy": 302,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "In many cases, human actions can be identified not only by the singular observation of the human body in motion, but also properties of the surrounding scene and the related objects. In this paper, we look into this problem and propose an approach for human action recognition that integrates multiple feature channels from several entities such as objects, scenes and people. We formulate the problem in a multiple instance learning (MIL) framework, based on multiple feature channels. By using a discriminative approach, we join multiple feature channels embedded to the MIL space. Our experiments over the large YouTube dataset show that scene and object information can be used to complement person features for human action recognition."
            },
            "slug": "Object,-Scene-and-Actions:-Combining-Multiple-for-Ikizler-Cinbis-Sclaroff",
            "title": {
                "fragments": [],
                "text": "Object, Scene and Actions: Combining Multiple Features for Human Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes an approach for human action recognition that integrates multiple feature channels from several entities such as objects, scenes and people, and forms the problem in a multiple instance learning (MIL) framework, based on several feature channels."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762649"
                        ],
                        "name": "V. Rabaud",
                        "slug": "V.-Rabaud",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Rabaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rabaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524603"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Dolla\u0301r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Dolla\u0301r et al (2005) rely on descriptors based on normalized brightness, gradient, and optical flow information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "Previous local descriptors (Dolla\u0301r et al, 2005; Kla\u0308ser et al, 2008; Laptev et al, 2008; Scovanner et al, 2007; Willems et al, 2008) are usually computed in a 3D video volume around interest points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 134
                            }
                        ],
                        "text": "Feature descriptors range from higher order derivatives (local jets), gradient information, optical flow, and brightness information (Dolla\u0301r et al, 2005; Laptev et al, 2008; Schu\u0308ldt et al, 2004) to spatio-temporal extensions of image descriptors, such as 3D-SIFT (Scovanner et al, 2007), HOG3D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "Other detection approaches are based on Gabor filters (Bregonzio et al, 2009; Dolla\u0301r et al, 2005) and on the determinant of the spatio-temporal Hessian matrix (Willems et al, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1956774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f1707caad72573633c2307fa26ec093e8f4bb03",
            "isKey": true,
            "numCitedBy": 2717,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior."
            },
            "slug": "Behavior-recognition-via-sparse-spatio-temporal-Doll\u00e1r-Rabaud",
            "title": {
                "fragments": [],
                "text": "Behavior recognition via sparse spatio-temporal features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and an alternative is proposed, and a recognition algorithm based on spatio-temporally windowed data is devised."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144474750"
                        ],
                        "name": "Subhabrata Bhattacharya",
                        "slug": "Subhabrata-Bhattacharya",
                        "structuredName": {
                            "firstName": "Subhabrata",
                            "lastName": "Bhattacharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhabrata Bhattacharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723884"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9105366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f0a367550441e314deee1e09bf56cb860b36f41",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an efficient alternative to the traditional vocabulary based on bag-of-visual words (BoW) used for visual classification tasks. Our representation is both conceptually and computationally superior to the bag-of-visual words: (1) We iteratively generate a Maximum Likelihood estimate of an image given a set of characteristic features in contrast to the BoW methods where an image is represented as a histogram of visual words, (2) We randomly sample a set of characteristic features instead of employing computation-intensive clustering algorithms used during the vocabulary generation step of BoW methods. Our performance compares favorably to the state-of-the-art on experiments over three challenging human action and a scene categorization dataset, demonstrating the universal applicability of our method."
            },
            "slug": "A-probabilistic-representation-for-efficient-large-Bhattacharya-Sukthankar",
            "title": {
                "fragments": [],
                "text": "A probabilistic representation for efficient large scale visual recognition tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper iteratively generates a Maximum Likelihood estimate of an image given a set of characteristic features in contrast to the BoW methods where an image is represented as a histogram of visual words."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1131,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories. To assure a dense coverage with trajectories, random points are sampled for tracking within the region of existing trajectories. Spatio-temporal statistics of the trajectories are then used to discriminate different actions. Raptis and Soatto (2010) track feature points in regions of interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also compare descriptors computed at space-time interest points extracted with the Harris3D detector (Laptev, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) has introduced space-time interest points by extending the Harris detector to video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center. The elements of the matrix are then used to represent the trajectories. Sun et al (2009) compute trajectories by matching SIFT descriptors between two consecutive frames. They impose a unique-match constraint among the descriptors and discard matches that are too far apart. Actions are described with intra- and inter-trajectory statistics. Sun et al (2010) combine both KLT tracker and SIFT descriptor matching to extract long-duration trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 88
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981). Trajectories are represented as sequences of log-polar quantized velocities and used for action classification. Matikainen et al (2009) extract trajectories using a standard KLT tracker, cluster the trajectories, and compute an affine transformation matrix for each cluster center."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Laptev et al (2008) combine histograms of oriented gradients (HOG) and histograms of optical flow (HOF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 415,
                                "start": 0
                            }
                        ],
                        "text": "Laptev (2005) have introduced spatio-temporal interest points, which are an extension of the Harris detector from image to video. Interest points are local maxima of a cornerness criterion based on the spatio-temporal second-moment matrix at each video point. Doll\u00e1r et al (2005) have proposed a cornerness function that combines a 2D Gaussian filter in space with a 1D Gabor filter in time. Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3148797,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2fb2fa53021b2776da0fb8b53c54820ed3982cc",
            "isKey": true,
            "numCitedBy": 1591,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and F\u00f6rstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "slug": "On-Space-Time-Interest-Points-Laptev",
            "title": {
                "fragments": [],
                "text": "On Space-Time Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper builds on the idea of the Harris and F\u00f6rstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time and illustrates how a video representation in terms of local space- time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "4.3 Spatio-temporal pyramids\nWe add structure information to the bag of features using spatio-temporal pyramids (Laptev et al, 2008; Ullah et al, 2010), an extension of spatial pyramids for images (Lazebnik et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving bag-offeatures action recognition with non-local cues. In British machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66971399"
                        ],
                        "name": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys",
                        "slug": "Svenska-S\u00e4llskapet-f\u00f6r-Automatiserad-Bildanalys",
                        "structuredName": {
                            "firstName": "Svenska",
                            "lastName": "Bildanalys",
                            "middleNames": [
                                "S\u00e4llskapet",
                                "f\u00f6r",
                                "Automatiserad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58872812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257a1fdf27448aa24a89e23ba8e58b5cb0db074e",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-...-Scandinavian-Conference-on-Bildanalys",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ... Scandinavian Conference on Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144690662"
                        ],
                        "name": "Liang Wang",
                        "slug": "Liang-Wang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757287"
                        ],
                        "name": "Guoying Zhao",
                        "slug": "Guoying-Zhao",
                        "structuredName": {
                            "firstName": "Guoying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150207998"
                        ],
                        "name": "Li Cheng",
                        "slug": "Li-Cheng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 185
                            }
                        ],
                        "text": "There are 16 sports actions: high-jump, long-jump, triple-jump, pole-vault, basketball lay-up, bowling, tennis-\n7 Note that here we use the same dataset as (Liu et al, 2009), whereas in (Wang et al, 2011) we used a different version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "A preliminary version of this article has appeared in (Wang et al, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "The resulting trajectories are more robust, in particular in the presence of fast irregular motions (Sand and Teller, 2008; Brox and Malik, 2010; Wang et al, 2011), see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58238834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "692db1510310d097f6e4b802befe7c16d7530905",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Learning-for-Vision-Based-Motion-Analysis-Wang-Zhao",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Vision-Based Motion Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064850418"
                        ],
                        "name": "E. Nowak",
                        "slug": "E.-Nowak",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nowak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nowak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "Dense sampling has shown to improve results over sparse interest points for image classification (Fei-Fei and Perona, 2005; Nowak et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 218459184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ae643b467ce873de1ce7962a7fa24dda1a28e68",
            "isKey": false,
            "numCitedBy": 1102,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Bag-of-features representations have recently become popular for content based image classification owing to their simplicity and good performance. They evolved from texton methods in texture analysis. The basic idea is to treat images as loose collections of independent patches, sampling a representative set of patches from the image, evaluating a visual descriptor vector for each patch independently, and using the resulting distribution of samples in descriptor space as a characterization of the image. The four main implementation choices are thus how to sample patches, how to describe them, how to characterize the resulting distributions and how to classify images based on the result. We concentrate on the first issue, showing experimentally that for a representative selection of commonly used test databases and for moderate to large numbers of samples, random sampling gives equal or better classifiers than the sophisticated multiscale interest operators that are in common use. Although interest operators work well for small numbers of samples, the single most important factor governing performance is the number of patches sampled from the test image and ultimately interest operators can not provide enough patches to compete. We also study the influence of other factors including codebook size and creation method, histogram normalization method and minimum scale for feature extraction."
            },
            "slug": "Sampling-Strategies-for-Bag-of-Features-Image-Nowak-Jurie",
            "title": {
                "fragments": [],
                "text": "Sampling Strategies for Bag-of-Features Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown experimentally that for a representative selection of commonly used test databases and for moderate to large numbers of samples, random sampling gives equal or better classifiers than the sophisticated multiscale interest operators that are in common use."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2870402"
                        ],
                        "name": "C. Jung",
                        "slug": "C.-Jung",
                        "structuredName": {
                            "firstName": "Cl\u00e1udio",
                            "lastName": "Jung",
                            "middleNames": [
                                "Rosito"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103297231"
                        ],
                        "name": "L. Hennemann",
                        "slug": "L.-Hennemann",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Hennemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hennemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679516"
                        ],
                        "name": "S. Musse",
                        "slug": "S.-Musse",
                        "structuredName": {
                            "firstName": "Soraia",
                            "lastName": "Musse",
                            "middleNames": [
                                "Raupp"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Musse"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Jung et al (2008) design a framework for event detection in video\nFig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9547202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0da5d922a015b17d87eabf8bff308f974fe1c60c",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a framework for event detection based on trajectory clustering and 4-D histograms. In the training period, captured trajectories are grouped into coherent clusters according to global motion flows. Within each cluster, the position and instantaneous velocity of each tracked object are used to build a 4-D motion histogram for the cluster. In the test period, each new trajectory is compared against the 4-D histograms of all clusters, so that its coherence with previously tracked objects can be evaluated. Experimental results showed that these criteria can be effectively used to measure the coherence of test trajectories with those in the training stage, allowing a range of events to be detected in surveillance and traffic applications."
            },
            "slug": "Event-Detection-Using-Trajectory-Clustering-and-4-D-Jung-Hennemann",
            "title": {
                "fragments": [],
                "text": "Event Detection Using Trajectory Clustering and 4-D Histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results showed that these criteria can be effectively used to measure the coherence of test trajectories with those in the training stage, allowing a range of events to be detected in surveillance and traffic applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 30
                            }
                        ],
                        "text": "Here, we use the criterion of Shi and Tomasi (1994), that is points on the grid are removed, if the eigenvalues of the auto-correlation matrix are very small."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 778478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab46391005cea85fa5c204b6e77a9c870fdbaed",
            "isKey": false,
            "numCitedBy": 8403,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments.<<ETX>>"
            },
            "slug": "Good-features-to-track-Shi-Tomasi",
            "title": {
                "fragments": [],
                "text": "Good features to track"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kl\u00e4ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 59
                            }
                        ],
                        "text": "Scovanner et al (2007) extend the popular SIFT descriptor (Lowe, 2004) to the spatiotemporal domain, and Kla\u0308ser et al (2008) introduce the HOG3D descriptor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66971399"
                        ],
                        "name": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys",
                        "slug": "Svenska-S\u00e4llskapet-f\u00f6r-Automatiserad-Bildanalys",
                        "structuredName": {
                            "firstName": "Svenska",
                            "lastName": "Bildanalys",
                            "middleNames": [
                                "S\u00e4llskapet",
                                "f\u00f6r",
                                "Automatiserad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58872812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257a1fdf27448aa24a89e23ba8e58b5cb0db074e",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-...-Scandinavian-Conference-on-Bildanalys",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ... Scandinavian Conference on Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144690662"
                        ],
                        "name": "Liang Wang",
                        "slug": "Liang-Wang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757287"
                        ],
                        "name": "Guoying Zhao",
                        "slug": "Guoying-Zhao",
                        "structuredName": {
                            "firstName": "Guoying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150207998"
                        ],
                        "name": "Li Cheng",
                        "slug": "Li-Cheng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 185
                            }
                        ],
                        "text": "There are 16 sports actions: high-jump, long-jump, triple-jump, pole-vault, basketball lay-up, bowling, tennis-\n7 Note that here we use the same dataset as (Liu et al, 2009), whereas in (Wang et al, 2011) we used a different version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "A preliminary version of this article has appeared in (Wang et al, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "The resulting trajectories are more robust, in particular in the presence of fast irregular motions (Sand and Teller, 2008; Brox and Malik, 2010; Wang et al, 2011), see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58238834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "692db1510310d097f6e4b802befe7c16d7530905",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Learning-for-Vision-Based-Motion-Analysis-Wang-Zhao",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Vision-Based Motion Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": "Other detection approaches are based on Gabor filters (Bregonzio et al, 2009; Dolla\u0301r et al, 2005) and on the determinant of the spatio-temporal Hessian matrix (Willems et al, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Bregonzio et al (2009) have extended this approach with 2D Gabor filters of different orientations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61363114,
            "fieldsOfStudy": [],
            "id": "7a73e44a09ef90484af666cd4cbb09628447480e",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognising action as clouds of space-time interest points"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "Dolla\u0301r et al (2005) rely on descriptors based on normalized brightness, gradient, and optical flow information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "Yeffet and Wolf (2009) propose Local Trinary Patterns for videos as extension of Local Binary Patterns (LBP) (Ojala et al, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiresolution grayscale and rotation invariant texture classification with local binary patterns"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 34
                            }
                        ],
                        "text": "A similar approach is proposed by Lu et al (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning dense opticalflow trajectory patterns for video object extraction. In IEEE advanced video and signal based surveillance conference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687325"
                        ],
                        "name": "Du Tran",
                        "slug": "Du-Tran",
                        "structuredName": {
                            "firstName": "Du",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Du Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144080148"
                        ],
                        "name": "A. Sorokin",
                        "slug": "A.-Sorokin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Sorokin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sorokin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "The UIUC dataset12 (Tran and Sorokin, 2008) is recorded\nin a controlled experimental setting with clean background and fixed cameras."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 21
                            }
                        ],
                        "text": "The UIUC dataset(12) (Tran and Sorokin, 2008) is recorded in a controlled experimental setting with clean background and fixed cameras."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14160859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a73c9533743da02a649e4d22fbe3bcde6f834cc8",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a metric learning based approach for human activity recognition with two main objectives: (1) reject unfamiliar activities and (2) learn with few examples. We show that our approach outperforms all state-of-the-art methods on numerous standard datasets for traditional action classification problem. Furthermore, we demonstrate that our method not only can accurately label activities but also can reject unseen activities and can learn from few examples with high accuracy. We finally show that our approach works well on noisy YouTube videos."
            },
            "slug": "Human-Activity-Recognition-with-Metric-Learning-Tran-Sorokin",
            "title": {
                "fragments": [],
                "text": "Human Activity Recognition with Metric Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper proposes a metric learning based approach for human activity recognition with two main objectives: (1) reject unfamiliar activities and (2) learn with few examples that outperforms all state-of-the-art methods on numerous standard datasets for traditional action classification problem."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "Dense sampling has shown to improve results over sparse interest points for image classification (Fei-Fei and Perona, 2005; Nowak et al, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 98
                            }
                        ],
                        "text": "Dense sampling has shown to improve results over sparse interest points for image classification (Fei-Fei and Perona, 2005; Nowak et al, 2006). The same is observed for action recognition in a recent evaluation by Wang et al (2009), where dense sampling at regular positions in space and time outperforms state-of-the-art spatio-temporal interest point de-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588702"
                        ],
                        "name": "B. D. Lucas",
                        "slug": "B.-D.-Lucas",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Lucas",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 85
                            }
                        ],
                        "text": "To obtain feature trajectories, either tracking techniques based on the KLT tracker (Lucas and Kanade, 1981) are used (Matikainen et al, 2009; Messing et al, 2009), or SIFT descriptors between consecutive frames are matched (Sun et al, 2009). Recently, Sun et al (2010) combined both approaches and added random trajectories in low density regions of both trackers in order to increase density."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 122
                            }
                        ],
                        "text": "Messing et al (2009) extract feature trajectories by tracking Harris3D interest points (Laptev, 2005) with a KLT tracker (Lucas and Kanade, 1981)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "To obtain feature trajectories, either tracking techniques based on the KLT tracker (Lucas and Kanade, 1981) are used (Matikainen et al, 2009; Messing et al, 2009), or SIFT descriptors between consecutive frames are matched (Sun et al, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "A standard KLT tracker (Lucas and Kanade, 1981) is employed to construct KLT trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2121536,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "a06547951c97b2a32f23a6c2b5f79c8c75c9b9bd",
            "isKey": true,
            "numCitedBy": 13329,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system."
            },
            "slug": "An-Iterative-Image-Registration-Technique-with-an-Lucas-Kanade",
            "title": {
                "fragments": [],
                "text": "An Iterative Image Registration Technique with an Application to Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration, and can be generalized to handle rotation, scaling and shearing."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1981
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 40,
            "methodology": 34,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 69,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Dense-Trajectories-and-Motion-Boundary-Descriptors-Wang-Kl\u00e4ser/bbe0819a47a9f3f11dd34bb3ab44a997ef111088?sort=total-citations"
}