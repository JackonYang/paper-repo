{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2268783"
                        ],
                        "name": "Chenghua Lin",
                        "slug": "Chenghua-Lin",
                        "structuredName": {
                            "firstName": "Chenghua",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenghua Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704133"
                        ],
                        "name": "Yulan He",
                        "slug": "Yulan-He",
                        "structuredName": {
                            "firstName": "Yulan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yulan He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 102
                            }
                        ],
                        "text": "Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6884774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a45973f8d5ede2bc700d3e6c0758e5c90455d5a0",
            "isKey": false,
            "numCitedBy": 969,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment analysis or opinion mining aims to use automated tools to detect subjective information such as opinions, attitudes, and feelings expressed in text. This paper proposes a novel probabilistic modeling framework based on Latent Dirichlet Allocation (LDA), called joint sentiment/topic model (JST), which detects sentiment and topic simultaneously from text. Unlike other machine learning approaches to sentiment classification which often require labeled corpora for classifier training, the proposed JST model is fully unsupervised. The model has been evaluated on the movie review dataset to classify the review sentiment polarity and minimum prior information have also been explored to further improve the sentiment classification accuracy. Preliminary experiments have shown promising results achieved by JST."
            },
            "slug": "Joint-sentiment/topic-model-for-sentiment-analysis-Lin-He",
            "title": {
                "fragments": [],
                "text": "Joint sentiment/topic model for sentiment analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel probabilistic modeling framework based on Latent Dirichlet Allocation (LDA) is proposed, called joint sentiment/topic model (JST), which detects sentiment and topic simultaneously from text, which is fully unsupervised."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389036863"
                        ],
                        "name": "Jordan L. Boyd-Graber",
                        "slug": "Jordan-L.-Boyd-Graber",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Boyd-Graber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordan L. Boyd-Graber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 102
                            }
                        ],
                        "text": "Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 218515777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "746c085477ec83bee8ca28cb4c1482439905ee15",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we develop multilingual supervised latent Dirichlet allocation (MlSLDA), a probabilistic generative model that allows insights gleaned from one language's data to inform how the model captures properties of other languages. MlSLDA accomplishes this by jointly modeling two aspects of text: how multilingual concepts are clustered into thematically coherent topics and how topics associated with text connect to an observed regression variable (such as ratings on a sentiment scale). Concepts are represented in a general hierarchical framework that is flexible enough to express semantic ontologies, dictionaries, clustering constraints, and, as a special, degenerate case, conventional topic models. Both the topics and the regression are discovered via posterior inference from corpora. We show MlSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment."
            },
            "slug": "Holistic-Sentiment-Analysis-Across-Languages:-Boyd-Graber-Resnik",
            "title": {
                "fragments": [],
                "text": "Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper develops multilingual supervised latent Dirichlet allocation (MlSLDA), a probabilistic generative model that allows insights gleaned from one language's data to inform how the model captures properties of other languages."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2967699"
                        ],
                        "name": "A. Andreevskaia",
                        "slug": "A.-Andreevskaia",
                        "structuredName": {
                            "firstName": "Alina",
                            "lastName": "Andreevskaia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Andreevskaia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209533"
                        ],
                        "name": "S. Bergler",
                        "slug": "S.-Bergler",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Bergler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bergler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 180
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2787775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d8b52f59945b4a3ef9d20ab44e108319eead6f",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement."
            },
            "slug": "Mining-WordNet-for-a-Fuzzy-Sentiment:-Sentiment-Tag-Andreevskaia-Bergler",
            "title": {
                "fragments": [],
                "text": "Mining WordNet for a Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794976"
                        ],
                        "name": "Fangtao Li",
                        "slug": "Fangtao-Li",
                        "structuredName": {
                            "firstName": "Fangtao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fangtao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730108"
                        ],
                        "name": "Minlie Huang",
                        "slug": "Minlie-Huang",
                        "structuredName": {
                            "firstName": "Minlie",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minlie Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145213540"
                        ],
                        "name": "Xiaoyan Zhu",
                        "slug": "Xiaoyan-Zhu",
                        "structuredName": {
                            "firstName": "Xiaoyan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyan Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "For example, while it learns thatwonderful andamazingare semantically close, it doesn\u2019t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible andawful."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 103
                            }
                        ],
                        "text": "Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6382866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75de299d2aeccfbadeced0e8fdbd54030f7cd927",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "With the development of Web 2.0, sentiment analysis has now become a popular research problem to tackle. Recently, topic models have been introduced for the simultaneous analysis for topics and the sentiment in a document. These studies, which jointly model topic and sentiment, take the advantage of the relationship between topics and sentiment, and are shown to be superior to traditional sentiment analysis tools. However, most of them make the assumption that, given the parameters, the sentiments of the words in the document are all independent. In our observation, in contrast, sentiments are expressed in a coherent way. The local conjunctive words, such as \"and\" or \"but\", are often indicative of sentiment transitions. \n \nIn this paper, we propose a major departure from the previous approaches by making two linked contributions. First, we assume that the sentiments are related to the topic in the document, and put forward a joint sentiment and topic model, i.e. Sentiment-LDA. Second, we observe that sentiments are dependent on local context. Thus, we further extend the Sentiment-LDA model to Dependency-Sentiment-LDA model by relaxing the sentiment independent assumption in Sentiment-LDA. The sentiments of words are viewed as a Markov chain in Dependency-Sentiment-LDA. Through experiments, we show that exploiting the sentiment dependency is clearly advantageous, and that the Dependency-Sentiment-LDA is an effective approach for sentiment analysis."
            },
            "slug": "Sentiment-Analysis-with-Global-Topics-and-Local-Li-Huang",
            "title": {
                "fragments": [],
                "text": "Sentiment Analysis with Global Topics and Local Dependency"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A major departure from the previous approaches to sentiment analysis is proposed by making two linked contributions, which assume that the sentiments are related to the topic in the document, and put forward a joint sentiment and topic model, i.e. Sentiment-LDA."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40378713"
                        ],
                        "name": "Justin Martineau",
                        "slug": "Justin-Martineau",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Martineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Martineau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144121212"
                        ],
                        "name": "Timothy W. Finin",
                        "slug": "Timothy-W.-Finin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Finin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy W. Finin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 188
                            }
                        ],
                        "text": "Weighting Variants We evaluate both binary (b) term frequency weighting with smoothed delta idf (\u2206t\u2019) and no idf (n) because these variants worked well in previous experiments in sentiment (Martineau and Finin, 2009; Pang et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 21
                            }
                        ],
                        "text": "Delta idf weighting (Martineau and Finin, 2009) is a supervised variant of idf weighting in which the idf calculation is done for each document class and then one value is subtracted from the other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 42
                            }
                        ],
                        "text": "well in previous experiments in sentiment (Martineau and Finin, 2009; Pang et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Mining opinions and sentiment from social networking sites is a popular application for social media systems. Common approaches use a machine learning system with a bag of words feature set. We present Delta TFIDF, an intuitive general purpose technique to efficiently weight word scores before classification. Delta TFIDF is easy to compute, implement, and understand. We use Support Vector Machines to show that Delta TFIDF significantly improves accuracy for sentiment analysis problems using three well known data sets."
            },
            "slug": "Delta-TFIDF:-An-Improved-Feature-Space-for-Analysis-Martineau-Finin",
            "title": {
                "fragments": [],
                "text": "Delta TFIDF: An Improved Feature Space for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Delta TFIDF is presented, an intuitive general purpose technique to efficiently weight word scores before classification to significantly improves accuracy for sentiment analysis problems using three well known data sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICWSM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 23
                            }
                        ],
                        "text": "We used the dataset of Pang and Lee (2004), which contains subjective sentences from movie review summaries and objective sentences from movie plot summaries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "We randomly split the 10,000 examples into 10 folds and report 10-fold cross validation accuracy using the SVM training protocol of Pang and Lee (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 166
                            }
                        ],
                        "text": "We use a linear support vector machine (SVM) classifier trained with LIBLINEAR (Fan et al., 2008), and set the SVM regularization parameter to the same value used by Pang and Lee (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "Our experiments involve the small, widely used sentiment and subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 47
                            }
                        ],
                        "text": "The polarity dataset version 2.0 introduced by Pang and Lee (2004)1 consists of 2,000 movie reviews, where each is associated with a binary sentiment polarity label."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "167e1359943b96b9e92ee73db1df69a1f65d731d",
            "isKey": true,
            "numCitedBy": 3565,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "slug": "A-Sentimental-Education:-Sentiment-Analysis-Using-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel machine-learning method is proposed that applies text-categorization techniques to just the subjective portions of the document, which greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718676"
                        ],
                        "name": "G. Paltoglou",
                        "slug": "G.-Paltoglou",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Paltoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Paltoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701298"
                        ],
                        "name": "M. Thelwall",
                        "slug": "M.-Thelwall",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Thelwall",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thelwall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Paltoglou and Thelwall (2010) perform an extensive analysis\nof such weighting variants for sentiment tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 161
                            }
                        ],
                        "text": "Our experiments involve the small, widely used sentiment and subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 98
                            }
                        ],
                        "text": "Martineau and Finin present evidence that this weighting helps with sentiment classification, and Paltoglou and Thelwall (2010) systematically explore a number of weighting schemes in the context of sentiment analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 469886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ac9bf53e4c5fae5ff6feacf96f2e2c474fcc6ca",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge."
            },
            "slug": "A-Study-of-Information-Retrieval-Weighting-Schemes-Paltoglou-Thelwall",
            "title": {
                "fragments": [],
                "text": "A Study of Information Retrieval Weighting Schemes for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081253"
                        ],
                        "name": "Casey Whitelaw",
                        "slug": "Casey-Whitelaw",
                        "structuredName": {
                            "firstName": "Casey",
                            "lastName": "Whitelaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Casey Whitelaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2838371"
                        ],
                        "name": "Navendu Garg",
                        "slug": "Navendu-Garg",
                        "structuredName": {
                            "firstName": "Navendu",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navendu Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "10 N/A N/A Appraisal Taxonomy (Whitelaw et al., 2005) 90."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1619039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0470303953de1d19765423f719d8314e3cb91278",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Little work to date in sentiment analysis (classifying texts by `positive' or `negative' orientation) has attempted to use fine-grained semantic distinctions in features used for classification. We present a new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny''. An appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on Appraisal Theory. Semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers. We classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%. In addition, we find that some types of appraisal appear to be more significant for sentiment classification than others."
            },
            "slug": "Using-appraisal-groups-for-sentiment-analysis-Whitelaw-Garg",
            "title": {
                "fragments": [],
                "text": "Using appraisal groups for sentiment analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny'' is presented, based on several task-independent semantic taxonomies based on Appraisal Theory."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066721"
                        ],
                        "name": "Shivakumar Vaithyanathan",
                        "slug": "Shivakumar-Vaithyanathan",
                        "structuredName": {
                            "firstName": "Shivakumar",
                            "lastName": "Vaithyanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shivakumar Vaithyanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 215
                            }
                        ],
                        "text": "Weighting Variants We evaluate both binary (b) term frequency weighting with smoothed delta idf (\u2206t\u2019) and no idf (n) because these variants worked well in previous experiments in sentiment (Martineau and Finin, 2009; Pang et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 42
                            }
                        ],
                        "text": "well in previous experiments in sentiment (Martineau and Finin, 2009; Pang et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7105713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc",
            "isKey": false,
            "numCitedBy": 8511,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging."
            },
            "slug": "Thumbs-up-Sentiment-Classification-using-Machine-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "Thumbs up? Sentiment Classification using Machine Learning Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work considers the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative, and concludes by examining factors that make the sentiment classification problem more challenging."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335225"
                        ],
                        "name": "Lev-Arie Ratinov",
                        "slug": "Lev-Arie-Ratinov",
                        "structuredName": {
                            "firstName": "Lev-Arie",
                            "lastName": "Ratinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lev-Arie Ratinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 210
                            }
                        ],
                        "text": "The general approach has proven useful in tasks such as word sense disambiguation, named entity\nrecognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 156
                            }
                        ],
                        "text": "The general approach has proven useful in tasks such as word sense disambiguation, named entity recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 629094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dac72f2c509aee67524d3321f77e97e8eff51de6",
            "isKey": false,
            "numCitedBy": 2163,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/"
            },
            "slug": "Word-Representations:-A-Simple-and-General-Method-Turian-Ratinov",
            "title": {
                "fragments": [],
                "text": "Word Representations: A Simple and General Method for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work evaluates Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeds of words on both NER and chunking, and finds that each of the three word representations improves the accuracy of these baselines."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34827767"
                        ],
                        "name": "Alistair Kennedy",
                        "slug": "Alistair-Kennedy",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Kennedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "00 Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16414995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14609a3a6c6f8ef3269d3e0728f88da57826698",
            "isKey": false,
            "numCitedBy": 774,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two methods for determining the sentiment expressed by a movie review. The semantic orientation of a review can be positive, negative, or neutral. We examine the effect of valence shifters on classifying the reviews. We examine three types of valence shifters: negations, intensifiers, and diminishers. Negations are used to reverse the semantic polarity of a particular term, while intensifiers and diminishers are used to increase and decrease, respectively, the degree to which a term is positive or negative. The first method classifies reviews based on the number of positive and negative terms they contain. We use the General Inquirer to identify positive and negative terms, as well as negation terms, intensifiers, and diminishers. We also use positive and negative terms from other sources, including a dictionary of synonym differences and a very large Web corpus. To compute corpus\u2010based semantic orientation values of terms, we use their association scores with a small group of positive and negative terms. We show that extending the term\u2010counting method with contextual valence shifters improves the accuracy of the classification. The second method uses a Machine Learning algorithm, Support Vector Machines. We start with unigram features and then add bigrams that consist of a valence shifter and another word. The accuracy of classification is very high, and the valence shifter bigrams slightly improve it. The features that contribute to the high accuracy are the words in the lists of positive and negative terms. Previous work focused on either the term\u2010counting method or the Machine Learning method. We show that combining the two methods achieves better results than either method alone."
            },
            "slug": "SENTIMENT-CLASSIFICATION-of-MOVIE-REVIEWS-USING-Kennedy-Inkpen",
            "title": {
                "fragments": [],
                "text": "SENTIMENT CLASSIFICATION of MOVIE REVIEWS USING CONTEXTUAL VALENCE SHIFTERS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that extending the term\u2010counting method with contextual valence shifters improves the accuracy of the classification, and combining the two methods achieves better results than either method alone."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 212
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3264224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
            "isKey": false,
            "numCitedBy": 2290,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."
            },
            "slug": "Seeing-Stars:-Exploiting-Class-Relationships-for-to-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A meta-algorithm is applied, based on a metric labeling formulation of the rating-inference problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417893"
                        ],
                        "name": "A. Goldberg",
                        "slug": "A.-Goldberg",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 140
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2795175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fec21a78eb9279c87cc89ef7efa0acf22ff4abd",
            "isKey": false,
            "numCitedBy": 353,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \"4 stars\"), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."
            },
            "slug": "Seeing-stars-when-there-aren\u2019t-many-stars:-learning-Goldberg-Zhu",
            "title": {
                "fragments": [],
                "text": "Seeing stars when there aren\u2019t many stars: Graph-based semi-supervised learning for sentiment categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A graph-based semi-supervised learning algorithm is presented to address the sentiment analysis task of rating inference and achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 102
                            }
                        ],
                        "text": "It is common to represent words as indices in a vocabulary, but this fails to capture the rich relational structure of the lexicon."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We can efficiently learn parameters for the joint objective function using alternating maximization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": false,
            "numCitedBy": 30944,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144648940"
                        ],
                        "name": "Cecilia Ovesdotter Alm",
                        "slug": "Cecilia-Ovesdotter-Alm",
                        "structuredName": {
                            "firstName": "Cecilia",
                            "lastName": "Alm",
                            "middleNames": [
                                "Ovesdotter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cecilia Ovesdotter Alm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 162
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1260035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "872352b0a53ab6cbb4420f81df64d215d86c7d9b",
            "isKey": false,
            "numCitedBy": 831,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a naive baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions."
            },
            "slug": "Emotions-from-Text:-Machine-Learning-for-Text-based-Alm-Roth",
            "title": {
                "fragments": [],
                "text": "Emotions from Text: Machine Learning for Text-based Emotion Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "The general approach has proven useful in tasks such as word sense disambiguation, named entity\nrecognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 115
                            }
                        ],
                        "text": "Much previous work in natural language processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5023,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 124
                            }
                        ],
                        "text": "Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6009,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47481818"
                        ],
                        "name": "Theresa Wilson",
                        "slug": "Theresa-Wilson",
                        "structuredName": {
                            "firstName": "Theresa",
                            "lastName": "Wilson",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Theresa Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726601"
                        ],
                        "name": "R. Hwa",
                        "slug": "R.-Hwa",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Hwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hwa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11463929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b419fb257870fe8024fb3f07dddd66328ae644cb",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a recent swell of interest in the automatic identification and extraction of opinions and emotions in text. In this paper, we present the first experimental results classifying the strength of opinions and other types of subjectivity and classifying the subjectivity of deeply nested clauses. We use a wide range of features, including new syntactic features developed for opinion recognition. In 10-fold cross-validation experiments using support vector regression, we achieve improvements in mean-squared error over baseline ranging from 57% to 64%."
            },
            "slug": "Just-How-Mad-Are-You-Finding-Strong-and-Weak-Wilson-Wiebe",
            "title": {
                "fragments": [],
                "text": "Just How Mad Are You? Finding Strong and Weak Opinion Clauses"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents the first experimental results classifying the strength of opinions and other types of subjectivity and classifies the subjectivity of deeply nested clauses using new syntactic features developed for opinion recognition."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144163566"
                        ],
                        "name": "Benjamin Snyder",
                        "slug": "Benjamin-Snyder",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Snyder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Snyder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 140
                            }
                        ],
                        "text": "Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2279432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab8faaa1ee1791e6da911e408dee97681cfa58e",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further conrm the strength of the model: the algorithm provides signicant improvement over both individual rankers and a state-of-the-art joint ranking model."
            },
            "slug": "Multiple-Aspect-Ranking-Using-the-Good-Grief-Snyder-Barzilay",
            "title": {
                "fragments": [],
                "text": "Multiple Aspect Ranking Using the Good Grief Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algorithm is presented that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks, and it is proved that the agreementbased joint model is more expressive than individual ranking models."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 115
                            }
                        ],
                        "text": ", 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 753304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7314be5cd836c8f06bd1ecab565b00b65259eac6",
            "isKey": false,
            "numCitedBy": 3888,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called \"topics\" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis."
            },
            "slug": "Probabilistic-Topic-Models-Blei",
            "title": {
                "fragments": [],
                "text": "Probabilistic topic models"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Surveying a suite of algorithms that offer a solution to managing large document archives suggests they are well-suited to handle large amounts of data."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831395"
                        ],
                        "name": "H. Wallach",
                        "slug": "H.-Wallach",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Wallach",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wallach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705700"
                        ],
                        "name": "David Mimno",
                        "slug": "David-Mimno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mimno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mimno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 129
                            }
                        ],
                        "text": "We leave the LDA hyperparameters at their default values, though some work suggests optimizing over priors for LDA is important (Wallach et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 111
                            }
                        ],
                        "text": "hyperparameters at their default values, though some work suggests optimizing over priors for LDA is important (Wallach et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8328649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5554c9d5fa92af69992d72ed1fdfbe953b03fb4",
            "isKey": false,
            "numCitedBy": 677,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such \"smoothing parameters\" have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic-word distributions provides no real benefit. Approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling."
            },
            "slug": "Rethinking-LDA:-Why-Priors-Matter-Wallach-Mimno",
            "title": {
                "fragments": [],
                "text": "Rethinking LDA: Why Priors Matter"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The prior structure advocated substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 97
                            }
                        ],
                        "text": "This idea is similar to the word vector inner product used in the log-bilinear language model of Mnih and Hinton (2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 170
                            }
                        ],
                        "text": "\u2026model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 577005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models."
            },
            "slug": "Three-new-graphical-models-for-statistical-language-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "Three new graphical models for statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784228"
                        ],
                        "name": "J. Finkel",
                        "slug": "J.-Finkel",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Finkel",
                            "middleNames": [
                                "Rose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Finkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 114
                            }
                        ],
                        "text": "Much previous work in natural language processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10473638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00cf902b27676cdc376e26567e70298b96c672a1",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser). This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system. We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser. Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree. The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition."
            },
            "slug": "Joint-Parsing-and-Named-Entity-Recognition-Finkel-Manning",
            "title": {
                "fragments": [],
                "text": "Joint Parsing and Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser that produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 271
                            }
                        ],
                        "text": "\u2026are conceptually similar to our own, as discussed in section 2:\nLatent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for146\nVSM induction (Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 157
                            }
                        ],
                        "text": "The general approach has proven useful in tasks such as word sense disambiguation, named entity\nrecognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 156
                            }
                        ],
                        "text": "The general approach has proven useful in tasks such as word sense disambiguation, named entity recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 57
                            }
                        ],
                        "text": "Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": true,
            "numCitedBy": 2721,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 25
                            }
                        ],
                        "text": "Latent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 7
                            }
                        ],
                        "text": "Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term\u2013document co-occurrence matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "For comparison, we implemented several alternative vector space models that are conceptually similar to our own, as discussed in section 2:\nLatent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for146\nVSM induction (Turney and Pantel, 2010)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7018,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849128"
                        ],
                        "name": "Rong-En Fan",
                        "slug": "Rong-En-Fan",
                        "structuredName": {
                            "firstName": "Rong-En",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong-En Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144799660"
                        ],
                        "name": "Xiang-Rui Wang",
                        "slug": "Xiang-Rui-Wang",
                        "structuredName": {
                            "firstName": "Xiang-Rui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang-Rui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "We use a linear support vector machine (SVM) classifier trained with LIBLINEAR (Fan et al., 2008), and set the SVM regularization parameter to the same value used by Pang and Lee (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3116168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "268a4f8da15a42f3e0e71691f760ff5edbf9cec8",
            "isKey": false,
            "numCitedBy": 7764,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."
            },
            "slug": "LIBLINEAR:-A-Library-for-Large-Linear-Fan-Chang",
            "title": {
                "fragments": [],
                "text": "LIBLINEAR: A Library for Large Linear Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "LIBLINEAR is an open source library for large-scale linear classification that supports logistic regression and linear support vector machines and provides easy-to-use command-line tools and library calls for users and developers."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922861"
                        ],
                        "name": "Christopher Potts",
                        "slug": "Christopher-Potts",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Potts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Potts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 163
                            }
                        ],
                        "text": "However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32589255,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "92d21c180641df9c5ceb82b07844a922799ff3d5",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Expressives like damn and bastard have, when uttered, an immediate and powerful impact on the context. They are performative, often destructively so. They are revealing of the perspective from which the utterance is made, and they can have a dramatic impact on how current and future utterances are perceived. This, despite the fact that speakers are invariably hard-pressed to articulate what they mean. I develop a general theory of these volatile, indispensable meanings. The theory is built around a class of expressive indices. These determine the expressive setting of the context of interpretation. Expressive morphemes act on that context, actively changing its expressive setting. The theory is multidimensional in the sense that descriptives and expressives are fundamentally different but receive a unified logical treatment."
            },
            "slug": "The-expressive-dimension-Potts",
            "title": {
                "fragments": [],
                "text": "The expressive dimension"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52142310"
                        ],
                        "name": "Timothy B. Jay",
                        "slug": "Timothy-B.-Jay",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Jay",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy B. Jay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221564279,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e41f3ca94308ddaba99dfb4efee61687a5690894",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Psychiatrists, psychologists, neurologists, linguists and speech pathologists currently have no coherent theory to explain why we curse and why we choose the words we do when we curse. The Neuro-Psycho-Social Theory of Speech draws together information about cursing from different disciplines and unites them to explain and describe the psychological, neurological, cultural and linguistic factors that underlie this startling phenomenon. Why We Curse is divided into five parts. Part 1 introduces the dimensions and scope of cursing and outlines the NPS Theory, while Part 2 covers neurological variables and offers evidence for right brain dominance during emotional speech events. Part 3 then focuses on psychological development including language acquisition, personality development, cognition and so forth, while Part 4 covers the wide variety of social and cultural forces that define curse words and restrict their usage. Finally, Part 5 concludes by examining the social and legal implications of cursing, treating misconceptions about cursing, and setting the agenda for future research. The work draws on new research by Dr. Jay and others and continues the research reported in his groundbreaking 1992 volume Cursing in America. A psycholinguistic study of dirty language in the courts, in the movies, in the schoolyards and on the streets ."
            },
            "slug": "Why-We-Curse-Jay",
            "title": {
                "fragments": [],
                "text": "Why We Curse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 25
                            }
                        ],
                        "text": "Latent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "\u2026several alternative vector space models that are conceptually similar to our own, as discussed in section 2:\nLatent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 7
                            }
                        ],
                        "text": "Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term\u2013document co-occurrence matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "For comparison, we implemented several alternative vector space models that are conceptually similar to our own, as discussed in section 2:\nLatent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for146\nVSM induction (Turney and Pantel, 2010)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indexing by latent semantic analysis.Journal of the American Society"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the ICML,"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073571274"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Kushner",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 152
                            }
                        ],
                        "text": "However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 147428545,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "04882a986d7155aae66b3be77e5b732e0b340d19",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Why-We-Curse.-A-Neuro-Psycho-Social-Theory-of-Kushner",
            "title": {
                "fragments": [],
                "text": "Why We Curse. A Neuro-Psycho-Social Theory of Speech."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 163
                            }
                        ],
                        "text": "However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The expressive dimension. Theoretical Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "The expressive dimension. Theoretical Linguistics"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is meaning? Explorations in the theory of Meaning as Use"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is meaning? Explorations in the theory of Meaning as Use. Brief version -draft 1"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2003. a neural probabilistic language model"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Machine Learning Research"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 18
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly/649d03490ef72c5274e3bccd03d7a299d2f8da91?sort=total-citations"
}