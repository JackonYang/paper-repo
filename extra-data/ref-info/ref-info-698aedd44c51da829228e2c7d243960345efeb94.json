{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62589626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "447f336b3b1b18c556500075b62154f7c1721fc0",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the ability of a simple neural network (a perceptron architecture, no hidden units, binary outputs) to process information in the context of an unsupervised learning task. The network is asked to provide the best possible neural representation of a given input distribution, according to some criterion taken from information theory. The authors compare various optimization criteria that have been proposed: maximum information transmission, minimum redundancy and closeness to factorial code. They show that for the perceptron one can compute the maximum information that the code (the output neural representation) can convey about the input. They show that one can use statistical mechanics techniques, such as replica techniques, to compute the typical mutual information between input and output distributions. More precisely, for a Gaussian input source with a given correlation matrix, they compute the typical mutual information when the couplings are chosen randomly. They determine the correlations b..."
            },
            "slug": "Information-processing-by-a-perceptron-in-an-task-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Information processing by a perceptron in an unsupervised learning task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The authors show that for the perceptron one can compute the maximum information that the code can convey about the input when the couplings are chosen randomly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42871496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "026e9b04bab73d3a34cd37f8b290f0c8f6da5f4e",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A network that develops to maximize the mutual information between its output and the signal portion of its input (which is admixed with noise) is useful for extracting salient input features, and may provide a model for aspects of biological neural network function. I describe a local synaptic Learning rule that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian. Feedforward connection strengths are modified by a Hebbian rule during a \"learning\" phase in which examples of input signal plus noise are presented to the network, and by an anti-Hebbian rule during an \"unlearning\" phase in which examples of noise alone are presented. Each recurrent lateral connection has two values of connection strength, one for each phase; these values are updated by an anti-Hebbian rule."
            },
            "slug": "Local-Synaptic-Learning-Rules-Suffice-to-Maximize-a-Linsker",
            "title": {
                "fragments": [],
                "text": "Local Synaptic Learning Rules Suffice to Maximize Mutual Information in a Linear Network"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A local synaptic Learning rule is described that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15546068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "214574b7eda0a9e3a8c4501fd2007e6b87b3047a",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An information-theoretic optimization principle ('infomax') has previously been used for unsupervised learning of statistical regularities in an input ensemble. The principle states that the input-output mapping implemented by a processing stage should be chosen so as to maximize the average mutual information between input and output patterns, subject to constraints and in the presence of processing noise. In the present work I show how infomax, when applied to a class of nonlinear input-output mappings, can under certain conditions generate optimal filters that have additional useful properties: (1) Output activity (for each input pattern) tends to be concentrated among a relatively small number of nodes. (2) The filters are sensitive to higher-order statistical structure (beyond pairwise correlations). If the input features are localized, the filters' receptive fields tend to be localized as well. (3) Multiresolution sets of filters with subsampling at low spatial frequencies - related to pyramid coding and wavelet representations - emerge as favored solutions for certain types of input ensembles."
            },
            "slug": "Deriving-Receptive-Fields-Using-an-Optimal-Encoding-Linsker",
            "title": {
                "fragments": [],
                "text": "Deriving Receptive Fields Using an Optimal Encoding Criterion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how infomax, when applied to a class of nonlinear input-output mappings, can under certain conditions generate optimal filters that have additional useful properties."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103005420"
                        ],
                        "name": "Schuster Hg",
                        "slug": "Schuster-Hg",
                        "structuredName": {
                            "firstName": "Schuster",
                            "lastName": "Hg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Schuster Hg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120854056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495d5dd4ec696fc338ae4186404efe5155f872bb",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The transmission of information through a nonlinear noisy neuron has been computed with the following results. The mutual information between input and output signals is, in the large-noise limit, rigorously given by the mean-squared variance of the fluctuations of the output of the nonlinear neuron. The changes of synaptic strengths that tend to maximize the mutual information are qualitatively similar to those obtained by Hebbian learning of the nonlinear neuron"
            },
            "slug": "Learning-by-maximizing-the-information-transfer-and-Hg",
            "title": {
                "fragments": [],
                "text": "Learning by maximizing the information transfer through nonlinear noisy neurons and \"noise breakdown"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The transmission of information through a nonlinear noisy neuron has been computed and the changes of synaptic strengths that tend to maximize the mutual information are qualitatively similar to those obtained by Hebbian learning of the nonlinear neuron."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207599521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1482c67fc0c96dbd1d190e5040ab113a53e544",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy (using the measure introduced in previous work) at every update and thus removes pairwise correlations between pixels. Since the measure of pixel-entropy has only a global minimum the algorithm is guaranteed to converge to the minimum entropy map. Such optimal maps have recently been shown to possess cognitively desirable properties and are likely to be used by the nervous system to organize sensory information. The algorithm derived here turns out to be one proposed by Goodall for pairwise decorrelation. It is biologically plausible since in a neural network implementation it requires only data available locally to a neuron. In training over ensembles of two-dimensional input signals with the same spatial power spectrum as natural scenes, networks develop output neurons with center-surround receptive fields similar to those of ganglion cells in the retina. Some technical issues pertinent to developmental algorithms of this sort, such as symmetry fixing, are also discussed."
            },
            "slug": "Convergent-Algorithm-for-Sensory-Receptive-Field-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Convergent Algorithm for Sensory Receptive Field Development"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy at every update and thus removes pairwise correlations between pixels, and is biologically plausible since in a neural network implementation it requires only data available locally to a neuron."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5062473"
                        ],
                        "name": "Zhaoping Li",
                        "slug": "Zhaoping-Li",
                        "structuredName": {
                            "firstName": "Zhaoping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56995449,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c36cc3ff28b9b493df50a7c2b14ed5e19c510caa",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the hypothesis that linear cortical neurons are concerned with building a particular type of representation of the visual world --- one which not only preserves the information and the efficiency achieved by the retina, but in addition preserves spatial relationships in the input --- both in the plane of vision and in the depth dimension. Focusing on the {\\it linear} cortical cells, we classify all transforms having these properties. They are given by representations of the scaling and translation group, and turn out to be labeled by rational numbers `$(p+q)/p$' ($p, q$ integers). Any given $(p,q)$ predicts a set of receptive fields which come at different spatial locations and scales (sizes) with a bandwidth of $\\log_2[(p+q)/p]$ octaves, and, most interestingly, with a diversity of `$q$' cell varieties. The bandwidth affects the trade-off between preservation of planar and depth relations, and, we think, should be selected to match structures in natural scenes. For bandwidths between $1$ and $2$ octaves, which are the ones we feel provide the best matching, we find for each scale a minimum of two distinct cell types that reside next to each other and in phase quadrature, i.e., differ by $90^o$ in the phases of their receptive fields, as are found in the cortex, they resemble the ``even-symmetric'' and ``odd-symmetric'' simple cells in special cases. An interesting consequence of the representations presented here is that the pattern of activation in the cells in response to a translation or scaling of an object remains the same but merely shifts its locus from one group of cells to another. This work also provides a new understanding of color coding changes from the retina to the cortex."
            },
            "slug": "Towards-a-theory-of-striate-cortex-Li-Atick",
            "title": {
                "fragments": [],
                "text": "Towards a theory of striate cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The hypothesis that linear cortical neurons are concerned with building a particular type of representation of the visual world --- one which not only preserves the information and the efficiency achieved by the retina, but in addition preserves spatial relationships in the input --- both in the plane of vision and in the depth dimension is explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28154878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03128ecdd3c3dfb9752861d8555b97535e1cfc14",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a generalized redundancy subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise."
            },
            "slug": "Towards-a-Theory-of-Early-Visual-Processing-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Early Visual Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A theory of the early processing in the mammalian visual pathway is proposed and the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations is found."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5062473"
                        ],
                        "name": "Zhaoping Li",
                        "slug": "Zhaoping-Li",
                        "structuredName": {
                            "firstName": "Zhaoping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9541864,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3f904e8ea7d8147d04a59fdcd21d63a502041ef7",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the hypothesis that linear cortical neurons are concerned with building a particular type of representation of the visual worldone that not only preserves the information and the efficiency achieved by the retina, but in addition preserves spatial relationships in the inputboth in the plane of vision and in the depth dimension. Focusing on the linear cortical cells, we classify all transforms having these properties. They are given by representations of the scaling and translation group and turn out to be labeled by rational numbers (p q)/p (p, q integers). Any given (p, q) predicts a set of receptive fields that comes at different spatial locations and scales (sizes) with a bandwidth of log2 [(p q)/p] octaves and, most interestingly, with a diversity of q cell varieties. The bandwidth affects the trade-off between preservation of planar and depth relations and, we think, should be selected to match structures in natural scenes. For bandwidths between 1 and 2 octaves, which are the ones we feel provide the best matching, we find for each scale a minimum of two distinct cell types that reside next to each other and in phase quadrature, that is, differ by 90 in the phases of their receptive fields, as are found in the cortex, they resemble the even-symmetric and odd-symmetric simple cells in special cases. An interesting consequence of the representations presented here is that the pattern of activation in the cells in response to a translation or scaling of an object remains the same but merely shifts its locus from one group of cells to another. This work also provides a new understanding of color coding changes from the retina to the cortex."
            },
            "slug": "Toward-a-Theory-of-the-Striate-Cortex-Li-Atick",
            "title": {
                "fragments": [],
                "text": "Toward a Theory of the Striate Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The hypothesis that linear cortical neurons are concerned with building a particular type of representation of the visual world one that not only preserves the information and the efficiency achieved by the retina, but in addition preserves spatial relationships in the input both in the plane of vision and in the depth dimension is explored."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17515861,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "841cd4a6cac86fa0cfb0e8542eac5ed164f23f50",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data."
            },
            "slug": "What-Does-the-Retina-Know-about-Natural-Scenes-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "What Does the Retina Know about Natural Scenes?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5242294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cceb0fdf30bc69b7af0d0e4f43541c7fafe67cbc",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A redundancy reduction strategy, which can be applied in stages, is proposed as a way to learn as efficiently as possible the statistical properties of an ensemble of sensory messages. The method works best for inputs consisting of strongly correlated groups, that is features, with weaker statistical dependence between different features. This is the case for localized objects in an image or for words in a text. A local feature measure determining how much a single feature reduces the total redundancy is derived which turns out to depend only on the probability of the feature and of its components, but not on the statistical properties of any other features. The locality of this measure makes it ideal as the basis for a \"neural\" implementation of redundancy reduction, and an example of a very simple non-Hebbian algorithm is given. The effect of noise on learning redundancy is also discussed."
            },
            "slug": "Redundancy-Reduction-as-a-Strategy-for-Unsupervised-Redlich",
            "title": {
                "fragments": [],
                "text": "Redundancy Reduction as a Strategy for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A local feature measure determining how much a single feature reduces the total redundancy is derived which turns out to depend only on the probability of the feature and of its components, but not on the statistical properties of any other features."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653849"
                        ],
                        "name": "F. Rieke",
                        "slug": "F.-Rieke",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Rieke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rieke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991348"
                        ],
                        "name": "R. Steveninck",
                        "slug": "R.-Steveninck",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Steveninck",
                            "middleNames": [
                                "R.",
                                "de",
                                "Ruyter",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Steveninck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125505"
                        ],
                        "name": "D. Warland",
                        "slug": "D.-Warland",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warland",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Warland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6604470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45",
            "isKey": false,
            "numCitedBy": 1002,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task--extracting information about an unknown time-dependent stimulus from short segments of a spike train. Here the neural code was characterized from the point of view of the organism, culminating in algorithms for real-time stimulus estimation based on a single example of the spike train. These methods were applied to an identified movement-sensitive neuron in the fly visual system. Such decoding experiments determined the effective noise level and fault tolerance of neural computation, and the structure of the decoding algorithms suggested a simple model for real-time analog signal processing with spiking neurons."
            },
            "slug": "Reading-a-Neural-Code-Bialek-Rieke",
            "title": {
                "fragments": [],
                "text": "Reading a Neural Code"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Here the neural code was characterized from the point of view of the organism, culminating in algorithms for real-time stimulus estimation based on a single example of the spike train, applied to an identified movement-sensitive neuron in the fly visual system."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62423253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "945b503a625efeaac2ea4a9335b622773dd4d5fe",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a basic problem for neural encoding: the stimulus should be accurately represented in the neural responses. We use this criterion to design the optimal receptive fields of a model visual system. Since reconstruction fidelity is an ensemble average over signals and noise, the statistics of natural stimuli play a central role. We compare our results with those of similar studies which apply optimization principles based on information theory."
            },
            "slug": "Designing-receptive-fields-for-highest-fidelity-Ruderman",
            "title": {
                "fragments": [],
                "text": "Designing receptive fields for highest fidelity"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A basic problem for neural encoding is formulated: the stimulus should be accurately represented in the neural responses, and this criterion is used to design the optimal receptive fields of a model visual system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1600874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb37769d72999bcbfb0582b73607fd8d23f4545",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy."
            },
            "slug": "Relations-between-the-statistics-of-natural-images-Field",
            "title": {
                "fragments": [],
                "text": "Relations between the statistics of natural images and the response properties of cortical cells."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy into first- order redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50359956"
                        ],
                        "name": "F. Attneave",
                        "slug": "F.-Attneave",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Attneave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Attneave"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8453552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d0198460198fdb49b89d1646049712b3a0683df",
            "isKey": false,
            "numCitedBy": 2762,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "xThe ideas of information theory are at present stimulating many different areas of psychological inquiry. In providing techniques for quantifying situations which have hitherto been difficult or impossible to quantify, they suggest new and more precise ways of conceptualizing these situations (see Miller [12] for a general discussion and bibliography). Events ordered in time are particularly amenable to informational analysis; thus language sequences are being extensively studied, and other sequences, such as those of music, plainly invite research. In this paper I shall indicate some of the ways in which the concepts and techniques of information theory may clarify our understanding of visual perception. When we begin to consider perception as an information-handling process, it quickly becomes clear that much of the information received by any higher organism is redundant. Sensory events are highly interdependent in both space and time: if we know at a given moment the states of a limited number of receptors (i.e., whether they are firing or not firing), we can make better-than-chance inferences with respect to the prior and subsequent states of these receptors, and also with respect to the present, prior, and subsequent states of other receptors. The preceding statement, taken in its broadest im1 The experimental work for this study was performed as part of the United States Air Force Human Resources Research and Development Program. The opinions and conclusions contained in this report are those of the author. They are not to be construed as reflecting the views or indorsement of the Department of the Air Force. plications, is precisely equivalent to an assertion that the world as we know it is lawful. In the present discussion, however, we shall restrict our attention to special types of lawfulness which may exist in space at a fixed time, and which seem particularly relevant to processes of visual perception."
            },
            "slug": "Some-informational-aspects-of-visual-perception.-Attneave",
            "title": {
                "fragments": [],
                "text": "Some informational aspects of visual perception."
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Special types of lawfulness which may exist in space at a fixed time, and which seem particularly relevant to processes of visual perception are focused on."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706393"
                        ],
                        "name": "J. Rospars",
                        "slug": "J.-Rospars",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Rospars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rospars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780350"
                        ],
                        "name": "J. Fort",
                        "slug": "J.-Fort",
                        "structuredName": {
                            "firstName": "Jean-Claude",
                            "lastName": "Fort",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fort"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124706260,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c8aff814d33d309eb73d888e2512fd08953909aa",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Coding of odour quality in the first two neuronal layers of olfactory systems is studied, taking especially insects as reference. First, odour quality is defined by K independent molecular properties that activate differentially the chemosensitive receptors of first-order neurons. Each odour quality discriminated is assumed to be coded by a distinct spatial activity pattern of the principal neurons (or equivalently glomeruli) of the second layer. Second, computer simulations show that the differential projections into glomeruli of several types of first-order neurons (convergence) is the main factor responsible for the generation of activity maps. These maps give a complete and unbiased representation of all odour qualities which make them suitable as an internal code. Third, lateral inhibition mediated by local neurons can significantly increase the difference of activity between the least and most active glomeruli for each odour, and consequently the total number of discriminated odours. Fourth, the mos..."
            },
            "slug": "Coding-of-odour-quality:-roles-of-convergence-and-Rospars-Fort",
            "title": {
                "fragments": [],
                "text": "Coding of odour quality: roles of convergence and inhibition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computer simulations show that the differential projections into glomeruli of several types of first-order neurons (convergence) is the main factor responsible for the generation of activity maps which give a complete and unbiased representation of all odour qualities which make them suitable as an internal code."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60106091,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a4bb28d30078bd409545fffe9d5e0c2228a7f577",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Biological sensory processing systems are exquisitely complex and varied. Nonetheless, optimization principles and methods rooted in information theory can be used to understand and to make predictions concerning certain aspects of sensory processing. A brief overview of some work in this field is presented. A particular principle, that of \u2018maximum information preservation,\u2019 states that a sensory system should preserve as much information as possible at each processing stage, in the presence of noise and subject to various constraints. This optimization principle is applied to a couple of model systems to illustrate how the principle generates ordered maps and processing units (filters) whose properties are similar to those found in biological systems, as well as being useful for constructing artificial learning networks."
            },
            "slug": "Sensory-Processing-and-Information-Theory-Linsker",
            "title": {
                "fragments": [],
                "text": "Sensory Processing and Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This optimization principle of \u2018maximum information preservation\u2019 is applied to a couple of model systems to illustrate how the principle generates ordered maps and processing units (filters) whose properties are similar to those found in biological systems, as well as being useful for constructing artificial learning networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315855"
                        ],
                        "name": "T. P. Kaushal",
                        "slug": "T.-P.-Kaushal",
                        "structuredName": {
                            "firstName": "Tej",
                            "lastName": "Kaushal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Kaushal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28107770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5074f2219ddac70995f0a5e81ee2e892cb884b56",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To determine whether a particular sensory event is a reliable predictor of reward or punishment it is necessary to know the prior probability of that event. If the variables of a sensory representation normally occur independently of each other, then it is possible to derive the prior probability of any logical function of the variables from the prior probabilities of the individual variables, without any additional knowledge; hence such a representation enormously enlarges the scope of definable events that can be searched for reliable predictors. Finding a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored in this paper. The main results are (1) to show how to find such a code when the probabilities of the input states form a geometric progression, as is shown to be nearly true for keyboard characters in normal text; (2) to show how a Minimum Entropy Code can be approximated by repeatedly recoding pairs, triples, etc. of an original 7-bit code for keyboard characters; (3) to prove that in some cases enlarging the capacity of the output channel can lower the entropy."
            },
            "slug": "Finding-Minimum-Entropy-Codes-Barlow-Kaushal",
            "title": {
                "fragments": [],
                "text": "Finding Minimum Entropy Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Find a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored, and the main results are to show how to find such a code when the probabilities of the input states form a geometric progression."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10210242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ea293ac6d42d09ccb9ffab7bd72dcf6102c3eab",
            "isKey": false,
            "numCitedBy": 974,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to best understand a visual system one should attempt to characterize the natural images it processes. We gather images from the woods and find that these scenes possess an ensemble scale invariance. Further, they are highly non-Gaussian, and this non-Gaussian character cannot be removed through local linear filtering. We find that including a simple \"gain control\" nonlinearity in the filtering process makes the filter output quite Gaussian, meaning information is maximized at fixed channel variance. Finally, we use the measured power spectrum to place an upper bound on the information conveyed about natural scenes by an array of receptors."
            },
            "slug": "Statistics-of-Natural-Images:-Scaling-in-the-Woods-Ruderman-Bialek",
            "title": {
                "fragments": [],
                "text": "Statistics of Natural Images: Scaling in the Woods"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work gathers images from the woods and finds that these scenes possess an ensemble scale invariance, and this non-Gaussian character cannot be removed through local linear filtering, meaning information is maximized at fixed channel variance."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161090"
                        ],
                        "name": "S. Laughlin",
                        "slug": "S.-Laughlin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Laughlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Laughlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8991866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "420c02bdc487338dbda64feb7491dcf9fb412f17",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes. Elementary information theory shows that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "slug": "A-Simple-Coding-Procedure-Enhances-a-Neuron's-Laughlin",
            "title": {
                "fragments": [],
                "text": "A Simple Coding Procedure Enhances a Neuron's Information Capacity"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes, showing that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Zeitschrift fur Naturforschung. Section C, Biosciences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5062473"
                        ],
                        "name": "Zhaoping Li",
                        "slug": "Zhaoping-Li",
                        "structuredName": {
                            "firstName": "Zhaoping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 22447536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "75eeeeb6ff4268f197f1c708db3a67256ab7ace6",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Stereo images are highly redundant; the left and right frames of typical scenes are very similar. We explore the consequences of the hypothesis that cortical cells\u2014in addition to their multiscale coding strategies\u2014are concerned with reducing binocular redundancy due to correlations between the two eyes. We derive the most efficient coding strategies that achieve binocular decorrelation. It is shown that multiscale coding combined with a binocular decorrelation strategy leads to a rich diversity of cell types. In particular, the theory predicts monocular/binocular cells as well as a family of disparity selective cells, among which one can identify cells that are tuned-zero-excitatory, near, far, and tuned inhibitory. The theory also predicts correlations between ocular dominance, cell size, orientation, and disparity selectivities. Consequences on cortical ocular dominance column formation from abnormal developmental conditions such as strabismus and monocular eye closure are also predicted. These findings..."
            },
            "slug": "Efficient-stereo-coding-in-the-multiscale-Li-Atick",
            "title": {
                "fragments": [],
                "text": "Efficient stereo coding in the multiscale representation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that multiscale coding combined with a binocular decorrelation strategy leads to a rich diversity of cell types, including monocular/binocular cells as well as a family of disparity selective cells, among which one can identify cells that are tuned-zero-excitatory, near, far, and tuned inhibitory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21243149,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3f19a131970681eebbb003ac6e9b50c7a4db352b",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Animals that are primarily dependent on olfaction must obtain a description of the spatial location and the individual odor quality of environmental odor sources through olfaction alone. The variable nature of turbulent air flow makes such a remote sensing problem solvable if the animal can make use of the information conveyed by the fluctuation with time of the mixture of odor sources. Behavioral evidence suggests that such analysis takes place. An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources. The network structure is an idealization of olfactory bulb circuitry. The dynamics of synapse change is essential to the computation. The synaptic variables themselves contain information needed by higher processing centers. The use of the same axons to convey intensity information and quality information requires time-coding of information. Covariation defines an individual odor source (object), and this may have a parallel in vision."
            },
            "slug": "Olfactory-computation-and-object-perception.-Hopfield",
            "title": {
                "fragments": [],
                "text": "Olfactory computation and object perception."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12045859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc3ac9afa3e6bcf489ee82e6281843b35789b5c",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We exhibit a duality between two perceptrons that allows us to compare the theoretical analysis of supervised and unsupervised learning tasks. The first perceptron has one output and is asked to learn a classification of p patterns. The second (dual) perceptron has p outputs and is asked to transmit as much information as possible on a distribution of inputs. We show in particular that the maximum information that can be stored in the couplings for the supervised learning task is equal to the maximum information that can be transmitted by the dual perceptron."
            },
            "slug": "Duality-Between-Learning-Machines:-A-Bridge-Between-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Duality Between Learning Machines: A Bridge Between Supervised and Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that the maximum information that can be stored in the couplings for the supervised learning task is equal to themaximum information that could be transmitted by the dual perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23408621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d60694bb4f0f8736aff49f0513a4dca1303526ae",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources:-A-nonlinear-neural-Burel",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources: A nonlinear neural algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55379485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a54194422c56399b2923b2ad706b8175c8c48258",
            "isKey": false,
            "numCitedBy": 34823,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases."
            },
            "slug": "A-mathematical-theory-of-communication-Shannon",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of communication"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This final installment of the paper considers the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now."
            },
            "venue": {
                "fragments": [],
                "text": "MOCO"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5062473"
                        ],
                        "name": "Zhaoping Li",
                        "slug": "Zhaoping-Li",
                        "structuredName": {
                            "firstName": "Zhaoping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8487334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bd9b030a5ef8071119fe237a6fa04e67e032ab8",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A previously proposed theory of visual processing, based on redundancy reduction, is used to derive the retinal transfer function including color. The predicted kernels show the nontrivial mixing of space-time with color coding observed in experiments. The differences in color-coding between species are found to be due to differences among the chromatic autocorrelators for natural scenes in different environments."
            },
            "slug": "Understanding-Retinal-Color-Coding-from-First-Atick-Li",
            "title": {
                "fragments": [],
                "text": "Understanding Retinal Color Coding from First Principles"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The predicted kernels show the nontrivial mixing of space-time with color coding observed in experiments, and differences in color-coding between species are found to be due to differences among the chromatic autocorrelators for natural scenes in different environments."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400931141"
                        ],
                        "name": "F. Chapeau-Blondeau",
                        "slug": "F.-Chapeau-Blondeau",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Chapeau-Blondeau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Chapeau-Blondeau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221961946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86c899401b2edc623e670026f7e6005e32914bd8",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-entropy-maximization-in-the-by-a-neuron-Chapeau-Blondeau",
            "title": {
                "fragments": [],
                "text": "Information entropy maximization in the transmission by a neuron nonlinearity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858054"
                        ],
                        "name": "P. F\u00f6ldi\u00e1k",
                        "slug": "P.-F\u00f6ldi\u00e1k",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "F\u00f6ldi\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. F\u00f6ldi\u00e1k"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60513567,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7947b19f7d5488b8d76664c8b2d70daff350babd",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptation-and-decorrelation-in-the-cortex-Barlow-F\u00f6ldi\u00e1k",
            "title": {
                "fragments": [],
                "text": "Adaptation and decorrelation in the cortex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805342"
                        ],
                        "name": "R. Blahut",
                        "slug": "R.-Blahut",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Blahut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blahut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53897375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d789b834bcb9734199b52fb589d9d4ad614a0b45",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-and-practice-of-information-theory-Blahut",
            "title": {
                "fragments": [],
                "text": "Principles and practice of information theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98430564"
                        ],
                        "name": "Bialek",
                        "slug": "Bialek",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bialek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119890679"
                        ],
                        "name": "Zee",
                        "slug": "Zee",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Zee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33044238,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "87b68cfc36b625564e72437760a7181e0b91fdd1",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Understanding-the-efficiency-of-human-perception.-Bialek-Zee",
            "title": {
                "fragments": [],
                "text": "Understanding the efficiency of human perception."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga/698aedd44c51da829228e2c7d243960345efeb94?sort=total-citations"
}