{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3447955"
                        ],
                        "name": "Stanley Kok",
                        "slug": "Stanley-Kok",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Kok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley Kok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021654"
                        ],
                        "name": "Daniel Lowd",
                        "slug": "Daniel-Lowd",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lowd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lowd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759772"
                        ],
                        "name": "Hoifung Poon",
                        "slug": "Hoifung-Poon",
                        "structuredName": {
                            "firstName": "Hoifung",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoifung Poon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422314"
                        ],
                        "name": "Matthew Richardson",
                        "slug": "Matthew-Richardson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35108153"
                        ],
                        "name": "Parag Singla",
                        "slug": "Parag-Singla",
                        "structuredName": {
                            "firstName": "Parag",
                            "lastName": "Singla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Parag Singla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12590148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "439a295063f73bf16edd85821ad058b4e9000cd8",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Most real-world machine learning problems have both statistical and relational aspects. Thus learners need representations that combine probability and relational logic. Markov logic accomplishes this by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the conjugate gradient algorithm, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system."
            },
            "slug": "Markov-Logic-Domingos-Kok",
            "title": {
                "fragments": [],
                "text": "Markov Logic"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Markov logic accomplishes this by attaching weights to first-order formulas and viewing them as templates for features of Markov networks, and is the basis of the open-source Alchemy system."
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic Inductive Logic Programming"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746871"
                        ],
                        "name": "K. Kersting",
                        "slug": "K.-Kersting",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Kersting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kersting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Knowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992; Ngo & Haddawy, 1997;  Kersting & De Raedt, 2001 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Current proposals typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996 ;N go & Haddawy, 1997 ;S ato K Cussens, 1999;  Kersting & De Raedt, 2001 ; S antos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 142
                            }
                        ],
                        "text": "\u2026logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003),\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 181
                            }
                        ],
                        "text": "Knowledge-based model construction\nKnowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992; Ngo & Haddawy, 1997; Kersting & De Raedt, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15085278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbcc245b22a31f3a344e2240cab641d34ccb80ae",
            "isKey": true,
            "numCitedBy": 154,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, new representation languages that integrate first order logic with Bayesian networks have been developed. Bayesian logic programs are one of these languages. In this paper, we present results on combining Inductive Logic Programming (ILP) with Bayesian networks to learn both the qualitative and the quantitative components of Bayesian logic programs. More precisely, we show how to combine the ILP setting learning from interpretations with score-based techniques for learning Bayesian networks. Thus, the paper positively answers Koller and Pfeffer's question, whether techniques from ILP could help to learn the logical component of first order probabilistic models."
            },
            "slug": "Towards-Combining-Inductive-Logic-Programming-with-Kersting-Raedt",
            "title": {
                "fragments": [],
                "text": "Towards Combining Inductive Logic Programming with Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper positively answers Koller and Pfeffer's question, whether techniques from ILP could help to learn the logical component of first order probabilistic models."
            },
            "venue": {
                "fragments": [],
                "text": "ILP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073245877"
                        ],
                        "name": "Aymeric Puech",
                        "slug": "Aymeric-Puech",
                        "structuredName": {
                            "firstName": "Aymeric",
                            "lastName": "Puech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aymeric Puech"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Puech and Muggleton (2003) showed that SLPs are a special case of KBMC, and thus they can be converted into MLNs in the same way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Puech and Muggleton (2003)  showed that SLPs are a special case of KBMC, and thus they can be converted into MLNs in the same way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115550795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306dfea047e91f5b2952fbd1aa0b4aeb422cf398",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "First-order probabilistic models are recognized as efficient frameworks to represent several realworld problems: they combine the expressive power of first-order logic, which serves as a knowledge representation language, and the capability to model uncertainty with probabilities. Among existing models, it is usual to distinguish the domain-frequency approach from the possible-worlds approach. Bayesian logic programs (BLPs, which conveniently encode possible-worlds semantics) and stochastic logic programs (SLPs, often referred to as a domain-frequency approach) are promising probabilistic logic models in their categories. This paper is aimed at comparing the respective expressive power of these frameworks. We demonstrate relations between SLPs\u2019 and BLPs\u2019 semantics, and argue that SLPs can encode the same knowledge as a subclass of BLPs. We introduce extended SLPs which lift the latter result to any BLP. Converse properties are reviewed, and we show how BLPs can define the same semantics as complete, range-restricted, non-recursive SLPs. Algorithms that translate BLPs into SLPs (and vice versa) are provided, as well as worked examples of the intertranslations of SLPs and BLPs."
            },
            "slug": "A-Comparison-of-Stochastic-Logic-Programs-and-Logic-Puech",
            "title": {
                "fragments": [],
                "text": "A Comparison of Stochastic Logic Programs and Bayesian Logic Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Relations between SLPs\u2019 and BLP\u2019 semantics are demonstrated, and it is argued that SLPs can encode the same knowledge as a subclass of BLPs, and extended SLPs are introduced which lift the latter result to any BLP."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16721087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "454742f723d5bdeb023d4a65ede020e881d68ae4",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken both within Bayesian learning schemes and the framework of U-learnability. However, it is not obvious how an Induc-tive Logic Programming (ILP) system should best be provided with a probability distribution. This paper extends the results of a previous paper by the author which introduced stochastic logic programs as a means of providing a structured deenition of such a probability distribution. Stochastic logic programs are a generalisation of stochastic grammars. A stochastic logic program consists of a set of labelled clauses p : C where p is from the interval 0; 1] and C is a range-restricted deenite clause. A stochastic logic program P has a distributional semantics, that is one which assigns a probability distribution to the atoms of each predicate in the Herbrand base of the clauses in P. These probabilities are assigned to atoms according to an SLD-resolution strategy which employs a stochastic selection rule. It is shown that the probabilities can be computed directly for fail-free logic programs and by normalisation for arbitrary logic programs. The stochastic proof strategy can be used to provide three distinct functions: 1) a method of sampling from the Herbrand base which can be used to provide selected targets or example sets for ILP experiments, 2) a measure of the information content of examples or hypotheses; this can be used to guide the search in an ILP system and 3) a simple method for conditioning a given stochastic logic program on samples of data. Functions 1) and 3) are used to measure the generality of hypotheses in the ILP system Progol4.2. This supports an implementation of a Bayesian technique for learning from positive examples only. This paper is an extension of a paper with the same title which appeared in 12]"
            },
            "slug": "Stochastic-Logic-Programs-Muggleton",
            "title": {
                "fragments": [],
                "text": "Stochastic Logic Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Stochastic logic programs are introduced as a means of providing a structured deenition of such a probability distribution and it is shown that the probabilities can be computed directly for fail-free logic programs and by normalisation for arbitrary logic programs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202401"
                        ],
                        "name": "H. Pasula",
                        "slug": "H.-Pasula",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Pasula",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pasula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 169
                            }
                        ],
                        "text": "\u20261996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et al., 2002; Popescul & Ungar, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 105
                            }
                        ],
                        "text": "However, many of the large number of techniques for efficient inference in either case are applicable to MLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 223
                            }
                        ],
                        "text": "While they focus on directed graphical models, some of the ideas (e.g., different MCMC steps for different types of predicates, combining unification with variable elimination, abstraction hierarchies) may be applicable to MLNs.\nMLNs have some interesting similarities with the KBANN system, which converts a propositional Horn KB into a neural network and uses backpropagation to learn the network\u2019s weights (Towell & Shavlik, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "This leads to an infinite number of new constants, requiring the corresponding extension of\nSpringer\nMLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Current proposals typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996 ;N go & Haddawy, 1997 ;S ato K Cussens, 1999; Kersting & De Raedt, 2001 ;S antos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999;  Pasula & Russell, 2001;  Cumby & Roth, 2003), or database query languages ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "These assumptions are quite reasonable in most practical applications, and greatly simplify the use of MLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Pasula and Russell (2001) , Poole (2003) and Sanghai et al. (2003) have studied efficient inference in first-order probabilistic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "We saw in Section 4 how to remove the unique names and domain closure assumptions in MLNs. (When there are unknown objects of multiple types, a random variable for the number of each type is introduced.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 153
                            }
                        ],
                        "text": "Unlike most other ILP systems, which\nSpringer\nlearn only Horn clauses, CLAUDIEN is able to learn arbitrary first-order clauses, making it well suited to MLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 100
                            }
                        ],
                        "text": "Empirical tests with real-world data and knowledge in a university domain illustrate the promise of MLNs. Source code for learning and inference in MLNs is available at http://www.cs.washington.edu/ai/alchemy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 214
                            }
                        ],
                        "text": "Using CLAUDIEN to refine the KB typically performs worse in AUC but better in CLL than using CLAUDIEN from scratch; overall, the bestperforming logical method is KB+CLB, but its results fall well short of the best MLNs\u2019."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 251
                            }
                        ],
                        "text": "The other question we wanted to answer with these experiments is whether existing (propositional) probabilistic models are already powerful enough to be used in relational domains without the need for the additional representational power provided by MLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "There is a very large literature relating logic and probability; here we will focus only on the approaches most relevant to statistical relational learning, and discuss how they relate to MLNs.\n9.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 92
                            }
                        ],
                        "text": "This section briefly considers some additional works that are potentially relevant to MLNs.\nPasula and Russell (2001), Poole (2003) and Sanghai et al. (2003) have studied efficient inference in first-order probabilistic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5515539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d171d8b815f5b758d7b0a4db7090e352d15f2b4",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A new, general approach is described for approximate inference in first-order probabilistic languages, using Markov chain Monte Carlo (MCMC) techniques in the space of concrete possible worlds underlying any given knowledge base. The simplicity of the approach and its lazy construction of possible worlds make it possible to consider quite expressive languages. In particular, we consider two extensions to the basic relational probability models (RPMs) defined by Koller and Pfeffer, both of which have caused difficulties for exact algorithms. The first extension deals with uncertainty about relations among objects, where MCMC samples over relational structures. The second extension deals with uncertainty about the identity of individuals, where MCMC samples over sets of equivalence classes of objects. In both cases, we identify types of probability distributions that allow local decomposition of inference while encoding possible domains in a plausible way. We apply our algorithms to simple examples and show that the MCMC approach scales well."
            },
            "slug": "Approximate-inference-for-first-order-probabilistic-Pasula-Russell",
            "title": {
                "fragments": [],
                "text": "Approximate inference for first-order probabilistic languages"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work considers two extensions to the basic relational probability models (RPMs) defined by Koller and Pfeffer, and identifies types of probability distributions that allow local decomposition of inference while encoding possible domains in a plausible way."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744430"
                        ],
                        "name": "J. Cussens",
                        "slug": "J.-Cussens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cussens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cussens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 33
                            }
                        ],
                        "text": "Stochastic logic programs (SLPs) (Muggleton, 1996; Cussens, 1999) are a combination of logic programming and log-linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "Other logic programming approaches\nStochastic logic programs (SLPs) (Muggleton, 1996; Cussens, 1999) are a combination of logic programming and log-linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 51
                            }
                        ],
                        "text": "Stochastic logic programs (SLPs) (Muggleton, 1996; Cussens, 1999) are a combination of logic programming and log-linear models. Puech and Muggleton (2003) showed that SLPs are a special case of KBMC, and thus they can be converted into MLNs in the same way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 150
                            }
                        ],
                        "text": "\u2026subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1816018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "555bbd43bc82725a96e029f79db8d4d003721dbb",
            "isKey": true,
            "numCitedBy": 64,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on loglinear models in probabilistic constraint logic programming is applied to first-order probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning."
            },
            "slug": "Loglinear-models-for-first-order-probabilistic-Cussens",
            "title": {
                "fragments": [],
                "text": "Loglinear models for first-order probabilistic reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work shows how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data and compares the presented framework with other approaches to first-order probabilistic reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065321832"
                        ],
                        "name": "L. Ngo",
                        "slug": "L.-Ngo",
                        "structuredName": {
                            "firstName": "Liem",
                            "lastName": "Ngo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ngo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681109"
                        ],
                        "name": "P. Haddawy",
                        "slug": "P.-Haddawy",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Haddawy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haddawy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Knowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992;  Ngo & Haddawy, 1997;  Kersting & De Raedt, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 160
                            }
                        ],
                        "text": "Knowledge-based model construction\nKnowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992; Ngo & Haddawy, 1997; Kersting & De Raedt, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16211249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a59ce04b6a0d923cd60dc1c16b6b7d99a60bffb",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Answering-Queries-from-Context-Sensitive-Knowledge-Ngo-Haddawy",
            "title": {
                "fragments": [],
                "text": "Answering Queries from Context-Sensitive Probabilistic Knowledge Bases"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Probabilistic CLP generalizes SLPs to CLP (Riezler, 1998), and CLP (BN ) combines CLP with Bayesian networks (Santos Costa et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5599296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f34268b463e0219a57d049aac6fd165f4afc4d9",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses two central problems for probabilistic processing models: parameter estimation from incomplete data and efficient retrieval of most probable analyses. These questions have been answered satisfactorily only for probabilistic regular and context-free models. We address these problems for a more expressive probabilistic constraint logic programming model. We present a log-linear probability model for probabilistic constraint logic programming. On top of this model we define an algorithm to estimate the parameters and to select the properties of log-linear models from incomplete data. This algorithm is an extension of the improved iterative scaling algorithm of Della-Pietra, Della-Pietra, and Lafferty (1995). Our algorithm applies to log-linear models in general and is accompanied with suitable approximation methods when applied to large data spaces. Furthermore, we present an approach for searching for most probable analyses of the probabilistic constraint logic programming model. This method can be applied to the ambiguity resolution problem in natural language processing applications."
            },
            "slug": "Probabilistic-Constraint-Logic-Programming-Riezler",
            "title": {
                "fragments": [],
                "text": "Probabilistic Constraint Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An algorithm to estimate the parameters and to select the properties of log-linear models from incomplete data and an approach for searching for most probable analyses of the probabilistic constraint logic programming model are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143715817"
                        ],
                        "name": "D. Poole",
                        "slug": "D.-Poole",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Poole",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Poole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11707680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9ef7893ae7ee6826a43b5f58365092194c0e213",
            "isKey": false,
            "numCitedBy": 644,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Horn-Abduction-and-Bayesian-Networks-Poole",
            "title": {
                "fragments": [],
                "text": "Probabilistic Horn Abduction and Bayesian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118447506"
                        ],
                        "name": "C. Roth",
                        "slug": "C.-Roth",
                        "structuredName": {
                            "firstName": "Chad",
                            "lastName": "Roth",
                            "middleNames": [
                                "Cumby",
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 193
                            }
                        ],
                        "text": "\u20261996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et al., 2002; Popescul & Ungar, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 88
                            }
                        ],
                        "text": "(1992); Poole (1993); Muggleton (1996); Ngo and Haddawy (1997); Sato and Kameya (1997); Cussens (1999); Kersting and De Raedt (2001); Santos Costa et al. (2003)), frame-based systems (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 88
                            }
                        ],
                        "text": "(1992); Poole (1993); Muggleton (1996); Ngo and Haddawy (1997); Sato and Kameya (1997); Cussens (1999); Kersting and De Raedt (2001); Santos Costa et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 35
                            }
                        ],
                        "text": "(1999); Pasula and Russell (2001); Cumby and Roth (2003)), or database query languages (e.g., Taskar et al. (2002); Popescul and Ungar (2003))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 35
                            }
                        ],
                        "text": "(1999); Pasula and Russell (2001); Cumby and Roth (2003)), or database query languages (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 88
                            }
                        ],
                        "text": "(1992); Poole (1993); Muggleton (1996); Ngo and Haddawy (1997); Sato and Kameya (1997); Cussens (1999); Kersting and De Raedt (2001); Santos Costa et al. (2003)), frame-based systems (e.g., Friedman et al. (1999); Pasula and Russell (2001); Cumby and Roth (2003)), or database query languages (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14179970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0f181370d9baa090048ea31c7763fcfbfae883a",
            "isKey": true,
            "numCitedBy": 16,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We study representations and relational learning over structured domains within a propositionalization framework that decouples feature construction and model construction. We describe two complementary approaches that address three aspects of the problem: First, we develop and study a flexible knowledge representation for structured data, with an associated language that provides the syntax and a well defined equivalent semantics for expressing complex structured data succinctly. Second, we use this language to automate the process of feature construction by expressing \u2018types\u2019 of objects in the language, which are instantiated in the ground data, allowing us to determine the level at which learning is done. Finally, this process of re-representation of the domain allows general purpose learning schemes, such as feature efficient linear algorithms and probabilistic representations, to be defined over the resulting space, yielding efficient and expressive learning of relational functions over a structured domain using propositional means."
            },
            "slug": "Feature-Extraction-Languages-for-Propositionalized-Roth",
            "title": {
                "fragments": [],
                "text": "Feature Extraction Languages for Propositionalized Relational Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work develops and study a flexible knowledge representation for structured data, with an associated language that provides the syntax and a well defined equivalent semantics for expressing complex structured data succinctly, and uses this language to automate the process of feature construction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144074891"
                        ],
                        "name": "Sumit K. Sanghai",
                        "slug": "Sumit-K.-Sanghai",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Sanghai",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sumit K. Sanghai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Pasula and Russell (2001), Poole (2003) and  Sanghai et al. (2003)  have studied efficient inference in first-order probabilistic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 136
                            }
                        ],
                        "text": "This section briefly considers some additional works that are potentially relevant to MLNs.\nPasula and Russell (2001), Poole (2003) and Sanghai et al. (2003) have studied efficient inference in first-order probabilistic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4745154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7907f8aff65a78143b58fb3922ece5f276aac418",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Intelligent agents must function in an uncertain world, containing multiple objects and relations that change over time. Unfortunately, no representation is currently available that can handle all these issues, while allowing for principled and efficient inference. This paper addresses this need by introducing dynamic probabilistic relational models (DPRMs). DPRMs are an extension of dynamic Bayesian networks (DBNs) where each time slice (and its dependences on previous slices) is represented by a probabilistic relational model (PRM). Particle filtering, the standard method for inference in DBNs, has severe limitations when applied to DPRMs, but we are able to greatly improve its performance through a form of relational Rao-Blackwellisation. Further gains in efficiency arc obtained through the use of abstraction trees, a novel data structure. We successfully apply DPRMs to execution monitoring and fault diagnosis of an assembly plan, in which a complex product is gradually constructed from subparts."
            },
            "slug": "Dynamic-Probabilistic-Relational-Models-Sanghai-Domingos",
            "title": {
                "fragments": [],
                "text": "Dynamic Probabilistic Relational Models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper successfully applies dynamic probabilistic relational models (DPRMs) to execution monitoring and fault diagnosis of an assembly plan, in which a complex product is gradually constructed from subparts."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057536"
                        ],
                        "name": "M. Jaeger",
                        "slug": "M.-Jaeger",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 18514732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ae7d1149ad0a7a54e297076d14803957647e9cb",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-complexity-of-inference-about-probabilistic-Jaeger",
            "title": {
                "fragments": [],
                "text": "On the complexity of inference about probabilistic relational models"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746871"
                        ],
                        "name": "K. Kersting",
                        "slug": "K.-Kersting",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Kersting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kersting"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 957577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00709ed019f7ddc5cd84740099066cd20e61625d",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic inductive logic programming aka. statistical relational learning addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with machine learning and first order and relational logic representations. A rich variety of different formalisms and learning techniques have been developed. A unifying characterization of the underlying learning settings, however, is missing so far. \n \nIn this chapter, we start from inductive logic programming and sketch how the inductive logic programming formalisms, settings and techniques can be extended to the statistical case. More precisely, we outline three classical settings for inductive logic programming, namely learning from entailment, learning from interpretations, and learning from proofs or traces, and show how they can be adapted to cover state-of-the-art statistical relational learning approaches."
            },
            "slug": "Probabilistic-Inductive-Logic-Programming-Raedt-Kersting",
            "title": {
                "fragments": [],
                "text": "Probabilistic Inductive Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This chapter outlines three classical settings for inductive logic programming, namely learning from entailment, learning from interpretations, and learning from proofs or traces, and shows how they can be adapted to cover state-of-the-art statistical relational learning approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic Inductive Logic Programming"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736882"
                        ],
                        "name": "F. Bacchus",
                        "slug": "F.-Bacchus",
                        "structuredName": {
                            "firstName": "Fahiem",
                            "lastName": "Bacchus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bacchus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30275497,
            "fieldsOfStudy": [
                "Computer Science",
                "Philosophy"
            ],
            "id": "59735d8798db201970434caecedf12ed4a9dc7dd",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic information has many uses in an intelligent system. This book explores logical formalisms for representing and reasoning with probabilistic information that will be of particular value to researchers in nonmonotonic reasoning, applications of probabilities, and knowledge representation. It demonstrates that probabilities are not limited to particular applications, like expert systems; they have an important role to play in the formal design and specification of intelligent systems in general.Fahiem Bacchus focuses on two distinct notions of probabilities: one propositional, involving degrees of belief, the other proportional, involving statistics. He constructs distinct logics with different semantics for each type of probability that are a significant advance in the formal tools available for representing and reasoning with probabilities. These logics can represent an extensive variety of qualitative assertions, eliminating requirements for exact point-valued probabilities, and they can represent first-order logical information. The logics also have proof theories which give a formal specification for a class of reasoning that subsumes and integrates most of the probabilistic reasoning schemes so far developed in AI.Using the new logical tools to connect statistical with propositional probability, Bacchus also proposes a system of direct inference in which degrees of belief can be inferred from statistical knowledge and demonstrates how this mechanism can be applied to yield a powerful and intuitively satisfying system of defeasible or default reasoning.Contents: Introduction. Propositional Probabilities. Statistical Probabilities. Combining Statistical and Propositional Probabilities Default Inferences from Statistical Knowledge."
            },
            "slug": "Representing-and-reasoning-with-probabilistic-a-to-Bacchus",
            "title": {
                "fragments": [],
                "text": "Representing and reasoning with probabilistic knowledge - a logical approach to probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This book explores logical formalisms for representing and reasoning with probabilistic information that will be of particular value to researchers in nonmonotonic reasoning, applications of probabilities, and knowledge representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796536"
                        ],
                        "name": "Michael P. Wellman",
                        "slug": "Michael-P.-Wellman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wellman",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Wellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778725"
                        ],
                        "name": "J. Breese",
                        "slug": "J.-Breese",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Breese",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Breese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2473054"
                        ],
                        "name": "R. Goldman",
                        "slug": "R.-Goldman",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Goldman",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goldman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 102
                            }
                        ],
                        "text": "Knowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992; Ngo & Haddawy, 1997; Kersting & De Raedt, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 86
                            }
                        ],
                        "text": "The algorithm proceeds in two phases, analogous to knowledgebased model construction (Wellman et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 138
                            }
                        ],
                        "text": "Knowledge-based model construction\nKnowledge-based model construction (KBMC) is a combination of logic programming and Bayesian networks (Wellman et al., 1992; Ngo & Haddawy, 1997; Kersting & De Raedt, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 130
                            }
                        ],
                        "text": "Current proposals typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6385392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8089729d8711b3ef0de37eb6016ca3a311b491b5",
            "isKey": true,
            "numCitedBy": 219,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest among AI researchers in probabilistic and decision modelling, spurred by significant advances in representation and computation with network modelling formalisms. In applying these techniques to decision support tasks, fixed network models have proven to be inadequately expressive when a broad range of situations must be handled. Hence many researchers have sought to combine the strengths of flexible knowledge representation languages with the normative status and well-understood computational properties of decision-modelling formalisms and algorithms. One approach is to encode general knowledge in an expressive language, then dynamically construct a decision model for each particular situation or problem instance. We have developed several systems adopting this approach, which illustrate a variety of interesting techniques and design issues."
            },
            "slug": "From-knowledge-bases-to-decision-models-Wellman-Breese",
            "title": {
                "fragments": [],
                "text": "From knowledge bases to decision models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Several systems adopting this approach to encode general knowledge in an expressive language, then dynamically construct a decision model for each particular situation or problem instance are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Knowl. Eng. Rev."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057536"
                        ],
                        "name": "M. Jaeger",
                        "slug": "M.-Jaeger",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "We believe it is possible to extend MLNs to infinite domains (see Jaeger, 1998), but this is an issue of chiefly theoretical interest, and we leave it for future work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 37
                            }
                        ],
                        "text": "Tuples involving constants of more than one area were discarded, to avoid train-test contamination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3186646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8d618fabcc26a5083dbbec5055df4cfe12f8947",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational Bayesian networks extend standard Bayesian networks by integrating some of the expressive power of first-order logic into the Bayesian network paradigm. As in the case of the related technique of knowledge based model construction, so far, decidable semantics only have been provided for finite stochastic domains. In this paper we extend the semantics of relational Bayesian networks, so that they also define probability distributions over countably infinite structures. Using a technique remeniscent of quantifier elimination methods in model theory, we show that probabilistic queries about these distributions are decidable."
            },
            "slug": "Reasoning-About-Infinite-Random-Structures-with-Jaeger",
            "title": {
                "fragments": [],
                "text": "Reasoning About Infinite Random Structures with Relational Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper extends the semantics of relational Bayesian networks, so that they also define probability distributions over countably infinite structures, and shows that probabilistic queries about these distributions are decidable."
            },
            "venue": {
                "fragments": [],
                "text": "KR"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110904481"
                        ],
                        "name": "Taisuke Sato",
                        "slug": "Taisuke-Sato",
                        "structuredName": {
                            "firstName": "Taisuke",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taisuke Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764638"
                        ],
                        "name": "Yoshitaka Kameya",
                        "slug": "Yoshitaka-Kameya",
                        "structuredName": {
                            "firstName": "Yoshitaka",
                            "lastName": "Kameya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshitaka Kameya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 155
                            }
                        ],
                        "text": "Similar remarks apply to a number of other representations that are essentially equivalent to SLPs, like independent choice logic (Poole, 1993) and PRISM (Sato & Kameya, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 447738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5f8adf4b0b8060e7d208effad3f6e03534ec2d1",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs."
            },
            "slug": "PRISM:-A-Language-for-Symbolic-Statistical-Modeling-Sato-Kameya",
            "title": {
                "fragments": [],
                "text": "PRISM: A Language for Symbolic-Statistical Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 743435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611dac316bf03112c778cf7365d08e4a9d171876",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases."
            },
            "slug": "Learning-Probabilistic-Relational-Models-Getoor",
            "title": {
                "fragments": [],
                "text": "Learning Probabilistic Relational Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model and shows how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "The latter condition is particularly troublesome to enforce in relational extensions (Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 3
                            }
                        ],
                        "text": "As Taskar et al. (2002) point out, the need to avoid cycles in PRMs causes significant representational and computational difficulties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 249
                            }
                        ],
                        "text": "\u20261996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et al., 2002; Popescul & Ungar, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 114
                            }
                        ],
                        "text": "Collective classification also takes into account the classes of related objects (e.g., Chakrabarti et al., 1998; Taskar et al., 2002; Neville & Jensen, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "Relational Markov networks\nRelational Markov networks (RMNs) use database queries as clique templates, and have a feature for each state of a clique (Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 122
                            }
                        ],
                        "text": "Relational Markov networks (RMNs) use database queries as clique templates, and have a feature for each state of a clique (Taskar et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2282762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc36397e1fef5c922d64e88211a7e08ecc64759",
            "isKey": true,
            "numCitedBy": 798,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."
            },
            "slug": "Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel",
            "title": {
                "fragments": [],
                "text": "Discriminative Probabilistic Models for Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach is presented, showing how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724065"
                        ],
                        "name": "D. M. Chickering",
                        "slug": "D.-M.-Chickering",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chickering",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Chickering"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": "We used two propositional learners: Naive Bayes (Domingos & Pazzani, 1997) and Bayesian networks (Heckerman et al., 1995) with structure and parameters learned using the VFBN2 algorithm (Hulten & Domingos, 2002) with a maximum of four parents per node."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12224032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2086fe71a443512cd7efe50828fef81be59fe719",
            "isKey": false,
            "numCitedBy": 3521,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen\u2014a prior network\u2014and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k = 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches."
            },
            "slug": "Learning-Bayesian-Networks:-The-Combination-of-and-Heckerman-Geiger",
            "title": {
                "fragments": [],
                "text": "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A methodology for assessing informative priors needed for learning Bayesian networks from a combination of prior knowledge and statistical data is developed and how to compute the relative posterior probabilities of network structures given data is shown."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1937845"
                        ],
                        "name": "M. Paskin",
                        "slug": "M.-Paskin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Paskin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Paskin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16327964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a23a7c0d04b406e79667561253ddfaa7b6e3b010",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research has shown there are two types of uncertainty that can be expressed in first-order logic\u2014 propositional and statistical uncertainty\u2014and that both types can be represented in terms of probability spaces. However, these efforts have fallen short of providing a general account of how to design probability measures for these spaces; as a result, we lack a crucial component of any system that reasons under these types of uncertainty. In this paper, we describe an automatic procedure for defining such measures in terms of a probabilistic knowledge base. In particular, we employ the principle of maximum entropy to select measures that are consistent with our knowledge and that make the fewest assumptions in doing so. This approach yields models of first-order uncertainty that are principled, intuitive, and economical in their representation."
            },
            "slug": "Maximum-Entropy-Probabilistic-Logic-Paskin",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Probabilistic Logic"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes an automatic procedure for defining probability measures in terms of a probabilistic knowledge base and employs the principle of maximum entropy to select measures that are consistent with the authors' knowledge and that make the fewest assumptions in doing so."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724065"
                        ],
                        "name": "D. M. Chickering",
                        "slug": "D.-M.-Chickering",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chickering",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Chickering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745567"
                        ],
                        "name": "Robert Rounthwaite",
                        "slug": "Robert-Rounthwaite",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Rounthwaite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Rounthwaite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772349"
                        ],
                        "name": "C. Kadie",
                        "slug": "C.-Kadie",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Kadie",
                            "middleNames": [
                                "Myers"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kadie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 189
                            }
                        ],
                        "text": "Every RDN has a corresponding MLN in the same way that every dependency network has a corresponding Markov network, given by the stationary distribution of a Gibbs sampler operating on it (Heckerman et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 211
                            }
                        ],
                        "text": "\u2026or relational) attribute of each class, where S(x, v) means \u201cThe value of attribute S in object x is v.\u201d A PRM is then translated into an\n4 Conversely, joint distributions can be built up from classifiers (e.g., Heckerman et al., 2000), but this would be a significant extension of MACCENT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 2
                            }
                        ],
                        "text": ", (Heckerman et al., 2000)), but this would be a significant extension of MACCENT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11581349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30afca3a4056bc54deadc1c5794048436d1c9eb4",
            "isKey": true,
            "numCitedBy": 594,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships."
            },
            "slug": "Dependency-Networks-for-Inference,-Collaborative-Heckerman-Chickering",
            "title": {
                "fragments": [],
                "text": "Dependency Networks for Inference, Collaborative Filtering, and Data Visualization"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work describes a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network and identifies several basic properties of this representation and describes a computationally efficient procedure for learning the graph and probability components from data."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691828"
                        ],
                        "name": "Joseph Y. Halpern",
                        "slug": "Joseph-Y.-Halpern",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Halpern",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Y. Halpern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16514075,
            "fieldsOfStudy": [
                "Computer Science",
                "Philosophy"
            ],
            "id": "71f4bdd0b59d68076419d81cca12dda49edd3b81",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Analysis-of-First-Order-Logics-of-Probability-Halpern",
            "title": {
                "fragments": [],
                "text": "An Analysis of First-Order Logics of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 141
                            }
                        ],
                        "text": "A Markov network (also known as Markov random field) is a model for the joint distribution of a set of variables X = (X1, X2, . . . , Xn) \u2208 X (Pearl, 1988), It is composed of an undirected graph G and a set of potential functions \u03c6k."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 6
                            }
                        ],
                        "text": "Satisfiability ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Early work\nAttempts to combine logic and probability in AI date back to at least Nilsson (1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9800575,
            "fieldsOfStudy": [
                "Philosophy",
                "Computer Science"
            ],
            "id": "35bbc5ce402e88705e6e599f9aa45f8889b9e369",
            "isKey": false,
            "numCitedBy": 1331,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Logic-Nilsson",
            "title": {
                "fragments": [],
                "text": "Probabilistic Logic"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1937845"
                        ],
                        "name": "M. Paskin",
                        "slug": "M.-Paskin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Paskin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Paskin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123427173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62f4b72d142c5642f4edbc3c440363620a8c9ae8",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I present probabilistic extensions to propositional and first-order logic. The general approach is to define a probability distribution over possible worlds; this distribution is chosen as the unique distribution that agrees with a set of probability constraints over a set of logical sentences and that has maximal entropy. Such distributions are well studied in the Statistics and Machine Learning literature, and using them not only results in simple, intuitive models, but makes available standard inference and learning algorithms."
            },
            "slug": "Maximum-Entropy-Probabilistic-Logics-Paskin",
            "title": {
                "fragments": [],
                "text": "Maximum-Entropy Probabilistic Logics"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Probabilistic extensions to propositional and first-order logic are presented, which results in simple, intuitive models, and makes available standard inference and learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763670"
                        ],
                        "name": "L. Dehaspe",
                        "slug": "L.-Dehaspe",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Dehaspe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Dehaspe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 210
                            }
                        ],
                        "text": "In the future we plan to more fully integrate structure learning into MLNs, by generalizing techniques like Della Pietra et al.\u2019s (1997) to the first-order realm, as done by MACCENT for classification problems (Dehaspe, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "We use the CLAUDIEN system for this purpose (De Raedt & Dehaspe, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 151
                            }
                        ],
                        "text": "Like any probability estimation approach, MLNs can be used for classification simply by issuing the appropriate conditional queries.4 In particular, a MACCENT model can be converted into an MLN simply by defining a class predicate (as in Section 8.1), adding the corresponding features and their weights to the MLN, and adding a formula with infinite weight stating that each object must have exactly one class."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 83
                            }
                        ],
                        "text": "\u2019s (1997) to the first-order realm, as done by MACCENT for classification problems (Dehaspe, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 368
                            }
                        ],
                        "text": "PRMs can be converted into MLNs by defining a predicate S(x, v) for each (propositional or relational) attribute of each class, where S(x, v) means \u201cThe value of attribute S in object x is v.\u201d A PRM is then translated into an\n4 Conversely, joint distributions can be built up from classifiers (e.g., Heckerman et al., 2000), but this would be a significant extension of MACCENT."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 142
                            }
                        ],
                        "text": "As described in Section 8.1, MLNs can be used for collective classification, where the classes of different objects can depend on each other; MACCENT, which requires that each object be represented in a separate Prolog knowledge base, does not have this capability."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 25
                            }
                        ],
                        "text": "A key difference between MACCENT and MLNs is that MACCENT is a classification system (i.e., it predicts the conditional distribution of an object\u2019s class given its properties), while an MLN represents the full joint distribution of a set of predicates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "MACCENT (Dehaspe, 1997) is a system that learns log-linear models with first-order features; each feature is a conjunction of a class and a Prolog query (clause with empty head)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MACCENT can make use of deterministic background knowledge in the form of Prolog clauses; these can be added to the MLN as formulas with infinite weight."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17063720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "121101b6fcc6f4952514c179559bce4f6163d884",
            "isKey": true,
            "numCitedBy": 50,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the learning system Maccent which addresses the novel task of stochastic MAximum ENTropy modeling with Clausal Constraints. Maximum Entropy method is a Bayesian method based on the principle that the target stochastic model should be as uniform as possible, subject to known constraints. Maccent incorporates clausal constraints that are based on the evaluation of Prolog clauses in examples represented as Prolog programs. We build on an existing maximum-likelihood approach to maximum entropy modeling, which we upgrade along two dimensions: (1) Maccent can handle larger search spaces, due to a partial ordering defined on the space of clausal constraints, and (2) uses a richer first-order logic format. In comparison with other inductive logic programming systems, MACCENT seems to be the first that explicitly constructs a conditional probability distribution p(C|I) based on an empirical distribution \\(\\tilde p\\)(C|I) (where p(C|I) (\\(\\tilde p\\)(C|I)) equals the induced (observed) probability of an instance I belonging to a class C). First experiments indicate MACCENT may be useful for prediction, and for classification in cases where the induced model should be combined with other stochastic information sources."
            },
            "slug": "Maximum-Entropy-Modeling-with-Clausal-Constraints-Dehaspe",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Modeling with Clausal Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "First experiments indicate MACCENT may be useful for prediction, and for classification in cases where the induced model should be combined with other stochastic information sources, and on an existing maximum-likelihood approach to maximum entropy modeling."
            },
            "venue": {
                "fragments": [],
                "text": "ILP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143715817"
                        ],
                        "name": "D. Poole",
                        "slug": "D.-Poole",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Poole",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Poole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 20
                            }
                        ],
                        "text": "After running for 24 hours (approximately 2 million Gibbs steps per chain), the average R value across training sets was 3.04, with no one training set having reached an R value less than 2 (other than briefly dipping to 1.5 in the early stages of the process)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "This section briefly considers some additional works that are potentially relevant to MLNs.\nPasula and Russell (2001), Poole (2003) and Sanghai et al. (2003) have studied efficient inference in first-order probabilistic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "Investigating lifted inference (where queries containing variables are answered without grounding them) is an important direction for future work (see Jaeger (2000) and Poole (2003) for initial results)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62068714,
            "fieldsOfStudy": [
                "Philosophy",
                "Computer Science"
            ],
            "id": "3d4606a66c81cf3a5fa647d80c5895e2301db4a9",
            "isKey": true,
            "numCitedBy": 489,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "There have been many proposals for first-order belief networks (i.e., where we quantify over individuals) but these typically only let us reason about the individuals that we know about. There are many instances where we have to quantify over all of the individuals in a population. When we do this the population size often matters and we need to reason about all of the members of the population (but not necessarily individually). This paper presents an algorithm to reason about multiple individuals, where we may know particular facts about some of them, but want to treat the others as a group. Combining unification with variable elimination lets us reason about classes of individuals without needing to ground out the theory."
            },
            "slug": "First-order-probabilistic-inference-Poole",
            "title": {
                "fragments": [],
                "text": "First-order probabilistic inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents an algorithm to reason about multiple individuals, where the authors may know particular facts about some of them, but want to treat the others as a group."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706989"
                        ],
                        "name": "V. S. Costa",
                        "slug": "V.-S.-Costa",
                        "structuredName": {
                            "firstName": "V\u00edtor",
                            "lastName": "Costa",
                            "middleNames": [
                                "Santos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. S. Costa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144334798"
                        ],
                        "name": "David Page",
                        "slug": "David-Page",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Page",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Page"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744430"
                        ],
                        "name": "J. Cussens",
                        "slug": "J.-Cussens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cussens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cussens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 55
                            }
                        ],
                        "text": "Acknowledgments We are grateful to Julian Besag, Vitor Santos Costa, James Cussens, Nilesh Dalvi, Alan Fern, Alon Halevy, Mark Handcock, Henry Kautz, Kristian Kersting, Tian Sang, Bart Selman, Dan Suciu, Jeremy Tantrum, and Wei Wei for helpful discussions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 266
                            }
                        ],
                        "text": "Current proposals typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et al., 2002; Popescul & Ungar, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 144
                            }
                        ],
                        "text": "\u2026Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 117
                            }
                        ],
                        "text": "Probabilistic CLP generalizes SLPs to CLP (Riezler, 1998), and CLP (BN ) combines CLP with Bayesian networks (Santos Costa et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3135523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fdc3e265fe4f33021dd0cd618539a25fa6557a3",
            "isKey": true,
            "numCitedBy": 122,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "In Datalog, missing values are represented by Skolem constants. More generally, in logic programming missing values, or existentially quantified variables, are represented by terms built from Skolem functors. The CLP(BN) language represents the joint probability distribution over missing values in a database or logic program by using constraints to represent Skolem functions. Algorithms from inductive logic programming (ILP) can be used with only minor modification to learn CLP(BN) programs. An implementation of CLP(BN) is publicly available as part of YAP Prolog at http://www.ncc.up.pt/~vsc/Yap."
            },
            "slug": "CLP(BN):-Constraint-Logic-Programming-for-Knowledge-Costa-Page",
            "title": {
                "fragments": [],
                "text": "CLP(BN): Constraint Logic Programming for Probabilistic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The CLP(BN) language represents the joint probability distribution over missing values in a database or logic program by using constraints to represent Skolem functions."
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic Inductive Logic Programming"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830699"
                        ],
                        "name": "D. Ourston",
                        "slug": "D.-Ourston",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Ourston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ourston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Early work\nAttempts to combine logic and probability in AI date back to at least Nilsson (1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2276266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8ac3efbe5b679d29516a002b54ad1c8a47ec8a8",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-Refinement-Combining-Analytical-and-Methods-Ourston-Mooney",
            "title": {
                "fragments": [],
                "text": "Theory Refinement Combining Analytical and Empirical Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744430"
                        ],
                        "name": "J. Cussens",
                        "slug": "J.-Cussens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cussens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cussens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Stochastic logic programs (SLPs) (Muggleton, 1996; Cussens, 1999) are a combination of logic programming and log-linear models. Puech and Muggleton (2003) showed that SLPs are a special case of KBMC, and thus they can be converted into MLNs in the same way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "individuals and their relations to be explicitly represented (see Cussens, 2003), and contextspecific independencies to be compactly written down, instead of left implicit in the node models. More recently, Heckerman et al. (2004) have proposed a language based on entityrelationship models that combines the features of plates and PRMs; this language is a special case of MLNs in the same way that ER models are a special case of logic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 169
                            }
                        ],
                        "text": "\u2026they allow\n5 Use of SQL aggregates requires that their definitions be imported into the MLN.\nSpringer\nindividuals and their relations to be explicitly represented (see Cussens, 2003), and contextspecific independencies to be compactly written down, instead of left implicit in the node models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2808964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d4473f9493b718cb297fef9f5aefbbbef982c7e",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational data is equivalent to non-relational structured data. It is this equivalence which permits probabilistic models of relational data. Learning of probabilistic models for relational data is possible because one item of structured data is generally equivalent to many related data items. Succession and inclusion are two relations that have been well explored in the statistical literature. A description of the relevant statistical approaches is given. The representation of relational data via Bayesian nets is examined, and compared with PRMs. The paper ends with some cursory remarks on structured objects. 1 Learning from iid samples Recall from[Cussens, 2000 ], the well-known correspondence between the mathematical abstractions used in statistics and the real world. This correspondence is given diagrammatically in Figure 1. This view sees Nature as a machine which probabilistically spits out data in response to questions (inputs) that we give it. In some cases (e.g. clustering, density estimation) the independent variables do not play an important role\u2014the machine does not require any input to produce an output. This probabilistic machine has many names in the literature, it is Hacking\u2019s \u201cchance set-up\u201d [Hacking, 1965] and Popper\u2019s \u201cgenerating conditions\u201d [Popper, 1983 ]. This probabilistic machine is often taken to produce output by selecting its output from some population of possible outputs. Such a reconceptualisation is sometimes strained: \u201cBut only excessive metaphor makes outcomes of every chance set-up into samples from an hypothetical population\u201d [Hacking, 1965, p. 25]. But it is pretty much hard-coded into the standard Kolmogorovian formalisation of probability. Kolmogorov\u2019s axiomatisation defines a probabilistic model to be a probability space(\u03a9,F , P ). Here\u03a9 is the population, and outputs (actually subsets of \u03a9 in F) are chosen according to P . In standard approaches to statistical inference (or \u2018learning\u2019; the terms will be used interchangeably in this paper) we assume that the observed data is composed of independent and identically distributed (iid) items sampled from\u03a9. The homogeneity of such data permits estimation of P . To Machine Nature Model Unknown"
            },
            "slug": "Individuals,-relations-and-structures-in-models-Cussens",
            "title": {
                "fragments": [],
                "text": "Individuals, relations and structures in probabilistic models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The representation of relational data via Bayesian nets is examined, and the standard Kolmogorovian formalisation of probability approaches are given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 128
                            }
                        ],
                        "text": "Plates and probabilistic ER models\nLarge graphical models with repeated structure are often compactly represented using plates (Buntine, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "Large graphical models with repeated structure are often compactly represented using plates (Buntine, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11672931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, andthe manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximizationalgorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "slug": "Operations-for-Learning-with-Graphical-Models-Buntine",
            "title": {
                "fragments": [],
                "text": "Operations for Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691703"
                        ],
                        "name": "J. Jaffar",
                        "slug": "J.-Jaffar",
                        "structuredName": {
                            "firstName": "Joxan",
                            "lastName": "Jaffar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jaffar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743545"
                        ],
                        "name": "J. Lassez",
                        "slug": "J.-Lassez",
                        "structuredName": {
                            "firstName": "Jean-Louis",
                            "lastName": "Lassez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lassez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Constraint logic programming (CLP) is an extension of logic programming where variables are constrained instead of being bound to specific values during inference ( Laffar & Lassez, 1987 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2088154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5323b0af5f3b48b60891a6a80ab587efe1411c6",
            "isKey": false,
            "numCitedBy": 1374,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of designing programming systems to reason with and about constraints. Taking a logic programming approach, we define a class of programming languages, the CLP languages, all of which share the same essential semantic properties. From a conceptual point of view, CLP programs are highly declarative and are soundly based within a unified framework of formal semantics. This framework not only subsumes that of logic programming, but satisfies the core properties of logic programs more naturally. From a user's point of view, CLP programs have great expressive power due to the constraints which they naturally manipulate. Intuition in the reasoning about programs is enhanced as a result of working directly in the intended domain of discourse. This contrasts with working in the Herbrand Universe wherein every semantic object has to be explicitly coded into a Herbrand term; this enforces reasoning at a primitive level. Finally, from an implementor's point of view, CLP systems can be efficient because of the exploitation of constraint solving techniques over specific domains."
            },
            "slug": "Constraint-logic-programming-Jaffar-Lassez",
            "title": {
                "fragments": [],
                "text": "Constraint logic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A class of programming languages, the CLP languages, are defined, all of which share the same essential semantic properties, and are highly declarative and are soundly based within a unified framework of formal semantics."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '87"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Heckerman et al. (2004) have proposed a language based on entityrelationship models that combines the features of plates and PRMs; this language is a special case of MLNs in the same way that ER models are a special case of logic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15412720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04757f50d0021c8351237fad2f4002e59d5d8430",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a graphical language for re- lational data called the probabilistic entity- relationship (PER) model. The model is an extension of the entity-relationship model, a common model for the abstract repre- sentation of database structure. We con- centrate on the directed version of this model\u2014the directed acyclic probabilistic entity-relationship (DAPER) model. The DAPER model is closely related to the plate model and the probabilistic relational model (PRM), existing models for relational data. The DAPER model is more expressive than either existing model, and also helps to demonstrate their similarity. dinary graphical models (e.g., directed-acyclic graphs and undirected graphs) are to flat data. In this paper, we introduce a new graphical model for relational data\u2014the probabilistic entity-relationship (PER) model. This model class is more expressive than either PRMs or plate models. We concentrate on a particular type of PER model\u2014the directed acyclic probabilistic entity-relationship (DAPER) model\u2014in which all probabilistic arcs are directed. It is this ver- sion of PER model that is most similar to the plate model and PRM. We define new versions of the plate model and PRM such their expressiveness is equivalent to the DAPER model, and then (in the expanded tech report, Heckerman, Meek, and Koller, 2004) compare the new and old definitions. Consequently, we both demonstrate the similarity among the original lan- guages as well as enhance their abilities to express con- ditional independence in relational data. Our hope is that this demonstration of similarity will foster greater communication and collaboration among statisticians who mostly use plate models and computer scientists who mostly use PRMs. We in fact began this work with an effort to unify traditional PRMs and plate models. In the process, we discovered that it was important to make both entities and relationships (concepts discussed in de- tail in the next section) first class objects in the lan- guage. We in turn discovered an existing language that does this\u2014the entity-relationship (ER) model\u2014a commonly used model for the abstract representation of database structure. We then extended this language to handle probabilistic relationships, creating the PER model. We should emphasize that the languages we discuss are neither meant to serve as a database schema nor meant to be built on top of one. In practice, database schemas are built up over a long period of time as the needs of the database consumers change. Conse-"
            },
            "slug": "Probabilistic-Entity-Relationship-Models,-PRMs,-and-Heckerman-Meek",
            "title": {
                "fragments": [],
                "text": "Probabilistic Entity-Relationship Models, PRMs, and Plate Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A graphical language for re- lational data called the probabilistic entity- relationship (PER) model is introduced, an extension of the entity-relationship model, a common model for the abstract repre- sentation of database structure."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 42
                            }
                        ],
                        "text": "Another alternative is iterative scaling (Della Pietra et al., 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 110
                            }
                        ],
                        "text": "Features can also be learned from data, for example by greedily constructing conjunctions of atomic features (Della Pietra et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 273
                            }
                        ],
                        "text": "\u2026of formulas.3 Nevertheless, the probabilities of all formulas collectively determine all weights, if we view them as constraints on a maximum entropy distribution, or treat them as empirical probabilities and learn the maximum likelihood weights (the two are equivalent) (Della Pietra et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": true,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740077"
                        ],
                        "name": "Geoff Hulten",
                        "slug": "Geoff-Hulten",
                        "structuredName": {
                            "firstName": "Geoff",
                            "lastName": "Hulten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoff Hulten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9230745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef947b0e25257abe57e8e44c5cc2c5dc09cee22f",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search. The result of applying the method to an algorithm is that its running time becomes independent of the size of the database, while the decisions made are essentially identical to those that would be made given infinite data. The method works within pre-specified memory limits and, as long as the data is iid, only requires accessing it sequentially. It gives anytime results, and can be used to produce batch, stream, time-changing and active-learning versions of an algorithm. We apply the method to learning Bayesian networks, developing an algorithm that is faster than previous ones by orders of magnitude, while achieving essentially the same predictive performance. We observe these gains on a series of large databases \"generated from benchmark networks, on the KDD Cup 2000 e-commerce data, and on a Web log containing 100 million requests."
            },
            "slug": "Mining-complex-models-from-arbitrarily-large-in-Hulten-Domingos",
            "title": {
                "fragments": [],
                "text": "Mining complex models from arbitrarily large databases in constant time"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A scaling-up method that is applicable to essentially any induction algorithm based on discrete search is proposed, developing an algorithm that is faster than previous ones by orders of magnitude, while achieving essentially the same predictive performance."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144050371"
                        ],
                        "name": "Jennifer Neville",
                        "slug": "Jennifer-Neville",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Neville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Neville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774129"
                        ],
                        "name": "David D. Jensen",
                        "slug": "David-D.-Jensen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jensen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David D. Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 159
                            }
                        ],
                        "text": "Relational dependency networks\nIn a relational dependency network (RDN), each node\u2019s probability conditioned on its Markov blanket is given by a decision tree (Neville & Jensen, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 135
                            }
                        ],
                        "text": "Collective classification also takes into account the classes of related objects (e.g., Chakrabarti et al., 1998; Taskar et al., 2002; Neville & Jensen, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Collective classification also takes into account the classes of related objects (e.g., Chakrabarti et al., 1998 ;T askar et al.,2002;  Neville & Jensen, 2003 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In a relational dependency network (RDN), each node\u2019s probability conditioned on its Markov blanket is given by a decision tree ( Neville & Jensen, 2003 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11015651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56eb42f079e2b596d6c2e6156a5e4cfc8a48a19f",
            "isKey": true,
            "numCitedBy": 134,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Collective classification models exploit the dependencies in a network of objects to improve predictions. For example, in a network of web pages, the topic of a page may depend on the topics of hyperlinked pages. A relational model capable of expressing and reasoning with such dependencies should achieve superior performance to relational models that ignore such dependencies. In this paper, we present relational dependency networks (RDNs), extending recent work in dependency networks to a relational setting. RDNs are a collective classification model that offers simple parameter estimation and efficient structure learning. On two real-world data sets, we compare RDNs to ordinary classification with relational probability trees and show that collective classification improves performance."
            },
            "slug": "Collective-Classification-with-Relational-Networks-Neville-Jensen",
            "title": {
                "fragments": [],
                "text": "Collective Classification with Relational Dependency Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents relational dependency networks (RDNs), a collective classification model that offers simple parameter estimation and efficient structure learning and shows that collective classification improves performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MLNs have some interesting similarities with the KBANN system, which converts a propositional Horn KB into a neural network and uses backpropagation to learn the network\u2019s weights ( Towell & Shavlik, 1994 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 272
                            }
                        ],
                        "text": "\u2026unification with variable elimination, abstraction hierarchies) may be applicable to MLNs.\nMLNs have some interesting similarities with the KBANN system, which converts a propositional Horn KB into a neural network and uses backpropagation to learn the network\u2019s weights (Towell & Shavlik, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2489891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9caf5445e47af0fc0782bec1f26e4de840a0b5a9",
            "isKey": false,
            "numCitedBy": 758,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Knowledge-Based-Artificial-Neural-Networks-Towell-Shavlik",
            "title": {
                "fragments": [],
                "text": "Knowledge-Based Artificial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 45
                            }
                        ],
                        "text": "Inference in Markov networks is #P-complete (Roth, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 87
                            }
                        ],
                        "text": "Proof: Counting satisfying assignments of propositional monotone 2-CNF is #P-complete (Roth, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Link prediction"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1838914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "937c3e93a15cc416af330f9fcbcf447f7ad77e1e",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Hardness-of-Approximate-Reasoning-Roth",
            "title": {
                "fragments": [],
                "text": "On the Hardness of Approximate Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 122
                            }
                        ],
                        "text": "This is potentially useful in areas like the Semantic Web (Berners-Lee et al., 2001) and mass collaboration (Richardson & Domingos, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "More generally, MLNs can be viewed as an extension to probability estimation of a long line of work on knowledge-intensive learning (e.g., Bergadano & Giordana 1988; Pazzani & Kibler 1992;  Ourston & Mooney 1994 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11307768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2d3a6598944463687115e369a0223d4826bbe26",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Data cleaning and integration is typically the most expen- sive step in the KDD process. A key part, known as record linkage or de-duplication, is identifying which records in a database refer to the same entities. This problem is traditionally solved separately for each candidate record pair (followed by transitive closure). We propose to use instead a multi-relational approach, performing simultaneous inference for all candidate pairs, and allowing information to propagate from one candidate match to another via the attributes they have in common. Our formulation is based on conditional random elds, and allows an optimal solution to be found in polynomial time using a graph cut algorithm. Pa- rameters are learned using a voted perceptron algorithm. Experiments on real and synthetic databases show that multi-relational record linkage outperforms the standard approach."
            },
            "slug": "Multi-Relational-Record-Linkage-Domingos",
            "title": {
                "fragments": [],
                "text": "Multi-Relational Record Linkage"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experiments on real and synthetic databases show that multi-relational record linkage outperforms the standard approach, and allows an optimal solution to be found in polynomial time using a graph cut algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736882"
                        ],
                        "name": "F. Bacchus",
                        "slug": "F.-Bacchus",
                        "structuredName": {
                            "firstName": "Fahiem",
                            "lastName": "Bacchus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bacchus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2187850"
                        ],
                        "name": "Adam J. Grove",
                        "slug": "Adam-J.-Grove",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Grove",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam J. Grove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691828"
                        ],
                        "name": "Joseph Y. Halpern",
                        "slug": "Joseph-Y.-Halpern",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Halpern",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Y. Halpern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bacchus (1990), Halpern (1990) and coworkers (e.g.,  Bacchus et al., 1996 ) studied the problem in detail from a theoretical standpoint."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 52
                            }
                        ],
                        "text": "Bacchus (1990), Halpern (1990) and coworkers (e.g., Bacchus et al., 1996) studied the problem in detail from a theoretical standpoint."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47035812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6f58567e134e11ee2fa8ae9eb088723c1b1bf03",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-Statistical-Knowledge-Bases-to-Degrees-of-Bacchus-Grove",
            "title": {
                "fragments": [],
                "text": "From Statistical Knowledge Bases to Degrees of Belief"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422314"
                        ],
                        "name": "Matthew Richardson",
                        "slug": "Matthew-Richardson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6023267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399edd2bc226fa1b9e5ff46b37b8b21f82919948",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Acquiring knowledge has long been the major bottleneck preventing the rapid spread of AI systems. Manual approaches are slow and costly. Machine-learning approaches have limitations in the depth and breadth of knowledge they can acquire. The spread of the Internet has made possible a third solution: building knowledge bases by mass collaboration, with thousands of volunteers contributing simultaneously. While this approach promises large improvements in the speed and cost of knowledge base development, it can only succeed if the problem of ensuring the quality, relevance and consistency of the knowledge is addressed, if contributors are properly motivated, and if the underlying algorithms scale. In this paper we propose an architecture that meets all these desiderata. It uses first-order probabilistic reasoning techniques to combine potentially inconsistent knowledge sources of varying quality, and it uses machine-learning techniques to estimate the quality of knowledge. We evaluate the approach using a series of synthetic knowledge bases and a pilot study in the domain of printer troubleshooting."
            },
            "slug": "Building-large-knowledge-bases-by-mass-Richardson-Domingos",
            "title": {
                "fragments": [],
                "text": "Building large knowledge bases by mass collaboration"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses first-order probabilistic reasoning techniques to combine potentially inconsistent knowledge sources of varying quality, and it uses machine-learning techniques to estimate the quality of knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "K-CAP '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080984"
                        ],
                        "name": "F. Bergadano",
                        "slug": "F.-Bergadano",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Bergadano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bergadano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683066"
                        ],
                        "name": "A. Giordana",
                        "slug": "A.-Giordana",
                        "structuredName": {
                            "firstName": "Attilio",
                            "lastName": "Giordana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Giordana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 2
                            }
                        ],
                        "text": ", Bergadano and Giordana (1988); Pazzani and Kibler (1992); Ourston and Mooney (1994))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 139
                            }
                        ],
                        "text": "More generally, MLNs can be viewed as an extension to probability estimation of a long line of work on knowledge-intensive learning (e.g., Bergadano & Giordana 1988; Pazzani & Kibler 1992; Ourston & Mooney 1994."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39671038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f5595b7babf8b7364732437f9e3842e6d112fd8",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Knowledge-Intensive-Approach-to-Concept-Induction-Bergadano-Giordana",
            "title": {
                "fragments": [],
                "text": "A Knowledge Intensive Approach to Concept Induction"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226208"
                        ],
                        "name": "Brian Milch",
                        "slug": "Brian-Milch",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Milch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Milch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711416"
                        ],
                        "name": "B. Marthi",
                        "slug": "B.-Marthi",
                        "structuredName": {
                            "firstName": "Bhaskara",
                            "lastName": "Marthi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Marthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Milch et al. (2004) have proposed a language, called BLOG, designed to avoid making the unique names and domain closure assumptions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5290125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01189ef908a078b8115d0d9e99d4d38af0efeb77",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In many real-world probabilistic reasoning problems, one of the questions we want to answer is: how many objects are out there? Examples of such problems range from multitarget tracking to extracting information from text documents. However, most probabilistic modeling formalisms \u2014 even firstorder ones \u2014 assume a fixed, known set of objects. We introduce a language called Blog for specifying probability distributions over relational structures that include varying sets of objects. In this paper we present Blog informally, by means of example models for multi-target tracking and citation matching. We discuss some attractive features of Blog models and some avenues of future work."
            },
            "slug": "BLOG:-Relational-Modeling-with-Unknown-Objects-Milch-Marthi",
            "title": {
                "fragments": [],
                "text": "BLOG: Relational Modeling with Unknown Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces a language called Blog for specifying probability distributions over relational structures that include varying sets of objects and discusses some attractive features of Blog models and some avenues of future work."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730104"
                        ],
                        "name": "N. Lavrac",
                        "slug": "N.-Lavrac",
                        "structuredName": {
                            "firstName": "Nada",
                            "lastName": "Lavrac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lavrac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693549"
                        ],
                        "name": "S. D\u017eeroski",
                        "slug": "S.-D\u017eeroski",
                        "structuredName": {
                            "firstName": "Sa\u0161o",
                            "lastName": "D\u017eeroski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D\u017eeroski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "We were also interested in automatic induction of clauses using ILP techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Unlike most other ILP systems, which\nSpringer\nlearn only Horn clauses, CLAUDIEN is able to learn arbitrary first-order clauses, making it well suited to MLNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 182
                            }
                        ],
                        "text": "Prolog programs can be learned from databases by searching for Horn clauses that (approximately) hold in the data; this is studied in the field of inductive logic programming (ILP) (Lavrac\u030c & Dz\u030ceroski, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, we show how a variety of statistical relational learning tasks can be cast as MLNs (Section 8), discuss how MLNs relate to previous approaches (Section 9) and list directions for future work (Section 10)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Inductive logic programming (ILP) techniques can be used to learn additional clauses, refine the ones already in the MLN, or learn an MLN from scratch."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36237350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58095bae1d836943bdaa52b76fa8d17cf77d06b3",
            "isKey": true,
            "numCitedBy": 931,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Empirical inductive logic programming: introduction empirical ILP systems - an overview LINUS - using attribute-value learners in an ILP framework experiments in learning relations with LINUS ILP as search for program clauses. Part 2 Learning relations from imperfect data: handling imperfect data in ILP using heuristics to handle noise in ILP mFOIL - extending noise-handling in FOIL experiments in learning relations from noisy examples. Part 3 Applications of inductive logic programming: learning rules for early diagnosis of rheumatic diseases finite element mesh design an overview of selected ILP applications."
            },
            "slug": "Inductive-logic-programming-techniques-and-Lavrac-D\u017eeroski",
            "title": {
                "fragments": [],
                "text": "Inductive logic programming - techniques and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Applications of inductive logic programming: learning rules for early diagnosis of rheumatic diseases finite element mesh design an overview of selected ILP applications."
            },
            "venue": {
                "fragments": [],
                "text": "Ellis Horwood series in artificial intelligence"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787844"
                        ],
                        "name": "D. Kibler",
                        "slug": "D.-Kibler",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Kibler",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kibler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More generally, MLNs can be viewed as an extension to probability estimation of a long line of work on knowledge-intensive learning (e.g., Bergadano & Giordana 1988;  Pazzani & Kibler 1992;  Ourston & Mooney 1994."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 166
                            }
                        ],
                        "text": "More generally, MLNs can be viewed as an extension to probability estimation of a long line of work on knowledge-intensive learning (e.g., Bergadano & Giordana 1988; Pazzani & Kibler 1992; Ourston & Mooney 1994."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 644974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90a10e330d136caf989afbbe96b0548e0d64c65b",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we demonstrate how different forms of background knowledge can be integrated with an inductive method for generating function-free Horn clause rules. Furthermore, we evaluate, both theoretically and empirically, the effect that these forms of knowledge have on the cost and accuracy of learning. Lastly, we demonstrate that a hybrid explanation-based and inductive learning method can advantageously use an approximate domain theory, even when this theory is incorrect and incomplete."
            },
            "slug": "The-Utility-of-Knowledge-in-Inductive-Learning-Pazzani-Kibler",
            "title": {
                "fragments": [],
                "text": "The Utility of Knowledge in Inductive Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper demonstrates how different forms of background knowledge can be integrated with an inductive method for generating function-free Horn clause rules, and demonstrates that a hybrid explanation-based and inductive learning method can advantageously use an approximate domain theory, even when this theory is incorrect and incomplete."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729548"
                        ],
                        "name": "Alexandrin Popescul",
                        "slug": "Alexandrin-Popescul",
                        "structuredName": {
                            "firstName": "Alexandrin",
                            "lastName": "Popescul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandrin Popescul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "Structural logistic regression\nIn structural logistic regression (SLR) (Popescul & Ungar, 2003), the predictors are the output of SQL queries over the input data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 270
                            }
                        ],
                        "text": "\u20261996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et al., 2002; Popescul & Ungar, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Just as a logistic regression model is a discriminativelytrained Markov network, an SLR model is a discriminatively-trained MLN.5\n9.7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "... proposals typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996 ;N go & Haddawy, 1997 ;S ato K Cussens, 1999; Kersting & De Raedt, 2001 ;S antos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula & Russell, 2001; Cumby & Roth, 2003), or database query languages (e.g., Taskar et ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In structural logistic regression (SLR) ( Popescul & Ungar, 2003 ), the predictors are the output of SQL queries over the input data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "advisor) from the properties of those objects and possibly other known relations (e.g.,  Popescul & Ungar, 2003 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 240
                            }
                        ],
                        "text": "Link prediction\nThe goal of link prediction is to determine whether a relation exists between two objects of interest (e.g., whether Anna is Bob\u2019s Ph.D. advisor) from the properties of those objects and possibly other known relations (e.g., Popescul & Ungar, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14546274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc4927900a9f1f06ddd80d2599353a3c5a253fda",
            "isKey": true,
            "numCitedBy": 76,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Structural Logistic Regression, an extension of logistic regression to modeling relational data. It is an integrated approach to building regression models from data stored in relational databases in which potential pre- dictors, both boolean and real-valued, are generated by structured search in the space of queries to the database, and then tested with statistical information crite- ria for inclusion in a logistic regression. Using statistics and relational represen- tation allows modeling in noisy domains with complex structure. Link prediction is a task of high interest with exactly such characteristics. Be it in the domain of scientific citations, social networks or hypertext, the underlying data are ex- tremely noisy and the features useful for prediction are not readily available in a \"flat\" file format. We propose the application of Structural Logistic Regression to building link prediction models, and present experimental results for the task of predicting citations made in scientific literature using relational data taken from the CiteSeer search engine. This data includes the citation graph, authorship and publication venues of papers, as well as their word content."
            },
            "slug": "Structural-Logistic-Regression-for-Link-Analysis-Popescul-Ungar",
            "title": {
                "fragments": [],
                "text": "Structural Logistic Regression for Link Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Structural Logistic Regression is an integrated approach to building regression models from data stored in relational databases in which potential pre- dictors are generated by structured search in the space of queries to the database, and then tested with statistical information for inclusion in a logistic regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820860"
                        ],
                        "name": "R. Neapolitan",
                        "slug": "R.-Neapolitan",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Neapolitan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neapolitan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2316614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc4d9febd19e30f376e4d26deeeb75047bde24d4",
            "isKey": false,
            "numCitedBy": 2642,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. I. BASICS. 1. Introduction to Bayesian Networks. 2. More DAG/Probability Relationships. II. INFERENCE. 3. Inference: Discrete Variables. 4. More Inference Algorithms. 5. Influence Diagrams. III. LEARNING. 6. Parameter Learning: Binary Variables. 7. More Parameter Learning. 8. Bayesian Structure Learning. 9. Approximate Bayesian Structure Learning. 10. Constraint-Based Learning. 11. More Structure Learning. IV. APPICATIONS. 12. Applications. Bibliography. Index."
            },
            "slug": "Learning-Bayesian-networks-Neapolitan",
            "title": {
                "fragments": [],
                "text": "Learning Bayesian networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This chapter discusses Bayesian Networks, a framework for Bayesian Structure Learning, and some of the algorithms used in this framework."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34654269"
                        ],
                        "name": "J. Lloyd",
                        "slug": "J.-Lloyd",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lloyd",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lloyd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7523281,
            "fieldsOfStudy": [],
            "id": "27f366b733ba0f75a93c06d5d7f0d1e06b467a4c",
            "isKey": false,
            "numCitedBy": 3285,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-Logic-Programming-Lloyd",
            "title": {
                "fragments": [],
                "text": "Foundations of Logic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Symbolic Computation"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Link prediction"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Another popular method for inference in Markov networks is belief propagation (Yedidia et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145282631"
                        ],
                        "name": "J. A. Robinson",
                        "slug": "J.-A.-Robinson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robinson",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14389185,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8",
            "isKey": false,
            "numCitedBy": 4367,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ":tb.~tract. Theorem-proving on the computer, using procedures based on the fund~mental theorem of Herbrand concerning the first-order predicate etdeulus, is examined with ~ view towards improving the efticieney and widening the range of practical applicability of these procedures. A elose analysis of the process of substitution (of terms for variables), and the process of t ruth-funct ional analysis of the results of such substitutions, reveals that both processes can be combined into a single new process (called resolution), i terating which is vastty more ef[ieient than the older cyclic procedures consisting of substitution stages alternating with truth-functional analysis stages. The theory of the resolution process is presented in the form of a system of first<~rder logic with .just one inference principle (the resolution principle). The completeness of the system is proved; the simplest proof-procedure based oil the system is then the direct implementation of the proof of completeness. Howew~r, this procedure is quite inefficient, ~nd the paper concludes with a discussion of several principles (called search principles) which are applicable to the design of efficient proof-procedures employing resolution as the basle logical process."
            },
            "slug": "A-Machine-Oriented-Logic-Based-on-the-Resolution-Robinson",
            "title": {
                "fragments": [],
                "text": "A Machine-Oriented Logic Based on the Resolution Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The paper concludes with a discussion of several principles which are applicable to the design of efficient proof-procedures employing resolution as the basle logical process."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763670"
                        ],
                        "name": "L. Dehaspe",
                        "slug": "L.-Dehaspe",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Dehaspe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Dehaspe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "We use the CLAUDIEN system for this purpose (De Raedt & Dehaspe, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11536037,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "a8101031f1a3fced5afdf38b219df45ce55dc4c2",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "The clausal discovery engine claudien is presented. CLAUDIEN is an inductive logic programming engine that fits in the descriptive data mining paradigm. CLAUDIEN addresses characteristic induction from interpretations, a task which is related to existing formalisations of induction in logic. In characteristic induction from interpretations, the regularities are represented by clausal theories, and the data using Herbrand interpretations. Because CLAUDIEN uses clausal logic to represent hypotheses, the regularities induced typically involve multiple relations or predicates. CLAUDIEN also employs a novel declarative bias mechanism to define the set of clauses that may appear in a hypothesis."
            },
            "slug": "Clausal-Discovery-Raedt-Dehaspe",
            "title": {
                "fragments": [],
                "text": "Clausal Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "CLAUDIEN is an inductive logic programming engine that fits in the descriptive data mining paradigm and employs a novel declarative bias mechanism to define the set of clauses that may appear in a hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3383546"
                        ],
                        "name": "Y. Jiang",
                        "slug": "Y.-Jiang",
                        "structuredName": {
                            "firstName": "YueYen",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Jiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 85
                            }
                        ],
                        "text": ", finding a truth assignment that maximizes the sum of weights of satisfied clauses) (Kautz et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 271
                            }
                        ],
                        "text": "When the MLN is in clausal form, we minimize burn-in time by starting each run from a mode found using MaxWalkSat, a local search algorithm for the weighted satisfiability problem (i.e., finding a truth assignment that maximizes the sum of weights of satisfied clauses) (Kautz et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18412311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d733fc29d102dbecf7414e713e505b1b042dc8c",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Many AI problems can be conveniently encoded as discrete constraint satisfaction problems. It is often the case that not all solutions to a CSP are equally desirable | in general, one is interested in a set of \\preferred\" solutions (for example, solutions that minimize some cost function). Preferences can be encoded by incorporating \\soft\" constraints in the problem instance. We show how both hard and soft constraints can be handled by encoding problems as instances of weighted MAX-SAT ((nd-ing a model that maximizes the sum of the weights of the satissed clauses that make up a problem instance). We generalize a local-search algorithm for satissability to handle weighted MAX-SAT. To demonstrate the eeec-tiveness of our approach, we present experimental results on encodings of a set of well-studied network Steiner-tree problems. This approach turns out to be competitive with some of the best current specialized algorithms developed in operations research."
            },
            "slug": "A-general-stochastic-approach-to-solving-problems-Kautz-Selman",
            "title": {
                "fragments": [],
                "text": "A general stochastic approach to solving problems with hard and soft constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work shows how both hard and soft constraints can be handled by encoding problems as instances of weighted MAX-SAT, and presents experimental results on encodings of a set of well-studied network Steiner-tree problems."
            },
            "venue": {
                "fragments": [],
                "text": "Satisfiability Problem: Theory and Applications"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40941894"
                        ],
                        "name": "Soumen Chakrabarti",
                        "slug": "Soumen-Chakrabarti",
                        "structuredName": {
                            "firstName": "Soumen",
                            "lastName": "Chakrabarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumen Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688317"
                        ],
                        "name": "P. Indyk",
                        "slug": "P.-Indyk",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Indyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Indyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Collective classification also takes into account the classes of related objects (e.g.,  Chakrabarti et al., 1998 ; T askar et al.,2002; Neville & Jensen, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "Collective classification also takes into account the classes of related objects (e.g., Chakrabarti et al., 1998; Taskar et al., 2002; Neville & Jensen, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207226010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3261bb81085f59efae1e1c72453c47daaee777ac",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%."
            },
            "slug": "Enhanced-hypertext-categorization-using-hyperlinks-Chakrabarti-Dom",
            "title": {
                "fragments": [],
                "text": "Enhanced hypertext categorization using hyperlinks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work has developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained and its technique also adapts gracefully to the fraction of neighboring documents having known topics."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A more efficient alternative, widely used in areas like spatial statistics, social network modeling and language processing, is to optimize instead the pseudo-likelihood (Besag, 1975)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 171
                            }
                        ],
                        "text": "A more efficient alternative, widely used in areas like spatial statistics, social network modeling and language processing, is to optimize instead the pseudo-likelihood (Besag, 1975)\nP\u2217w(X = x) = n\u220f\nl=1 Pw(Xl = xl |MBx (Xl )) (7)\nwhere MBx(Xl) is the state of the Markov blanket of Xl in the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116757950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1406b6d771c270aff4dcb1c96e4f5c62c02c00a5",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In rather formal terms, the situation with which this paper is concerned may be described as follows. We are given a fixed system of n sites, labelled by the first n positive integers, and an associated vector x of observations, Xi, . . ., Xn, which, in turn, is presumed to be a realization of a vector X of (dependent) random variables, Xi, . . ., X.. In practice, the sites may represent points or regions in space and the random variables may be either continuous or discrete. The main statistical objectives are the following: firstly, to provide a means of using the available concomitant information, particularly the configuration of the sites, to attach a plausible probability distribution to the random vector X; secondly, to estimate any unknown parameters in the distribution from the realization x; thirdly, where possible, to quantify the extent of disagreement between hypothesis and observation."
            },
            "slug": "Statistical-Analysis-of-Non-Lattice-Data-Besag",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis of Non-Lattice Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693549"
                        ],
                        "name": "S. D\u017eeroski",
                        "slug": "S.-D\u017eeroski",
                        "structuredName": {
                            "firstName": "Sa\u0161o",
                            "lastName": "D\u017eeroski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D\u017eeroski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19099552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65b96efe441cc43862b49d8ea96ae13a1687930c",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Data mining algorithms look for patterns in data . Most existing data mining approaches are propositional and look for patterns in a single data table . Most real-world databases, however, store information in multiple tables. Relational data mining (RDM) approaches (D2eroski and Lavrac 2001), look for patterns that involve multiple tables (relations) from a relational database. To emphasize this fact, RDM is often referred to as multi-relational data mining (MRDM) (D2eroski et al . 2002) . We will adopt this term in the present special issue. When we are looking for patterns in multi-relational data, it is natural that the patterns involve multiple relations. They are typically stated in a more expressive language than patterns defined on a single data table . The major types of multi-relational patterns extend the types of propositional patterns considered in single table data mining. We can thus have multi-relational classification rules, multi-relational regression trees, and multi-relational association rules, among others . Just as many data mining algorithms come from the field of machine learning, many MRDM algorithms come form the field of inductive logic programming (ILP, Muggleton 1992 ; Lavrac and Dzeroski 1994) . Situated at the intersection of machine learning and logic programming, ILP has been concerned with finding patterns expressed as logic programs . Initially, ILP focussed on automated program synthesis from examples, formulated as a binary classification task . In recent years, however, the scope of ILP has broadened to cover the whole spectrum ofdata mining tasks (classification, regression, clustering, association analysis) . The most common types of patterns have been extended to their multi-relational versions and so have the major data mining algorithms (decision tree induction, distance-based clustering and prediction, etc.) . There is also a growing interest in the development of data mining algorithms for various types of structured data . These include, for example, graph-based data mining . There is also an increasing body of work on mining tree-structured and XML documents. Mining data which consists of complex/structured objects also falls within the scope ofMRDM, as the normalized representation of such objects in a relational database requires multiple tables . The rise of several KDD application areas that are intrinsically relational has provided and continues to provide a strong motivation for the development of MRDM approaches . Luc De Raedt Institut fur Informatik, Albert-Ludwigs-University, Georges-Koehler-Allee, Building 079, D-79110 Freiburg, Germany"
            },
            "slug": "Multi-relational-data-mining:-the-current-frontiers-D\u017eeroski-Raedt",
            "title": {
                "fragments": [],
                "text": "Multi-relational data mining: the current frontiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "RDM approaches, which look for patterns that involve multiple tables (relations) from a relational database, are often referred to as multi-relational data mining (MRDM) and will be adopted in the present special issue."
            },
            "venue": {
                "fragments": [],
                "text": "SKDD"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 213
                            }
                        ],
                        "text": "Discriminative training of MLNs is straightforward (in fact, easier than the generative training used in this paper), and we have carried out successful preliminary experiments using a voted perceptron algorithm (Collins, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 77139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "700666f0c59a4fedc8b08294424c47cb99a8e2ff",
            "isKey": false,
            "numCitedBy": 3124,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically."
            },
            "slug": "On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani",
            "title": {
                "fragments": [],
                "text": "On the Optimality of the Simple Bayesian Classifier under Zero-One Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Bayesian classifier is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption, and will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687227"
                        ],
                        "name": "G. Flake",
                        "slug": "G.-Flake",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Flake",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Flake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5716473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aabcf3c91db0b8230d081c245a9ccf968a8f6a9",
            "isKey": false,
            "numCitedBy": 941,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We de ne a communit y on the web as a set of sites that have more links (in either direction) to members of the community than to non-members. Members of such a community can be eAEciently iden ti ed in a maximum ow / minim um cut framework, where the source is composed of known members, and the sink consists of well-kno wn non-members. A focused crawler that crawls to a xed depth can approximate community membership by augmenting the graph induced by the cra wl with links to a virtual sink node.The effectiveness of the approximation algorithm is demonstrated with several crawl results that iden tify hubs, authorities, w eb rings, and other link topologies that are useful but not easily categorized. Applications of our approach include focused cra wlers and search engines, automatic population of portal categories, and improved ltering."
            },
            "slug": "Efficient-identification-of-Web-communities-Flake-Lawrence",
            "title": {
                "fragments": [],
                "text": "Efficient identification of Web communities"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A focused crawler that crawls to a depth can approximate community membership by augmenting the graph induced by the cra wl with links to a virtual sink node."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145684209"
                        ],
                        "name": "S. Soliman",
                        "slug": "S.-Soliman",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Soliman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soliman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822220"
                        ],
                        "name": "F. Fages",
                        "slug": "F.-Fages",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Fages",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fages"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876179"
                        ],
                        "name": "Nicolas Beldiceanu",
                        "slug": "Nicolas-Beldiceanu",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Beldiceanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Beldiceanu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145892,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "fb1986e5dff3399e8fcd081e97bbe0f48f97dca9",
            "isKey": false,
            "numCitedBy": 696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this constraint logic programming tells you. It will add more knowledge of you to life and work better. Try it and prove it."
            },
            "slug": "Constraint-Logic-Programming-Soliman-Fages",
            "title": {
                "fragments": [],
                "text": "Constraint Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability and this constraint logic programming tells you, that any book will give certain knowledge to take all benefits."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4940444"
                        ],
                        "name": "M. Genesereth",
                        "slug": "M.-Genesereth",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Genesereth",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Genesereth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29423399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23945b28552a35a03ae65f282cc84da0dc5dd332",
            "isKey": false,
            "numCitedBy": 1633,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Logical-foundations-of-artificial-intelligence-Genesereth-Nilsson",
            "title": {
                "fragments": [],
                "text": "Logical foundations of artificial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35345313"
                        ],
                        "name": "S. Wasserman",
                        "slug": "S.-Wasserman",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Wasserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wasserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721288"
                        ],
                        "name": "Katherine Faust",
                        "slug": "Katherine-Faust",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Faust",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Faust"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "Systems\nIn order to evaluate MLNs, which use logic and probability for inference, we wished to compare with methods that use only logic or only probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 25
                            }
                        ],
                        "text": "Social network analysis (Wasserman & Faust, 1994) is concerned with building models relating actors\u2019 properties and their links."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "In fact, an MLN like the one in Table 1 succinctly represents a type of model that is a staple of social network analysis (Wasserman & Faust, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121177082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9301eab07d6c64ee86651bc15ffab9663a6995b6",
            "isKey": true,
            "numCitedBy": 15836,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I. Introduction: Networks, Relations, and Structure: 1. Relations and networks in the social and behavioral sciences 2. Social network data: collection and application Part II. Mathematical Representations of Social Networks: 3. Notation 4. Graphs and matrixes Part III. Structural and Locational Properties: 5. Centrality, prestige, and related actor and group measures 6. Structural balance, clusterability, and transitivity 7. Cohesive subgroups 8. Affiliations, co-memberships, and overlapping subgroups Part IV. Roles and Positions: 9. Structural equivalence 10. Blockmodels 11. Relational algebras 12. Network positions and roles Part V. Dyadic and Triadic Methods: 13. Dyads 14. Triads Part VI. Statistical Dyadic Interaction Models: 15. Statistical analysis of single relational networks 16. Stochastic blockmodels and goodness-of-fit indices Part VII. Epilogue: 17. Future directions."
            },
            "slug": "Social-Network-Analysis:-Methods-and-Applications-Wasserman-Faust",
            "title": {
                "fragments": [],
                "text": "Social network analysis - methods and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper presents mathematical representation of social networks in the social and behavioral sciences through the lens of dyadic and triadic interaction models, which provide insights into the structure and dynamics of relationships between actors and groups."
            },
            "venue": {
                "fragments": [],
                "text": "Structural analysis in the social sciences"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876131"
                        ],
                        "name": "C. Geyer",
                        "slug": "C.-Geyer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Geyer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Geyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145414560"
                        ],
                        "name": "E. Thompson",
                        "slug": "E.-Thompson",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Thompson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117634170,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ad695cf64d77838e76dd8495f352f253d7f75c86",
            "isKey": false,
            "numCitedBy": 891,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum likelihood estimates (MLEs) in autologistic models and other exponential family models for dependent data can be calculated with Markov chain Monte Carlo methods (the Metropolis algorithm or the Gibbs sampler), which simulate ergodic Markov chains having equilibrium distributions in the model. From one realization of such a Markov chain, a Monte Carlo approximant to the whole likelihood function can be constructed. The parameter value (if any) maximizing this function approximates the MLE"
            },
            "slug": "Constrained-Monte-Carlo-Maximum-Likelihood-for-Data-Geyer-Thompson",
            "title": {
                "fragments": [],
                "text": "Constrained Monte Carlo Maximum Likelihood for Dependent Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49297224"
                        ],
                        "name": "W. Winkler",
                        "slug": "W.-Winkler",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Winkler",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Winkler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "Object identification\nObject identification (also known as record linkage, de-duplication, and others) is the problem of determining which records in a database refer to the same real-world entity (e.g., which entries in a bibliographic database represent the same publication) (Winkler, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 76
                            }
                        ],
                        "text": ", which entries in a bibliographic database represent the same publication) (Winkler, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7844523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54dc61b6d5154bee538605bae023a1f7c5999c4a",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides an overview of methods and systems developed for record linkage. Modern record linkage begins with the pioneering work of Newcombe and is especially based on the formal mathematical model of Fellegi and Sunter. In their seminal work, Fellegi and Sunter introduced many powerful ideas for estimating record linkage parameters and other ideas that still influence record linkage today. Record linkage research is characterized by its synergism of statistics, computer science, and operations research. Many difficult algorithms have been developed and put in software systems. Record linkage practice is still very limited. Some limits are due to existing software. Other limits are due to the difficulty in automatically estimating matching parameters and error rates, with current research highlighted by the work of Larsen and Rubin."
            },
            "slug": "The-State-of-Record-Linkage-and-Current-Research-Winkler",
            "title": {
                "fragments": [],
                "text": "The State of Record Linkage and Current Research Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper provides an overview of methods and systems developed for record linkage based on the formal mathematical model of Fellegi and Sunter, and highlights the work of Larsen and Rubin."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409964566"
                        ],
                        "name": "Dong C. Liu",
                        "slug": "Dong-C.-Liu",
                        "structuredName": {
                            "firstName": "Dong C.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong C. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We optimize the pseudo-log-likelihood using the limited-memory BFGS algorithm ( Liu & Nocedal, 1989 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 79
                            }
                        ],
                        "text": "We optimize the pseudo-log-likelihood using the limited-memory BFGS algorithm (Liu & Nocedal, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5681609,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1267fe36b5ece49a9d8f913eb67716a040bbcced",
            "isKey": false,
            "numCitedBy": 5863,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems."
            },
            "slug": "On-the-limited-memory-BFGS-method-for-large-scale-Liu-Nocedal",
            "title": {
                "fragments": [],
                "text": "On the limited memory BFGS method for large scale optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence, and the convergence properties are studied to prove global convergence on uniformly convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34728746"
                        ],
                        "name": "Ciyou Zhu",
                        "slug": "Ciyou-Zhu",
                        "structuredName": {
                            "firstName": "Ciyou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciyou Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337678"
                        ],
                        "name": "P. Lu",
                        "slug": "P.-Lu",
                        "structuredName": {
                            "firstName": "Peihuang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 68
                            }
                        ],
                        "text": "For optimization, we used the Fortran implementation of L-BFGS from Zhu et al. (1997) and Byrd et al. (1995), leaving all parameters at their default values, and with a convergence criterion (ftol) of 10\u22125."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207228122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1076a9b8181a7f9eb069d38ca10876a3202d2e89",
            "isKey": false,
            "numCitedBy": 2503,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77."
            },
            "slug": "Algorithm-778:-L-BFGS-B:-Fortran-subroutines-for-Zhu-Byrd",
            "title": {
                "fragments": [],
                "text": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables, intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337678"
                        ],
                        "name": "P. Lu",
                        "slug": "P.-Lu",
                        "structuredName": {
                            "firstName": "Peihuang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34728746"
                        ],
                        "name": "Ciyou Zhu",
                        "slug": "Ciyou-Zhu",
                        "structuredName": {
                            "firstName": "Ciyou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciyou Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "and Byrd et al. (1995), leaving all parameters at their default values, and with a convergence criterion (ftol) of 10\u22125."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 90
                            }
                        ],
                        "text": "For optimization, we used the Fortran implementation of L-BFGS from Zhu et al. (1997) and Byrd et al. (1995), leaving all parameters at their default values, and with a convergence criterion (ftol) of 10\u22125."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6398414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a9ab6f97ab05fcd49aaf2864c97538b55e6268",
            "isKey": false,
            "numCitedBy": 3770,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported."
            },
            "slug": "A-Limited-Memory-Algorithm-for-Bound-Constrained-Byrd-Lu",
            "title": {
                "fragments": [],
                "text": "A Limited Memory Algorithm for Bound Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An algorithm for solving large nonlinear optimization problems with simple bounds is described, based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120928761"
                        ],
                        "name": "Edwards",
                        "slug": "Edwards",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Edwards",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edwards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122245306"
                        ],
                        "name": "Sokal",
                        "slug": "Sokal",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sokal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sokal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "With a better choice of initial state, approximate counting, and improved MCMC techniques such as the Swendsen-Wang algorithm ( Edwards & Sokal, 1988 ), MC-MLE may become practical, but it is not a viable option for training in the current version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 127
                            }
                        ],
                        "text": "With a better choice of initial state, approximate counting, and improved MCMC techniques such as the Swendsen-Wang algorithm (Edwards & Sokal, 1988), MC-MLE may become practical, but it is not a viable option for training in the current version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21262870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ea0ff7e6f423496c84223c7aff4bec34f590cd3",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a simple explanation of the Swendsen-Wang algorithm for Potts models in terms of a joint model of Potts spin variables interacting with bond occupation variables. We then show how to generalize this representation, as well as the corresponding Monte Carlo algorithm, to arbitrary models. We give initial results of tests of the new algorithm on the two-dimensional XY model."
            },
            "slug": "Generalization-of-the-representation-and-Monte-Edwards-Sokal",
            "title": {
                "fragments": [],
                "text": "Generalization of the Fortuin-Kasteleyn-Swendsen-Wang representation and Monte Carlo algorithm."
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A simple explanation of the Swendsen-Wang algorithm for Potts models in terms of a joint model of Potts spin variables interacting with bond occupation variables is given and how to generalize this representation to arbitrary models is shown."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. D, Particles and fields"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157892296"
                        ],
                        "name": "D. K. Smith",
                        "slug": "D.-K.-Smith",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Smith",
                            "middleNames": [
                                "K.",
                                "Skip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 265
                            }
                        ],
                        "text": "Maximum-likelihood or MAP estimates of Markov network weights cannot be computed in closed form, but, because the log-likelihood is a concave function of the weights, they can be found efficiently using standard gradient-based or quasi-Newton optimization methods (Nocedal & Wright, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Link prediction"
                    },
                    "intents": []
                }
            ],
            "corpusId": 189864167,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bf86896c23300a46b7fc76298e365984c0b05105",
            "isKey": false,
            "numCitedBy": 10989,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "no exception. MRP II and JIT=TQC in purchasing and supplier education are covered in Chapter 15. Without proper education MRP II and JIT=TQC will not be successful and will not generate their true bene\u00aets. Suppliers are key to the success of MRP II and JIT=TQC. They therefore need to understand these disciplines. Purchasing in the 21st century is going to be marked by continuous changes, by who can gain the competitive edge \u00aerst, who will be the most \u0304exible and who will build the best supplier relationships. This will only be achieved by following the process as described in Schorr in a step by step fashion. An organization must however be willing to, as Schorr states in Chapter 16, `create the spark, ignite change'! Only then can it happen! If you really want to know something about purchasing then this is the book to read. It is most de\u00aenitely relevant and more importantly up to date. It will certainly be a handy reference book for a course on purchasing."
            },
            "slug": "Numerical-Optimization-Smith",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "J. Oper. Res. Soc."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102844155"
                        ],
                        "name": "Walter L. Smith",
                        "slug": "Walter-L.-Smith",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Smith",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter L. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": "We use the CLAUDIEN system for this purpose (De Raedt & Dehaspe, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4213259,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dfeb5d5c79f0dcdbddb789c00d940dd7034594ef",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "IF k1(x) is any non-negative function in L1 (\u2212 \u221e, + \u221e), let us write: for n = 2, 3, \u2026 Then the function: is defined almost everywhere (although it is possibly infinite for some, or all, x)."
            },
            "slug": "Probability-and-Statistics-Smith",
            "title": {
                "fragments": [],
                "text": "Probability and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398329396"
                        ],
                        "name": "E. Lloyd-Richardson",
                        "slug": "E.-Lloyd-Richardson",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Lloyd-Richardson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lloyd-Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3178690"
                        ],
                        "name": "G. Papandonatos",
                        "slug": "G.-Papandonatos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandonatos",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandonatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47434139"
                        ],
                        "name": "A. Kazura",
                        "slug": "A.-Kazura",
                        "structuredName": {
                            "firstName": "Alessandra",
                            "lastName": "Kazura",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kazura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33421944"
                        ],
                        "name": "C. Stanton",
                        "slug": "C.-Stanton",
                        "structuredName": {
                            "firstName": "Cassandra",
                            "lastName": "Stanton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47800900"
                        ],
                        "name": "R. Niaura",
                        "slug": "R.-Niaura",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Niaura",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Niaura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 88
                            }
                        ],
                        "text": "For example, it is well known that teenage friends tend to have similar smoking habits (Lloyd-Richardson et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17068365,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "c1780bd78b95c69d2ced1e0a693987e77c2a511e",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Researchers' understanding of the impact of sociocultural and psychological factors on the various stages of adolescent smoking uptake is limited. Using national data, the authors examined transitions across smoking stages among adolescents (N = 20,747) as a function of interpersonal, familial, and peer domains. Peer smoking was particularly influential on differentiating regular smoking, whereas alcohol use was most influential on earlier smoking. Although significant, depression and delinquency were attenuated in the context of other variables. Higher school grade was more likely to differentiate regular smoking from earlier smoking stages, whereas African American ethnicity and connectedness to school and family were protective of smoking initiation. Results lend support for an interactional approach to adolescent smoking, with implications for stage-matched prevention and intervention applications."
            },
            "slug": "Differentiating-stages-of-smoking-intensity-among-Lloyd-Richardson-Papandonatos",
            "title": {
                "fragments": [],
                "text": "Differentiating stages of smoking intensity among adolescents: stage-specific psychological and social influences."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Transitions across smoking stages among adolescents as a function of interpersonal, familial, and peer domains lend support for an interactional approach to adolescent smoking, with implications for stage-matched prevention and intervention applications."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of consulting and clinical psychology"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "While they focus on directed graphical models, some of the ideas (e.g., different MCMC steps for different types of predicates, combining unification with variable elimination, abstraction hierarchies) may be applicable to MLNs.\nMLNs have some interesting similarities with the KBANN system, which converts a propositional Horn KB into a neural network and uses backpropagation to learn the network\u2019s weights (Towell & Shavlik, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "The most widely used method for approximate inference in Markov networks is Markov chain Monte Carlo (MCMC) (Gilks et al., 1996), and in particular Gibbs sampling, which proceeds by sampling each variable in turn given its Markov blanket."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Link prediction"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 69
                            }
                        ],
                        "text": "We used the maximum across all predicates of the Gelman criterion R (Gilks et al., 1996) to determine when the chains had reached their stationary distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "In principle, P(F1|F2, L, C) can be approximated using an MCMC algorithm that rejects all moves to states where F2 does not hold, and counts the number of samples in which F1 holds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "(Notice that during learning MCMC is performed over the full ground network, which is too large to apply MaxWalkSat to.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Directions for future work fall into three main areas:\nInference: We plan to develop more efficient forms of MCMC for MLNs, study the use of belief propagation, identify and exploit useful special cases, and investigate the possibility of lifted inference."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "With a better choice of initial state, approximate counting, and improved MCMC techniques such as the Swendsen-Wang algorithm (Edwards & Sokal, 1988), MC-MLE may become practical, but it is not a viable option for training in the current version."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125093681,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f1bb45d5d20c107daa9dbc489019cf22a3a6e6b",
            "isKey": true,
            "numCitedBy": 4620,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-chain-Monte-Carlo-in-Practice-Green",
            "title": {
                "fragments": [],
                "text": "Markov chain Monte Carlo in Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729741"
                        ],
                        "name": "K. J. Evans",
                        "slug": "K.-J.-Evans",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Evans",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J. Evans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 189865228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5426ddbb3d42e85f83ab96f6ebc9de3eb7901d24",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Representing-and-Reasoning-with-Probabilistic-Evans",
            "title": {
                "fragments": [],
                "text": "Representing and Reasoning with Probabilistic Knowledge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 126189700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9598b553d981f7ab08d5f7627040fd36180d76c",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Entity-Relationship-Models,-PRMs,-and-Getoor-Taskar",
            "title": {
                "fragments": [],
                "text": "Probabilistic Entity-Relationship Models, PRMs, and Plate Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071978015"
                        ],
                        "name": "\u5927\u897f \u4ec1",
                        "slug": "\u5927\u897f-\u4ec1",
                        "structuredName": {
                            "firstName": "\u5927\u897f",
                            "lastName": "\u4ec1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5927\u897f \u4ec1"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 204167032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c36143936e3048818d881d9c5ebcae68e88ad70",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pearl,-J.-(1988,-second-printing-1991).-Reasoning-\u5927\u897f",
            "title": {
                "fragments": [],
                "text": "Pearl, J. (1988, second printing 1991). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan-Kaufmann."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34654269"
                        ],
                        "name": "J. Lloyd",
                        "slug": "J.-Lloyd",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lloyd",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lloyd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "The Prolog programming language is based on Horn clause logic (Lloyd, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59640235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8feda402d6a4ffd601ca08f47ae0eaf3465ec3ba",
            "isKey": false,
            "numCitedBy": 1517,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-logic-programming;-(2nd-extended-Lloyd",
            "title": {
                "fragments": [],
                "text": "Foundations of logic programming; (2nd extended ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403176300"
                        ],
                        "name": "Tim Berners-Lee",
                        "slug": "Tim-Berners-Lee",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Berners-Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Berners-Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701341"
                        ],
                        "name": "J. Hendler",
                        "slug": "J.-Hendler",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hendler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hendler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35003282"
                        ],
                        "name": "O. Lassila",
                        "slug": "O.-Lassila",
                        "structuredName": {
                            "firstName": "Ora",
                            "lastName": "Lassila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Lassila"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 59
                            }
                        ],
                        "text": "This is potentially useful in areas like the Semantic Web (Berners-Lee et al., 2001) and mass collaboration (Richardson & Domingos, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56818714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57820e6f974d198bf4bbdf26ae7e1063bac190c3",
            "isKey": false,
            "numCitedBy": 8576,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Semantic-Web\"-in-Scientific-American-Berners-Lee-Hendler",
            "title": {
                "fragments": [],
                "text": "The Semantic Web\" in Scientific American"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 263
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Third International Workshop on Multi-Relational Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Third International Workshop on Multi-Relational Data Mining"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 234
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the First International Workshop on Multi-Relational Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First International Workshop on Multi-Relational Data Mining"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The task used in our experiments was an example of link prediction"
            },
            "venue": {
                "fragments": [],
                "text": "The task used in our experiments was an example of link prediction"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "8.3. Link-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "8.3. Link-based clustering"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "8.2. Link prediction"
            },
            "venue": {
                "fragments": [],
                "text": "8.2. Link prediction"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 108
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the IJCAI-2003 Workshop on Learning Statistical Models from Relational Data. Acapulco, Mexico: IJCAII"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 43
                            }
                        ],
                        "text": "Probabilistic CLP generalizes SLPs to CLP (Riezler, 1998), and CLP (BN ) combines CLP with Bayesian networks (Santos Costa et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Constraint logic programming (CLP) is an extension of logic programming where variables are constrained instead of being bound to specific values during inference (Laffar & Lassez, 1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Unlike in MLNs, constraints in CLP (BN ) are hard (i.e., they cannot be violated; rather, they define the form of the probability distribution)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic constraint logic programming. Doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic constraint logic programming. Doctoral dissertation"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Paskin (2002) extended the work of Bacchus et al. by associating a probability with each first-order formula, and taking the maximum entropy distribution compatible with those probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy probabilistic logic (Technical Report UCB/CSD-01-1161)"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Science Division, University of California, Berkeley, CA."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 207
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special issue on multirelational data mining : The current frontiers"
            },
            "venue": {
                "fragments": [],
                "text": "SIGKDD Explorations"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 107
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000; Getoor & Jensen, 2003; Dietterich et al., 2003), also known as multi-relational data mining (D\u017eeroski & De Raedt, 2003; D\u017eeroski et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 137
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ICML2004 Workshop on Statistical Relational Learning and its Connections to Other Fields. Banff, Canada: IMLS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards combining inductive logic programming"
            },
            "venue": {
                "fragments": [],
                "text": "(pp. 573\u2013586)"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 57
                            }
                        ],
                        "text": "Nevertheless, as a tradeoff between incorporating as much potentially relevant information as possible and avoiding extremely long feature vectors, we defined two sets of propositional attributes: order-1 and order-2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Second International Workshop on Multi-Relational Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Workshop on Multi-Relational Data Mining"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The goal of clustering is to group together objects with similar attributes In model-based clustering, we assume a generative model"
            },
            "venue": {
                "fragments": [],
                "text": "X|C), where X is an object"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 137
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ICML-2004 Workshop on Statistical Relational Learning and its Connections to Other Fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the ICML-2004 Workshop on Statistical Relational Learning and its Connections to Other Fields"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 234
                            }
                        ],
                        "text": "Interest in this problem has grown in recent years due to its relevance to statistical relational learning (Getoor & Jensen, 2000, 2003; Dietterich et al., 2003), also known as multi-relational data mining (Dz\u030ceroski & De Raedt, 2003; Dz\u030ceroski et al., 2002, 2003; Dz\u030ceroski & Blockeel, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Second International Workshop on Multi-Relational Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Workshop on Multi-Relational Data Mining"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 127
                            }
                        ],
                        "text": "With a better choice of initial state, approximate counting, and improved MCMC techniques such as the Swendsen-Wang algorithm (Edwards & Sokal, 1988), MC-MLE may become practical, but it is not a viable option for training in the current version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of the Fortuin-KasteleynSwendsen-Wang representation and Monte Carlo algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Physics Review D (pp. 2009\u20132012)"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 127
                            }
                        ],
                        "text": "With a better choice of initial state, approximate counting, and improved MCMC techniques such as the Swendsen-Wang algorithm (Edwards & Sokal, 1988), MC-MLE may become practical, but it is not a viable option for training in the current version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of the FortuinKasteleynSwendsenWang representation and Monte Carlo algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Physics Review D"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 69
                            }
                        ],
                        "text": "Other logic programming approaches\nStochastic logic programs (SLPs) (Muggleton, 1996; Cussens, 1999) are a combination of logic programming and log-linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 148
                            }
                        ],
                        "text": "\u2026typically focus on combining probability with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic logic programs Advances in inductive logic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic logic programs Advances in inductive logic programming"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": "We used two propositional learners: Naive Bayes (Domingos & Pazzani, 1997) and Bayesian networks (Heckerman et al., 1995) with structure and parameters learned using the VFBN2 algorithm (Hulten & Domingos, 2002) with a maximum of four parents per node."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Bayesian networks: The combination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 155
                            }
                        ],
                        "text": "Similar remarks apply to a number of other representations that are essentially equivalent to SLPs, like independent choice logic (Poole, 1993) and PRISM (Sato & Kameya, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with restricted subsets of first-order logic, like Horn clauses (e.g., Wellman et al., 1992; Poole, 1993; Muggleton, 1996; Ngo & Haddawy, 1997; Sato & Kameya, 1997; Cussens, 1999; Kersting & De Raedt, 2001; Santos Costa et al., 2003), frame-based systems (e.g., Friedman et al., 1999; Pasula &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PRISM: A symbolic-statistical modeling language"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 47,
            "methodology": 30
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 97,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Markov-logic-networks-Richardson-Domingos/950a3c89dacbc3e7ddcd43d7ff6f985697e41cdb?sort=total-citations"
}