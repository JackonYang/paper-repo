{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114316817"
                        ],
                        "name": "M. La Cascia",
                        "slug": "M.-La-Cascia",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "La Cascia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. La Cascia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120772285"
                        ],
                        "name": "S. Sethi",
                        "slug": "S.-Sethi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6349236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78462bcbcd3e8592dc45f41db56e697a2e26209d",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A system is proposed that combines textual and visual statistics in a single index vector for content-based search of a WWW image database. Textual statistics are captured in vector form using latent semantic indexing (LSI) based on text in the containing HTML document. Visual statistics are captured in vector form using color and orientation histograms. By using an integrated approach, it becomes possible to take advantage of possible statistical couplings between the content of the document (latent semantic content) and the contents of images (visual statistics). The combined approach allows improved performance in conducting content-based search. Search performance experiments are reported for a database containing 100,000 images collected from the WWW."
            },
            "slug": "Combining-textual-and-visual-cues-for-content-based-Cascia-Sethi",
            "title": {
                "fragments": [],
                "text": "Combining textual and visual cues for content-based image retrieval on the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A system is proposed that combines textual and visual statistics in a single index vector for content- based search of a WWW image database and allows improved performance in conducting content-based search."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. IEEE Workshop on Content-Based Access of Image and Video Libraries (Cat. No.98EX173)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011596"
                        ],
                        "name": "D. Burhans",
                        "slug": "D.-Burhans",
                        "structuredName": {
                            "firstName": "Debra",
                            "lastName": "Burhans",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burhans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1433969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fabd30916cba34f9711ab9ba18d8868e3df346ae",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This research explores the interaction of textual and photographic information in document understanding. The problem of performing general-purpose vision without a priori knowledge is difficult at best. The use of collateral information in scene understanding has been explored in computer vision systems that use scene context in the task of object identification. The work described here extends this notion by defining visual semantics, a theory of systematically extracting picture-specific information from text accompanying a photograph. Specifically, this paper discusses the multi-stage processing of textual captions with the following objectives: (i) predicting which objects (implicitly or explicitly mentioned in the caption) are present in the picture and (ii) generating constraints useful in locating/identifying these objects. The implementation and use of a lexicon specifically designed for the integration of linguistic and visual information is discussed. Finally, the research described here has been successfully incorporated into PICTION, a caption-based face identification system."
            },
            "slug": "Visual-Semantics:-Extracting-Visual-information-Srihari-Burhans",
            "title": {
                "fragments": [],
                "text": "Visual Semantics: Extracting Visual information from Text Accompanying Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper discusses the multi-stage processing of textual captions with the following objectives: predicting which objects are present in the picture and generating constraints useful in locating/identifying these objects."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60622931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "697c7f995188ef30b8687e4458c2c7fd33163184",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many situations where linguistic and pictorial data are jointly presented to communicate information. In the general case, each of these two sources conveys orthogonal information. A computer model for integrating information from the two sources requires an initial interpretation of both the text and the picture followed by consolidation of information. The problem of performing general-purpose vision without apriori knowledge (needed in such a situation) is nearly impossible. However, in some situations, the text describes salient aspects of the picture. In such situations, it is possible to extract visual information from the text, resulting in a conceptualised graph describing the structure of the accompanying picture. This graph can then be used by a computer vision system in the top-down interpretation of the picture. \nIn this dissertation, a computational model for understanding pictures based on information in accompanying captions is presented. The use of SNePS (Semantic Network Processing System) as the common intermediate representation for both linguistic and pictorial information is discussed. Specifically, we present the knowledge representations and interpretations that comprise the model. \nThe focus of this dissertation is on the generation of a conceptualised graph, a SNePS network which reflects a cognitive agent's conceptualisation of a picture based on information contained in a descriptive caption. This representation includes information about objects appearing in the picture and spatial constraints between them, information used in the subsequent task of labelling objects in the picture. A substantial portion of the dissertation is devoted to techniques of extracting such visual information from text. The techniques are based on both syntactic and semantic considerations. We classify linguistic methods of identifying objects in pictures into several broad categories and, for each category, discuss the manner in which visual information can be extracted. The problem of dynamically generating model descriptions for objects (and for entire pictures) is illustrated. A theoretical solution to this problem is presented and illustrated through an example. \nAs a test of the model, we present an implementation, PICTION, whereby information obtained from parsing a caption of a newspaper photograph is used to identify human faces in the photograph. A key component of the system is the utilisation of spatial constraints in order to reduce the number of possible labels that could be associated with a face. These constraints are generated by a natural-language processing system that examines the caption in detail. We report on the extensive testing of the system and discuss the results obtained. The method of evaluating the performance of PICTION can be used by any face-identification system."
            },
            "slug": "Extracting-visual-information-from-text:-using-to-Srihari",
            "title": {
                "fragments": [],
                "text": "Extracting visual information from text: using captions to label faces in newspaper photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A computational model for understanding pictures based on information in accompanying captions, PICTION, whereby information obtained from parsing a caption of a newspaper photograph is used to identify human faces in the photograph."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969789"
                        ],
                        "name": "C. Carson",
                        "slug": "C.-Carson",
                        "structuredName": {
                            "firstName": "Chad",
                            "lastName": "Carson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Carson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143942875"
                        ],
                        "name": "H. Greenspan",
                        "slug": "H.-Greenspan",
                        "structuredName": {
                            "firstName": "Hayit",
                            "lastName": "Greenspan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Greenspan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work includes search by text [1, 2], search by image feature similarity [3-6], search by segment features [7], search for specific types of images using more compressive methods [8, 9], and search by image sketch [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this work we model documents as a sequence of words and a sequence of segments, with the segments being taken from the Blobworld representation [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Search using a simple conjunction of keywords and image features is provided in Blobworld [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14715074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fedf7729b620ec2cf4e79705d2898f82e9a2ba66",
            "isKey": false,
            "numCitedBy": 1629,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions that roughly correspond to objects, we allow querying at the level of objects rather than global image properties. We present results indicating that querying for images using Blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects."
            },
            "slug": "Blobworld:-Image-Segmentation-Using-and-Its-to-Carson-Belongie",
            "title": {
                "fragments": [],
                "text": "Blobworld: Image Segmentation Using Expectation-Maximization and Its Application to Image Querying"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results indicating that querying for images using Blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683556"
                        ],
                        "name": "G. Wiederhold",
                        "slug": "G.-Wiederhold",
                        "structuredName": {
                            "firstName": "Gio",
                            "lastName": "Wiederhold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wiederhold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836317"
                        ],
                        "name": "O. Firschein",
                        "slug": "O.-Firschein",
                        "structuredName": {
                            "firstName": "Oscar",
                            "lastName": "Firschein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Firschein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8776998"
                        ],
                        "name": "S. Wei",
                        "slug": "S.-Wei",
                        "structuredName": {
                            "firstName": "Sha",
                            "lastName": "Wei",
                            "middleNames": [
                                "Xin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work includes search by text [1, 2], search by image feature similarity [3-6], search by segment features [7], search for specific types of images using more compressive methods [8, 9], and search by image sketch [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2269151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "647801bdf050f6046f98d9b97b56fc1df4d06af6",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies' wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then refines the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial-sketch queries. This technique performs much better in capturing coherence of image, object granularity, local color/texture, and bias avoidance than traditional color layout algorithms. WBIIS is much faster and more accurate than traditional algorithms. When tested on a database of more than 10 000 general-purpose images, the best 100 matches were found in 3.3 seconds."
            },
            "slug": "Content-based-image-indexing-and-searching-using-Wang-Wiederhold",
            "title": {
                "fragments": [],
                "text": "Content-based image indexing and searching using Daubechies' wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases, which performs much better in capturing coherence of image, object granularity, local color/texture, and bias avoidance than traditional color layout algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Digital Libraries"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883679"
                        ],
                        "name": "J. Ashley",
                        "slug": "J.-Ashley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ashley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39311329"
                        ],
                        "name": "J. Hafner",
                        "slug": "J.-Hafner",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hafner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499047"
                        ],
                        "name": "Denis Lee",
                        "slug": "Denis-Lee",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867341"
                        ],
                        "name": "D. Petkovic",
                        "slug": "D.-Petkovic",
                        "structuredName": {
                            "firstName": "Dragutin",
                            "lastName": "Petkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028064"
                        ],
                        "name": "David Steele",
                        "slug": "David-Steele",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70341848"
                        ],
                        "name": "P. Yanker",
                        "slug": "P.-Yanker",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 110716,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "dc139f901c869f80b54b41f89d5b7f35c7dfa3c7",
            "isKey": false,
            "numCitedBy": 4258,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >"
            },
            "slug": "Query-by-Image-and-Video-Content:-The-QBIC-System-Flickner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content: The QBIC System"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The QBIC system is described and its query capabilities are demonstrated, which allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099025541"
                        ],
                        "name": "R. Chopra",
                        "slug": "R.-Chopra",
                        "structuredName": {
                            "firstName": "Rajiv",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, in the area of using associated text for image understanding the work of Srihari and others [18-22] bears mentioning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 906540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cba4740274e6bf4d5f4197b0c8da3072a26f5c0",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an efficient control mechanism for incorporating picture-specific context in the task of image interpretation. Although other knowledge-based vision systems use general domain context in reducing the computational burden of image interpretation, to our knowledge, this is the first effort in exploring picture-specific collateral information. We assume that constraints on the picture are generated from a natural language understanding module which processes descriptive text accompanying the pictures. We have developed a unified framework for exploiting these constraints both in the object location and identification (labeling) stage. In particular, we describe a technique for incorporating constrained search in context-based vision. Finally, we demonstrate the effectiveness of this approach in PICTION, a system that uses captions to label human faces in newspaper photographs."
            },
            "slug": "Control-Structures-for-Incorporating-Context-in-Chopra-Srihari",
            "title": {
                "fragments": [],
                "text": "Control Structures for Incorporating Picture-Specific Context in Image Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper describes an efficient control mechanism for incorporating picture-specific context in the task of image interpretation and describes a technique for incorporating constrained search in context-based vision."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3174044"
                        ],
                        "name": "L. H. Armitage",
                        "slug": "L.-H.-Armitage",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Armitage",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. H. Armitage"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2137152"
                        ],
                        "name": "P. Enser",
                        "slug": "P.-Enser",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Enser",
                            "middleNames": [
                                "G.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Enser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45350741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "316440b3a78715902ebdf9a433867143c70f1b85",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a project in which an analysis was undertaken of user queries addressed to seven libraries which manage archives of widely varying still and moving image material. The sampling procedure is described, in which queries obtained from each library were broadly categorised by image content, identification and accessibility. Attention is focused on the image content requests, for which a categorisation based on facet analysis is developed. The analytical tool which is used for this purpose is based on a schema already well established for the analysis of levels of meaning in images. The project demonstrates the possibility of formulating a general categorisation of requests which seek widely different still and moving image material. The paper concludes with observations on the potential value of embedding such a schema within the user interface of unmediated-query visual information retrieval systems."
            },
            "slug": "Analysis-of-user-need-in-image-archives-Armitage-Enser",
            "title": {
                "fragments": [],
                "text": "Analysis of user need in image archives"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The project demonstrates the possibility of formulating a general categorisation of requests which seek widely different still and moving image material, and the potential value of embedding such a schema within the user interface of unmediated-query visual information retrieval systems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Inf. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2137152"
                        ],
                        "name": "P. Enser",
                        "slug": "P.-Enser",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Enser",
                            "middleNames": [
                                "G.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Enser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206393711,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ef94d01afd7e972fa0ad8401c91e5ccd792e89f2",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 128,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys theoretical and practical issues associated with a particular type of information retrieval problem, namely that where the information need is pictorial. The paper is contextualised by the notion of a visually stimulated society, in which the ease of record creation and transmission in the visual medium is contrasted with the difficulty of gaining effective subject access to the world's stores of such records. The technological developments which, in casting the visual image in electronic form, have contributed so significantly to its availability are reviewed briefly, as a prelude to the main thrust of the paper. Concentrating on still and moving pictorial forms of the visual image, the paper dwells on issues related to the subject indexing of pictorial material and discusses four models of pictorial information retrieval corresponding with permutations of the verbal and visual modes for the representation of picture content and of information need."
            },
            "slug": "Progress-in-Documentation-Pictorial-Information-Enser",
            "title": {
                "fragments": [],
                "text": "Progress in Documentation Pictorial Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper surveys theoretical and practical issues associated with a particular type of information retrieval problem, namely that where the information need is pictorial, and discusses four models of pictorial information retrieval corresponding with permutations of the verbal and visual modes for the representation of picture content and of information need."
            },
            "venue": {
                "fragments": [],
                "text": "J. Documentation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Others also argue for statistical models of data for image retrieval [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10837883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15281680463698dca403697bd627af4efebc98a2",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or \"translate\" a given document into a query. To assess the relevance of a document to a user's query, we estimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justification of the \"language modeling\" strategy recently proposed by Ponte and Croft. In a series of experiments on TREC data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques. This prototype system only begins to tap the full potential of translation-based retrieval."
            },
            "slug": "Information-retrieval-as-statistical-translation-Berger-Lafferty",
            "title": {
                "fragments": [],
                "text": "Information retrieval as statistical translation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple, well motivated model of the document-to-query translation process is proposed, and an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents is described."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work includes search by text [1, 2], search by image feature similarity [3-6], search by segment features [7], search for specific types of images using more compressive methods [8, 9], and search by image sketch [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206411936,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "eb581367272797bd700fbb28699299b49d7a15c6",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Major search engines such as Hotbot (http://www.hotbot.com) help us find text on the Web, but typically have few or no capabilities for finding visual media. Yet many Web users, such as magazine editors or professional Web site designers, need to find images using just a few global features. My colleagues and I developed a prototype system called ImageScape (http://skynet.liacs.nl) to find visual media over intranets and the Web. The system integrates technologies such as vector quantization-based compression of the image database and k-d trees for fast searching over high-dimensional spaces."
            },
            "slug": "Next-Generation-Web-Searches-for-Visual-Content-Lew",
            "title": {
                "fragments": [],
                "text": "Next-Generation Web Searches for Visual Content"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A prototype system called ImageScape is developed to find visual media over intranets and the Web using vector quantization-based compression of the image database and k-d trees for fast searching over high-dimensional spaces."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116439170"
                        ],
                        "name": "J. Z. Wang",
                        "slug": "J.-Z.-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Zijun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Z. Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12233545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2b9b626b05419cef188a6df13f2c932ba130ebf",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this demonstration, we present SIMPLIcity, an image retrieval system for picture libraries and biomedical image databases. The system uses a wavelet-based approach for feature extraction, real-time region segmentation, the Integration Region Matching (IRM) metric, and image classification methods. Tested on large-scale picture libraries and a database of pathology images, the system has demonstrated accurate and fast retrieval. It also exceptionally robust to image alterations."
            },
            "slug": "SIMPLIcity:-a-region-based-retrieval-system-for-and-Wang",
            "title": {
                "fragments": [],
                "text": "SIMPLIcity: a region-based retrieval system for picture libraries and biomedical image databases"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "SIMPLIcity, an image retrieval system for picture libraries and biomedical image databases, uses a wavelet-based approach for feature extraction, real-time region segmentation, the Integration Region Matching (IRM) metric, and image classification methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121941586"
                        ],
                        "name": "C. Frankel",
                        "slug": "C.-Frankel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Frankel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Frankel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987811"
                        ],
                        "name": "M. Swain",
                        "slug": "M.-Swain",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Swain",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Swain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720747"
                        ],
                        "name": "V. Athitsos",
                        "slug": "V.-Athitsos",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Athitsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Athitsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7811959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "105b158b73511030ff10ac0f0f1cbee65236e4a6",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. PC-Meters March, 1996, survey found that three of the five most visited Web sites were search engines. However, while Web pages typically contain both text and images, all the currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs."
            },
            "slug": "WebSeer:-An-Image-Search-Engine-for-the-World-Wide-Frankel-Swain",
            "title": {
                "fragments": [],
                "text": "WebSeer: An Image Search Engine for the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs on the Web."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683556"
                        ],
                        "name": "G. Wiederhold",
                        "slug": "G.-Wiederhold",
                        "structuredName": {
                            "firstName": "Gio",
                            "lastName": "Wiederhold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wiederhold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116439170"
                        ],
                        "name": "J. Z. Wang",
                        "slug": "J.-Z.-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Zijun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Z. Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13936056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27558a61df91a16bf5b90a67e0f08c02dcdee2c",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 150,
            "paperAbstract": {
                "fragments": [],
                "text": "The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. In the biomedical domain, content-based image retrieval can be used in patient digital libraries, clinical diagnosis, searching of 2-D electrophoresis gels, and pathology slides. In this thesis, we present a wavelet-based approach for feature extraction, combined with integrated region matching. An image in the database, or a portion of an image, is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. A measure for the overall similarity between images is developed as a region-matching scheme that integrates properties of all the regions in the images. The advantage of using such a \u201csoft matching\u201d is that it makes the metric robust to poor segmentation, an important property that previous work has not solved. An experimental image retrieval system, SIMPLIcity (Semantics-sensitive Integrated Matching for Picture LIbraries), has been built to validate these methods on various image databases, including a database of about 200,000 general-purpose images and a database of more than 70,000 pathology image fragments. We have shown that our methods perform much better and much faster than existing methods. The system is exceptionally robust to image alterations such as intensity variation, sharpness variation, intentional distortions, cropping, shifting, and rotation. These features are important to biomedical image databases because visual features in the query image are not exactly the same as the visual features in the images in the database. The work has also been applied to the classification of on-line images and web sites."
            },
            "slug": "Semantics-sensitive-integrated-matching-for-picture-Wiederhold-Wang",
            "title": {
                "fragments": [],
                "text": "Semantics-sensitive integrated matching for picture libraries and biomedical image databases"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A wavelet-based approach for feature extraction, combined with integrated region matching for Picture LIbraries, which is exceptionally robust to image alterations such as intensity variation, sharpness variation, intentional distortions, cropping, shifting, and rotation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829696"
                        ],
                        "name": "Y. Rubner",
                        "slug": "Y.-Rubner",
                        "structuredName": {
                            "firstName": "Yossi",
                            "lastName": "Rubner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rubner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "work emphasizing this philosophy include the application of multidimensional scaling using the Earth Mover\u2019s distance to image displays [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18648233,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b7934a6f3a23940b7562df4cf58366b1adce55a3",
            "isKey": false,
            "numCitedBy": 1779,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving \"distribution mass\" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search."
            },
            "slug": "A-metric-for-distributions-with-applications-to-Rubner-Tomasi",
            "title": {
                "fragments": [],
                "text": "A metric for distributions with applications to image databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper uses the Earth Mover's Distance to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays, and proposes a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144689737"
                        ],
                        "name": "M. Das",
                        "slug": "M.-Das",
                        "structuredName": {
                            "firstName": "Madirakshi",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work includes search by text [1, 2], search by image feature similarity [3-6], search by segment features [7], search for specific types of images using more compressive methods [8, 9], and search by image sketch [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2076108,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "8148831d9e94e42b86b10013a49a08b664e95712",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The article explains how to query a database of flower patent images using both an example flower image and color names. This database consists of images that have been digitized from photographs submitted as a part of applications for flower patents to the US Patents and Trademark Office. This database must be queried by both example images and color name, so that both those checking new patent applications and those buying patents for cultivation can use it."
            },
            "slug": "Indexing-Flower-Patent-Images-Using-Domain-Das-Manmatha",
            "title": {
                "fragments": [],
                "text": "Indexing Flower Patent Images Using Domain Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This article explains how to query a database of flower patent images using both an example flower image and color names, so that both those checking new patent applications and those buying patents for cultivation can use it."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692121"
                        ],
                        "name": "Matthew L. Miller",
                        "slug": "Matthew-L.-Miller",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Miller",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew L. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2838104"
                        ],
                        "name": "T. Papathomas",
                        "slug": "T.-Papathomas",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Papathomas",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Papathomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203897"
                        ],
                        "name": "P. Yianilos",
                        "slug": "P.-Yianilos",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yianilos",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yianilos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "This work includes search by text [1, 2], search by image feature similarity [3-6], search by segment features [7], search for specific types of images using more compressive methods [8, 9], and search by image sketch [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 550483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a60d352b1477fb9cd650510e3185104d82596221",
            "isKey": false,
            "numCitedBy": 809,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the theory, design principles, implementation and performance results of PicHunter, a prototype content-based image retrieval (CBIR) system. In addition, this document presents the rationale, design and results of psychophysical experiments that were conducted to address some key issues that arose during PicHunter's development. The PicHunter project makes four primary contributions to research on CBIR. First, PicHunter represents a simple instance of a general Bayesian framework which we describe for using relevance feedback to direct a search. With an explicit model of what users would do, given the target image they want, PicHunter uses Bayes's rule to predict the target they want, given their actions. This is done via a probability distribution over possible image targets, rather than by refining a query. Second, an entropy-minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search. Third, PicHunter makes use of hidden annotation rather than a possibly inaccurate/inconsistent annotation structure that the user must learn and make queries in. Finally, PicHunter introduces two experimental paradigms to quantitatively evaluate the performance of the system, and psychophysical experiments are presented that support the theoretical claims."
            },
            "slug": "The-Bayesian-image-retrieval-system,-PicHunter:-and-Cox-Miller",
            "title": {
                "fragments": [],
                "text": "The Bayesian image retrieval system, PicHunter: theory, implementation, and psychophysical experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The theory, design principles, implementation and performance results of PicHunter are presented, two experimental paradigms to quantitatively evaluate the performance of the system are introduced, and psychophysical experiments are presented that support the theoretical claims."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735767"
                        ],
                        "name": "Margaret M. Fleck",
                        "slug": "Margaret-M.-Fleck",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Fleck",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret M. Fleck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1979750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ccda575c3c3e96bb99522c5a8ab158474a9f2f3",
            "isKey": false,
            "numCitedBy": 552,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources."
            },
            "slug": "Finding-Naked-People-Fleck-Forsyth",
            "title": {
                "fragments": [],
                "text": "Finding Naked People"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A content-based retrieval strategy that can tell whether there are naked people present in an image and an effective mask for skin regions is demonstrated, which introduces a new view of object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118627231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1d0496f5d4a5bc8563cc377728242f494d797a3",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The human face is an object that is easily located in complex scenes by infants and adults alike. Yet the development of an automated system to perform this task is extremely challenging. An attempt to solve this problem raises two important issues in object location. First, natural objects such as human faces tend to have boundaries which as yet have not been accurately described by analytical functions. This renders the commonly used parameter-based techniques like the Hough transform inadequate for extracting the shape. Second, the object of interest could occur in a scene in various sizes, thus requiring scale independent techniques which can detect instances of the object at all scales. \nAlthough, the task of identifying a well-framed face (as one of a set of labeled faces) has been well researched, the task of locating a face in a natural scene is relatively unexplored. We present a computational theory for locating human faces in scenes with certain constraints. Our experiments will be confined to instances where people's faces are the primary subject of the scene, occlusion is minimal, and the faces contrast well against the background. \nA hypothesis generate-and-test paradigm is proposed and justified as a methodology for face location. Alternative methods of hypothesis testing by either performing rigorous face-specific analysis or using collateral information have been addressed. The shape of the object is defined in terms of features selected using cognitive principles of human perception. Geometrical relationships between features are not rigid. Rather, they are represented by spring functionals that allow several configurations of the features to match against the model. The framework of spring functionals provides a mathematical basis for evaluating the \"goodness\" of matches between the data and the model."
            },
            "slug": "A-computational-theory-for-locating-human-faces-in-Govindaraju",
            "title": {
                "fragments": [],
                "text": "A computational theory for locating human faces in photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A hypothesis generate-and-test paradigm is proposed and justified as a methodology for face location and alternative methods of hypothesis testing by either performing rigorous face-specific analysis or using collateral information have been addressed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Such structure is part of semantics, and therefore we proposes that a hierarchical system is better poised to capture semantics than a flat one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "We present a method which organizes image databases using both image features and associated text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8069201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7682b0fe481c345ea973ca07d2d979e003fd20a2",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "ion levels of words document partitioning abstraction levels (a) (b)"
            },
            "slug": "Learning-and-representing-topic-a-hierarchical-for-Hofmann",
            "title": {
                "fragments": [],
                "text": "Learning and representing topic-a hierarchical mixture model for word occurences in document databas"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "In this chapter three levels of words document partitioning abstraction levels are calculated using a model based on the model developed in [Bouchut-Boyaval, M3AS, 2013]."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To train the model we use the ExpectationMaximization algorithm [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48403,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60793310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "561f6561fd47b531f3694f708cd5d41bd5f0a443",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Query-by-Image-and-Video-Content-Flickner",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692121"
                        ],
                        "name": "Matthew L. Miller",
                        "slug": "Matthew-L.-Miller",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Miller",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew L. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2838104"
                        ],
                        "name": "T. Papathomas",
                        "slug": "T.-Papathomas",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Papathomas",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Papathomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203897"
                        ],
                        "name": "P. Yianilos",
                        "slug": "P.-Yianilos",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yianilos",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yianilos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2025979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "851cc451d24997d7807248dbc1849942e4a2f4de",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Correction-to-\"the-Bayesian-image-retrieval-system,-Cox-Miller",
            "title": {
                "fragments": [],
                "text": "Correction to \"the Bayesian image retrieval system, pichunter: theory, implementation, and psychophysical experiments\""
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083462951"
                        ],
                        "name": "Fran ine Chena",
                        "slug": "Fran-ine-Chena",
                        "structuredName": {
                            "firstName": "Fran",
                            "lastName": "Chena",
                            "middleNames": [
                                "ine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran ine Chena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103312311"
                        ],
                        "name": "Ullas Gargib",
                        "slug": "Ullas-Gargib",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargib",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ullas Gargib"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096374698"
                        ],
                        "name": "Les Nilesa",
                        "slug": "Les-Nilesa",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Nilesa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Les Nilesa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17630005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c196925c50862a363f5c87446832c8c7ca7ad181",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-Modal-Browsing-of-Images-in-Web-Do-uments-Chena-Gargib",
            "title": {
                "fragments": [],
                "text": "Multi-Modal Browsing of Images in Web Do uments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099025541"
                        ],
                        "name": "R. Chopra",
                        "slug": "R.-Chopra",
                        "structuredName": {
                            "firstName": "Rajiv",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011596"
                        ],
                        "name": "D. Burhans",
                        "slug": "D.-Burhans",
                        "structuredName": {
                            "firstName": "Debra",
                            "lastName": "Burhans",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burhans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92876288"
                        ],
                        "name": "M. Venkataraman",
                        "slug": "M.-Venkataraman",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Venkataraman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Venkataraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, in the area of using associated text for image understanding the work of Srihari and others [18-22] bears mentioning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59669097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1720b6e3e41edd77a86ef45ad5fc49bc73990b80",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Use-of-Collateral-Text-in-Image-Interpretation-Srihari-Chopra",
            "title": {
                "fragments": [],
                "text": "Use of Collateral Text in Image Interpretation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "[16] T. Hofmann, \u201cLearning and representing topic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 91
                            }
                        ],
                        "text": "Our model is a generative hierarchical model, inspired by one proposed for text by Hofmann [16, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "[24] T. Hofmann and J. Puzicha, \u201cStatistical models fro cooccurance data,\u201d MIssachusetts institute of technology, A.I. Memo 1635,, 1998."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical models fro cooccurance data"
            },
            "venue": {
                "fragments": [],
                "text": "MIssachusetts institute of technology, A.I. Memo 1635,, 1998."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Leaning and representing topic. A hierarchical mixture model for word occurrence in document databases,"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Workshop on learning,from text and the web,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Query analysis in a visual information retrieval context"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Document and Text Management"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting Visual Information from Text: Using Captions to Label Human Faces in Newspaper Photographs, SUNY at Buffalo"
            },
            "venue": {
                "fragments": [],
                "text": "Extracting Visual Information from Text: Using Captions to Label Human Faces in Newspaper Photographs, SUNY at Buffalo"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictorial Information Retrieval (Progress in Documentation)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 96
                            }
                        ],
                        "text": "We present a method which organizes image databases using both image features and associated text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical models fro cooccurrence data Massachusetts institute of technology, A.I. Memo 1635"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical models fro cooccurrence data Massachusetts institute of technology, A.I. Memo 1635"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Progress in documentation pictoral information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Documentation, vol. 51, pp. 126-170, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 96
                            }
                        ],
                        "text": "We present a method which organizes image databases using both image features and associated text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical models fro cooccurance data MIssachusetts institute of technology, A.I. Memo 1635"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical models fro cooccurance data MIssachusetts institute of technology, A.I. Memo 1635"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "[16] T. Hofmann, \u201cLearning and representing topic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 91
                            }
                        ],
                        "text": "Our model is a generative hierarchical model, inspired by one proposed for text by Hofmann [16, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "[24] T. Hofmann and J. Puzicha, \u201cStatistical models fro cooccurance data,\u201d MIssachusetts institute of technology, A.I. Memo 1635,, 1998."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical models fro cooccurrence data"
            },
            "venue": {
                "fragments": [],
                "text": "Massachusetts institute of technology, A.I. Memo 1635,, 1998."
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 22
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-the-semantics-of-words-and-pictures-Barnard-Forsyth/4e36d141e2964817c3d926c380793e404a3a3367?sort=total-citations"
}