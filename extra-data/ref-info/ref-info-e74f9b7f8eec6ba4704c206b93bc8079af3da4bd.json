{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "In (Everingham et al., 2010) this algorithm uses thr(Bk) = 0.5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In (Everingham et al., 2010) this algorithm uses thr(Bk) = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 62
                            }
                        ],
                        "text": "The criteria for object detection was adopted from PASCAL VOC (Everingham et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 78
                            }
                        ],
                        "text": "Prior to ILSVRC, the object detection benchmark was the PASCAL VOC challenge (Everingham et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 246
                            }
                        ],
                        "text": "In Sections 3.2.2 and 3.3.4 we tried to put those concerns to rest by analyzing the statistics of the ILSVRC dataset and concluding that it is comparable with, and in many cases much more challenging than, the long-standing PASCAL VOC benchmark (Everingham et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 84
                            }
                        ],
                        "text": "4.3 Object detection\nThe criteria for object detection was adopted from PASCAL VOC (Everingham et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 49
                            }
                        ],
                        "text": "The closest to ILSVRC is the PASCAL VOC dataset (Everingham et al., 2010, 2014), which provides a standardized test bed for object detection, image classification, object segmentation, person layout, and action classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 1
                            }
                        ],
                        "text": "(Everingham et al., 2010) The error of an algorithm is computed as in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": true,
            "numCitedBy": 11683,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62181,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677488"
                        ],
                        "name": "\u00c0. Lapedriza",
                        "slug": "\u00c0.-Lapedriza",
                        "structuredName": {
                            "firstName": "\u00c0gata",
                            "lastName": "Lapedriza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c0. Lapedriza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ied, there are many errors, making it less suitable for algorithm evaluation. Datasets such as 15 Scenes (Oliva and Torralba, 2001; Fei-Fei and Perona, 2005; Lazebnik et al., 2006) or recent Places (Zhou et al., 2014) provide a single scene category label (as opposed to an object category). The ImageNet dataset (Deng et al., 2009) is the backbone of ILSVRC. ImageNet is an image dataset organized according to the W"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1849990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9667f8264745b626c6173b1310e2ff0298b09cfc",
            "isKey": false,
            "numCitedBy": 2610,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks."
            },
            "slug": "Learning-Deep-Features-for-Scene-Recognition-using-Zhou-Lapedriza",
            "title": {
                "fragments": [],
                "text": "Learning Deep Features for Scene Recognition using Places Database"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new scene-centric database called Places with over 7 million labeled pictures of scenes is introduced with new methods to compare the density and diversity of image datasets and it is shown that Places is as dense as other scene datasets and has more diversity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 13
                            }
                        ],
                        "text": "Caltech 256 (Griffin et al., 2007) increased the number of object classes to 256 and added images with greater scale and background variability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865091"
                        ],
                        "name": "Krista A. Ehinger",
                        "slug": "Krista-A.-Ehinger",
                        "structuredName": {
                            "firstName": "Krista",
                            "lastName": "Ehinger",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krista A. Ehinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "The SUN2012 (Xiao et al., 2010) dataset contains 16,873 manually cleaned up and fully annotated images suitable for object detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 173
                            }
                        ],
                        "text": "It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et al., 2004; Criminisi, 2004; Everingham et al., 2012; Xiao et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1309931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "isKey": false,
            "numCitedBy": 2352,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes."
            },
            "slug": "SUN-database:-Large-scale-scene-recognition-from-to-Xiao-Hays",
            "title": {
                "fragments": [],
                "text": "SUN database: Large-scale scene recognition from abbey to zoo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images and uses 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 179
                            }
                        ],
                        "text": "7 The Third Research Institute of the Ministry of Public Security Jie Shao, Xiaoteng Zhang, JianYing Zhou, Jian Wang, Jian Chen, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu (Girshick et al., 2014; Manen et al., 2013; Howard, 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "4 The University of Tokyo Riku Togashi, Keita Iwamoto, Tomoaki Iwase, Hideki Nakayama (Girshick et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "7 Orange Labs Beijing\u2020, BUPT China\u2021 Hongliang Bai\u2020, Yinan Liu\u2020, Bo Liu\u2021, Yanchao Feng\u2021, Kun Tao\u2020, Yuan Dong\u2020(Girshick et al., 2014) PassBy 16."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "6 Chinese Academy of Science\u2020, Southeast University\u2021 Peihao Huang\u2020, Yongzhen Huang\u2020, Feng Liu\u2021, Zifeng Wu\u2020, Fang Zhao\u2020, Liang Wang\u2020, Tieniu Tan\u2020(Girshick et al., 2014) CASIAWS - 11."
                    },
                    "intents": []
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": true,
            "numCitedBy": 17075,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207252270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616b246e332573af1f4859aa91440280774c183a",
            "isKey": false,
            "numCitedBy": 3766,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "slug": "The-Pascal-Visual-Object-Classes-Challenge:-A-Everingham-Eslami",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes Challenge: A Retrospective"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A review of the Pascal Visual Object Classes challenge from 2008-2012 and an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119916460"
                        ],
                        "name": "Yuanqing Lin",
                        "slug": "Yuanqing-Lin",
                        "structuredName": {
                            "firstName": "Yuanqing",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanqing Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682028"
                        ],
                        "name": "Shenghuo Zhu",
                        "slug": "Shenghuo-Zhu",
                        "structuredName": {
                            "firstName": "Shenghuo",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghuo Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41216159"
                        ],
                        "name": "Ming Yang",
                        "slug": "Ming-Yang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807482"
                        ],
                        "name": "Timoth\u00e9e Cour",
                        "slug": "Timoth\u00e9e-Cour",
                        "structuredName": {
                            "firstName": "Timoth\u00e9e",
                            "lastName": "Cour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timoth\u00e9e Cour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48749954"
                        ],
                        "name": "Liangliang Cao",
                        "slug": "Liangliang-Cao",
                        "structuredName": {
                            "firstName": "Liangliang",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangliang Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 917,
                                "start": 907
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 235
                            }
                        ],
                        "text": "2 NEC Labs America\u2020, University of Illinois at UrbanaChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1346314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d48a42a34f368240b79e7c98c7d9283f79b350",
            "isKey": true,
            "numCitedBy": 412,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Most research efforts on image classification so far have been focused on medium-scale datasets, which are often defined as datasets that can fit into the memory of a desktop (typically 4G\u223c48G). There are two main reasons for the limited effort on large-scale image classification. First, until the emergence of ImageNet dataset, there was almost no publicly available large-scale benchmark data for image classification. This is mostly because class labels are expensive to obtain. Second, large-scale classification is hard because it poses more challenges than its medium-scale counterparts. A key challenge is how to achieve efficiency in both feature extraction and classifier training without compromising performance. This paper is to show how we address this challenge using ImageNet dataset as an example. For feature extraction, we develop a Hadoop scheme that performs feature extraction in parallel using hundreds of mappers. This allows us to extract fairly sophisticated features (with dimensions being hundreds of thousands) on 1.2 million images within one day. For SVM training, we develop a parallel averaging stochastic gradient descent (ASGD) algorithm for training one-against-all 1000-class SVM classifiers. The ASGD algorithm is capable of dealing with terabytes of training data and converges very fast\u2013typically 5 epochs are sufficient. As a result, we achieve state-of-the-art performance on the ImageNet 1000-class classification, i.e., 52.9% in classification accuracy and 71.8% in top 5 hit rate."
            },
            "slug": "Large-scale-image-classification:-Fast-feature-and-Lin-Lv",
            "title": {
                "fragments": [],
                "text": "Large-scale image classification: Fast feature extraction and SVM training"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A parallel averaging stochastic gradient descent (ASGD) algorithm for training one-against-all 1000-class SVM classifiers and a Hadoop scheme that performs feature extraction in parallel using hundreds of mappers, which achieves state-of-the-art performance on the ImageNet 1000- class classification."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19766,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 23
                            }
                        ],
                        "text": "The TinyImages dataset (Torralba et al. 2008) contains 80 million 32\u00d7 32 low resolution images collected from the internet using synsets in WordNet (Miller 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 28
                            }
                        ],
                        "text": "Another dataset TinyImages (Torralba et al., 2008) contains 80 million 32x32 low resolution images collected from the internet using synsets in WordNet (Miller, 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7487588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d2b5c64a67f65c5dd812b89e07973f97699552",
            "isKey": false,
            "numCitedBy": 1868,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "slug": "80-Million-Tiny-Images:-A-Large-Data-Set-for-Object-Torralba-Fergus",
            "title": {
                "fragments": [],
                "text": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For certain classes that are particularly prevalent in the dataset, such as people, this work is able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722052"
                        ],
                        "name": "Thomas Mensink",
                        "slug": "Thomas-Mensink",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Mensink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Mensink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "The honorable mention XRCE team (Perronnin et al., 2010) used an improved Fisher vector representation (Perronnin and Dance, 2007) along with PCA dimensionality reduction and data compression followed by a linear SVM. Fisher vectorbased methods have evolved over five years of the challenge and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1364,
                                "start": 1316
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 118
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10402702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39f3b1804b8df5be645a1dcb4a876e128385d9be",
            "isKey": true,
            "numCitedBy": 2662,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets."
            },
            "slug": "Improving-the-Fisher-Kernel-for-Large-Scale-Image-Perronnin-S\u00e1nchez",
            "title": {
                "fragments": [],
                "text": "Improving the Fisher Kernel for Large-Scale Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "In an evaluation involving hundreds of thousands of training images, it is shown that classifiers learned on Flickr groups perform surprisingly well and that they can complement classifier learned on more carefully annotated datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554424"
                        ],
                        "name": "Asako Kanezaki",
                        "slug": "Asako-Kanezaki",
                        "structuredName": {
                            "firstName": "Asako",
                            "lastName": "Kanezaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asako Kanezaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101352"
                        ],
                        "name": "Sho Inaba",
                        "slug": "Sho-Inaba",
                        "structuredName": {
                            "firstName": "Sho",
                            "lastName": "Inaba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sho Inaba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250559"
                        ],
                        "name": "Y. Ushiku",
                        "slug": "Y.-Ushiku",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Ushiku",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ushiku"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10855898"
                        ],
                        "name": "Yuya Yamashita",
                        "slug": "Yuya-Yamashita",
                        "structuredName": {
                            "firstName": "Yuya",
                            "lastName": "Yamashita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuya Yamashita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056738783"
                        ],
                        "name": "Hiroshi Muraoka",
                        "slug": "Hiroshi-Muraoka",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Muraoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroshi Muraoka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744602"
                        ],
                        "name": "Y. Kuniyoshi",
                        "slug": "Y.-Kuniyoshi",
                        "structuredName": {
                            "firstName": "Yasuo",
                            "lastName": "Kuniyoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kuniyoshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790553"
                        ],
                        "name": "T. Harada",
                        "slug": "T.-Harada",
                        "structuredName": {
                            "firstName": "Tatsuya",
                            "lastName": "Harada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Harada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 137
                            }
                        ],
                        "text": "4 The University of Tokyo\u2020, IIT Guwahati\u2021 Senthil Purushwalkam\u2020\u2021, Yuichiro Tsuchiya\u2020, Atsushi Kanehira\u2020, Asako Kanezaki\u2020, Tatsuya Harada\u2020(Kanezaki et al., 2014; Girshick et al., 2013) MPG UT - - - - - 26."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1284533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5710ddc7254500901111394e968cfc41b4fadbe1",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an efficient method to train multiple object detectors simultaneously using a large scale image dataset. The one-vs-all approach that optimizes the boundary between positive samples from a target class and negative samples from the others has been the most standard approach for object detection. However, because this approach trains each object detector independently, the scores are not balanced between object classes. The proposed method combines ideas derived from both detection and classification in order to balance the scores across all object classes. We optimized the boundary between target classes and their \u201chard negative\u201d samples, just as in detection, while simultaneously balancing the detector scores across object classes, as done in multi-class classification. We evaluated the performances on multi-class object detection using a subset of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2011 dataset and showed our method outperformed a de facto standard method."
            },
            "slug": "Hard-negative-classes-for-multiple-object-detection-Kanezaki-Inaba",
            "title": {
                "fragments": [],
                "text": "Hard negative classes for multiple object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed method optimized the boundary between target classes and their \u201chard negative\u201d samples, just as in detection, while simultaneously balancing the detector scores across object classes, as done in multi-class classification."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2218,
                                "start": 2210
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 111
                            }
                        ],
                        "text": "3 \u2013 \u2013 \u2013 Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou 2014; Papandreou et al. 2014; Jojic et al. 2003; Krizhevsky et al. 2012; Sermanet et al. 2013; Dubout and Fleuret 2012; Iandola et al. 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "For localization they used per-class bounding box regression similar to OverFeat (Sermanet et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "For localization they used per-class bounding box regression similar to OverFeat (Sermanet et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al. 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 216
                            }
                        ],
                        "text": "The winning single-object localization OverFeat submissionwas basedon an integrated framework for using convolutional networks for classification, localization and detection with a multiscale sliding window approach (Sermanet et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 219
                            }
                        ],
                        "text": "The winning single-object localization OverFeat submission was based on an integrated framework for using convolutional networks for classification, localization and detection with a multiscale sliding window approach (Sermanet et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 536,
                                "start": 528
                            }
                        ],
                        "text": "Given the results of all the bootstrapping rounds we\nImage classification Year Codename Error (percent) 99.9% Conf Int 2014 GoogLeNet 6.66 6.40 - 6.92 2014 VGG 7.32 7.05 - 7.60 2014 MSRA 8.06 7.78 - 8.34 2014 AHoward 8.11 7.83 - 8.39 2014 DeeperVision 9.51 9.21 - 9.82 2013 Clarifai\u2020 11.20 10.87 - 11.53 2014 CASIAWS\u2020 11.36 11.03 - 11.69 2014 Trimps\u2020 11.46 11.13 - 11.80 2014 Adobe\u2020 11.58 11.25 - 11.91 2013 Clarifai 11.74 11.41 - 12.08 2013 NUS 12.95 12.60 - 13.30 2013 ZF 13.51 13.14 - 13.87 2013 AHoward 13.55 13.20 - 13.91 2013 OverFeat 14.18 13.83 - 14.54 2014 Orange\u2020 14.80 14.43 - 15.17 2012 SuperVision\u2020 15.32 14.94 - 15.69 2012 SuperVision 16.42 16.04 - 16.80 2012 ISI 26.17 25.71 - 26.65 2012 VGG 26.98 26.53 - 27.43 2012 XRCE 27.06 26.60 - 27.52 2012 UvA 29.58 29.09 - 30.04 Single-object localization Year Codename Error (percent) 99.9% Conf Int 2014 VGG 25.32 24.87 - 25.78 2014 GoogLeNet 26.44 25.98 - 26.92 2013 OverFeat 29.88 29.38 - 30.35 2014 Adobe\u2020 30.10 29.61 - 30.58 2014 SYSU 31.90 31.40 - 32.40 2012 SuperVision\u2020 33.55 33.05 - 34.04 2014 MIL 33.74 33.24 - 34.25 2012 SuperVision 34.19 33.67 - 34.69 2014 MSRA 35.48 34.97 - 35.99 2014 Trimps\u2020 42.22 41.69 - 42.75 2014 Orange\u2020 42.70 42.18 - 43.24 2013 VGG 46.42 45.90 - 46.95 2012 VGG 50.03 49.50 - 50.57 2012 ISI 53.65 53.10 - 54.17 2014 CASIAWS\u2020 61.96 61.44 - 62.48\nObject detection Year Codename AP (percent) 99.9% Conf Int 2014 GoogLeNet\u2020 43.93 42.92 - 45.65 2014 CUHK\u2020 40.67 39.68 - 42.30 2014 DeepInsight\u2020 40.45 39.49 - 42.06 2014 NUS 37.21 36.29 - 38.80 2014 UvA\u2020 35.42 34.63 - 36.92 2014 MSRA 35.11 34.36 - 36.70 2014 Berkeley\u2020 34.52 33.67 - 36.12 2014 UvA 32.03 31.28 - 33.49 2014 Southeast 30.48 29.70 - 31.93 2014 HKUST 28.87 28.03 - 30.20 2013 UvA 22.58 22.00 - 23.82 2013 NEC\u2020 20.90 20.40 - 22.15 2013 NEC 19.62 19.14 - 20.85 2013 OverFeat\u2020 19.40 18.82 - 20.61 2013 Toronto 11.46 10.98 - 12.34 2013 SYSU 10.45 10.04 - 11.32 2013 UCLA 9.83 9.48 - 10.77\nTable 8 We use bootstrapping to construct 99.9% confidence intervals around the result of up to top 5 submissions to each ILSVRC task in 2012-2014."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1109b663453e78a59e4f66446d71720ac58cec25",
            "isKey": true,
            "numCitedBy": 4352,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "\u2026by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1886,
                                "start": 1882
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 126
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11442196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37e41557932cc0035eab23fd767bde68f6475c3a",
            "isKey": true,
            "numCitedBy": 711,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge."
            },
            "slug": "Segmentation-as-selective-search-for-object-Sande-Uijlings",
            "title": {
                "fragments": [],
                "text": "Segmentation as selective search for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work adapt segmentation as a selective search by reconsidering segmentation to generate many approximate locations over few and precise object delineations because an object whose location is never generated can not be recognised and appearance and immediate nearby context are most effective for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 104
                            }
                        ],
                        "text": "The honorable mention XRCE team (Perronnin et al., 2010) used an improved Fisher vector representation (Perronnin and Dance, 2007) along with PCA dimensionality reduction and data compression followed by a linear SVM. Fisher vectorbased methods have evolved over five years of the challenge and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 54
                            }
                        ],
                        "text": ", 2010) used an improved Fisher vector representation (Perronnin and Dance, 2007) along with PCA dimensionality reduction and data compression followed by a linear SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12795415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23694b6d61668e62bb11f17c1d75dde3b4951948",
            "isKey": false,
            "numCitedBy": 1613,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance."
            },
            "slug": "Fisher-Kernels-on-Visual-Vocabularies-for-Image-Perronnin-Dance",
            "title": {
                "fragments": [],
                "text": "Fisher Kernels on Visual Vocabularies for Image Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms, and proposes to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346186"
                        ],
                        "name": "Forrest N. Iandola",
                        "slug": "Forrest-N.-Iandola",
                        "structuredName": {
                            "firstName": "Forrest",
                            "lastName": "Iandola",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Forrest N. Iandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318023"
                        ],
                        "name": "M. Moskewicz",
                        "slug": "M.-Moskewicz",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Moskewicz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moskewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 111
                            }
                        ],
                        "text": "3 \u2013 \u2013 \u2013 Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou 2014; Papandreou et al. 2014; Jojic et al. 2003; Krizhevsky et al. 2012; Sermanet et al. 2013; Dubout and Fleuret 2012; Iandola et al. 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8803949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fc662287842e5cb2d23b5fa917354e957c573bf",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper presents DenseNet, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier. Future work will involve training efficient object detectors with DenseNet feature descriptors."
            },
            "slug": "DenseNet:-Implementing-Efficient-ConvNet-Descriptor-Iandola-Moskewicz",
            "title": {
                "fragments": [],
                "text": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "DenseNet is presented, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 9
                            }
                        ],
                        "text": "LabelMe (Russell et al., 2007) contains general photographs with multiple objects per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "reconstruction and KITTI (Geiger et al., 2013) for computer vision in autonomous driving. These datasets along with ILSVRC help benchmark progress in dierent areas of computer vision. Works such as (Torralba and Efros, 2011) emphasize the importance of examining the bias inherent in any standardized dataset. 1.2 Paper layout We begin with a brief overview of ILSVRC challenge tasks in Section 2. Dataset collection and ann"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2777306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0302bb2d5476540cfb21467473f5eca843caf90b",
            "isKey": true,
            "numCitedBy": 1756,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue."
            },
            "slug": "Unbiased-look-at-dataset-bias-Torralba-Efros",
            "title": {
                "fragments": [],
                "text": "Unbiased look at dataset bias"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118775664"
                        ],
                        "name": "Xiaoyu Wang",
                        "slug": "Xiaoyu-Wang",
                        "structuredName": {
                            "firstName": "Xiaoyu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41216159"
                        ],
                        "name": "Ming Yang",
                        "slug": "Ming-Yang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682028"
                        ],
                        "name": "Shenghuo Zhu",
                        "slug": "Shenghuo-Zhu",
                        "structuredName": {
                            "firstName": "Shenghuo",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghuo Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119916460"
                        ],
                        "name": "Yuanqing Lin",
                        "slug": "Yuanqing-Lin",
                        "structuredName": {
                            "firstName": "Yuanqing",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanqing Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1782,
                                "start": 1747
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6194123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f99408de2ae6c5c036e1825bdadf7b193c3ba734",
            "isKey": true,
            "numCitedBy": 363,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as region lets. A region let is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These region lets are organized in small groups with stable relative positions to delineate fine grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detection mean average precision of 41.7% on the PASCAL VOC 2007 dataset and 39.7% on the VOC 2010 for 20 object categories. It achieves 14.7% mean average precision on the Image Net dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4.7%."
            },
            "slug": "Regionlets-for-Generic-Object-Detection-Wang-Yang",
            "title": {
                "fragments": [],
                "text": "Regionlets for Generic Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as region lets, which significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 100
                            }
                        ],
                        "text": "Instead we turn to designing novel crowdsourcing approaches for collecting large-scale annotations (Su et al., 2012; Deng et al., 2009, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 85
                            }
                        ],
                        "text": "We summarize the crowdsourced bounding box annotation system described in detail in (Su et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "tation system described in detail in (Su et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "Additional evaluation of the overall cost and an analysis of quality control can be found in (Su et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 70
                            }
                        ],
                        "text": "Worker training for each of these subtasks is described in detail in (Su et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3621240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf9daa503798c3d4e04131bc7bf01544666265d1",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of images with ground truth object bounding boxes are critical for learning object detectors, which is a fundamental task in compute vision. In this paper, we study strategies to crowd-source bounding box annotations. The core challenge of building such a system is to effectively control the data quality with minimal cost. Our key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions. Thus quality control through additional verification tasks is more cost effective than consensus based algorithms. In particular, we present a system that consists of three simple sub-tasks \u2014 a drawing task, a quality verification task and a coverage verification task. Experimental results demonstrate that our system is scalable, accurate, and cost-effective."
            },
            "slug": "Crowdsourcing-Annotations-for-Visual-Object-Su-Deng",
            "title": {
                "fragments": [],
                "text": "Crowdsourcing Annotations for Visual Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions, so quality control through additional verification tasks is more cost effective than consensus based algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "HCOMP@AAAI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "research have emerged, such as large-scale weakly supervised localization work of (Kuettel et al., 2012) which was awarded the best paper award in ECCV 2012 and large-scale zero-shot learning, e.g., (Frome et al., 2013). 6 Results and analysis 6.1 Improvements over the years State-of-the-art accuracy has improved signicantly from ILSVRC2010 to ILSVRC2014, showcasing the massive progress that has been made in large-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 242
                            }
                        ],
                        "text": "Additionally, several influential lines of research have emerged, such as large-scale weakly supervised localization work of (Kuettel et al., 2012) which was awarded the best paper award in ECCV 2012 and largescale zero-shot learning, e.g., (Frome et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 261138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aa4069693bee00d1b0759ca3df35e59284e9845",
            "isKey": true,
            "numCitedBy": 1950,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model."
            },
            "slug": "DeViSE:-A-Deep-Visual-Semantic-Embedding-Model-Frome-Corrado",
            "title": {
                "fragments": [],
                "text": "DeViSE: A Deep Visual-Semantic Embedding Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text and shows that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681442"
                        ],
                        "name": "Ce Liu",
                        "slug": "Ce-Liu",
                        "structuredName": {
                            "firstName": "Ce",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ce Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143738177"
                        ],
                        "name": "Jenny Yuen",
                        "slug": "Jenny-Yuen",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Yuen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenny Yuen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ame each object. The SUN2012 (Xiao et al., 2010) dataset contains 16,873 manually cleaned up and fully annotated images more suitable for standard object detection training and evaluation. SIFT Flow (Liu et al., 2011) contains 2,688 images labeled using the LabelMe system. The LotusHill dataset (Yao et al., 2007) contains very detailed annotations of objects in 636,748 images and video frames, but it is not availa"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18477260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5a52b69dde106cb69cb7c35dd8ca23071966876",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "While there has been a lot of recent work on object recognition and image understanding, the focus has been on carefully establishing mathematical models for images, scenes, and objects. In this paper, we propose a novel, nonparametric approach for object recognition and scene parsing using a new technology we name label transfer. For an input image, our system first retrieves its nearest neighbors from a large database containing fully annotated images. Then, the system establishes dense correspondences between the input image and each of the nearest neighbors using the dense SIFT flow algorithm [28], which aligns two images based on local image structures. Finally, based on the dense scene correspondences obtained from SIFT flow, our system warps the existing annotations and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on challenging databases. Compared to existing object recognition approaches that require training classifiers or appearance models for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure."
            },
            "slug": "Nonparametric-Scene-Parsing-via-Label-Transfer-Liu-Yuen",
            "title": {
                "fragments": [],
                "text": "Nonparametric Scene Parsing via Label Transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a novel, nonparametric approach for object recognition and scene parsing using a new technology the authors name label transfer, which is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2893664"
                        ],
                        "name": "Zeynep Akata",
                        "slug": "Zeynep-Akata",
                        "structuredName": {
                            "firstName": "Zeynep",
                            "lastName": "Akata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeynep Akata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753355"
                        ],
                        "name": "Z. Harchaoui",
                        "slug": "Z.-Harchaoui",
                        "structuredName": {
                            "firstName": "Za\u00efd",
                            "lastName": "Harchaoui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harchaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 116
                            }
                        ],
                        "text": "1 Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2341,
                                "start": 2336
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1312964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69d4430a8a70b53fe0b71482193262995b6e27b",
            "isKey": true,
            "numCitedBy": 190,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a benchmark of several objective functions for large-scale image classification: we compare the one-vs-rest, multiclass, ranking and weighted average ranking SVMs. Using stochastic gradient descent optimization, we can scale the learning to millions of images and thousands of classes. Our experimental evaluation shows that ranking based algorithms do not outperform a one-vs-rest strategy and that the gap between the different algorithms reduces in case of high-dimensional data. We also show that for one-vs-rest, learning through cross-validation the optimal degree of imbalance between the positive and the negative samples can have a significant impact. Furthermore, early stopping can be used as an effective regularization strategy when training with stochastic gradient algorithms. Following these \"good practices\", we were able to improve the state-of-the-art on a large subset of 10K classes and 9M of images of lmageNet from 16.7% accuracy to 19.1%."
            },
            "slug": "Towards-good-practice-in-large-scale-learning-for-Akata-Perronnin",
            "title": {
                "fragments": [],
                "text": "Towards good practice in large-scale learning for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that for one-vs-rest, learning through cross-validation the optimal degree of imbalance between the positive and the negative samples can have a significant impact and early stopping can be used as an effective regularization strategy when training with stochastic gradient algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 142
                            }
                        ],
                        "text": "\u2026by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 45
                            }
                        ],
                        "text": "2010), pooling with spatial pyramid matching (Lazebnik et al. 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik 2009) trained on a GPU (van de Sande et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 27
                            }
                        ],
                        "text": "Datasets such as 15 Scenes (Oliva and Torralba 2001; Fei-Fei and Perona 2005; Lazebnik et al. 2006) or recent Places (Zhou et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": false,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40462943"
                        ],
                        "name": "D. K\u00fcttel",
                        "slug": "D.-K\u00fcttel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "K\u00fcttel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K\u00fcttel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737253"
                        ],
                        "name": "M. Guillaumin",
                        "slug": "M.-Guillaumin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Guillaumin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guillaumin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 73
                            }
                        ],
                        "text": "have emerged, such as large-scale weakly supervised localization work of (Kuettel et al., 2012) which was awarded the best paper award in ECCV 2012 and large-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 126
                            }
                        ],
                        "text": "Additionally, several influential lines of research have emerged, such as large-scale weakly supervised localization work of (Kuettel et al., 2012) which was awarded the best paper award in ECCV 2012 and largescale zero-shot learning, e.g., (Frome et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9346547,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "50bf30469f4d95f28acf80cf7c4ba5ab2f1c422e",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "ImageNet is a large-scale hierarchical database of object classes. We propose to automatically populate it with pixelwise segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes. The key idea is to recursively exploit images segmented so far to guide the segmentation of new images. At each stage this propagation process expands into the images which are easiest to segment at that point in time, e.g. by moving to the semantically most related classes to those segmented so far. The propagation of segmentation occurs both (a) at the image level, by transferring existing segmentations to estimate the probability of a pixel to be foreground, and (b) at the class level, by jointly segmenting images of the same class and by importing the appearance models of classes that are already segmented. Through an experiment on 577 classes and 500k images we show that our technique (i) annotates a wide range of classes with accurate segmentations; (ii) effectively exploits the hierarchical structure of ImageNet; (iii) scales efficiently; (iv) outperforms a baseline GrabCut [1] initialized on the image center, as well as our recent segmentation transfer technique [2] on which this paper is based. Moreover, our method also delivers state-of-the-art results on the recent iCoseg dataset for co-segmentation."
            },
            "slug": "Segmentation-Propagation-in-ImageNet-K\u00fcttel-Guillaumin",
            "title": {
                "fragments": [],
                "text": "Segmentation Propagation in ImageNet"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper proposes to automatically populate ImageNet with pixelwise segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes, and effectively exploits the hierarchical structure of ImageNet."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Large-scale image classification: Fast feature extraction and SVM training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "togram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 224
                            }
                        ],
                        "text": "\u2026by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 240
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 375
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 213
                            }
                        ],
                        "text": "The honorable mention XRCE team (Perronnin et al., 2010) used an improved Fisher vector representation (Perronnin and Dance, 2007) along with PCA dimensionality reduction and data compression followed by a linear SVM. Fisher vectorbased methods have evolved over five years of the challenge and continued performing strongly in every ILSVRC from 2010 to 2014."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 211
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 238
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 107221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1adec942e0225bc6fde6560ffea5d496b122cdcc",
            "isKey": true,
            "numCitedBy": 150,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a discriminative Hough transform based object detector where each local part casts a weighted vote for the possible locations of the object center. We show that the weights can be learned in a max-margin framework which directly optimizes the classification performance. The discriminative training takes into account both the codebook appearance and the spatial distribution of its position with respect to the object center to derive its importance. On various datasets we show that the discriminative training improves the Hough detector. Combined with a verification step using a SVM based classifier, our approach achieves a detection rate of 91.9% at 0.3 false positives per image on the ETHZ shape dataset, a significant improvement over the state of the art, while running the verification step on at least an order of magnitude fewer windows than in a sliding window approach."
            },
            "slug": "Object-detection-using-a-max-margin-Hough-transform-Maji-Malik",
            "title": {
                "fragments": [],
                "text": "Object detection using a max-margin Hough transform"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A discriminative Hough transform based object detector where each local part casts a weighted vote for the possible locations of the object center and it is shown that the weights can be learned in a max-margin framework which directly optimizes the classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 664,
                                "start": 609
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 95
                            }
                        ],
                        "text": "4 Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 262
                            }
                        ],
                        "text": "\u2026Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2161130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d77482b5e3478f4616f7467054ad50505207958",
            "isKey": true,
            "numCitedBy": 191,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore the extent of this analogy, proposing a version of the state-of-the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture significantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a smaller computational learning cost. Our hybrid architecture allows us to assess how the performance of a conventional hand-crafted image classification pipeline changes with increased depth. We also show that convolutional networks and Fisher vector encodings are complementary in the sense that their combination further improves the accuracy."
            },
            "slug": "Deep-Fisher-Networks-for-Large-Scale-Image-Simonyan-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Deep Fisher Networks for Large-Scale Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a version of the state-of-the-art Fisher vector image encoding that can be stacked in multiple layers, and significantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a smaller computational learning cost."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722052"
                        ],
                        "name": "Thomas Mensink",
                        "slug": "Thomas-Mensink",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Mensink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Mensink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721683"
                        ],
                        "name": "J. Verbeek",
                        "slug": "J.-Verbeek",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Verbeek",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Verbeek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808423"
                        ],
                        "name": "G. Csurka",
                        "slug": "G.-Csurka",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Csurka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csurka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 143
                            }
                        ],
                        "text": "\u2026Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "5 \u2013 LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021(Mensink et al. 2012)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2451,
                                "start": 2331
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9296691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a4a53fe47036ac89dad070ab87a9d8795b139b1",
            "isKey": true,
            "numCitedBy": 260,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We are interested in large-scale image classification and especially in the setting where images corresponding to new or existing classes are continuously added to the training set. Our goal is to devise classifiers which can incorporate such images and classes on-the-fly at (near) zero cost. We cast this problem into one of learning a metric which is shared across all classes and explore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers. We learn metrics on the ImageNet 2010 challenge data set, which contains more than 1.2M training images of 1K classes. Surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier, and has comparable performance to linear SVMs. We also study the generalization performance, among others by using the learned metric on the ImageNet-10K dataset, and we obtain competitive performance. Finally, we explore zero-shot classification, and show how the zero-shot model can be combined very effectively with small training datasets."
            },
            "slug": "Metric-Learning-for-Large-Scale-Image-Generalizing-Mensink-Verbeek",
            "title": {
                "fragments": [],
                "text": "Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal is to devise classifiers which can incorporate images and classes on-the-fly at (near) zero cost and to explore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": "To quantify clutter we employ the objectness measure of (Alexe et al., 2012), which is a class-generic object detector evaluating how likely a window in the image contains a coherent object (of any class) as opposed to background (sky, water, grass)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7316529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2eb6caace8296fd4dfd4947efa4fe911c8e133b2",
            "isKey": false,
            "numCitedBy": 1196,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image."
            },
            "slug": "Measuring-the-Objectness-of-Image-Windows-Alexe-Deselaers",
            "title": {
                "fragments": [],
                "text": "Measuring the Objectness of Image Windows"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A generic objectness measure, quantifying how likely it is for an image window to contain an object of any class, and uses objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068068778"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": ", 2014a) incorporating improvements of (Howard, 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 179
                            }
                        ],
                        "text": "7 The Third Research Institute of the Ministry of Public Security Jie Shao, Xiaoteng Zhang, JianYing Zhou, Jian Wang, Jian Chen, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu (Girshick et al., 2014; Manen et al., 2013; Howard, 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "1 - \u25e6 - - Howard Vision Technologies Andrew Howard (Howard, 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 205
                            }
                        ],
                        "text": "In the object detection with provided data track, the winning team NUS used the RCNN framework (Girshick et al., 2013) with the network-in-network method (Lin et al., 2014a) incorporating improvements of (Howard, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 117
                            }
                        ],
                        "text": "of Singapore\u2020, Beijing Samsung Telecom R&D Center\u2020 Min Lin\u2020, Jian Dong\u2020, Hanjiang Lai\u2020, Junjun Xiong\u2021, Shuicheng Yan\u2020(Lin et al., 2014a; Howard, 2014; Krizhevsky et al., 2012)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 651286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67175d17c450ab0ac9c256103828f9e9a0acb85",
            "isKey": true,
            "numCitedBy": 362,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner."
            },
            "slug": "Some-Improvements-on-Deep-Convolutional-Neural-Howard",
            "title": {
                "fragments": [],
                "text": "Some Improvements on Deep Convolutional Neural Network Based Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper summarizes the entry in the Imagenet Large Scale Visual Recognition Challenge 2013, which achieved a top 5 classification error rate and achieved over a 20% relative improvement on the previous year's winner."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 75
                            }
                        ],
                        "text": "\u2213 Cewu Lu\u2020, Hei Law*\u2020, Hao Chen*\u2021, Qifeng Chen*\u2213, Yao Xiao*\u2020Chi Keung Tang\u2020(Uijlings et al., 2013; Girshick et al., 2013; Perronnin et al., 2010; Felzenszwalb et al., 2010)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 82
                            }
                        ],
                        "text": ", 2010) pooled using a multilevel spatial pyramid in a selective search framework (Uijlings et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 265
                            }
                        ],
                        "text": "The winner of object detection task was UvA team, which utilized a new way of efficient encoding (van de Sande et al., 2014) of densely sampled color descriptors (van de Sande et al., 2010) pooled using a multilevel spatial pyramid in a selective search framework (Uijlings et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 216077384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38b6540ddd5beebffd05047c78183f7575559fb2",
            "isKey": false,
            "numCitedBy": 4748,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html)."
            },
            "slug": "Selective-Search-for-Object-Recognition-Uijlings-Sande",
            "title": {
                "fragments": [],
                "text": "Selective Search for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper introduces selective search which combines the strength of both an exhaustive search and segmentation, and shows that its selective search enables the use of the powerful Bag-of-Words model for recognition."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5626877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d1813749fcb36351fff850f0391968b62fc73b7",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a large scale general purpose image database with human annotated ground truth. Firstly, an all-in-all labeling framework is proposed to group visual knowledge of three levels: scene level (global geometric description), object level (segmentation, sketch representation, hierarchical decomposition), and low-mid level (2.1D layered representation, object boundary attributes, curve completion, etc.). Much of this data has not appeared in previous databases. In addition, And-Or Graph is used to organize visual elements to facilitate top-down labeling. An annotation tool is developed to realize and integrate all tasks. With this tool, we've been able to create a database consisting of more than 636,748 annotated images and video frames. Lastly, the data is organized into 13 common subsets to serve as benchmarks for diverse evaluation endeavors."
            },
            "slug": "Introduction-to-a-Large-Scale-General-Purpose-Truth-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "Introduction to a Large-Scale General Purpose Ground Truth Database: Methodology, Annotation Tool and Benchmarks"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A large scale general purpose image database with human annotated ground truth consisting of more than 636,748 annotated images and video frames is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMMCVPR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50196944"
                        ],
                        "name": "Judy Hoffman",
                        "slug": "Judy-Hoffman",
                        "structuredName": {
                            "firstName": "Judy",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judy Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368132"
                        ],
                        "name": "Eric Tzeng",
                        "slug": "Eric-Tzeng",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Tzeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Tzeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6161478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8de958fead0d8a9619b55c7299df3257c624a96",
            "isKey": false,
            "numCitedBy": 4234,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "slug": "DeCAF:-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia",
            "title": {
                "fragments": [],
                "text": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DeCAF, an open-source implementation of deep convolutional activation features, along with all associated network parameters, are released to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2004053"
                        ],
                        "name": "Vicente Ordonez",
                        "slug": "Vicente-Ordonez",
                        "structuredName": {
                            "firstName": "Vicente",
                            "lastName": "Ordonez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicente Ordonez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 269
                            }
                        ],
                        "text": "Best paper awards at top vision conferences in 2013 were awarded to large-scale recognition methods: at CVPR 2013 to \u201dFast, Accurate Detection of 100,000 Object Classes on a Single Machine\u201d (Dean et al., 2013) and at ICCV 2013 to \u201dFrom Large Scale Image Categorization to Entry-Level Categories\u201d (Ordonez et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 86
                            }
                        ],
                        "text": "and at ICCV 2013 to \u201dFrom Large Scale Image Categorization to Entry-Level Categories\u201d (Ordonez et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 273
                            }
                        ],
                        "text": "\u2026top vision conferences in 2013 were awarded to large-scale recognition methods: at CVPR 2013 to \u201dFast, Accurate Detection of 100,000 Object Classes on a Single Machine\u201d (Dean et al., 2013) and at ICCV 2013 to \u201dFrom Large Scale Image Categorization to Entry-Level Categories\u201d (Ordonez et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2271818,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3bfeecf2aa26efe211985e19a967b2cb28012482",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Entry level categories - the labels people will use to name an object - were originally defined and studied by psychologists in the 1980s. In this paper we study entry-level categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word \"naturalness\" mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval."
            },
            "slug": "From-Large-Scale-Image-Categorization-to-Categories-Ordonez-Deng",
            "title": {
                "fragments": [],
                "text": "From Large Scale Image Categorization to Entry-Level Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The first models for predicting entry-level categories for images, which combine visual recognition predictions with proxies for word \"naturalness\" mined from the enormous amounts of text on the web are learned."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 834
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 75
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CVPR. Yang, J., Yu, K., Gong, Y., and Huang, T. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "In CVPR. Yao, B., Yang, X., and Zhu, S.-C. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 63
                            }
                        ],
                        "text": "2 - \u25e6 - - The University of Queensland Zhongwen Xu and Yi Yang (Krizhevsky et al., 2012; Jia, 2013; Zeiler and Fergus, 2013; Lin et al., 2014a)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 53
                            }
                        ],
                        "text": "5 - New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": true,
            "numCitedBy": 11803,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154604"
                        ],
                        "name": "Nityananda Jayadevaprakash",
                        "slug": "Nityananda-Jayadevaprakash",
                        "structuredName": {
                            "firstName": "Nityananda",
                            "lastName": "Jayadevaprakash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nityananda Jayadevaprakash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38916673"
                        ],
                        "name": "B. Yao",
                        "slug": "B.-Yao",
                        "structuredName": {
                            "firstName": "Bangpeng",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 211
                            }
                        ],
                        "text": "A subset of categories and images was chosen and\n3 In addition, ILSVRC in 2012 also included a taster finegrained classification task, where algorithms would classify dog photographs into one of 120 dog breeds (Khosla et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 161
                            }
                        ],
                        "text": "3 In addition, ILSVRC in 2012 also included a taster finegrained classification task, where algorithms would classify dog photographs into one of 120 dog breeds (Khosla et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3181866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5e3beb791cc17cdaf131d5cca6ceb796226d832",
            "isKey": false,
            "numCitedBy": 767,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a 120 class Stanford Dogs dataset, a challenging and large-scale dataset aimed at fine-grained image categorization. Stanford Dogs includes over 22,000 annotated images of dogs belonging to 120 species. Each image is annotated with a bounding box and object class label. Fig. 1 shows examples of images from Stanford Dogs. This dataset is extremely challenging due to a variety of reasons. First, being a fine-grained categorization problem, there is little inter-class variation. For example the basset hound and bloodhound share very similar facial characteristics but differ significantly in their color, while the Japanese spaniel and papillion share very similar color but greatly differ in their facial characteristics. Second, there is very large intra-class variation. The images show that dogs within a class could have different ages (e.g. beagle), poses (e.g. blenheim spaniel), occlusion/self-occlusion and even color (e.g. Shih-tzu). Furthermore, compared to other animal datasets that tend to exist in natural scenes, a large proportion of the images contain humans and are taken in manmade environments leading to greater background variation. The aforementioned reasons make this an extremely challenging dataset."
            },
            "slug": "Novel-Dataset-for-Fine-Grained-Image-Categorization-Khosla-Jayadevaprakash",
            "title": {
                "fragments": [],
                "text": "Novel Dataset for Fine-Grained Image Categorization : Stanford Dogs"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A 120 class Stanford Dogs dataset, a challenging and large-scale dataset aimed at fine-grained image categorization, is introduced, which includes over 22,000 annotated images of dogs belonging to 120 species."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 109
                            }
                        ],
                        "text": "3 - - Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou, 2014; Papandreou et al., 2014; Jojic et al., 2003; Krizhevsky et al., 2012; Sermanet et al., 2013; Dubout and Fleuret, 2012; Iandola et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6583291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80fee61150c4643dc8ae45223359ba9fdc4099f6",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to question the necessity of features like SIFT in categorical visual recognition tasks. As an alternative, we develop a generative model for the raw intensity of image patches and show that it can support image classification performance on par with optimized SIFT-based techniques in a bag-of-visual-words setting. Key ingredient of the proposed model is a compact dictionary of mini-epitomes, learned in an unsupervised fashion on a large collection of images. The use of epitomes allows us to explicitly account for photometric and position variability in image appearance. We show that this flexibility considerably increases the capacity of the dictionary to accurately approximate the appearance of image patches and support recognition tasks. For image classification, we develop histogram-based image encoding methods tailored to the epitomic representation, as well as an \"epitomic footprint\" encoding which is easy to visualize and highlights the generative nature of our model. We discuss in detail computational aspects and develop efficient algorithms to make the model scalable to large tasks. The proposed techniques are evaluated with experiments on the challenging PASCAL VOC 2007 image classification benchmark."
            },
            "slug": "Modeling-Image-Patches-with-a-Generic-Dictionary-of-Papandreou-Chen",
            "title": {
                "fragments": [],
                "text": "Modeling Image Patches with a Generic Dictionary of Mini-epitomes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generative model for the raw intensity of image patches is developed and shown that it can support image classification performance on par with optimized SIFT-based techniques in a bag-of-visual-words setting."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109615009"
                        ],
                        "name": "Xi Zhou",
                        "slug": "Xi-Zhou",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49104973"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7405065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b",
            "isKey": true,
            "numCitedBy": 555,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks."
            },
            "slug": "Image-Classification-Using-Super-Vector-Coding-of-Zhou-Yu",
            "title": {
                "fragments": [],
                "text": "Image Classification Using Super-Vector Coding of Local Image Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper introduces a new framework for image classification using local visual descriptors that first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "9 Google Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Drago Anguelov, Dumitru Erhan, Andrew Rabinovich (Szegedy et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29470,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "Appendix B and (Russakovsky et al., 2013) have\nadditional comparisons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "The other properties were computed by asking human subjects to annotate each of the 1000 object categories (Russakovsky et al. 2013)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 1
                            }
                        ],
                        "text": "(Russakovsky et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 94
                            }
                        ],
                        "text": "The x-axis corresponds to object properties annotated by human labelers for each object class (Russakovsky et al. 2013) and illustrated in Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 151
                            }
                        ],
                        "text": "Human subjects annotated each of the 1000 image classification and single-object localization object classes from ILSVRC2012-2014with these properties (Russakovsky et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "A detailed analysis and comparison of the SuperVision and VGG submissions on the single-object localization task can be found in (Russakovsky et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 164786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4eeec2093a08b9d5a965776ad0e11eab749bd019",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The growth of detection datasets and the multiple directions of object detection research provide both an unprecedented need and a great opportunity for a thorough evaluation of the current state of the field of categorical object detection. In this paper we strive to answer two key questions. First, where are we currently as a field: what have we done right, what still needs to be improved? Second, where should we be going in designing the next generation of object detectors? Inspired by the recent work of Hoiem et al. on the standard PASCAL VOC detection dataset, we perform a large-scale study on the Image Net Large Scale Visual Recognition Challenge (ILSVRC) data. First, we quantitatively demonstrate that this dataset provides many of the same detection challenges as the PASCAL VOC. Due to its scale of 1000 object categories, ILSVRC also provides an excellent test bed for understanding the performance of detectors as a function of several key properties of the object classes. We conduct a series of analyses looking at how different detection methods perform on a number of image-level and object-class-level properties such as texture, color, deformation, and clutter. We learn important lessons of the current object detection methods and propose a number of insights for designing the next generation object detectors."
            },
            "slug": "Detecting-Avocados-to-Zucchinis:-What-Have-We-Done,-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A large-scale study on the Image Net Large Scale Visual Recognition Challenge data, inspired by the recent work of Hoiem et al, shows that this dataset provides many of the same detection challenges as the PASCAL VOC."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38916673"
                        ],
                        "name": "B. Yao",
                        "slug": "B.-Yao",
                        "structuredName": {
                            "firstName": "Bangpeng",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18014460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e7a689b57415342cd6c6dc57b4d0074868e8042",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the problem of fine-grained image categorization. The goal of our method is to explore fine image statistics and identify the discriminative image patches for recognition. We achieve this goal by combining two ideas, discriminative feature mining and randomization. Discriminative feature mining allows us to model the detailed information that distinguishes different classes of images, while randomization allows us to handle the huge feature space and prevents over-fitting. We propose a random forest with discriminative decision trees algorithm, where every tree node is a discriminative classifier that is trained by combining the information in this node as well as all upstream nodes. Our method is tested on both subordinate categorization and activity recognition datasets. Experimental results show that our method identifies semantically meaningful visual information and outperforms state-of-the-art algorithms on various datasets."
            },
            "slug": "Combining-randomization-and-discrimination-for-Yao-Khosla",
            "title": {
                "fragments": [],
                "text": "Combining randomization and discrimination for fine-grained image categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results show that the proposed random forest with discriminative decision trees algorithm identifies semantically meaningful visual information and outperforms state-of-the-art algorithms on various datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001348"
                        ],
                        "name": "Wanli Ouyang",
                        "slug": "Wanli-Ouyang",
                        "structuredName": {
                            "firstName": "Wanli",
                            "lastName": "Ouyang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanli Ouyang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47571885"
                        ],
                        "name": "Ping Luo",
                        "slug": "Ping-Luo",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550719"
                        ],
                        "name": "Xingyu Zeng",
                        "slug": "Xingyu-Zeng",
                        "structuredName": {
                            "firstName": "Xingyu",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingyu Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146345714"
                        ],
                        "name": "Shi Qiu",
                        "slug": "Shi-Qiu",
                        "structuredName": {
                            "firstName": "Shi",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476765"
                        ],
                        "name": "Yonglong Tian",
                        "slug": "Yonglong-Tian",
                        "structuredName": {
                            "firstName": "Yonglong",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonglong Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404547"
                        ],
                        "name": "Hongsheng Li",
                        "slug": "Hongsheng-Li",
                        "structuredName": {
                            "firstName": "Hongsheng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongsheng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92887925"
                        ],
                        "name": "Shuo Yang",
                        "slug": "Shuo-Yang",
                        "structuredName": {
                            "firstName": "Shuo",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuo Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40072288"
                        ],
                        "name": "Zhe Wang",
                        "slug": "Zhe-Wang",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331521"
                        ],
                        "name": "Yuanjun Xiong",
                        "slug": "Yuanjun-Xiong",
                        "structuredName": {
                            "firstName": "Yuanjun",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanjun Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005026919"
                        ],
                        "name": "C. Qian",
                        "slug": "C.-Qian",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042558"
                        ],
                        "name": "Zhenyao Zhu",
                        "slug": "Zhenyao-Zhu",
                        "structuredName": {
                            "firstName": "Zhenyao",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenyao Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108694893"
                        ],
                        "name": "Ruohui Wang",
                        "slug": "Ruohui-Wang",
                        "structuredName": {
                            "firstName": "Ruohui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruohui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717179"
                        ],
                        "name": "Chen Change Loy",
                        "slug": "Chen-Change-Loy",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Loy",
                            "middleNames": [
                                "Change"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Change Loy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 230
                            }
                        ],
                        "text": "7 The Chinese University of Hong Kong Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang (Ouyang et al. 2014; Ouyang and Wang 2013) DeepCNet 17."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10426070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51a1a02688d00ead31b6f5177fc7a7cd2e08477f",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose multi-stage and deformable deep convolutional neural networks for object detection. This new deep learning object detection diagram has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (defpooling) layer models the deformation of object parts with geometric constraint and penalty. With the proposed multistage training strategy, multiple classifiers are jointly optimized to process samples at different difficulty levels. A ne w pre-training strategy is proposed to learn feature represe ntations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of modeling averaging. The proposed approach ranked #2 in ILSVRC 2014. It improves the mean averaged precision obtained by RCNN, which is the stateof-the-art of object detection, from31% to 45%. Detailed component-wise analysis is also provided through extensiv e experimental evaluation."
            },
            "slug": "DeepID-Net:-multi-stage-and-deformable-deep-neural-Ouyang-Luo",
            "title": {
                "fragments": [],
                "text": "DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A set of models with large diversity are obtained, which significantly improves the effectiveness of modeling averaging, and improves the mean averaged precision obtained by RCNN, which is the state of the art of object detection, from31% to 45%."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 142
                            }
                        ],
                        "text": "\u2026Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 264
                            }
                        ],
                        "text": "\u2026by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "They trained a large, deep convolutional neural network on RGB values, with 60 million parameters using an efficient GPU implementation and a novel hidden-unit dropout trick (Krizhevsky et al., 2012; Hinton et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 418,
                                "start": 415
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2956488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "338f33472ccaddbde45736a50885a6f9bb28db10",
            "isKey": true,
            "numCitedBy": 106,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual categorization is important to manage large collections of digital images and video, where textual metadata is often incomplete or simply unavailable. The bag-of-words model has become the most powerful method for visual categorization of images and video. Despite its high accuracy, a severe drawback of this model is its high computational cost. As the trend to increase computational power in newer CPU and GPU architectures is to increase their level of parallelism, exploiting this parallelism becomes an important direction to handle the computational cost of the bag-of-words approach. When optimizing a system based on the bag-of-words approach, the goal is to minimize the time it takes to process batches of images. this paper, we analyze the bag-of-words model for visual categorization in terms of computational cost and identify two major bottlenecks: the quantization step and the classification step. We address these two bottlenecks by proposing two efficient algorithms for quantization and classification by exploiting the GPU hardware and the CUDA parallel programming model. The algorithms are designed to (1) keep categorization accuracy intact, (2) decompose the problem, and (3) give the same numerical results. In the experiments on large scale datasets, it is shown that, by using a parallel implementation on the Geforce GTX260 GPU, classifying unseen images is 4.8 times faster than a quad-core CPU version on the Core i7 920, while giving the exact same numerical results. In addition, we show how the algorithms can be generalized to other applications, such as text retrieval and video retrieval. Moreover, when the obtained speedup is used to process extra video frames in a video retrieval benchmark, the accuracy of visual categorization is improved by 29%."
            },
            "slug": "Empowering-Visual-Categorization-With-the-GPU-Sande-Gevers",
            "title": {
                "fragments": [],
                "text": "Empowering Visual Categorization With the GPU"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper analyzed the bag-of-words model for visual categorization in terms of computational cost and identified two major bottlenecks: the quantization step and the classification step and proposes two efficient algorithms for quantization and classification by exploiting the GPU hardware and the CUDA parallel programming model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299479"
                        ],
                        "name": "R. Arandjelovi\u0107",
                        "slug": "R.-Arandjelovi\u0107",
                        "structuredName": {
                            "firstName": "Relja",
                            "lastName": "Arandjelovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arandjelovi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 258
                            }
                        ],
                        "text": "\u2026went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14678946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faae9ed25339426c7b03acf640a106ee84e50703",
            "isKey": true,
            "numCitedBy": 1285,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improvement of the image augmentation method proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of standard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate retrieval speeds. Combining these complementary methods achieves a new state-of-the-art performance on these datasets."
            },
            "slug": "Three-things-everyone-should-know-to-improve-object-Arandjelovi\u0107-Zisserman",
            "title": {
                "fragments": [],
                "text": "Three things everyone should know to improve object retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements, and a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 135
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 196
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 59
                            }
                        ],
                        "text": "Xiaolong Wang (Felzenszwalb et al., 2010) Toronto - - 11.5 University of Toronto Yichuan Tang*, Nitish Srivastava*, Ruslan Salakhutdinov (* = equal contribution) Trimps 26.2 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 79
                            }
                        ],
                        "text": "0 - - Brno University of Technology Martin Kol\u00e1\u0159, Michal Hrad\u01d0s, Pavel Svoboda (Krizhevsky et al., 2012; Mikolov et al., 2013; Jia, 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 432,
                                "start": 423
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 131
                            }
                        ],
                        "text": "0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 15
                            }
                        ],
                        "text": "H. Su Stanford University, Stanford, CA, USA"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 43
                            }
                        ],
                        "text": "4 - \u25e6 - - Fengjun Lv Consulting Fengjun Lv (Krizhevsky et al., 2012; Harel et al., 2007)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 106
                            }
                        ],
                        "text": "2 - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021 (Krizhevsky et al., 2012)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 9
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 35
                            }
                        ],
                        "text": "Acknowledgements We thank Stanford University, UNC Chapel Hill, Google and Facebook for sponsoring the challenges, and NVIDIA for providing computational resources to participants of ILSVRC2014."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 175
                            }
                        ],
                        "text": "They trained a large, deep convolutional neural network on RGB values, with 60 million parameters using an efficient GPU implementation and a novel hidden-unit dropout trick (Krizhevsky et al., 2012; Hinton et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 73
                            }
                        ],
                        "text": "2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "\u2026Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "\u2026Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 142
                            }
                        ],
                        "text": "\u2026Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 35
                            }
                        ],
                        "text": "Paek\u2021, In So Kweon\u2020, Seong Dae Kim\u2020(Krizhevsky et al., 2012; Perronnin et al., 2010) CUHK - - - - - 40."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 87
                            }
                        ],
                        "text": "9 - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 24
                            }
                        ],
                        "text": "Technical Report 07-49, University of Massachusetts, Amherst."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 15
                            }
                        ],
                        "text": "S. Ma Stanford University, Stanford, CA, USA"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang (Krizhevsky et al., 2012)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 9
                            }
                        ],
                        "text": "Stanford University, Stanford, CA, USA\nlenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 135
                            }
                        ],
                        "text": "The major breakthrough came in 2012 with the win of the SuperVision team on image classification and single-object localization tasks (Krizhevsky et al., 2012), and by 2014 all of the top contestants were relying heavily on convolutional neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "3 - \u25e6 - - Institute for Infocomm Research\u2020, Universit Pierre et Marie Curie\u2021 Olivier Morre\u2020\u2021, Hanlin Goh\u2020, Antoine Veillard\u2021, Vijay Chandrasekhar\u2020(Krizhevsky et al., 2012)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 80897,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1600,
                                "start": 1596
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "Xiaolong Wang (Felzenszwalb et al., 2010) Toronto - - 11.5 University of Toronto Yichuan Tang*, Nitish Srivastava*, Ruslan Salakhutdinov (* = equal contribution) Trimps 26.2 - -"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 620,
                                "start": 616
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "In CVPR. Wang, M., Xiao, T., Li, J., Hong, C., Zhang, J., and\nZhang, Z. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Ouyang, W. and Wang, X. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 75
                            }
                        ],
                        "text": "\u2213 Cewu Lu\u2020, Hei Law*\u2020, Hao Chen*\u2021, Qifeng Chen*\u2213, Yao Xiao*\u2020Chi Keung Tang\u2020(Uijlings et al., 2013; Girshick et al., 2013; Perronnin et al., 2010; Felzenszwalb et al., 2010)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 1
                            }
                        ],
                        "text": "(Felzenszwalb et al., 2010) for object localization; SuperVision used a regression model trained to predict bounding box locations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": true,
            "numCitedBy": 9371,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764761"
                        ],
                        "name": "K. Chatfield",
                        "slug": "K.-Chatfield",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Chatfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chatfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7204540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14d9be7962a4ec5a6e55755f4c7588ea00793652",
            "isKey": false,
            "numCitedBy": 3045,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available."
            },
            "slug": "Return-of-the-Devil-in-the-Details:-Delving-Deep-Chatfield-Simonyan",
            "title": {
                "fragments": [],
                "text": "Return of the Devil in the Details: Delving Deep into Convolutional Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost, and it is identified that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "Instead we turn to designing novel crowdsourcing approaches for collecting large-scale annotations (Su et al., 2012; Deng et al., 2009, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 22
                            }
                        ],
                        "text": "The ImageNet dataset (Deng et al., 2009) is the backbone of ILSVRC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 168
                            }
                        ],
                        "text": "Constructing ImageNet was an effort to scale up an image classification dataset to cover most nouns in English using tens of millions of manually verified photographs (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ei-Fei et al., 2004; Criminisi, 2004; Everingham et al., 2012; Xiao et al., 2010). Instead we turn to designing novel crowdsourcing approaches for collecting large-scale annotations (Su et al., 2012; Deng et al., 2009, 2014). Some of the 1000 object classes may not be as easy to annotate as the 20 categories of PASCAL VOC: e.g., bananas which appear in bunches may not be as easy to delineate as the basic-level cat"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 137
                            }
                        ],
                        "text": "New test images are collected and labeled especially for this competition and are not part of the previously published ImageNet dataset (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 92
                            }
                        ],
                        "text": "The 1000 categories used for the image classification task were selected from the ImageNet (Deng et al., 2009) categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 35
                            }
                        ],
                        "text": "We briefly summarize the process; (Deng et al., 2009)\ncontains further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ng synsets in WordNet (Miller, 1995) as queries. However, since this data has not been manually veried, there are many errors, making it less suitable for algorithm evaluation. The ImageNet dataset (Deng et al., 2009) is the backbone of ILSVRC. ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995). Each concept in WordNet, possibly described by multiple words or word phrases, is "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "l for the image (see Section 4.1). Constructing ImageNet was an eort to scale up an image classication dataset to cover most nouns in English using tens of millions of manually veried photographs (Deng et al., 2009). The image classication task of ILSVRC came as a direct extension of this effort. A subset of categories and images was chosen and 3 In addition, ILSVRC in 2012 also included a taster negrained clas"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "e labelings specifying what objects are present in the images. New test images are collected and labeled especially for this competition and are not part of the previously published ImageNet dataset (Deng et al., 2009). 4 Olga Russakovsky* et al. Task Image classication Single-object localization Object detection Manual labeling on training set Number of object classes annotated per image 1 1 or more Locations of "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 116
                            }
                        ],
                        "text": "Evaluation of the accuracy of the large-scale crowdsourced image annotation system was done on the entire ImageNet (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 258
                            }
                        ],
                        "text": "\u2026a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected images using the carefully designed crowdsourcing strategy of ImageNet (Deng et al., 2009) (Section 3.1.3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 95
                            }
                        ],
                        "text": "Annotating images with corresponding object classes follows the strategy employed by ImageNet (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ructing large-scale object recognition image datasets consists of three key steps. The rst step is dening the set of target object categories. To do this, we select from among the existing ImageNet (Deng et al., 2009) categories. By using WordNet as a backbone (Miller, 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category. Since the sele"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 112
                            }
                        ],
                        "text": "Image collection for ILSVRC classification task is the same as the strategy employed for constructing ImageNet (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 56
                            }
                        ],
                        "text": "The system is evaluated on 10 categories with ImageNet (Deng et al., 2009): balloon, bear, bed, bench, beach, bird, bookshelf, basketball hoop, bottle, and people."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 56
                            }
                        ],
                        "text": "To do this, we select from among the existing ImageNet (Deng et al., 2009) categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "isKey": false,
            "numCitedBy": 27367,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066267536"
                        ],
                        "name": "Santiago Man\u00e9n",
                        "slug": "Santiago-Man\u00e9n",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Man\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Man\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737253"
                        ],
                        "name": "M. Guillaumin",
                        "slug": "M.-Guillaumin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Guillaumin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guillaumin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 179
                            }
                        ],
                        "text": "7 The Third Research Institute of the Ministry of Public Security Jie Shao, Xiaoteng Zhang, JianYing Zhou, Jian Wang, Jian Chen, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu (Girshick et al. 2014; Manen et al. 2013; Howard 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4749697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aa259af5b7f1778eca7926ed1b6a6cbaebf28ab",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Generic object detection is the challenging task of proposing windows that localize all the objects in an image, regardless of their classes. Such detectors have recently been shown to benefit many applications such as speeding-up class-specific object detection, weakly supervised learning of object detectors and object discovery. In this paper, we introduce a novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm. Using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object, the algorithm generates random partial spanning trees with large expected sum of edge weights. Object localizations are proposed as bounding-boxes of those partial trees. Our method has several benefits compared to the state-of-the-art. Thanks to the efficiency of Prim's algorithm, it samples proposals very quickly: 1000 proposals are obtained in about 0.7s. With proposals bound to super pixel boundaries yet diversified by randomization, it yields very high detection rates and windows that tightly fit objects. In extensive experiments on the challenging PASCAL VOC 2007 and 2012 and SUN2012 benchmark datasets, we show that our method improves over state-of-the-art competitors for a wide range of evaluation scenarios."
            },
            "slug": "Prime-Object-Proposals-with-Randomized-Prim's-Man\u00e9n-Guillaumin",
            "title": {
                "fragments": [],
                "text": "Prime Object Proposals with Randomized Prim's Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm, using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39971338"
                        ],
                        "name": "Thomas L. Dean",
                        "slug": "Thomas-L.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas L. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154722"
                        ],
                        "name": "Mark A. Ruzon",
                        "slug": "Mark-A.-Ruzon",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ruzon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark A. Ruzon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060304831"
                        ],
                        "name": "Mark E. Segal",
                        "slug": "Mark-E.-Segal",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Segal",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark E. Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259154"
                        ],
                        "name": "Sudheendra Vijayanarasimhan",
                        "slug": "Sudheendra-Vijayanarasimhan",
                        "structuredName": {
                            "firstName": "Sudheendra",
                            "lastName": "Vijayanarasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudheendra Vijayanarasimhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1842163"
                        ],
                        "name": "J. Yagnik",
                        "slug": "J.-Yagnik",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Yagnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yagnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 173
                            }
                        ],
                        "text": "Best paper awards at top vision conferences in 2013 were awarded to large-scale recognition methods: at CVPR 2013 to \u201dFast, Accurate Detection of 100,000 Object Classes on a Single Machine\u201d (Dean et al., 2013) and at ICCV 2013 to \u201dFrom Large Scale Image Categorization to Entry-Level Categories\u201d (Ordonez et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "2013 were awarded to large-scale recognition methods: at CVPR 2013 to \u201cFast, Accurate Detection of 100,000 Object Classes on a Single Machine\u201d (Dean et al. 2013) and at"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 169
                            }
                        ],
                        "text": "\u2026top vision conferences in 2013 were awarded to large-scale recognition methods: at CVPR 2013 to \u201dFast, Accurate Detection of 100,000 Object Classes on a Single Machine\u201d (Dean et al., 2013) and at ICCV 2013 to \u201dFrom Large Scale Image Categorization to Entry-Level Categories\u201d (Ordonez et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2568065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "774f67303ea4a3a94874f08cf9a9dacc69b40782",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes."
            },
            "slug": "Fast,-Accurate-Detection-of-100,000-Object-Classes-Dean-Ruzon",
            "title": {
                "fragments": [],
                "text": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine"
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 13
                            }
                        ],
                        "text": "Caltech 101 (Fei-Fei et al., 2004) was among the first standardized datasets for multi-category image classification, with 101 object classes and commonly 15-30 training images per class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "Some measures have already been established by datasets such as the Caltech 101 (Fei-Fei et al., 2004) for image classification and PASCAL VOC (Everingham et al., 2012) for both image classification and object detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et al., 2004; Criminisi, 2004; Everingham et al., 2012; Xiao et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2",
            "isKey": false,
            "numCitedBy": 2318,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 828465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa5a8ad5b7031ba39e1dc0537484694364a1312",
            "isKey": false,
            "numCitedBy": 2099,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Image category recognition is important to access visual information on the level of objects and scene types. So far, intensity-based descriptors have been widely used for feature extraction at salient points. To increase illumination invariance and discriminative power, color descriptors have been proposed. Because many different descriptors exist, a structured overview is required of color invariant descriptors in the context of image category recognition. Therefore, this paper studies the invariance properties and the distinctiveness of color descriptors (software to compute the color descriptors from this paper is available from http://www.colordescriptors.com) in a structured way. The analytical invariance properties of color descriptors are explored, using a taxonomy based on invariance properties with respect to photometric transformations, and tested experimentally using a data set with known illumination conditions. In addition, the distinctiveness of color descriptors is assessed experimentally using two benchmarks, one from the image domain and one from the video domain. From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition. The results further reveal that, for light intensity shifts, the usefulness of invariance is category-specific. Overall, when choosing a single descriptor and no prior knowledge about the data set and object and scene categories is available, the OpponentSIFT is recommended. Furthermore, a combined set of color descriptors outperforms intensity-based SIFT and improves category recognition by 8 percent on the PASCAL VOC 2007 and by 7 percent on the Mediamill Challenge."
            },
            "slug": "Evaluating-Color-Descriptors-for-Object-and-Scene-Sande-Gevers",
            "title": {
                "fragments": [],
                "text": "Evaluating Color Descriptors for Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition and the usefulness of invariance is category-specific."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 63
                            }
                        ],
                        "text": "of China\u2213 Kaiming He\u2020, Xiangyu Zhang\u2021, Shaoqing Ren\u2213, Jian Sun\u2020(He et al., 2014)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 436933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb19236820a96038d000dc629225d36e0b6294a",
            "isKey": false,
            "numCitedBy": 4764,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition."
            },
            "slug": "Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work equips the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement, and develops a new network structure, called SPP-net, which can generate a fixed-length representation regardless of image size/scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790553"
                        ],
                        "name": "T. Harada",
                        "slug": "T.-Harada",
                        "structuredName": {
                            "firstName": "Tatsuya",
                            "lastName": "Harada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Harada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744602"
                        ],
                        "name": "Y. Kuniyoshi",
                        "slug": "Y.-Kuniyoshi",
                        "structuredName": {
                            "firstName": "Yasuo",
                            "lastName": "Kuniyoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kuniyoshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2239,
                                "start": 2157
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 159
                            }
                        ],
                        "text": "6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya Harada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 176
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 175
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1406064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e33348dad1bad456159d7274c0d86bb7e08743",
            "isKey": true,
            "numCitedBy": 8,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel image representation called a Graphical Gaussian Vector (GGV), which is a counterpart of the codebook and local feature matching approaches. We model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Then we define a new image feature by embedding the proper metric into the parameters, which can be directly applied to scalable linear classifiers. We show that the GGV obtains better performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset."
            },
            "slug": "Graphical-Gaussian-Vector-for-Image-Categorization-Harada-Kuniyoshi",
            "title": {
                "fragments": [],
                "text": "Graphical Gaussian Vector for Image Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper proposes a novel image representation called a Graphical Gaussian Vector (GGV), which is a counterpart of the codebook and local feature matching approaches and obtains better performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2161592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "398c296d0cc7f9d180f84969f8937e6d3a413796",
            "isKey": false,
            "numCitedBy": 3369,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks."
            },
            "slug": "Multi-column-deep-neural-networks-for-image-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Multi-column deep neural networks for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "On the very competitive MNIST handwriting benchmark, this method is the first to achieve near-human performance and improves the state-of-the-art on a plethora of common image classification benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144639556"
                        ],
                        "name": "Graham W. Taylor",
                        "slug": "Graham-W.-Taylor",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Taylor",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham W. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 75
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 145
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u2026Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 53
                            }
                        ],
                        "text": "5 - New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 975170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d743430cb2329caa5d446c17fc9ec07f5e916ab0",
            "isKey": true,
            "numCitedBy": 1027,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods."
            },
            "slug": "Adaptive-deconvolutional-networks-for-mid-and-high-Zeiler-Taylor",
            "title": {
                "fragments": [],
                "text": "Adaptive deconvolutional networks for mid and high level feature learning"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling, relying on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918391"
                        ],
                        "name": "Yodsawalai Chodpathumwan",
                        "slug": "Yodsawalai-Chodpathumwan",
                        "structuredName": {
                            "firstName": "Yodsawalai",
                            "lastName": "Chodpathumwan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yodsawalai Chodpathumwan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279233"
                        ],
                        "name": "Qieyun Dai",
                        "slug": "Qieyun-Dai",
                        "structuredName": {
                            "firstName": "Qieyun",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qieyun Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 16
                            }
                        ],
                        "text": "As described in (Hoiem et al., 2012), smaller objects tend to be significantly more difficult to local-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 17
                            }
                        ],
                        "text": "As described in (Hoiem et al., 2012), smaller objects tend to be significantly more difficult to local-\nImageNet Large Scale Visual Recognition Challenge 9\nize."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6650709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f877575217f0868f496a6123954b2ea933fb21e",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives. In particular, we examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects, other labeled objects, and background. We analyze two classes of detectors: the Vedaldi et al. multiple kernel learning detector and different versions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error. Our analysis also reveals that many different kinds of improvement are necessary to achieve large gains, making more detailed analysis essential for the progress of recognition research. By making our software and annotations available, we make it effortless for future researchers to perform similar analysis."
            },
            "slug": "Diagnosing-Error-in-Object-Detectors-Hoiem-Chodpathumwan",
            "title": {
                "fragments": [],
                "text": "Diagnosing Error in Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives, and shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1344,
                                "start": 1316
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 151
                            }
                        ],
                        "text": "\u2026went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 390,
                                "start": 385
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 90
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 187
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16199577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175",
            "isKey": true,
            "numCitedBy": 298,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. We report results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art."
            },
            "slug": "High-dimensional-signature-compression-for-image-S\u00e1nchez-Perronnin",
            "title": {
                "fragments": [],
                "text": "High-dimensional signature compression for large-scale image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work reports results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that it can reduce the storage of the authors' signatures by a factor 64 to 128 with little loss in accuracy and integrating the decompression in the classifier learning yields an efficient and scalable training algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108863279"
                        ],
                        "name": "Thomas Huang",
                        "slug": "Thomas-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "5 - \u25e6 - - University of Isfahan Fatemeh Shafizadegan, Elham Shabaninia (Yang et al., 2009)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 440212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c9633aedafe4ee8cf238fa06c40b84f47e17362",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors."
            },
            "slug": "Linear-spatial-pyramid-matching-using-sparse-coding-Yang-Yu",
            "title": {
                "fragments": [],
                "text": "Linear spatial pyramid matching using sparse coding for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An extension of the SPM method is developed, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and a linear SPM kernel based on SIFT sparse codes is proposed, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390562671"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144228782"
                        ],
                        "name": "T. Campos",
                        "slug": "T.-Campos",
                        "structuredName": {
                            "firstName": "Te\u00f3filo",
                            "lastName": "Campos",
                            "middleNames": [
                                "Em\u00eddio",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Campos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 275
                            }
                        ],
                        "text": "\u2026went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2544,
                                "start": 2503
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9892293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3babec90d7234913e792462bd336637ef7ae3651",
            "isKey": true,
            "numCitedBy": 123,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-the-spatial-layout-of-images-beyond-S\u00e1nchez-Perronnin",
            "title": {
                "fragments": [],
                "text": "Modeling the spatial layout of images beyond spatial pyramids"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985062"
                        ],
                        "name": "Marwan A. Mattar",
                        "slug": "Marwan-A.-Mattar",
                        "structuredName": {
                            "firstName": "Marwan",
                            "lastName": "Mattar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marwan A. Mattar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404579703"
                        ],
                        "name": "Eric Learned-Miller",
                        "slug": "Eric-Learned-Miller",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Learned-Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "d 3.3.3. Standardized challenges. There are several datasets with standardized online evaluation similar to ILSVRC: the aforementioned PASCAL VOC (Everingham et al., 2012), Labeled Faces in the Wild (Huang et al., 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al., 2014) for 3D reconstruction and KITTI (Geiger et al., 2013) for computer vision in autonomous driving. These data"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
            "isKey": true,
            "numCitedBy": 4896,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version."
            },
            "slug": "Labeled-Faces-in-the-Wild:-A-Database-forStudying-Huang-Mattar",
            "title": {
                "fragments": [],
                "text": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life, and exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302783"
                        ],
                        "name": "Sirion Vittayakorn",
                        "slug": "Sirion-Vittayakorn",
                        "structuredName": {
                            "firstName": "Sirion",
                            "lastName": "Vittayakorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sirion Vittayakorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 58
                            }
                        ],
                        "text": "Works such as (Welinder et al., 2010; Sheng et al., 2008; Vittayakorn and Hays, 2011) describe quality control mechanisms for this marketplace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1616464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de26a4f0d88d0c831aecca97cc77c359427a0390",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "As computer vision datasets grow larger the community is increasingly relying on crowdsourced annotations to train and test our algorithms. Due to the heterogeneous and unpredictable capability of online annotators, various strategies have been proposed to \u201cclean\u201d crowdsourced annotations. However, these strategies typically involve getting more annotations, perhaps different types of annotations (e.g. a grading task), rather than computationally assessing the annotation or image content. In this paper we propose and evaluate several strategies for automatically estimating the quality of a spatial object annotation. We show that one can significantly outperform simple baselines, such as that used by LabelMe, by combining multiple image-based annotation assessment strategies."
            },
            "slug": "Quality-Assessment-for-Crowdsourced-Object-Vittayakorn-Hays",
            "title": {
                "fragments": [],
                "text": "Quality Assessment for Crowdsourced Object Annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that one can significantly outperform simple baselines, such as that used by LabelMe, by combining multiple image-based annotation assessment strategies and is proposed and evaluated for automatically estimating the quality of a spatial object annotation."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001348"
                        ],
                        "name": "Wanli Ouyang",
                        "slug": "Wanli-Ouyang",
                        "structuredName": {
                            "firstName": "Wanli",
                            "lastName": "Ouyang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanli Ouyang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 230
                            }
                        ],
                        "text": "7 The Chinese University of Hong Kong Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang (Ouyang et al., 2014; Ouyang and Wang, 2013) DeepCNet 17."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14860163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b9e0187c0a3850c9d0e6833640b6ab8a1623bfc",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset."
            },
            "slug": "Joint-Deep-Learning-for-Pedestrian-Detection-Ouyang-Wang",
            "title": {
                "fragments": [],
                "text": "Joint Deep Learning for Pedestrian Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper forms these four important components in pedestrian detection into a joint deep learning framework and proposes a new deep network architecture that achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sets in WordNet (Miller, 1995) as queries. However, since this data has not been manually veried, there are many errors, making it less suitable for algorithm evaluation. Datasets such as 15 Scenes (Oliva and Torralba, 2001; Fei-Fei and Perona, 2005; Lazebnik et al., 2006) or recent Places (Zhou et al., 2014) provide a single scene category label (as opposed to an object category). The ImageNet dataset (Deng et al., 200"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11664336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "869171b2f56cfeaa9b81b2626cb4956fea590a57",
            "isKey": false,
            "numCitedBy": 6522,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "slug": "Modeling-the-Shape-of-the-Scene:-A-Holistic-of-the-Oliva-Torralba",
            "title": {
                "fragments": [],
                "text": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 109
                            }
                        ],
                        "text": "3 - - Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou, 2014; Papandreou et al., 2014; Jojic et al., 2003; Krizhevsky et al., 2012; Sermanet et al., 2013; Dubout and Fleuret, 2012; Iandola et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18815368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "665da8c71dfc8147ef89430c5e5eb486f4077ed0",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the small-image MNIST and CIFAR-10 datasets."
            },
            "slug": "Deep-Epitomic-Convolutional-Neural-Networks-Papandreou",
            "title": {
                "fragments": [],
                "text": "Deep Epitomic Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes the epitomic convolution as a new building block for deep neural networks and shows that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114790150"
                        ],
                        "name": "Zheng Song",
                        "slug": "Zheng-Song",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35370244"
                        ],
                        "name": "Qiang Chen",
                        "slug": "Qiang-Chen",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109669984"
                        ],
                        "name": "Zhongyang Huang",
                        "slug": "Zhongyang-Huang",
                        "structuredName": {
                            "firstName": "Zhongyang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhongyang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057199302"
                        ],
                        "name": "Yang Hua",
                        "slug": "Yang-Hua",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 136
                            }
                        ],
                        "text": "2 \u2013 National University of Singapore\u2020, IBM Research Australia\u2021 Jian Dong\u2020, Yunchao Wei\u2020, Min Lin\u2020, Qiang Chen\u2021, Wei Xia\u2020, Shuicheng Yan\u2020(Lin et al. 2014a; Chen et al. 2014) NUS-BST 9."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "mation was incorporated following (Chen et al. 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "Global context information was incorporated following (Chen et al., 2014)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3806655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2279e4dd2b073c6e314cf80e690130c1412b0df0",
            "isKey": true,
            "numCitedBy": 147,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate how to iteratively and mutually boost object classification and detection performance by taking the outputs from one task as the context of the other one. While context models have been quite popular, previous works mainly concentrate on co-occurrence relationship within classes and few of them focus on contextualization from a top-down perspective, i.e. high-level task context. In this paper, our system adopts a new method for adaptive context modeling and iterative boosting. First, the contextualized support vector machine (Context-SVM) is proposed, where the context takes the role of dynamically adjusting the classification score based on the sample ambiguity, and thus the context-adaptive classifier is achieved. Then, an iterative training procedure is presented. In each step, Context-SVM, associated with the output context from one task (object classification or detection), is instantiated to boost the performance for the other task, whose augmented outputs are then further used to improve the former task by Context-SVM. The proposed solution is evaluated on the object classification and detection tasks of PASCAL Visual Object Classes Challenge (VOC) 2007, 2010 and SUN09 data sets, and achieves the state-of-the-art performance."
            },
            "slug": "Contextualizing-Object-Detection-and-Classification-Song-Chen",
            "title": {
                "fragments": [],
                "text": "Contextualizing Object Detection and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper adopts a new method for adaptive context modeling and iterative boosting that achieves the state-of-the-art performance on object classification and detection tasks of PASCAL Visual Object Classes Challenge (VOC) 2007, 2010 and SUN09 data sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39810944"
                        ],
                        "name": "Jonathan Harel",
                        "slug": "Jonathan-Harel",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Harel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Harel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 43
                            }
                        ],
                        "text": "4 - \u25e6 - - Fengjun Lv Consulting Fengjun Lv (Krizhevsky et al., 2012; Harel et al., 2007)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 629401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f412bb31ec9ef8bbef70eefc7ffd04420c1365d9",
            "isKey": false,
            "numCitedBy": 3394,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: first forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2], [3], [4]) achieve only 84%."
            },
            "slug": "Graph-Based-Visual-Saliency-Harel-Koch",
            "title": {
                "fragments": [],
                "text": "Graph-Based Visual Saliency"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed, which powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch achieve only 84%."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97911988"
                        ],
                        "name": "Richard Fulton",
                        "slug": "Richard-Fulton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Fulton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Fulton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 47
                            }
                        ],
                        "text": "23 object classes, Stanford Background Dataset (Gould et al., 2009) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "\u2026provide pixel-level segmentations: for example, MSRC dataset (Criminisi, 2004) with 591 images and 23 object classes, Stanford Background Dataset (Gould et al., 2009) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et al., 2011) with 500 images annotated with\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Several datasets provide pixel-level segmentations: for example, MSRC dataset (Criminisi, 2004) with 591 images and 23 object classes, Stanford Background Dataset (Gould et al., 2009) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et al., 2011) with 500 images annotated with object boundaries."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17448963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc08847b65953ef2ae3542e47b08b57a46b5ba34",
            "isKey": true,
            "numCitedBy": 709,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene."
            },
            "slug": "Decomposing-a-scene-into-geometric-and-semantically-Gould-Fulton",
            "title": {
                "fragments": [],
                "text": "Decomposing a scene into geometric and semantically consistent regions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions and which achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760556"
                        ],
                        "name": "C. Stiller",
                        "slug": "C.-Stiller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Stiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 230
                            }
                        ],
                        "text": "\u2026the aforementioned PASCAL VOC (Everingham et al., 2012), Labeled Faces in the Wild (Huang et al., 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al., 2014) for 3D reconstruction and KITTI (Geiger et al., 2013) for computer vision in autonomous driving."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 309,
                                "start": 304
                            }
                        ],
                        "text": "There are several datasets with standardized online evaluation similar to ILSVRC: the aforementioned PASCAL VOC (Everingham et al., 2012), Labeled Faces in the Wild (Huang et al., 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al., 2014) for 3D reconstruction and KITTI (Geiger et al., 2013) for computer vision in autonomous driving."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "tion and KITTI (Geiger et al., 2013) for computer vision in autonomous driving."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9455111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b949d9b35c3f51dd20fb5c746cc81fc87147eb",
            "isKey": true,
            "numCitedBy": 4616,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10\u2013100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide."
            },
            "slug": "Vision-meets-robotics:-The-KITTI-dataset-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Vision meets robotics: The KITTI dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research, using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras and a high-precision GPS/IMU inertial navigation system."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403171438"
                        ],
                        "name": "J. Pont-Tuset",
                        "slug": "J.-Pont-Tuset",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Pont-Tuset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pont-Tuset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065306202"
                        ],
                        "name": "Jonathan T. Barron",
                        "slug": "Jonathan-T.-Barron",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Barron",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan T. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854725"
                        ],
                        "name": "F. Marqu\u00e9s",
                        "slug": "F.-Marqu\u00e9s",
                        "structuredName": {
                            "firstName": "Ferran",
                            "lastName": "Marqu\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Marqu\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4517687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83d1118c2b2995a3e0cf9b6159e4c59e85cabb7e",
            "isKey": false,
            "numCitedBy": 1039,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates."
            },
            "slug": "Multiscale-Combinatorial-Grouping-Arbel\u00e1ez-Pont-Tuset",
            "title": {
                "fragments": [],
                "text": "Multiscale Combinatorial Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work first develops a fast normalized cuts algorithm, then proposes a high-performance hierarchical segmenter that makes effective use of multiscale information, and proposes a grouping strategy that combines the authors' multiscales regions into highly-accurate object candidates by exploring efficiently their combinatorial space."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "\u2026Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "The winner of object detection task was UvA team, which utilized a new way of efficient encoding (van de Sande et al., 2014) of densely sampled color descriptors (van de Sande et al., 2010) pooled using a multilevel spatial pyramid in a selective search framework (Uijlings et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12316633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "744e5c8de8703b9ba28a798c0f2c2781000ea93c",
            "isKey": true,
            "numCitedBy": 64,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A major computational bottleneck in many current algorithms is the evaluation of arbitrary boxes. Dense local analysis and powerful bag-of-word encodings, such as Fisher vectors and VLAD, lead to improved accuracy at the expense of increased computation time. Where a simplification in the representation is tempting, we exploit novel representations while maintaining accuracy. We start from state-of-the-art, fast selective search, but our method will apply to any initial box-partitioning. By representing the picture as sparse integral images, one per codeword, we achieve a Fast Local Area Independent Representation. FLAIR allows for very fast evaluation of any box encoding and still enables spatial pooling. In FLAIR we achieve exact VLAD's difference coding, even with L2 and power-norms. Finally, by multiple codeword assignments, we achieve exact and approximate Fisher vectors with FLAIR. The results are a 18x speedup, which enables us to set a new state-of-the-art on the challenging 2010 PASCAL VOC objects and the fine-grained categorization of the CUB-2011 200 bird species. Plus, we rank number one in the official ImageNet 2013 detection challenge."
            },
            "slug": "Fisher-and-VLAD-with-FLAIR-Sande-Snoek",
            "title": {
                "fragments": [],
                "text": "Fisher and VLAD with FLAIR"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work starts from state-of-the-art, fast selective search, and achieves a Fast Local Area Independent Representation with FLAIR, which allows for very fast evaluation of any box encoding and still enables spatial pooling."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9739979"
                        ],
                        "name": "P. Arbel\u00e1ez",
                        "slug": "P.-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1505240324"
                        ],
                        "name": "C. Fowlkes",
                        "slug": "C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charlotte",
                            "lastName": "Fowlkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787589"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206764694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
            "isKey": false,
            "numCitedBy": 4196,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications."
            },
            "slug": "Contour-Detection-and-Hierarchical-Image-Arbel\u00e1ez-Maire",
            "title": {
                "fragments": [],
                "text": "Contour Detection and Hierarchical Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation and presents state-of-the-art algorithms for both of these tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720207"
                        ],
                        "name": "Charles Dubout",
                        "slug": "Charles-Dubout",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dubout",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Dubout"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721983"
                        ],
                        "name": "F. Fleuret",
                        "slug": "F.-Fleuret",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Fleuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fleuret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 109
                            }
                        ],
                        "text": "3 - - Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou, 2014; Papandreou et al., 2014; Jojic et al., 2003; Krizhevsky et al., 2012; Sermanet et al., 2013; Dubout and Fleuret, 2012; Iandola et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 928791,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "eae19367105ad161f2003d34d6f3bd05ccc8c624",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general and exact method to considerably speed up linear object detection systems operating in a sliding, multi-scale window fashion, such as the individual part detectors of part-based models. The main bottleneck of many of those systems is the computational cost of the convolutions between the multiple rescalings of the image to process, and the linear filters. We make use of properties of the Fourier transform and of clever implementation strategies to obtain a speedup factor proportional to the filters' sizes. The gain in performance is demonstrated on the well known Pascal VOC benchmark, where we accelerate the speed of said convolutions by an order of magnitude."
            },
            "slug": "Exact-Acceleration-of-Linear-Object-Detectors-Dubout-Fleuret",
            "title": {
                "fragments": [],
                "text": "Exact Acceleration of Linear Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A general and exact method to considerably speed up linear object detection systems operating in a sliding, multi-scale window fashion, such as the individual part detectors of part-based models, by making use of properties of the Fourier transform and of clever implementation strategies."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71563118"
                        ],
                        "name": "Jinjun Wang",
                        "slug": "Jinjun-Wang",
                        "structuredName": {
                            "firstName": "Jinjun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinjun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 175
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 58
                            }
                        ],
                        "text": "2006) features with two non-linear coding representations (Zhou et al. 2010; Wang et al. 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6718692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7713dcc35e7c05becf3be5522f36c9546b0364",
            "isKey": false,
            "numCitedBy": 3240,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications."
            },
            "slug": "Locality-constrained-Linear-Coding-for-image-Wang-Yang",
            "title": {
                "fragments": [],
                "text": "Locality-constrained Linear Coding for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM, using the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144499674"
                        ],
                        "name": "Sean Bell",
                        "slug": "Sean-Bell",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222840"
                        ],
                        "name": "P. Upchurch",
                        "slug": "P.-Upchurch",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Upchurch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Upchurch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374926"
                        ],
                        "name": "K. Bala",
                        "slug": "K.-Bala",
                        "structuredName": {
                            "firstName": "Kavita",
                            "lastName": "Bala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " images annotated with object boundaries. OpenSurfaces segments surfaces from consumer photographs and annotates them with surface properties, including material, texture, and contextual information (Bell et al., 2013) . The closest to ILSVRC is the PASCAL VOC dataset (Everingham et al., 2010, 2014), which provides a standardized test bed for object detection, image classi- cation, object segmentation, person layo"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12176541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43e48b702fbe1feba53afbf82ec322cc9a61ae6c",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The appearance of surfaces in real-world scenes is determined by the materials, textures, and context in which the surfaces appear. However, the datasets we have for visualizing and modeling rich surface appearance in context, in applications such as home remodeling, are quite limited. To help address this need, we present OpenSurfaces, a rich, labeled database consisting of thousands of examples of surfaces segmented from consumer photographs of interiors, and annotated with material parameters (reflectance, material names), texture information (surface normals, rectified textures), and contextual information (scene category, and object names). Retrieving usable surface information from uncalibrated Internet photo collections is challenging. We use human annotations and present a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk). Because of the noise and variability inherent in Internet photos and novice annotators, designing this annotation engine was a key challenge; we present a multi-stage set of annotation tasks with quality checks and validation. We demonstrate the use of this database in proof-of-concept applications including surface retexturing and material and image browsing, and discuss future uses. OpenSurfaces is a public resource available at http://opensurfaces.cs.cornell.edu/."
            },
            "slug": "OpenSurfaces:-a-richly-annotated-catalog-of-surface-Bell-Upchurch",
            "title": {
                "fragments": [],
                "text": "OpenSurfaces: a richly annotated catalog of surface appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work uses human annotations and presents a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk), and designs a multi-stage set of annotation tasks with quality checks and validation."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34312504"
                        ],
                        "name": "Yichuan Tang",
                        "slug": "Yichuan-Tang",
                        "structuredName": {
                            "firstName": "Yichuan",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichuan Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 89
                            }
                        ],
                        "text": "9 \u2013 \u2013 Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al. 2012; Wan et al. 2013; Tang 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 897,
                                "start": 815
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 142
                            }
                        ],
                        "text": "\u2026et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2895645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96c9f11fd9901f2edeaab8cf6bbff2590cea93c4",
            "isKey": true,
            "numCitedBy": 146,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, fully-connected and convolutional neural networks have been trained to reach state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics data. For classification tasks, much of these \u201cdeep learning\u201d models employ the softmax activation functions to learn output labels in 1-of-K format. In this paper, we demonstrate a small but consistent advantage of replacing softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs. In contrast to those models, we are proposing to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers. Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop\u2019s face expression recognition challenge."
            },
            "slug": "Deep-Learning-using-Support-Vector-Machines-Tang",
            "title": {
                "fragments": [],
                "text": "Deep Learning using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers, and demonstrates a small but consistent advantage of replacing softmax layer with a linear support vector machine."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930640"
                        ],
                        "name": "P. Welinder",
                        "slug": "P.-Welinder",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Welinder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Welinder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3251767"
                        ],
                        "name": "Steve Branson",
                        "slug": "Steve-Branson",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Branson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Branson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "Works such as (Welinder et al., 2010; Sheng et al., 2008; Vittayakorn and Hays, 2011) describe quality control mechanisms for this marketplace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16484321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2e250b4b49a9aa04b68dfd40dc69b022b1f8b3d",
            "isKey": false,
            "numCitedBy": 782,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different \"schools of thought\" amongst the annotators, and can group together images belonging to separate categories."
            },
            "slug": "The-Multidimensional-Wisdom-of-Crowds-Welinder-Branson",
            "title": {
                "fragments": [],
                "text": "The Multidimensional Wisdom of Crowds"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method for estimating the underlying value of each image from (noisy) annotations provided by multiple annotators, based on a model of the image formation and annotation process, which predicts ground truth labels on both synthetic and real data more accurately than state of the art methods."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613438"
                        ],
                        "name": "W. Scheirer",
                        "slug": "W.-Scheirer",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Scheirer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Scheirer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996078"
                        ],
                        "name": "Neeraj Kumar",
                        "slug": "Neeraj-Kumar",
                        "structuredName": {
                            "firstName": "Neeraj",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neeraj Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32163276"
                        ],
                        "name": "T. Boult",
                        "slug": "T.-Boult",
                        "structuredName": {
                            "firstName": "Terrance",
                            "lastName": "Boult",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Boult"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. van de Sande, K. E. A., Uijlings, J. R. R., Gevers, T.,\nand Smeulders, A. W. M. (2011b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1834,
                                "start": 1828
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "Uijlings, J., van de Sande, K., Gevers, T., and Smeul-\nders, A. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1582\u20131596. van de Sande, K. E. A., Gevers, T., and Snoek, C. G. M.\n(2011a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 52
                            }
                        ],
                        "text": "IEEE Transactions on Multimedia, 13(1):60\u201370.\nvan de Sande, K. E. A., Snoek, C. G. M., and Smeul-\nders, A. W. M. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 442,
                                "start": 436
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "Reconstruction meets recognition challenge. http://ttic.uchicago.edu/ ~rurtasun/rmrc/. van de Sande, K. E. A., Gevers, T., and Snoek, C.\nG. M. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "\u2026of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10497591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7999992e32be8c3616028023faf5b138bcb6b4",
            "isKey": true,
            "numCitedBy": 181,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that visual attributes are a powerful approach for applications such as recognition, image description and retrieval. However, fusing multiple attribute scores - as required during multi-attribute queries or similarity searches - presents a significant challenge. Scores from different attribute classifiers cannot be combined in a simple way; the same score for different attributes can mean different things. In this work, we show how to construct normalized \u201cmulti-attribute spaces\u201d from raw classifier outputs, using techniques based on the statistical Extreme Value Theory. Our method calibrates each raw score to a probability that the given attribute is present in the image. We describe how these probabilities can be fused in a simple way to perform more accurate multiattribute searches, as well as enable attribute-based similarity searches. A significant advantage of our approach is that the normalization is done after-the-fact, requiring neither modification to the attribute classification system nor ground truth attribute annotations. We demonstrate results on a large data set of nearly 2 million face images and show significant improvements over prior work. We also show that perceptual similarity of search results increases by using contextual attributes."
            },
            "slug": "Multi-attribute-spaces:-Calibration-for-attribute-Scheirer-Kumar",
            "title": {
                "fragments": [],
                "text": "Multi-attribute spaces: Calibration for attribute fusion and similarity search"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work shows how to construct normalized \u201cmulti-attribute spaces\u201d from raw classifier outputs, using techniques based on the statistical Extreme Value Theory, and shows that perceptual similarity of search results increases by using contextual attributes."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145721096"
                        ],
                        "name": "A. Kannan",
                        "slug": "A.-Kannan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kannan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 109
                            }
                        ],
                        "text": "3 - - Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou, 2014; Papandreou et al., 2014; Jojic et al., 2003; Krizhevsky et al., 2012; Sermanet et al., 2013; Dubout and Fleuret, 2012; Iandola et al., 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 793640,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "581db5af31e5c557015aefc993c6bbfca4e9150f",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present novel simple appearance and shape models that we call epitomes. The epitome of an image is its miniature, condensed version containing the essence of the textural and shape properties of the image. As opposed to previously used simple image models, such as templates or basis functions, the size of the epitome is considerably smaller than the size of the image or object it represents, but the epitome still contains most constitutive elements needed to reconstruct the image. A collection of images often shares an epitome, e.g., when images are a few consecutive frames from a video sequence, or when they are photographs of similar objects. A particular image in a collection is defined by its epitome and a smooth mapping from the epitome to the image pixels. When the epitomic representation is used within a hierarchical generative model, appropriate inference algorithms can be derived to extract the epitome from a single image or a collection of images and at the same time perform various inference tasks, such as image segmentation, motion estimation, object removal and super-resolution."
            },
            "slug": "Epitomic-analysis-of-appearance-and-shape-Jojic-Frey",
            "title": {
                "fragments": [],
                "text": "Epitomic analysis of appearance and shape"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The epitome of an image is its miniature, condensed version containing the essence of the textural and shape properties of the image, as opposed to previously used simple image models, such as templates or basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115913164"
                        ],
                        "name": "Min Lin",
                        "slug": "Min-Lin",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35370244"
                        ],
                        "name": "Qiang Chen",
                        "slug": "Qiang-Chen",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 48
                            }
                        ],
                        "text": "The recently released large-scale COCO dataset (Lin et al., 2014b) is already taking a step in that direction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 834
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 36
                            }
                        ],
                        "text": "The recently released COCO dataset (Lin et al., 2014b) contains more than 328,000 images with 2.5 million object instances manually segmented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CVPR. Yang, J., Yu, K., Gong, Y., and Huang, T. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "In CVPR. Yao, B., Yang, X., and Zhu, S.-C. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 63
                            }
                        ],
                        "text": "2 - \u25e6 - - The University of Queensland Zhongwen Xu and Yi Yang (Krizhevsky et al., 2012; Jia, 2013; Zeiler and Fergus, 2013; Lin et al., 2014a)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 134
                            }
                        ],
                        "text": "2 National University of Singapore\u2020, IBM Research Australia\u2021 Jian Dong\u2020, Yunchao Wei\u2020, Min Lin\u2020, Qiang Chen\u2021, Wei Xia\u2020, Shuicheng Yan\u2020(Lin et al., 2014a; Chen et al., 2014)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 155
                            }
                        ],
                        "text": "In the object detection with provided data track, the winning team NUS used the RCNN framework (Girshick et al., 2013) with the network-in-network method (Lin et al., 2014a) incorporating improvements of (Howard, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 43
                            }
                        ],
                        "text": ", 2013) with the network-in-network method (Lin et al., 2014a) incorporating improvements of (Howard, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 117
                            }
                        ],
                        "text": "of Singapore\u2020, Beijing Samsung Telecom R&D Center\u2020 Min Lin\u2020, Jian Dong\u2020, Hanjiang Lai\u2020, Junjun Xiong\u2021, Shuicheng Yan\u2020(Lin et al., 2014a; Howard, 2014; Krizhevsky et al., 2012)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16636683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "isKey": true,
            "numCitedBy": 4208,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
            },
            "slug": "Network-In-Network-Lin-Chen",
            "title": {
                "fragments": [],
                "text": "Network In Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "With enhanced local modeling via the micro network, the proposed deep network structure NIN is able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13755,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37825612"
                        ],
                        "name": "Harsh Agrawal",
                        "slug": "Harsh-Agrawal",
                        "structuredName": {
                            "firstName": "Harsh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harsh Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085140"
                        ],
                        "name": "Clint Solomon Mathialagan",
                        "slug": "Clint-Solomon-Mathialagan",
                        "structuredName": {
                            "firstName": "Clint",
                            "lastName": "Mathialagan",
                            "middleNames": [
                                "Solomon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clint Solomon Mathialagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37226164"
                        ],
                        "name": "Yash Goyal",
                        "slug": "Yash-Goyal",
                        "structuredName": {
                            "firstName": "Yash",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yash Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942727"
                        ],
                        "name": "Neelima Chavali",
                        "slug": "Neelima-Chavali",
                        "structuredName": {
                            "firstName": "Neelima",
                            "lastName": "Chavali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neelima Chavali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954423"
                        ],
                        "name": "Prakriti Banik",
                        "slug": "Prakriti-Banik",
                        "structuredName": {
                            "firstName": "Prakriti",
                            "lastName": "Banik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prakriti Banik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884573"
                        ],
                        "name": "Akrit Mohapatra",
                        "slug": "Akrit-Mohapatra",
                        "structuredName": {
                            "firstName": "Akrit",
                            "lastName": "Mohapatra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akrit Mohapatra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144261075"
                        ],
                        "name": "Ahmed A. A. Osman",
                        "slug": "Ahmed-A.-A.-Osman",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Osman",
                            "middleNames": [
                                "A.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmed A. A. Osman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13419445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2019216f08b576e1d5503d291a9cca3d3ceafb12",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We are witnessing a proliferation of massive visual data. Unfortunately scaling existing computer vision algorithms to large datasets leaves researchers repeatedly solving the same algorithmic, logistical, and infrastructural problems. Our goal is to democratize computer vision; one should not have to be a computer vision, big data and distributed computing expert to have access to state-of-the-art distributed computer vision algorithms. We present CloudCV, a comprehensive system to provide access to state-of-the-art distributed computer vision algorithms as a cloud service through a Web Interface and APIs."
            },
            "slug": "CloudCV:-Large-Scale-Distributed-Computer-Vision-as-Agrawal-Mathialagan",
            "title": {
                "fragments": [],
                "text": "CloudCV: Large-Scale Distributed Computer Vision as a Cloud Service"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The goal is to democratize computer vision; one should not have to be a computer vision, big data and distributed computing expert to have access to state-of-the-art distributed computer vision algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Mobile Cloud Visual Media Computing"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712289"
                        ],
                        "name": "Donald J. Patterson",
                        "slug": "Donald-J.-Patterson",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Patterson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald J. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Vondrick et al., 2012) provides a detailed overview of crowdsourcing video annotation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2315620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a061e7eab865fc8d2ef00e029b7070719ad2e9a",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extensive three year study on economically annotating video with crowdsourced marketplaces. Our public framework has annotated thousands of real world videos, including massive data sets unprecedented for their size, complexity, and cost. To accomplish this, we designed a state-of-the-art video annotation user interface and demonstrate that, despite common intuition, many contemporary interfaces are sub-optimal. We present several user studies that evaluate different aspects of our system and demonstrate that minimizing the cognitive load of the user is crucial when designing an annotation platform. We then deploy this interface on Amazon Mechanical Turk and discover expert and talented workers who are capable of annotating difficult videos with dense and closely cropped labels. We argue that video annotation requires specialized skill; most workers are poor annotators, mandating robust quality control protocols. We show that traditional crowdsourced micro-tasks are not suitable for video annotation and instead demonstrate that deploying time-consuming macro-tasks on MTurk is effective. Finally, we show that by extracting pixel-based features from manually labeled key frames, we are able to leverage more sophisticated interpolation strategies to maximize performance given a fixed budget. We validate the power of our framework on difficult, real-world data sets and we demonstrate an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling. We further introduce a novel, cost-based evaluation criteria that compares vision algorithms by the budget required to achieve an acceptable performance. We hope our findings will spur innovation in the creation of massive labeled video data sets and enable novel data-driven computer vision applications."
            },
            "slug": "Efficiently-Scaling-up-Crowdsourced-Video-Vondrick-Patterson",
            "title": {
                "fragments": [],
                "text": "Efficiently Scaling up Crowdsourced Video Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that video annotation requires specialized skill; most workers are poor annotators, mandating robust quality control protocols and an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": false,
            "numCitedBy": 6189,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48345000"
                        ],
                        "name": "T. Ahonen",
                        "slug": "T.-Ahonen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ahonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ahonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144979251"
                        ],
                        "name": "A. Hadid",
                        "slug": "A.-Hadid",
                        "structuredName": {
                            "firstName": "Abdenour",
                            "lastName": "Hadid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hadid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 369876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
            "isKey": false,
            "numCitedBy": 5461,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed"
            },
            "slug": "Face-Description-with-Local-Binary-Patterns:-to-Ahonen-Hadid",
            "title": {
                "fragments": [],
                "text": "Face Description with Local Binary Patterns: Application to Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features that is assessed in the face recognition problem under different challenges."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9286705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ab9d01469b542434e0e47428aa25e233824ea80",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We study strategies for scalable multi-label annotation, or for efficiently acquiring multiple labels from humans for a collection of items. We propose an algorithm that exploits correlation, hierarchy, and sparsity of the label distribution. A case study of labeling 200 objects using 20,000 images demonstrates the effectiveness of our approach. The algorithm results in up to 6x reduction in human computation time compared to the naive method of querying a human annotator for the presence of every object in every image."
            },
            "slug": "Scalable-multi-label-annotation-Deng-Russakovsky",
            "title": {
                "fragments": [],
                "text": "Scalable multi-label annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm that exploits correlation, hierarchy, and sparsity of the label distribution is proposed that results in up to 6x reduction in human computation time compared to the naive method of querying a human annotator for the presence of every object in every image."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2919556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fbed28aba6e9a0967b57d8ce87847bf623d4ba8",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, would improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and would help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don\u2019t work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "ESP:-Labeling-Images-with-a-Computer-Game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "ESP: Labeling Images with a Computer Game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143853801"
                        ],
                        "name": "Benjamin Graham",
                        "slug": "Benjamin-Graham",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 45
                            }
                        ],
                        "text": "5 \u2013 \u25e6 \u2013 \u2013 \u2013 University of Warwick Ben Graham (Graham 2013; Schmidhuber 2012)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1071800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f464d85f25fb5ec36a3194f27a31b6dbf91323f",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In mathematics the signature of a path is a collection of iterated integrals, commonly used for solving dierential equations. We show that the path signature, used as a set of features for consumption by a convolutional neural network (CNN), improves the accuracy of online character recognition|that is the task of reading characters represented as a collection of paths. Using datasets of letters, numbers, Assamese and Chinese characters, we show that the rst, second, and even the third iterated integrals contain useful information for consumption by a CNN. On the CASIA-OLHWDB1.1 3755 Chinese character dataset, our approach gave a test error of 3.58%, compared with 5.61%[4] for a traditional CNN. A CNN trained on the CASIA-OLHWDB1.0-1.2 datasets won the ICDAR2013 Online Isolated Chinese Character recognition competition. Computationally, we have developed a sparse CNN implementation that make it practical to train CNNs with many layers of maxpooling. Extending the MNIST dataset by translations, our sparse CNN gets a test error of 0.31%."
            },
            "slug": "Sparse-arrays-of-signatures-for-online-character-Graham",
            "title": {
                "fragments": [],
                "text": "Sparse arrays of signatures for online character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A sparse CNN implementation is developed that make it practical to train CNNs with many layers of maxpooling and extends the MNIST dataset by translations, which gets a test error of 0.31%."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34180590"
                        ],
                        "name": "S. Thorpe",
                        "slug": "S.-Thorpe",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thorpe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50780533"
                        ],
                        "name": "D. Fize",
                        "slug": "D.-Fize",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Fize",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fize"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3677347"
                        ],
                        "name": "Catherine Marlot",
                        "slug": "Catherine-Marlot",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Marlot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Marlot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4303570,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "addbd39fc775c12aa453ebd0cb77ea1bd3389572",
            "isKey": false,
            "numCitedBy": 2537,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speed-of-processing-in-the-human-visual-system-Thorpe-Fize",
            "title": {
                "fragments": [],
                "text": "Speed of processing in the human visual system"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2858764"
                        ],
                        "name": "V. Sheng",
                        "slug": "V.-Sheng",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sheng",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752722"
                        ],
                        "name": "F. Provost",
                        "slug": "F.-Provost",
                        "structuredName": {
                            "firstName": "Foster",
                            "lastName": "Provost",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Provost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2942126"
                        ],
                        "name": "Panagiotis G. Ipeirotis",
                        "slug": "Panagiotis-G.-Ipeirotis",
                        "structuredName": {
                            "firstName": "Panagiotis",
                            "lastName": "Ipeirotis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panagiotis G. Ipeirotis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 38
                            }
                        ],
                        "text": "Works such as (Welinder et al., 2010; Sheng et al., 2008; Vittayakorn and Hays, 2011) describe quality control mechanisms for this marketplace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 279332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3f853572e12b51c4c227590168c95b7cd0ca666",
            "isKey": false,
            "numCitedBy": 1112,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial."
            },
            "slug": "Get-another-label-improving-data-quality-and-data-Sheng-Provost",
            "title": {
                "fragments": [],
                "text": "Get another label? improving data quality and data mining using multiple, noisy labelers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 151
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 18
                            }
                        ],
                        "text": ", 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 216
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Section 5.1 documents the progress, starting from coded SIFT features and evolving to large-scale convolutional neural networks dominating at all three tasks of image classification, single-object localization, and object detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 149
                            }
                        ],
                        "text": "\u2026second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 62
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 81
                            }
                        ],
                        "text": "0 \u2013 \u2013 \u2013 Brno University of Technology Martin Kol\u00e1\u0159, Michal Hradi\u0161, Pavel Svoboda (Krizhevsky et al. 2012; Mikolov et al. 2013; Jia 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21868,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053671127"
                        ],
                        "name": "Li Wan",
                        "slug": "Li-Wan",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Wan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Wan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33551113"
                        ],
                        "name": "Sixin Zhang",
                        "slug": "Sixin-Zhang",
                        "structuredName": {
                            "firstName": "Sixin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sixin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 897,
                                "start": 815
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 87
                            }
                        ],
                        "text": "9 - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2936324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
            "isKey": true,
            "numCitedBy": 2091,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models."
            },
            "slug": "Regularization-of-Neural-Networks-using-DropConnect-Wan-Zeiler",
            "title": {
                "fragments": [],
                "text": "Regularization of Neural Networks using DropConnect"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work introduces DropConnect, a generalization of Dropout, for regularizing large fully-connected layers within neural networks, and derives a bound on the generalization performance of both Dropout and DropConnect."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1508337194"
                        ],
                        "name": "Minjie Wang",
                        "slug": "Minjie-Wang",
                        "structuredName": {
                            "firstName": "Minjie",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjie Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39102205"
                        ],
                        "name": "Tianjun Xiao",
                        "slug": "Tianjun-Xiao",
                        "structuredName": {
                            "firstName": "Tianjun",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianjun Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49297890"
                        ],
                        "name": "Jianpeng Li",
                        "slug": "Jianpeng-Li",
                        "structuredName": {
                            "firstName": "Jianpeng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianpeng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2478181"
                        ],
                        "name": "Jiaxing Zhang",
                        "slug": "Jiaxing-Zhang",
                        "structuredName": {
                            "firstName": "Jiaxing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaxing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107017154"
                        ],
                        "name": "Chuntao Hong",
                        "slug": "Chuntao-Hong",
                        "structuredName": {
                            "firstName": "Chuntao",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuntao Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38448016"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 246
                            }
                        ],
                        "text": "7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2 Tianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1640,
                                "start": 1564
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13970571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37b07566d53b8533c57d707c4913aae505a93a66",
            "isKey": true,
            "numCitedBy": 28,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The tooling landscape of deep learning is fragmented by a growing gap between the generic and productivity-oriented tools that optimize for algorithm development and the task-specific ones that optimize for speed and scale. This creates an artificial barrier to bring new innovations into real-world applications. Minerva addresses this issue with a layered design that provides language flexibility and execution efficiency simultaneously within one coherent framework. It proposes a matrix-based API, resulting in compact codes and the Matlab-like, imperative and procedural coding style. The code is dynamically translated into an internal dataflow representation, which is then efficiently executed against different hardware. The same user code runs on modern laptop and workstation, high-end multi-core server, or server clusters, with and without GPU acceleration, delivering performance and scalability better than or competitive with existing tools on different platforms."
            },
            "slug": "Minerva:-A-Scalable-and-Highly-Efficient-Training-Wang-Xiao",
            "title": {
                "fragments": [],
                "text": "Minerva: A Scalable and Highly Efficient Training Platform for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Minerva proposes a matrix-based API, resulting in compact codes and the Matlab-like, imperative and procedural coding style, and provides language flexibility and execution efficiency simultaneously within one coherent framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771345"
                        ],
                        "name": "Joseph Keshet",
                        "slug": "Joseph-Keshet",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Keshet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5919882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ca4d4229b8a843c0847fc70531790df6bd017ec",
            "isKey": false,
            "numCitedBy": 1772,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified view for online classification, regression, and uni-class problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss."
            },
            "slug": "Online-Passive-Aggressive-Algorithms-Crammer-Dekel",
            "title": {
                "fragments": [],
                "text": "Online Passive-Aggressive Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a unified view for online classification, regression, and uni-class problems, and proves worst case loss bounds for various algorithms for both the realizable case and the non-realizable case."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144080148"
                        ],
                        "name": "A. Sorokin",
                        "slug": "A.-Sorokin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Sorokin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sorokin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 85
                            }
                        ],
                        "text": "ILSVRC makes extensive use of Amazon Mechanical Turk to obtain accurate annotations (Sorokin and Forsyth, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1206581,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d513d8b6470c7cbbeca8563505de8711325a3179",
            "isKey": false,
            "numCitedBy": 645,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to outsource data annotation to Amazon Mechanical Turk. Doing so has produced annotations in quite large numbers relatively cheaply. The quality is good, and can be checked and controlled. Annotations are produced quickly. We describe results for several different annotation problems. We describe some strategies for determining when the task is well specified and properly priced."
            },
            "slug": "Utility-data-annotation-with-Amazon-Mechanical-Turk-Sorokin-Forsyth",
            "title": {
                "fragments": [],
                "text": "Utility data annotation with Amazon Mechanical Turk"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work shows how to outsource data annotation to Amazon Mechanical Turk, and describes results for several different annotation problems, including some strategies for determining when the task is well specified and properly priced."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "We obtain accurate translations using WordNets in those languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "Each concept in WordNet, possibly described by multiple words or word phrases, is called a \u201csynonym set\u201d or \u201csynset\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 383,
                                "start": 376
                            }
                        ],
                        "text": "The rationale is that the object detection system developed for this task can later be combined with a fine-grained classification model to further classify the objects if a finer subdivision is desired.5 As with the 1000 classification classes, the synsets are selected such that there is no overlap between synsets: for any synsets i and j, i is not an ancestor of j in the WordNet hierarchy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "For each synset, the queries are the set of WordNet synonyms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 135
                            }
                        ],
                        "text": "The 1000 synsets are selected such that there is no overlap between synsets: for any synsets i and j, i is not an ancestor of j in the WordNet hierarchy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "For example, when querying \u201cwhippet\u201d, according to WordNet\u2019s glossary a \u201csmall slender dog of greyhound type developed in England\u201d, we also use \u201cwhippet dog\u201d and \u201cwhippet greyhound.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": ", 2008) contains 80 million 32x32 low resolution images collected from the internet using synsets in WordNet (Miller, 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "By using WordNet as a backbone (Miller, 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "As a result, ImageNet contains\nImageNet Large Scale Visual Recognition Challenge 3\n14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "Another dataset TinyImages (Torralba et al., 2008) contains 80 million 32x32 low resolution images collected from the internet using synsets in WordNet (Miller, 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 26
                            }
                        ],
                        "text": "ing WordNet as a backbone (Miller, 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same ob-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": true,
            "numCitedBy": 13886,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144701020"
                        ],
                        "name": "Christopher N. Johnson",
                        "slug": "Christopher-N.-Johnson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Johnson",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher N. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123402576"
                        ],
                        "name": "Menna E. Jones",
                        "slug": "Menna-E.-Jones",
                        "structuredName": {
                            "firstName": "Menna",
                            "lastName": "Jones",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menna E. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 134476640,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "cc370505d854b8168c16e4d8d865eaa8e8031839",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Until just a few thousand years ago, Tasmanian Devils lived throughout most of mainland Australia. Should we bring them back? Before deciding this, we would need to answer a few questions."
            },
            "slug": "Return-of-the-devil-Johnson-Jones",
            "title": {
                "fragments": [],
                "text": "Return of the devil"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1344,
                                "start": 1316
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 50
                            }
                        ],
                        "text": "2010) with compression using product quantization (Sanchez and Perronnin 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 151
                            }
                        ],
                        "text": "\u2026went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 74
                            }
                        ],
                        "text": "5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021(Sanchez and Perronnin 2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 195
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe 2004), a Fisher vector representation (Sanchez and Perronnin 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman 2012; Sanchez et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 89
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 390,
                                "start": 385
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 90
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 187
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "High-dim. signature compression for large-scale image classification"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 90
                            }
                        ],
                        "text": "0 - \u25e6 - - KAIST department of EE Jun-Cheol Park, Yunhun Jang, Hyungwon Choi, JaeYoung Jun (Chatfield et al., 2014; Jia, 2013)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 287
                            }
                        ],
                        "text": "\u2026with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 79
                            }
                        ],
                        "text": "0 - - Brno University of Technology Martin Kol\u00e1\u0159, Michal Hrad\u01d0s, Pavel Svoboda (Krizhevsky et al., 2012; Mikolov et al., 2013; Jia, 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 834
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 103
                            }
                        ],
                        "text": "19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 313
                            }
                        ],
                        "text": "In the single-object localization with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CVPR. Yang, J., Yu, K., Gong, Y., and Huang, T. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Caffe: An open source convolutional\narchitecture for fast feature embedding. http:// caffe.berkeleyvision.org/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "In CVPR. Yao, B., Yang, X., and Zhu, S.-C. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 63
                            }
                        ],
                        "text": "2 - \u25e6 - - The University of Queensland Zhongwen Xu and Yi Yang (Krizhevsky et al., 2012; Jia, 2013; Zeiler and Fergus, 2013; Lin et al., 2014a)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Caffe: An open source convolutional architecture for fast feature embedding"
            },
            "venue": {
                "fragments": [],
                "text": "http:// caffe.berkeleyvision.org/."
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 834
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CVPR. Yang, J., Yu, K., Gong, Y., and Huang, T. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 75
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus 2013), and they were trained on the GPU following (Zeiler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "In CVPR. Yao, B., Yang, X., and Zhu, S.-C. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 55
                            }
                        ],
                        "text": "5 \u2013 \u2013 New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus 2013; Zeiler et al. 2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 65
                            }
                        ],
                        "text": "2 \u2013 \u25e6 \u2013 \u2013 \u2013 The University of Queensland Zhongwen Xu and Yi Yang (Krizhevsky et al. 2012; Jia 2013; Zeiler and Fergus 2013; Lin et al. 2014a)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks. CoRR, abs/1311.2901"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "We obtain accurate translations using WordNets in those languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "Each concept in WordNet, possibly described by multiple words or word phrases, is called a \u201csynonym set\u201d or \u201csynset\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 383,
                                "start": 376
                            }
                        ],
                        "text": "The rationale is that the object detection system developed for this task can later be combined with a fine-grained classification model to further classify the objects if a finer subdivision is desired.5 As with the 1000 classification classes, the synsets are selected such that there is no overlap between synsets: for any synsets i and j, i is not an ancestor of j in the WordNet hierarchy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "For each synset, the queries are the set of WordNet synonyms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "ImageNet is an image dataset organized according to the WordNet hierarchy (Miller 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 135
                            }
                        ],
                        "text": "The 1000 synsets are selected such that there is no overlap between synsets: for any synsets i and j, i is not an ancestor of j in the WordNet hierarchy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "For example, when querying \u201cwhippet\u201d, according to WordNet\u2019s glossary a \u201csmall slender dog of greyhound type developed in England\u201d, we also use \u201cwhippet dog\u201d and \u201cwhippet greyhound.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "By using WordNet as a backbone (Miller, 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "As a result, ImageNet contains\nImageNet Large Scale Visual Recognition Challenge 3\n14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "By using WordNet as a backbone (Miller 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "Another dataset TinyImages (Torralba et al., 2008) contains 80 million 32x32 low resolution images collected from the internet using synsets in WordNet (Miller, 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": "2008) contains 80 million 32\u00d7 32 low resolution images collected from the internet using synsets in WordNet (Miller 1995) as queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Wordnet: A lexical database for English.Commun"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "Instead we turn to designing novel crowdsourcing approaches for collecting large-scale annotations (Su et al., 2012; Deng et al., 2009, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "A detailed description of the generic algorithm, along with theoretical analysis and empirical evaluation, is presented in (Deng et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": "This is a special case of the algorithm described in (Deng et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 4
                            }
                        ],
                        "text": "In (Deng et al., 2014) we study strategies for scalable multilabel annotation, or for efficiently acquiring multiple labels from humans for a collection of items."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable multilabel annotation"
            },
            "venue": {
                "fragments": [],
                "text": "CHI."
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1344,
                                "start": 1316
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 196
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 75
                            }
                        ],
                        "text": "5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 44
                            }
                        ],
                        "text": "with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 151
                            }
                        ],
                        "text": "\u2026went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 390,
                                "start": 385
                            }
                        ],
                        "text": "The Third Research Institute of the Ministry of Public Security,\nP.R. China Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA - - 9.8 University of California Los Angeles Yukun Zhu, Jun Zhu, Alan Yuille UIUC - - 1.0 University of Illinois at Urbana-Champaign Thomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA 14.3 - 22.6 University of Amsterdam, Euvision Technologies Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et al., 2014) VGG 15.2 46.4 - Visual Geometry Group, University of Oxford Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et al., 2013) ZF 13.5 - -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 90
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 187
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "\u2026SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 89
                            }
                        ],
                        "text": "The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin, 2011) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi, 2012), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "High-dim"
            },
            "venue": {
                "fragments": [],
                "text": "signature compression for large-scale image classification. In CVPR."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "1 8\nO lg a R u ssa k o v sk y * et a l.\nILSVRC 2013\nCodename CLS LOC DET Insitutions Contributors and references Adobe 15.2 - - Adobe\u2020, University of Illinois at Urbana-Champaign\u2021 Hailin Jin\u2020, Zhe Lin\u2020, Jianchao Yang\u2020, Tom Paine\u2021\n(Krizhevsky et al., 2012)\nAHoward 13.6 - - Andrew Howard Consulting Andrew Howard BUPT 25.2 - - Beijing University of Posts and Telecommunications\u2020, Orange Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020, Yalong Bai\u2020, Yong Rui\u2021 decaf 19.2 - - University of California Berkeley Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et al., 2013) Deep Punx 20.9 - - Saint Petersburg State University Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013) Delta - - 6.1 National Tsing Hua University Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, HaoChe Kao IBM 20.7 - - University of Illinois at Urbana-Champaign\u2020, IBM Watson Research Center\u2021, IBM Haifa Research Center\u2213 Zhicheng Yan\u2020, Liangliang Cao\u2021, John R Smith\u2021, Noel Codella\u2021,Michele Merler\u2021, Sharath Pankanti\u2021, Sharon Alpert\u2213, Yochay Tzur\u2213, MIL 24.4 - - University of Tokyo Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada Minerva 21.7 Peking University\u2020, Microsoft Research\u2021, Shanghai Jiao Tong University\u2213, XiDian University\u00a7, Harbin Institute of Technology\u03c2\nTianjun Xiao\u2020\u2021, Minjie Wang\u2213\u2021, Jianpeng Li\u00a7\u2021, Yalong Bai\u03c2\u2021, Jiaxing Zhang\u2021, Kuiyuan Yang\u2021, Chuntao Hong\u2021, Zheng Zhang\u2021 (Wang et al., 2014)\nNEC - - 19.6 NEC Labs America\u2020, University of Missouri \u2021 Xiaoyu Wang\u2020, Miao Sun\u2021, Tianbao Yang\u2020, Yuanqing Lin\u2020, Tony X. Han\u2021, Shenghuo Zhu\u2020 (Wang et al., 2013) NUS 13.0 National University of Singapore Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et al., 2012) Orange 25.2 Orange Labs International Center Beijing\u2020, Beijing University of Posts and Telecommunications\u2021 Hongliang BAI\u2020, Lezi Wang\u2021, Shusheng Cen\u2021, YiNan Liu\u2021, Kun Tao\u2020, Wei Liu\u2020, Peng Li\u2020, Yuan Dong\u2020 OverFeat 14.2 30.0 (19.4) New York University Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et al., 2013) Quantum 82.0 - - Self-employed\u2020, Student in Troy High School, Fullerton, CA\u2021 Henry Shu\u2020, Jerry Shu\u2021 (Batra et al., 2013) SYSU - - 10.5 Sun Yat-Sen University, China."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 287
                            }
                        ],
                        "text": "\u2026with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 834
                            }
                        ],
                        "text": "Im a g eN et L a rg e S ca le V isu a l R eco g n itio n C h a llen g e\n1 7\nILSVRC 2010\nCodename CLS LOC Insitutions Contributors and references Hminmax 54.4 Massachusetts Institute of Technology Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang IBM 70.1 IBM research\u2020, Georgia Tech\u2021 Lexing Xie\u2020, Hua Ouyang\u2021, Apostol Natsev\u2020 ISIL 44.6 Intelligent Systems and Informatics Lab., The University of Tokyo Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi ITNLP 78.7 Harbin Institute of Technology Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun LIG 60.7 Laboratoire d\u2019Informatique de Grenoble Georges Que\u0301not NEC 28.2 NEC Labs America\u2020, University of Illinois at Urbana-\nChampaign\u2021, Rutgers\u2213 Yuanqing Lin\u2020, Fengjun Lv\u2020, Shenghuo Zhu\u2020, Ming Yang\u2020, Timothee Cour\u2020, Kai Yu\u2020, LiangLiang Cao\u2021, Zhen Li\u2021, Min-Hsuan Tsai\u2021, Xi Zhou\u2021, Thomas Huang\u2021, Tong Zhang\u2213 (Lin et al., 2011)\nNII 74.2 National Institute of Informatics, Tokyo,Japan\u2020, Hefei Normal Univ. Heifei, China\u2021 Cai-Zhi Zhu\u2020, Xiao Zhou\u2021, Shin\u0301\u0131chi Satoh\u2020 NTU 58.3 CeMNet, SCE, NTU, Singapore Zhengxiang Wang, Liang-Tien Chia Regularities 75.1 SRI International Omid Madani, Brian Burns UCI 46.6 University of California Irvine Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes XRCE 33.6 Xerox Research Centre Europe Jorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename CLS LOC Institutions Contributors and references ISI 36.0 - Intelligent Systems and Informatics lab, University of Tokyo Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII 50.5 - National Institute of Informatics, Japan Duy-Dinh Le, Shin\u0301\u0131chi Satoh UvA 31.0 42.5 University of Amsterdam\u2020, University of Trento\u2021 Koen E. A. van de Sande\u2020, Jasper R. R. Uijlings\u2021, Arnold W. M. Smeulders\u2020, Theo Gevers\u2020, Nicu Sebe\u2021,\nCees Snoek\u2020 (van de Sande et al., 2011b)\nXRCE 25.8 56.5 Xerox Research Centre Europe\u2020, CIII\u2021 Florent Perronnin\u2020, Jorge Sanchez\u2020\u2021 (Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename CLS LOC Institutions Contributors and references ISI 26.2 53.6 University of Tokyo\u2020, JST PRESTO\u2021 Naoyuki Gunji\u2020, Takayuki Higuchi\u2020, Koki Yasumoto\u2020, Hiroshi Muraoka\u2020, Yoshitaka Ushiku\u2020, Tatsuya\nHarada\u2020\u2021, Yasuo Kuniyoshi\u2020 (Harada and Kuniyoshi, 2012)\nLEAR 34.5 - LEAR INRIA Grenoble\u2020, TVPA Xerox Research Centre Europe\u2021 Thomas Mensink\u2020\u2021, Jakob Verbeek\u2020, Florent Perronnin\u2021, Gabriela Csurka\u2021 (Mensink et al., 2012) VGG 27.0 50.0 University of Oxford Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012) SuperVision 16.4 34.2 University of Toronto Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et al., 2012) UvA 29.6 - University of Amsterdam Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin, 2011; Scheirer et al., 2012) XRCE 27.1 - Xerox Research Centre Europe\u2020, LEAR INRIA \u2021 Florent Perronnin\u2020, Zeynep Akata\u2020\u2021, Zaid Harchaoui\u2021, Cordelia Schmid\u2021 (Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 313
                            }
                        ],
                        "text": "In the single-object localization with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CVPR. Yang, J., Yu, K., Gong, Y., and Huang, T. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Caffe: An open source convolutional\narchitecture for fast feature embedding. http:// caffe.berkeleyvision.org/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 81
                            }
                        ],
                        "text": "0 \u2013 \u2013 \u2013 Brno University of Technology Martin Kol\u00e1\u0159, Michal Hradi\u0161, Pavel Svoboda (Krizhevsky et al. 2012; Mikolov et al. 2013; Jia 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 319
                            }
                        ],
                        "text": "In the single-object localization with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., and Tang, X. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 92
                            }
                        ],
                        "text": "0 \u2013 \u25e6 \u2013 \u2013 \u2013 KAIST department of EE Jun-Cheol Park, Yunhun Jang, Hyungwon Choi, JaeYoung Jun (Chatfield et al. 2014; Jia 2013)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, X., Yang, M., Zhu, S., and Lin, Y. (2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "In CVPR. Yao, B., Yang, X., and Zhu, S.-C. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 65
                            }
                        ],
                        "text": "2 \u2013 \u25e6 \u2013 \u2013 \u2013 The University of Queensland Zhongwen Xu and Yi Yang (Krizhevsky et al. 2012; Jia 2013; Zeiler and Fergus 2013; Lin et al. 2014a)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.berkeleyvision.org"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 17
                            }
                        ],
                        "text": "region proposals (Arbel\u00e1ez et al. 2014) pretrained on PASCAL VOC 2012 data are used to extract region proposals, regions are represented using convolutional networks, and a multiple instance learning strategy is used to learn weakly supervised object detectors to represent the image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 87
                            }
                        ],
                        "text": "4 \u2013 \u25e6 \u2013 \u2013 CRIPAC, CASIA Weiqiang Ren, Chong Wang, Yanhua Chen, Kaiqi Huang, Tieniu Tan (Arbel\u00e1ez et al. 2014)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "MCG region proposals (Arbela\u0301ez et al., 2014) pretrained on PASCAL VOC 2012 data are used to extract region proposals, regions are represented using convolutional networks, and a multiple instance learning strategy is used to learn weakly supervised object detectors to represent the image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiscale combinatorial grouping. In Computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Large-scale image classification: Fast feature extraction and SVM training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "2006), and classifying with a histogram intersection kernel SVM (Maji and Malik 2009) trained on a GPU (van de Sande et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 224
                            }
                        ],
                        "text": "\u2026by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 240
                            }
                        ],
                        "text": "The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe, 2004), a Fisher vector representation (Sanchez and Perronnin, 2011), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 375
                            }
                        ],
                        "text": "The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et al., 2011b), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et al., 2010), pooling with spatial pyramid matching (Lazebnik et al., 2006), and classifying with a histogram intersection kernel SVM (Maji and Malik, 2009) trained on a GPU (van de Sande et al., 2011a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 213
                            }
                        ],
                        "text": "The honorable mention XRCE team (Perronnin et al., 2010) used an improved Fisher vector representation (Perronnin and Dance, 2007) along with PCA dimensionality reduction and data compression followed by a linear SVM. Fisher vectorbased methods have evolved over five years of the challenge and continued performing strongly in every ILSVRC from 2010 to 2014."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 211
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 238
                            }
                        ],
                        "text": "The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying highdimensional image signatures (Perronnin et al., 2010) with compression using product quantization (Sanchez and Perronnin, 2011) and one-vs-all linear SVMs."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2009).Object detection using amax-margin hough transform"
            },
            "venue": {
                "fragments": [],
                "text": "In CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "The network architectures were chosen using the visualization technique of (Zeiler and Fergus, 2013), and they were trained on the GPU following (Zeiler et al., 2011) using the dropout technique (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "New York University Matthew D Zeiler, Rob Fergus (Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Labs\nInternational Center Beijing\u2021 Chong Huang\u2020, Yunlong Bian\u2020, Hongliang Bai\u2021, Bo Liu\u2020, Yanchao Feng\u2020, Yuan Dong\u2020\nClarifai 11.7 - - Clarifai Matthew Zeiler (Zeiler and Fergus, 2013; Zeiler et al., 2011) CogVision 16.1 - - Microsoft Research\u2020, Harbin Institute of Technology\u2021 Kuiyuan Yang\u2020,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks. CoRR, abs/1311"
            },
            "venue": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks. CoRR, abs/1311"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "Appendix B and (Russakovsky et al., 2013) have\nadditional comparisons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "For each synset, the queries are the set of WordNet synonyms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 1
                            }
                        ],
                        "text": "(Russakovsky et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201cPotted plant\u201d was one of the most challenging object classes to annotate consistently among the PASCAL VOC classes, and in order to obtain accurate annotations using crowdsourcing we had to restrict the definition to a more concrete object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "A detailed analysis and comparison of the SuperVision and VGG submissions on the single-object localization task can be found in (Russakovsky et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detecting avocados to zucchinis: What have we done, & where are we going? In ICCV"
            },
            "venue": {
                "fragments": [],
                "text": "Detecting avocados to zucchinis: What have we done, & where are we going? In ICCV"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 78
                            }
                        ],
                        "text": "Several datasets provide pixel-level segmentations: for example, MSRC dataset (Criminisi 2004) with 591 images and 23 object classes, Stanford Background Dataset (Gould et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "Several datasets provide pixel-level segmentations: for example, MSRC dataset (Criminisi, 2004) with 591 images and 23 object classes, Stanford Background Dataset (Gould et al., 2009) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et al., 2011) with 500 images\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 108
                            }
                        ],
                        "text": "It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et al. 2004; Criminisi 2004; Everingham et al. 2012; Xiao et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 131
                            }
                        ],
                        "text": "It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et al., 2004; Criminisi, 2004; Everingham et al., 2012; Xiao et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Microsoft Research Cambridge ( MSRC ) object recognition image database ( version 2 . 0 )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141732326"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63925014,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0ec48ac86456cea3d6d6172ca81ef68e98b21a61",
            "isKey": false,
            "numCitedBy": 3322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-Zhang",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Common Objects in Context"
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ImageNet: a large-scale hierarchi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 (147) car, automobile (not a golf cart or a bus)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction meets recognition challenge. http://ttic.uchicago.edu/ ~rurtasun/rmrc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 200
                            }
                        ],
                        "text": "They trained a large, deep convolutional neural network on RGB values, with 60 million parameters using an efficient GPU implementation and a novel hidden-unit dropout trick (Krizhevsky et al., 2012; Hinton et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing coadaptation of feature detectors. CoRR, abs/1207.0580"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "gionlets for generic object detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 111
                            }
                        ],
                        "text": "3 \u2013 \u2013 \u2013 Toyota Technological Institute at Chicago\u2020, Ecole Centrale Paris\u2021 George Papandreou\u2020, Iasonas Kokkinos\u2021(Papandreou 2014; Papandreou et al. 2014; Jojic et al. 2003; Krizhevsky et al. 2012; Sermanet et al. 2013; Dubout and Fleuret 2012; Iandola et al. 2014)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep epitomic convolutional neural networks. CoRR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 (21) flute: a high-pitched musical instrument that looks like a straight tube and is usually played sideways (please do not confuse with oboes, which have a distinctive straw"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 242
                            }
                        ],
                        "text": "Additionally, several influential lines of research have emerged, such as large-scale weakly supervised localization work of (Kuettel et al., 2012) which was awarded the best paper award in ECCV 2012 and largescale zero-shot learning, e.g., (Frome et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Devise: A deep visual-semantic embeddingmodel. InAdvances in neural information processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction meets recognition challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "( 2005 - 2012 ) . PASCAL Visual Object Classes Challenge ( VOC )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 (165) screwdriver"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PASCAL Visual Object Classes Challenge (VOC). http://www.pascal-network.org/challenges/VOC/ voc2012/workshop/index.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object detection with discrimina"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 (72) bowl: a dish for serving food that is round, open at the top, and has no handles (please do not confuse with a cup, which usually has a handle and is used for serving drinks)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 23
                            }
                        ],
                        "text": "2006) or recent Places (Zhou et al. 2014) provide a single scene category label (as opposed to an object category)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learningdeep features for scene recognitionusingplaces database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PASCAL Visual Object Classes Challenge ( VOC )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction meets recognition challenge. http://ttic.uchicago.edu/ rurtasun/rmrc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 27
                            }
                        ],
                        "text": "Datasets such as 15 Scenes (Oliva and Torralba, 2001; Fei-Fei and Perona, 2005; Lazebnik et al., 2006) or recent Places (Zhou et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A bayesian hierar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 99
                            }
                        ],
                        "text": "Fine-grained classification has evolved into its own Fine-Grained classification challenge in 2013 (Berg et al. 2013), which is outside the scope of this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 100
                            }
                        ],
                        "text": "Fine-grained classification has evolved into its own Fine-Grained classification challenge in 2013 (Berg et al., 2013), which is outside the scope of this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fine-grained competition. https://sites.google.com/site/ fgcomp2013"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 22
                            }
                        ],
                        "text": "The LotusHill dataset (Yao et al., 2007) contains very"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 23
                            }
                        ],
                        "text": "The LotusHill dataset (Yao et al., 2007) contains very detailed annotations of objects in 636,748 images and video frames, but it is not available for free."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to a large scale general purpose ground truth dataset: methodology, annotation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 175
                            }
                        ],
                        "text": "The winning entry from NEC team (Lin et al., 2011) used SIFT (Lowe, 2004) and LBP (Ahonen et al., 2006) features with two nonlinear coding representations (Zhou et al., 2010; Wang et al., 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 58
                            }
                        ],
                        "text": "2006) features with two non-linear coding representations (Zhou et al. 2010; Wang et al. 2010) and a stochastic SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "F.,Huang,T.,&Gong,Y. (2010).Localityconstrained linear coding for image classification"
            },
            "venue": {
                "fragments": [],
                "text": "In CVPR"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PASCAL Visual Object Classes Challenge (VOC). http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 (22) oboe: a slender musical instrument roughly 65cm long with metal keys, a distinctive straw-like mouthpiece and often a slightly flared end"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 100
                            }
                        ],
                        "text": "Fine-grained classification has evolved into its own Fine-Grained classification challenge in 2013 (Berg et al., 2013), which is outside the scope of this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fine-Grained Competition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "Several datasets provide pixel-level segmentations: for example, MSRC dataset (Criminisi, 2004) with 591 images and 23 object classes, Stanford Background Dataset (Gould et al., 2009) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et al., 2011) with 500 images\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 131
                            }
                        ],
                        "text": "It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et al., 2004; Criminisi, 2004; Everingham et al., 2012; Xiao et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "H. Su Stanford University, Stanford, CA, USA"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Microsoft Research Cambridge (MSRC) object recognition image database (version 2.0). http://research. microsoft.com/vision/cambridge/recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Microsoft Research Cambridge (MSRC) object recognition image database (version 2.0). http://research. microsoft.com/vision/cambridge/recognition"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 68,
            "methodology": 56,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 138,
        "totalPages": 14
    },
    "page_url": "https://www.semanticscholar.org/paper/ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd?sort=total-citations"
}