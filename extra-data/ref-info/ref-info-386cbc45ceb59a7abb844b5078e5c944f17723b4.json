{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46607442"
                        ],
                        "name": "Yoshinori Uesaka",
                        "slug": "Yoshinori-Uesaka",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Uesaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshinori Uesaka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 39007138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1fb17dc0a4656aae5b0bb3f2c21cd5e5190f4f1",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analog-Perceptrons:-On-Additive-Representation-of-Uesaka",
            "title": {
                "fragments": [],
                "text": "Analog Perceptrons: On Additive Representation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724922"
                        ],
                        "name": "Bunpei Irie",
                        "slug": "Bunpei-Irie",
                        "structuredName": {
                            "firstName": "Bunpei",
                            "lastName": "Irie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bunpei Irie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Irie and Miyake (1988) obtained an integral formula which suggests the realization of functions of several variables by three-layer networks by analogy with the principle of the computerized tomography (CT)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "A(X, . . . . , x , ) - f ( x , , . . . , X,)IIL= = O.\nConnecting this formula with three-layer networks, Irie and Miyake (1988) assert that arbitrary functions can be represented by a three-layer network with an infinite number of computational units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "In this paper, we started from an integral formula recently proposed by Irie and Miyake (1988) and proved the theorem which guarantees the approximate realization of continuous mappings by threelayer (one hidden layer) networks whose output functions for hidden layer are sigmoid, and whose output\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15092998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb73ff39bf5e42e03b5428ce03c43f451288d534",
            "isKey": true,
            "numCitedBy": 418,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A theorem is proved to the effect that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints. The proof is constructive, and each coefficient is explicitly presented. The theorem theoretically guarantees a kind of universality for three-layered perceptrons. Although two-layered perceptrons (simple perceptrons) cannot represent arbitrary functions, three-layers prove necessary and sufficient. The relationship between the model used in the proof and the distributed storage and processing of information is also discussed.<<ETX>>"
            },
            "slug": "Capabilities-of-three-layered-perceptrons-Irie-Miyake",
            "title": {
                "fragments": [],
                "text": "Capabilities of three-layered perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A theorem is proved that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314979"
                        ],
                        "name": "W. Pitts",
                        "slug": "W.-Pitts",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pitts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pitts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15619658,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "090c5a5df345ab60c41d6de02b3e366e1a27cf43",
            "isKey": false,
            "numCitedBy": 6084,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed."
            },
            "slug": "A-logical-calculus-of-the-ideas-immanent-in-nervous-McCulloch-Pitts",
            "title": {
                "fragments": [],
                "text": "A logical calculus of the ideas immanent in nervous activity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Huang and Lippmann (1987) demonstrated by simulations that three-layer networks can form several complex decision regions in pattern recognition application."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 43
                            }
                        ],
                        "text": "On the application to pattern recognition, Lippmann (1987) asserts that arbitrary complex decision regions, including concave regions, can be formed using four-layer networks, but this is only an intuitive assertion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3817,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112105858"
                        ],
                        "name": "William Y. Huang",
                        "slug": "William-Y.-Huang",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Huang",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Y. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Huang and Lippmann (1987) demonstrated by simulations that three-layer networks can form several complex decision regions in pattern recognition application."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11607279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1382a29539b3de419d567f679b5f28cee459a49",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on nets with continuous-valued inputs led to generative procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third \"feature map classifier\" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available. The architecture of this classifier can be used to implement a neural net k-nearest neighbor classifier."
            },
            "slug": "Neural-Net-and-Traditional-Classifiers-Huang-Lippmann",
            "title": {
                "fragments": [],
                "text": "Neural Net and Traditional Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66980418"
                        ],
                        "name": "S. Tamura",
                        "slug": "S.-Tamura",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 185
                            }
                        ],
                        "text": "Presently, for application of neural networks to pattern recognition or related engineering fields, up to four-layer networks are used (Waibel, Hanazawa, Hinton, Shikano, & Lang, 1988; Tamura & Waibel, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195708690,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "f258b49b4a65d5b30a9bd539067ded2a3b2c5531",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using a back propagation network learning algorithm, a four-layered feed-forward network is trained on learning samples to realize a mapping from the set of noisy signals a set of noise-free signals. Computer experiments were carried out on 12 kHz sampled Japanese speech data, using stationary and nonstationary noise. The experiments showed that the network can indeed learn to perform noise reduction. Even for noisy speech signals that had not been part of the training data, the network successfully produced noise-suppressed output signals.<<ETX>>"
            },
            "slug": "Noise-reduction-using-connectionist-models-Tamura-Waibel",
            "title": {
                "fragments": [],
                "text": "Noise reduction using connectionist models"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Using a back propagation network learning algorithm, a four-layered feed-forward network is trained on learning samples to realize a mapping from the set of noisy signals a set of noise-free signals, showing that the network can indeed learn to perform noise reduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73429655"
                        ],
                        "name": "Hanazawa G. Hinton",
                        "slug": "Hanazawa-G.-Hinton",
                        "structuredName": {
                            "firstName": "Hanazawa",
                            "lastName": "Hinton",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanazawa G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71055078"
                        ],
                        "name": "Ic Shikano Ic",
                        "slug": "Ic-Shikano-Ic",
                        "structuredName": {
                            "firstName": "Ic",
                            "lastName": "Ic",
                            "middleNames": [
                                "Shikano"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ic Shikano Ic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 778,
                                "start": 111
                            }
                        ],
                        "text": "INTRODUCTION Since McCulloch-Pitts (1943), there have been many studies of mathematical models of neural networks. Recently, Hopfield, Hinton, Rumelhart, Sejnowski and others have tried many concrete applications such as pattern recognition, and have shown that it is possible to clarify the mechanism of human information processing by the use of these models. In particular, the back propagation algorithm (generalized delta rule) proposed by Rumelhart, Hinton, and Williams (1986) provides a learning rule for multilayer networks. Many applications of this algorithm have been shown recently. However, there has been little theoretical research on the capability of the Rumelhart-Hinton-Williams multilayer network. On the application to pattern recognition, Lippmann (1987) asserts that arbitrary complex decision regions, including concave regions, can be formed using four-layer networks, but this is only an intuitive assertion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 964,
                                "start": 111
                            }
                        ],
                        "text": "INTRODUCTION Since McCulloch-Pitts (1943), there have been many studies of mathematical models of neural networks. Recently, Hopfield, Hinton, Rumelhart, Sejnowski and others have tried many concrete applications such as pattern recognition, and have shown that it is possible to clarify the mechanism of human information processing by the use of these models. In particular, the back propagation algorithm (generalized delta rule) proposed by Rumelhart, Hinton, and Williams (1986) provides a learning rule for multilayer networks. Many applications of this algorithm have been shown recently. However, there has been little theoretical research on the capability of the Rumelhart-Hinton-Williams multilayer network. On the application to pattern recognition, Lippmann (1987) asserts that arbitrary complex decision regions, including concave regions, can be formed using four-layer networks, but this is only an intuitive assertion. Wieland and Leighton (1987) showed an example of a three-layer network with thresholding"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 61
                            }
                        ],
                        "text": "Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 173
                            }
                        ],
                        "text": "Presently, for application of neural networks to pattern recognition or related engineering fields, up to four-layer networks are used (Waibel, Hanazawa, Hinton, Shikano, & Lang, 1988; Tamura & Waibel, 1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 47
                            }
                        ],
                        "text": "Connecting this formula with three-layer networks, Irie and Miyake (1988) assert that arbitrary functions can be represented by a three-layer network with an infinite number of computational units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 484,
                                "start": 111
                            }
                        ],
                        "text": "INTRODUCTION Since McCulloch-Pitts (1943), there have been many studies of mathematical models of neural networks. Recently, Hopfield, Hinton, Rumelhart, Sejnowski and others have tried many concrete applications such as pattern recognition, and have shown that it is possible to clarify the mechanism of human information processing by the use of these models. In particular, the back propagation algorithm (generalized delta rule) proposed by Rumelhart, Hinton, and Williams (1986) provides a learning rule for multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62490901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "918aeead4adb3052bd0c437ac40939c116ba65db",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "neme recognition which is characterized by two important properties: 1.) Using a 3 layer arrangement of simple computing units, it can represent arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error back-propagatioii[l]. 2.) he time-delay arrangement enables the network to discover acoustichonetic features and the temporal relationships between them indeendent of position in time and hence not blurred by temporal shifts in the input. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task, i.e., the speakerdependent recognition of the phonemes \"B\", \"D\", and \"G\" extracted We show that the TDNN \"invented\" well-known acoustic-phonetic"
            },
            "slug": "Phoneme-Recognition:-Neural-Networks-vs-Waibel-Hinton",
            "title": {
                "fragments": [],
                "text": "Phoneme Recognition: Neural Networks vs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the TDNN \"invented\" well-known acoustic-phonetic features and the temporal relationships between them are indeendent of position in time and hence not blurred by temporal shifts in the input."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100859010"
                        ],
                        "name": "V. Tikhomirov",
                        "slug": "V.-Tikhomirov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Tikhomirov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tikhomirov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 105
                            }
                        ],
                        "text": "Therefore we show another proof for the four-layer case by using the Kolmogorov-Arnold-Sprecher theorem (Kolmogorov, 1957; Sprecher, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Kolmogorov (1957) and Arnold refuted this conjecture and proved the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116968444,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "300328d09233d3ada652d6aace66353c3bdb5762",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is to present a brief proof of the following theorem: Theorem. For any integer n \u2265 2 there are continuous real functions \u03c8 p q (x) on the closed unit interval E 1 = [0;1] such that each continuous real function f(x 1 ,\u2026,x n ) on the n-dimensional unit cube E n is representable as \n \n$$f\\left( {{{x}_{1}}, \\ldots ,{{x}_{n}}} \\right) = \\sum\\limits_{{q = 1}}^{{q = 2n + 1}} {Xq\\left[ {\\sum\\limits_{{p = 1}}^{n} {{{\\psi }^{{pq}}}\\left( {{{x}_{p}}} \\right)} } \\right]} ,$$ \n \n(1) \n \nwhere x q (y) are continuous real functions."
            },
            "slug": "On-the-Representation-of-Continuous-Functions-of-as-Tikhomirov",
            "title": {
                "fragments": [],
                "text": "On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of one Variable and Addition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12251177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "834b3738673dacc767563c2714239852a8a6d4b4",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.<<ETX>>"
            },
            "slug": "Phoneme-recognition:-neural-networks-vs.-hidden-vs.-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A time-delay neural network for phoneme recognition that was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation and does not rely on precise alignment or segmentation of the input."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 43
                            }
                        ],
                        "text": "It's learning algorithm was also proposed (Amari, 1967) based on the same principle as the generalized delta rule. There are also other applications of multilayer networks for forming mappings, such as NETtalk by Sejnowski and Rosenberg (1987). Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 745,
                                "start": 43
                            }
                        ],
                        "text": "It's learning algorithm was also proposed (Amari, 1967) based on the same principle as the generalized delta rule. There are also other applications of multilayer networks for forming mappings, such as NETtalk by Sejnowski and Rosenberg (1987). Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network. Uesaka (1971) and Poggio (1983) have also pointed this out. However, the assertion has a problem in that the output function of each unit of this network is not a given sigmoid function. Irie and Miyake (1988) obtained an integral formula which suggests the realization of functions of several variables by three-layer networks by analogy with the principle of the computerized tomography (CT)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 549,
                                "start": 43
                            }
                        ],
                        "text": "It's learning algorithm was also proposed (Amari, 1967) based on the same principle as the generalized delta rule. There are also other applications of multilayer networks for forming mappings, such as NETtalk by Sejnowski and Rosenberg (1987). Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network. Uesaka (1971) and Poggio (1983) have also pointed this out."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 43
                            }
                        ],
                        "text": "It's learning algorithm was also proposed (Amari, 1967) based on the same principle as the generalized delta rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": true,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150122018"
                        ],
                        "name": "H. Fossum",
                        "slug": "H.-Fossum",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Fossum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fossum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "However, it has been known that any piecewise-linear decision region (which is not necessarily convex) can be realized by a multilayer network (Duda & Fossum, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Huang and Lippmann (1987) demonstrated by simulations that three-layer networks can form several complex decision regions in pattern recognition application."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9623590,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aa9f847b10c5a1fb5b85c0096e2d765927ef72c1",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes iterative procedures for determining linear and piecewise linear discriminant functions for multicategory pattern classifiers. While classifiers with the same structure have often been proposed, it is less well known that their parameters can be efficiently determined by simple adjustment procedures. For linear discriminant functions, convergence proofs are given for procedures that are guaranteed to yield error-free solutions on design samples, provided only that such solutions exist. While no similar results are known for piecewise linear discriminant functions, simple procedures are given that have been effective in various experiments. The results of experiments with artificially generated multimodal data and with hand-printed alphanumeric characters are given to show that this approach compares favorably with other classification methods."
            },
            "slug": "Pattern-Classification-by-Iteratively-Determined-Duda-Fossum",
            "title": {
                "fragments": [],
                "text": "Pattern Classification by Iteratively Determined Linear and Piecewise Linear Discriminant Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper describes iterative procedures for determining linear and piecewise linear discriminant functions for multicategory pattern classifiers and shows that this approach compares favorably with other classification methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 83
                            }
                        ],
                        "text": "In particular, the back propagation algorithm (generalized delta rule) proposed by Rumelhart, Hinton, and Williams (1986) provides a learning rule for multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853990"
                        ],
                        "name": "O. Braddick",
                        "slug": "O.-Braddick",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Braddick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Braddick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49060290"
                        ],
                        "name": "A. C. Sleigh",
                        "slug": "A.-C.-Sleigh",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Sleigh",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. Sleigh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "Uesaka (1971) and Poggio (1983) have also pointed this out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63361955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c06b00a573ce45467be1fb04f2373e9b28d55ef",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Physical-and-Biological-Processing-of-Images-Braddick-Sleigh",
            "title": {
                "fragments": [],
                "text": "Physical and Biological Processing of Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that this theorem means that any continuous mapping f : x I\" ---> (fl(x) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that this theorem means that any continuous mapping f : x I\" ---> (fl(x) . . . . . f r o (X) ) E R m is represented by a form of four-layer neural network with hidden units whose output functions are 0, \u00d7~(i = 1 . . . . . m), where ~ is used for the first hidden\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118526925,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "257843bde91001dc94fffdba287879fcb3b2f89a",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An improved version of Kolmogorov's powerful 1957 theorem concerning the representation of arbitrary continuous functions from the n-dimensional cube to the real numbers in terms of one dimensional continuous functions is reinterpreted to yield an existence theorem for mapping neural networks."
            },
            "slug": "Kolmogorov''s-Mapping-Neural-Network-Existence-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "Kolmogorov''s Mapping Neural Network Existence Theorem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102088724"
                        ],
                        "name": "D. Sprecher",
                        "slug": "D.-Sprecher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sprecher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sprecher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 123
                            }
                        ],
                        "text": "Therefore we show another proof for the four-layer case by using the Kolmogorov-Arnold-Sprecher theorem (Kolmogorov, 1957; Sprecher, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Sprecher (1965) refined the above theorem and obtained the following:\nTheorem (Sprecher)\nFor each integer n -> 2, there exists a real, monotone increasing function \u00a2(x), \u00a2([0, 1]) = [0, 1], dependent on n and having the following property: For each preassigned number ~ > 0 there is a rational\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 105
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52217396,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b98c8948cb54384e4ef6e864cad1cc704251e516",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-structure-of-continuous-functions-of-several-Sprecher",
            "title": {
                "fragments": [],
                "text": "On the structure of continuous functions of several variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143880192"
                        ],
                        "name": "V. Dyck",
                        "slug": "V.-Dyck",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Dyck",
                            "middleNames": [
                                "Arnie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Dyck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068725286"
                        ],
                        "name": "Smith",
                        "slug": "Smith",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83099779"
                        ],
                        "name": "Barbara Lawson",
                        "slug": "Barbara-Lawson",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Lawson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Lawson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59771503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "068fcf47be1c4c3b3f5416e0002a2d41888fd172",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Computing-Dyck-Smith",
            "title": {
                "fragments": [],
                "text": "Introduction to Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Then d ~b'(x) = d~x ~b~(x) converge to the delta function as ct ~ 0. We consider the convolution c*~b'(x) of c(x) and ~b'(x)"
            },
            "venue": {
                "fragments": [],
                "text": "For a sigmoid function ~b(x), set (b~(x) = \u00a2(x/ot)(ot > 0)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "Uesaka (1971) and Poggio (1983) have also pointed this out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "O. J. Braddick & A. C."
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 83
                            }
                        ],
                        "text": "In particular, the back propagation algorithm (generalized delta rule) proposed by Rumelhart, Hinton, and Williams (1986) provides a learning rule for multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 105
                            }
                        ],
                        "text": "Therefore we show another proof for the four-layer case by using the Kolmogorov-Arnold-Sprecher theorem (Kolmogorov, 1957; Sprecher, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Kolmogorov (1957) and Arnold refuted this conjecture and proved the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "Hecht-Nielsen (1987) pointed out that Kolmogorov's theorem (Kolmogorov, 1957) and Sprecher's refinement (Sprecher, 1965), which are both known as negative solutions of Hilbert's thirteenth problem, show that any continuous mapping can be represented by a form of four-layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the representation of continuous"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "There is a continuous function g(x) on R which has a compact support such that g(x) = g(x) on K. We may prove the proposition for 8(x) and so we may initially suppose that g(x) has a compact support"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Wieland and Leighton (1987) showed an example of a three-layer network with thresholding\nThe author wishes to thank Drs. Y. Tohkura, T. Inui and S. Miyake, and Mr. T. Okamoto for their valuable comments on the manuscript."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric analysis of neural network capabilities"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE First International Conference on Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 185
                            }
                        ],
                        "text": "Presently, for application of neural networks to pattern recognition or related engineering fields, up to four-layer networks are used (Waibel, Hanazawa, Hinton, Shikano, & Lang, 1988; Tamura & Waibel, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Noise reduction using"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "on R and whose step variances are less than e/4. Here c(x) is represented using the Heaviside function H(x) as follows: c(x) : ~ ciH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Wieland and Leighton (1987) showed an example of a three-layer network with thresholding\nThe author wishes to thank Drs. Y. Tohkura, T. Inui and S. Miyake, and Mr. T. Okamoto for their valuable comments on the manuscript."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric analysis of neural"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "APPENDIX (DIRECT PROOF OF THE PROPOSITION IN SECTION 9 BY A DIFFERENT METHOD)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-approximate-realization-of-continuous-by-Funahashi/386cbc45ceb59a7abb844b5078e5c944f17723b4?sort=total-citations"
}