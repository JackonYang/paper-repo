{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145736272"
                        ],
                        "name": "David Guthrie",
                        "slug": "David-Guthrie",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Guthrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Guthrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695497"
                        ],
                        "name": "M. Hepple",
                        "slug": "M.-Hepple",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hepple",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hepple"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 171
                            }
                        ],
                        "text": "Of particular importance is language modeling, where web-scale language models have been shown to improve machine translation and automatic speech recognition performance (Brants et al., 2007; Chelba and Schalkwyk, 2013; Guthrie and Hepple, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1839061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc1ae71b79870a7a48f49a8d19a600561c5effaa",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current state-of-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram."
            },
            "slug": "Storing-the-Web-in-Memory:-Space-Efficient-Language-Guthrie-Hepple",
            "title": {
                "fragments": [],
                "text": "Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Three novel methods of compactly storing very large n-gram language models are presented, which use substantially less space than all known approaches and allow n- gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683714"
                        ],
                        "name": "S. Bergsma",
                        "slug": "S.-Bergsma",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Bergsma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bergsma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585932"
                        ],
                        "name": "Emily Pitler",
                        "slug": "Emily-Pitler",
                        "structuredName": {
                            "firstName": "Emily",
                            "lastName": "Pitler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emily Pitler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 425954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88f56fc68d70181c954ed7890912960848e3e916",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance."
            },
            "slug": "Creating-Robust-Supervised-Classifiers-via-N-Gram-Bergsma-Pitler",
            "title": {
                "fragments": [],
                "text": "Creating Robust Supervised Classifiers via Web-Scale N-Gram Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part- of-speech disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702066"
                        ],
                        "name": "Kenneth Heafield",
                        "slug": "Kenneth-Heafield",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Heafield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Heafield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837145"
                        ],
                        "name": "Ivan Pouzyrevsky",
                        "slug": "Ivan-Pouzyrevsky",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Pouzyrevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Pouzyrevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144797264"
                        ],
                        "name": "J. Clark",
                        "slug": "J.-Clark",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 142
                            }
                        ],
                        "text": "\u2026length of 80, grow-diag-final-and symmetrization of\nGIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 31
                            }
                        ],
                        "text": "By using disk-based streaming (Heafield et al., 2013) we are able to efficiently estimate language models much larger than the physical memory on our machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 88
                            }
                        ],
                        "text": "GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 247
                            }
                        ],
                        "text": "The baseline systems were trained using Moses (Koehn et al., 2007) with the following features: maximum sentence length of 80, grow-diag-final-and symmetrization of\nGIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2561041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "774e560a2cadcb84f4b1def7b152e5398b062efb",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM."
            },
            "slug": "Scalable-Modified-Kneser-Ney-Language-Model-Heafield-Pouzyrevsky",
            "title": {
                "fragments": [],
                "text": "Scalable Modified Kneser-Ney Language Model Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient algorithm to estimate large modified Kneser-Ney models including interpolation using Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119125158"
                        ],
                        "name": "Jason R. Smith",
                        "slug": "Jason-R.-Smith",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404569159"
                        ],
                        "name": "Herve Saint-Amand",
                        "slug": "Herve-Saint-Amand",
                        "structuredName": {
                            "firstName": "Herve",
                            "lastName": "Saint-Amand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Herve Saint-Amand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2899897"
                        ],
                        "name": "Magdalena Plamada",
                        "slug": "Magdalena-Plamada",
                        "structuredName": {
                            "firstName": "Magdalena",
                            "lastName": "Plamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magdalena Plamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144871732"
                        ],
                        "name": "Adam Lopez",
                        "slug": "Adam-Lopez",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lopez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lopez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "\u2026Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al., 2011), and the news crawls provided by the evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 223
                            }
                        ],
                        "text": "The truecaser was trained on data from the 2014 Workshop on Statistical Machine Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al., 2011), and the news crawls provided by the evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 235
                            }
                        ],
                        "text": "The truecaser was trained on data from the 2014 Workshop on Statistical Machine Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13420142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31e8337daa0bfd7aa7737cd9383adaaa4275ead0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon\u2019s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 1"
            },
            "slug": "Dirt-Cheap-Web-Scale-Parallel-Text-from-the-Common-Smith-Saint-Amand",
            "title": {
                "fragments": [],
                "text": "Dirt Cheap Web-Scale Parallel Text from the Common Crawl"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This large-scale experiment brings web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon\u2019s Elastic Cloud using an open-source extension of the STRAND algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895952"
                        ],
                        "name": "N. Bertoldi",
                        "slug": "N.-Bertoldi",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Bertoldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bertoldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3077970"
                        ],
                        "name": "M. Cettolo",
                        "slug": "M.-Cettolo",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Cettolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cettolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "We emphasize that this approach is not new, but rather standard practice recommended by IRSTLM (Federico et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "IRSTLM: an open source toolkit for handling large scale language models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 34745880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93f6dd2c761fdeac0af6d2253d57834439d7794f",
            "isKey": false,
            "numCitedBy": 361,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle language models with billions of n-grams on conventional machines. The IRSTLM toolkit supports distribution of ngram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk. IRSTLM has been so far successfully deployed with the Moses toolkit for statistical machine translation and with the FBK-irst speech recognition system. Efficiency of the tool is reported on a speech transcription task of Italian political speeches using a language model of 1.1 billion four-grams."
            },
            "slug": "IRSTLM:-an-open-source-toolkit-for-handling-large-Federico-Bertoldi",
            "title": {
                "fragments": [],
                "text": "IRSTLM: an open source toolkit for handling large scale language models"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The IRSTLM toolkit supports distribution of ngram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748169"
                        ],
                        "name": "Kuansan Wang",
                        "slug": "Kuansan-Wang",
                        "structuredName": {
                            "firstName": "Kuansan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuansan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3077650"
                        ],
                        "name": "Christopher Thrasher",
                        "slug": "Christopher-Thrasher",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Thrasher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Thrasher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2832706"
                        ],
                        "name": "E. Viegas",
                        "slug": "E.-Viegas",
                        "structuredName": {
                            "firstName": "Evelyne",
                            "lastName": "Viegas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Viegas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108672800"
                        ],
                        "name": "Xiaolong Li",
                        "slug": "Xiaolong-Li",
                        "structuredName": {
                            "firstName": "Xiaolong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10171248"
                        ],
                        "name": "B. Hsu",
                        "slug": "B.-Hsu",
                        "structuredName": {
                            "firstName": "Bo-June",
                            "lastName": "Hsu",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 34
                            }
                        ],
                        "text": "Microsoft provides a web service (Wang et al., 2010) that can be queried for language model probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1326585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fce9075c2a186614e784a1df606ea32431863d08",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This document describes the properties and some applications of the Microsoft Web N-gram corpus. The corpus is designed to have the following characteristics. First, in contrast to static data distribution of previous corpus releases, this N-gram corpus is made publicly available as an XML Web Service so that it can be updated as deemed necessary by the user community to include new words and phrases constantly being added to the Web. Secondly, the corpus makes available various sections of a Web document, specifically, the body, title, and anchor text, as separates models as text contents in these sections are found to possess significantly different statistical properties and therefore are treated as distinct languages from the language modeling point of view. The usages of the corpus are demonstrated here in two NLP tasks: phrase segmentation and word breaking."
            },
            "slug": "An-Overview-of-Microsoft-Web-N-gram-Corpus-and-Wang-Thrasher",
            "title": {
                "fragments": [],
                "text": "An Overview of Microsoft Web N-gram Corpus and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The properties and some applications of the Microsoft Web N-gram corpus are described and the usages of the corpus are demonstrated here in two NLP tasks: phrase segmentation and word breaking."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784037"
                        ],
                        "name": "T. Brants",
                        "slug": "T.-Brants",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brants"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054252"
                        ],
                        "name": "Ashok Popat",
                        "slug": "Ashok-Popat",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Popat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashok Popat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 172
                            }
                        ],
                        "text": "Of particular importance is language modeling, where web-scale language models have been shown to improve machine translation and automatic speech recognition performance (Brants et al., 2007; Chelba and Schalkwyk, 2013; Guthrie and Hepple, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 633992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba786c46373892554b98df42df7af6f5da343c9d",
            "isKey": false,
            "numCitedBy": 533,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "slug": "Large-Language-Models-in-Machine-Translation-Brants-Popat",
            "title": {
                "fragments": [],
                "text": "Large Language Models in Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Systems, methods, and computer program products for machine translation are provided for backoff score determination as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145938140"
                        ],
                        "name": "Nadir Durrani",
                        "slug": "Nadir-Durrani",
                        "structuredName": {
                            "firstName": "Nadir",
                            "lastName": "Durrani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadir Durrani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277248"
                        ],
                        "name": "Alexander M. Fraser",
                        "slug": "Alexander-M.-Fraser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Fraser",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Fraser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360495"
                        ],
                        "name": "Helmut Schmid",
                        "slug": "Helmut-Schmid",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143620134"
                        ],
                        "name": "Hieu Hoang",
                        "slug": "Hieu-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu Hoang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 76
                            }
                        ],
                        "text": ", 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5907276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76232f8fa3377c7382220a196470d11cb30fb45c",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve."
            },
            "slug": "Can-Markov-Models-Over-Minimal-Translation-Units-Durrani-Fraser",
            "title": {
                "fragments": [],
                "text": "Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work investigates whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption, and shows that performance does significantly improve."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2955842"
                        ],
                        "name": "E. Hasler",
                        "slug": "E.-Hasler",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Hasler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hasler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259100"
                        ],
                        "name": "B. Haddow",
                        "slug": "B.-Haddow",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Haddow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Haddow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "\u20265-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 89
                            }
                        ],
                        "text": ", 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15881205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "444f86ad01fa21a0e3bcd9054d3e1129ee086342",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to domain adaptation for SMT that enriches standard phrase-based models with lexicalised word and phrase pair features to help the model select appropriate translations for the target domain (TED talks). In addition, we show how source-side sentence-level topics can be incorporated to make the features differentiate between more fine-grained topics within the target domain (topic adaptation). We compare tuning our sparse features on a development set versus on the entire in-domain corpus and introduce a new method of porting them to larger mixed-domain models. Experimental results show that our features improve performance over a MIRA baseline and that in some cases we can get additional improvements with topic features. We evaluate our methods on two language pairs, English-French and German-English, showing promising results."
            },
            "slug": "Sparse-lexicalised-features-and-topic-adaptation-Hasler-Haddow",
            "title": {
                "fragments": [],
                "text": "Sparse lexicalised features and topic adaptation for SMT"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new approach to domain adaptation for SMT is presented that enriches standard phrase-based models with lexicalised word and phrase pair features to help the model select appropriate translations for the target domain (TED talks)."
            },
            "venue": {
                "fragments": [],
                "text": "IWSLT"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 121
                            }
                        ],
                        "text": "The truecaser was trained on data from the 2014 Workshop on Statistical Machine Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al., 2011), and the news crawls provided by the evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 217
                            }
                        ],
                        "text": "In addition to deduplicating, we restricted the data to printable Unicode characters, replaced all e-mail addresses with the same address, stripped out remaining HTML, and split sentences using the Europarl splitter (Koehn, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "The truecaser was trained on data from the 2014 Workshop on Statistical Machine Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Europarl: A parallel corpus for sta-\ntistical machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 131
                            }
                        ],
                        "text": "The truecaser was trained on data from the 2014 Workshop on Statistical Machine Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38407095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "694b3c58712deefb59502847ba1b52b192c413e5",
            "isKey": true,
            "numCitedBy": 3396,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead."
            },
            "slug": "Europarl:-A-Parallel-Corpus-for-Statistical-Machine-Koehn",
            "title": {
                "fragments": [],
                "text": "Europarl: A Parallel Corpus for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A corpus of parallel text in 11 languages from the proceedings of the European Parliament is collected and its acquisition and application as training data for statistical machine translation (SMT) is focused on."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698491"
                        ],
                        "name": "J. Schalkwyk",
                        "slug": "J.-Schalkwyk",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Schalkwyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schalkwyk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 193
                            }
                        ],
                        "text": "Of particular importance is language modeling, where web-scale language models have been shown to improve machine translation and automatic speech recognition performance (Brants et al., 2007; Chelba and Schalkwyk, 2013; Guthrie and Hepple, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59871995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "310d10bb2e769c045fab251c8fd664d1bb593be4",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Mobile is poised to become the predominant platform over which people access the World Wide Web. Recent developments in speech recognition and understanding, backed by high bandwidth coverage and high quality speech signal acquisition on smartphones and tablets are presenting the users with the choice of speaking their web search queries instead of typing them. A critical component of a speech recognition system targeting web search is the language model. The chapter presents an empirical exploration of the google.com query stream with the end goal of high quality statistical language modeling for mobile voice search. Our experiments show that after text normalization the query stream is not as \u201cwild\u201d as it seems at first sight. One can achieve out-of-vocabulary rates below 1% using a 1 million word vocabulary, and excellent n-gram hit ratios of 77/88% even at high orders such as \\( n=5/4\\), respectively. A more careful analysis shows that a significantly larger vocabulary (approx. 10 million words) may be required to guarantee at most 1% out-of-vocabulary rate for a large percentage (95%) of users. Using large scale, distributed language models can improve performance significantly\u2014up to 10% relative reductions in word-error-rate over conventional models used in speech recognition. We also find that the query stream is non-stationary, which means that adding more past training data beyond a certain point provides diminishing returns, and may even degrade performance slightly. Perhaps less surprisingly, we have shown that locale matters significantly for English query data across USA, Great Britain and Australia. In an attempt to leverage the speech data in voice search logs, we successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance."
            },
            "slug": "Empirical-Exploration-of-Language-Modeling-for-the-Chelba-Schalkwyk",
            "title": {
                "fragments": [],
                "text": "Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In an attempt to leverage the speech data in voice search logs, this chapter successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 165
                            }
                        ],
                        "text": "For all languages for which we have sufficient data and a preprocessing pipeline, we produce unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143832874"
                        ],
                        "name": "Ondrej Bojar",
                        "slug": "Ondrej-Bojar",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Bojar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ondrej Bojar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064348170"
                        ],
                        "name": "C. Buck",
                        "slug": "C.-Buck",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Buck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3359291"
                        ],
                        "name": "C. Federmann",
                        "slug": "C.-Federmann",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Federmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Federmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259100"
                        ],
                        "name": "B. Haddow",
                        "slug": "B.-Haddow",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Haddow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Haddow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696402"
                        ],
                        "name": "Christof Monz",
                        "slug": "Christof-Monz",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Monz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christof Monz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38842528"
                        ],
                        "name": "Matt Post",
                        "slug": "Matt-Post",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Post",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Post"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702974"
                        ],
                        "name": "Lucia Specia",
                        "slug": "Lucia-Specia",
                        "structuredName": {
                            "firstName": "Lucia",
                            "lastName": "Specia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucia Specia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 136
                            }
                        ],
                        "text": "Before building language models, we normalized punctuation using the script provided by the Workshop on Statistical Machine Translation (Bojar et al., 2013), tokenized using the Moses tokenizer (Koehn et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "Before building language models, we normalized punctuation using the script provided by the Workshop on Statistical Machine Translation (Bojar et al., 2013), tokenized using the Moses tokenizer (Koehn et al., 2007), and applied the Moses truecaser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1009868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7de66a09cd23f05859a95fa55616b515acab71e9",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries."
            },
            "slug": "Findings-of-the-2013-Workshop-on-Statistical-Bojar-Buck",
            "title": {
                "fragments": [],
                "text": "Findings of the 2013 Workshop on Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task are presented."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152378023"
                        ],
                        "name": "Hieu T. Hoang",
                        "slug": "Hieu-T.-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu T. Hoang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895952"
                        ],
                        "name": "N. Bertoldi",
                        "slug": "N.-Bertoldi",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Bertoldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bertoldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46898156"
                        ],
                        "name": "Brooke Cowan",
                        "slug": "Brooke-Cowan",
                        "structuredName": {
                            "firstName": "Brooke",
                            "lastName": "Cowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brooke Cowan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2529583"
                        ],
                        "name": "Wade Shen",
                        "slug": "Wade-Shen",
                        "structuredName": {
                            "firstName": "Wade",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wade Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055137469"
                        ],
                        "name": "C. Moran",
                        "slug": "C.-Moran",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Moran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983801"
                        ],
                        "name": "R. Zens",
                        "slug": "R.-Zens",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143832874"
                        ],
                        "name": "Ondrej Bojar",
                        "slug": "Ondrej-Bojar",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Bojar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ondrej Bojar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057195055"
                        ],
                        "name": "Alexandra Constantin",
                        "slug": "Alexandra-Constantin",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Constantin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Constantin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082901914"
                        ],
                        "name": "Evan Herbst",
                        "slug": "Evan-Herbst",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Herbst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Herbst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 794019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
            "isKey": false,
            "numCitedBy": 5929,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
            },
            "slug": "Moses:-Open-Source-Toolkit-for-Statistical-Machine-Koehn-Hoang",
            "title": {
                "fragments": [],
                "text": "Moses: Open Source Toolkit for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An open-source toolkit for statistical machine translation whose novel contributions are support for linguistically motivated factors, confusion network decoding, and efficient data formats for translation models and language models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145938140"
                        ],
                        "name": "Nadir Durrani",
                        "slug": "Nadir-Durrani",
                        "structuredName": {
                            "firstName": "Nadir",
                            "lastName": "Durrani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadir Durrani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145775792"
                        ],
                        "name": "Hassan Sajjad",
                        "slug": "Hassan-Sajjad",
                        "structuredName": {
                            "firstName": "Hassan",
                            "lastName": "Sajjad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hassan Sajjad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143620134"
                        ],
                        "name": "Hieu Hoang",
                        "slug": "Hieu-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu Hoang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "The Hindi-English system uses transliteration for unknown words (Durrani et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9407699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa144b01862baa5de61d22fd3f922a3ddd54ac4d",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate three methods for integrating an unsupervised transliteration model into an end-to-end SMT system. We induce a transliteration model from parallel data and use it to translate OOV words. Our approach is fully unsupervised and language independent. In the methods to integrate transliterations, we observed improvements from 0.23-0.75 ( 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora."
            },
            "slug": "Integrating-an-Unsupervised-Transliteration-Model-Durrani-Sajjad",
            "title": {
                "fragments": [],
                "text": "Integrating an Unsupervised Transliteration Model into Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work induces a transliteration model from parallel data and uses it to translate OOV words and shows that the mined transliterations provide better rule coverage and translation quality compared to the gold standard transliterated corpora."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768480"
                        ],
                        "name": "Liang Huang",
                        "slug": "Liang-Huang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": ", 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 592,
                                "start": 580
                            }
                        ],
                        "text": "The baseline systems were trained using Moses (Koehn et al., 2007) with the following features: maximum sentence length of 80, grow-diag-final-and symmetrization of\nGIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3510512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy."
            },
            "slug": "Forest-Rescoring:-Faster-Decoding-with-Integrated-Huang-Chiang",
            "title": {
                "fragments": [],
                "text": "Forest Rescoring: Faster Decoding with Integrated Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work develops faster approaches for efficient decoding based on k-best parsing algorithms and demonstrates their effectiveness on both phrase-based and syntax-based MT systems."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1839533"
                        ],
                        "name": "Matthias Huck",
                        "slug": "Matthias-Huck",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Huck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Huck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145938140"
                        ],
                        "name": "Nadir Durrani",
                        "slug": "Nadir-Durrani",
                        "structuredName": {
                            "firstName": "Nadir",
                            "lastName": "Durrani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadir Durrani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444222"
                        ],
                        "name": "Nikolay Bogoychev",
                        "slug": "Nikolay-Bogoychev",
                        "structuredName": {
                            "firstName": "Nikolay",
                            "lastName": "Bogoychev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolay Bogoychev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "Additionally target-side language models over automatically built word-classes (Birch et al., 2013) were built."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12222565,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "dd8a63a1ed6ab755e0a860f0086416af13dceaa2",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the University of Edinburgh\u2019s spoken language translation (SLT) and machine translation (MT) systems for the IWSLT 2014 evaluation campaign. In the SLT track, we participated in the German\u2194English and English\u2192French tasks. In the MT track, we participated in the German\u2194English, English\u2192French, Arabic\u2194English, Farsi\u2192English, Hebrew\u2192English, Spanish\u2194English, and Portuguese-Brazil\u2194English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination."
            },
            "slug": "English-SLT-and-MT-system-description-for-the-IWSLT-Birch-Huck",
            "title": {
                "fragments": [],
                "text": "English SLT and MT system description for the IWSLT 2013 evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The University of Edinburgh\u2019s spoken language translation and machine translation systems for the IWSLT 2014 evaluation campaign are described and using unsupervised transliteration for languages which have a different script than English is explored."
            },
            "venue": {
                "fragments": [],
                "text": "IWSLT"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072975661"
                        ],
                        "name": "Heng Ji",
                        "slug": "Heng-Ji",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683714"
                        ],
                        "name": "S. Bergsma",
                        "slug": "S.-Bergsma",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Bergsma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bergsma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38177465"
                        ],
                        "name": "Kailash Patil",
                        "slug": "Kailash-Patil",
                        "structuredName": {
                            "firstName": "Kailash",
                            "lastName": "Patil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kailash Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585932"
                        ],
                        "name": "Emily Pitler",
                        "slug": "Emily-Pitler",
                        "structuredName": {
                            "firstName": "Emily",
                            "lastName": "Pitler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emily Pitler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446232"
                        ],
                        "name": "Rachel Lathbury",
                        "slug": "Rachel-Lathbury",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Lathbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Lathbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066762526"
                        ],
                        "name": "Vikram Rao",
                        "slug": "Vikram-Rao",
                        "structuredName": {
                            "firstName": "Vikram",
                            "lastName": "Rao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikram Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2774234"
                        ],
                        "name": "Kapil Dalwani",
                        "slug": "Kapil-Dalwani",
                        "structuredName": {
                            "firstName": "Kapil",
                            "lastName": "Dalwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kapil Dalwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597322"
                        ],
                        "name": "Sushant Narsale",
                        "slug": "Sushant-Narsale",
                        "structuredName": {
                            "firstName": "Sushant",
                            "lastName": "Narsale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sushant Narsale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 85
                            }
                        ],
                        "text": "Google has shared a deduplicated version (Bergsma et al., 2010) in limited contexts (Lin et al., 2010), but it was never publicly released (Lin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 209
                            }
                        ],
                        "text": "The second issue with the publicly available Google ngram counts (Brants and Franz, 2006) is that the training data was not deduplicated, so boilerplate such as copyright notices has unreasonably high counts (Lin et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 47
                            }
                        ],
                        "text": "Moreover, all words that appeared less than 200 times were replaced with the unknown word."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "Both forms of pruning make the counts unsuitable for estimating a language model with the popular and successful Kneser-Ney smoothing algorithm, which requires unpruned counts even if the final model is to be pruned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15828105,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b268eb411c846b04159a0bef2c30e30489517713",
            "isKey": true,
            "numCitedBy": 7,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "Google, Inc. ( lindek@google.com ), Johns Hopkins University ( kenneth.church@jhu.edu,yarowsky@cs.jhu.edu, kailash@jhu.edu, kapild@cs.jhu.edu, sushant@jhu.edu ), City University of New York (hengji@cs.qc.cuny.edu ), New York University (sekine@cs.nyu.edu ), University of Alberta (sbergsma@ualberta.ca ), University of Pennsylvania ( epitler@seas.upenn.edu ), University of Virginia (rlathbury@virginia.edu ), Cornell University (vr59@cornell.edu )"
            },
            "slug": "Unsupervised-Acquisition-of-Lexical-Knowledge-From-Lin-Church",
            "title": {
                "fragments": [],
                "text": "Unsupervised Acquisition of Lexical Knowledge From N-grams : Final Report of the 2009 JHU CLSP Workshop"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "The baseline systems performed well in the shared task as measured by BLEU yielding an improvement between 0.5% and 1.4% over the baseline as shown in Table 8."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "BLEU: A method for automatic evalution of machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "These results5 are based on automatic BLEU (Papineni et al., 2002) scores as human evaluations were not available at time of writing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": true,
            "numCitedBy": 16617,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143620134"
                        ],
                        "name": "Hieu Hoang",
                        "slug": "Hieu-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu Hoang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 176
                            }
                        ],
                        "text": "ical target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2330566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "659f1f754954d093e684ead4842832052f7bf748",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level \u2014 may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence."
            },
            "slug": "Factored-Translation-Models-Koehn-Hoang",
            "title": {
                "fragments": [],
                "text": "Factored Translation Models"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "In a number of experiments, it is shown that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9567965"
                        ],
                        "name": "Shankar Kumar",
                        "slug": "Shankar-Kumar",
                        "structuredName": {
                            "firstName": "Shankar",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shankar Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716907"
                        ],
                        "name": "W. Byrne",
                        "slug": "W.-Byrne",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Byrne",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Byrne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 92
                            }
                        ],
                        "text": ", 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11706155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2a68774f92d1e894cbbbef2c819e4592990eb4b",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions."
            },
            "slug": "Minimum-Bayes-Risk-Decoding-for-Statistical-Machine-Kumar-Byrne",
            "title": {
                "fragments": [],
                "text": "Minimum Bayes-Risk Decoding for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The results show that MBR decoding can be used to tune statistical MT performance for specific loss functions, and a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 165
                            }
                        ],
                        "text": "For all languages for which we have sufficient data and a preprocessing pipeline, we produce unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": false,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208875690,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "8cf24826e5bbe8e9e3161bdb98c0523b3ace5443",
            "isKey": false,
            "numCitedBy": 25208,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "11. Brunck W: Die systematische untersuchung des sprachorgans bei angeborenen gaumendefekte. Diss Leipiz 1906 (from Meissner). 12. Commichau G, Oeken FW: Pneumatisationsverhaitnisse bel gaumenspaltentragem. HNO (Bed) 7:73-75, 1958 13. Compere WE, Jr: Tympanic cavity clearance studies. Trans AAOO 62:444-454, 1958 14. Compere WE, Jr: The radiologic evaluation of eustachian tube function. AMA Arch Otolaryngol 7 :386-389, 1960 15. Day KM: Management of deafness: Wherry Memorial Lecture. Trans AAOO 55: 22, 1950 16. Donaldson JA: The role of artificial Eustachian tube in cleft palate patients. Cleft Palate J 61-66, 1966 17. Drettner B: The nasal air way and hearing in patients with cleft palate. Acta Otolaryng 52: 131-142, 1960 18. Duncan RB: Positional otitis media. Arch Otolaryng 72:455-463, 1960"
            },
            "slug": "References",
            "title": {
                "fragments": [],
                "text": "References"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "11. Brunck W: Die systematische untersuchung des sprachorgans bei angeborenen gaumendefekte ist wirklich ein wirkliches Problem gegen \u00e2\u201a\u00ac\u201d Pneumatisationsverhaitnisse bel gaumenspaltentragem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 195
                            }
                        ],
                        "text": "Before building language models, we normalized punctuation using the script provided by the Workshop on Statistical Machine Translation (Bojar et al., 2013), tokenized using the Moses tokenizer (Koehn et al., 2007), and applied the Moses truecaser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 45
                            }
                        ],
                        "text": ", 2013), tokenized using the Moses tokenizer (Koehn et al., 2007), and applied the Moses truecaser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "To this end we train a system using Moses and standard settings but all available parallel data as detailed in Section 2.3. with the exception of 2013 news data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "The baseline systems were trained using Moses (Koehn et al., 2007) with the following features: maximum sentence length of 80, grow-diag-final-and symmetrization of\nGIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 47
                            }
                        ],
                        "text": "The baseline systems were trained using Moses (Koehn et al., 2007) with the following features: maximum sentence length of 80, grow-diag-final-and symmetrization of\nGIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield et al., 2013) used at runtime, a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Moses: Open source toolkit for statistical machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 46
                            }
                        ],
                        "text": "The baseline systems were trained using Moses (Koehn et al., 2007) with the following features: maximum sentence length of 80, grow-diag-final-and symmetrization of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Open source toolkit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 34
                            }
                        ],
                        "text": "Google has released n-gram counts (Brants and Franz, 2006) trained on one trillion tokens of text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "The second issue with the publicly available Google ngram counts (Brants and Franz, 2006) is that the training data was not deduplicated, so boilerplate such as copyright notices has unreasonably high counts (Lin et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "In this work, we contribute n-gram counts and language models trained on the Common Crawl corpus.1\nGoogle has released n-gram counts (Brants and Franz, 2006) trained on one trillion tokens of text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": "The second issue with the publicly available Google ngram counts (Brants and Franz, 2006) is that the training data was not deduplicated, so boilerplate such as copyright notices has unreasonably high counts (Lin et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Google web 1T 5-gram corpus version"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation Association for Computational Linguis- tics"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eighth Workshop on Statistical Machine Translation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 44
                            }
                        ],
                        "text": ", 2010), but it was never publicly released (Lin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 140
                            }
                        ],
                        "text": "Google has shared a deduplicated version (Bergsma et al., 2010) in limited contexts (Lin et al., 2010), but it was never publicly released (Lin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Personal communication, October"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 228
                            }
                        ],
                        "text": "\u2026Translation for each language, including Europarl (Koehn, 2005), United Nations parallel data, the Giga Fr-En corpus, parallel data mined from CommonCrawl (Smith et al., 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al., 2011), and the news crawls provided by the evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 58
                            }
                        ],
                        "text": ", 2013), the news commentary corpus, LDC Gigaword corpora (Parker et al., 2011), and the news crawls provided by the evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English gigaword fifth edition, June"
            },
            "venue": {
                "fragments": [],
                "text": "LDC2011T07."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "The second issue with the publicly available Google ngram counts (Brants and Franz, 2006) is that the training data was not deduplicated, so boilerplate such as copyright notices has unreasonably high counts (Lin et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "In this work, we contribute n-gram counts and language models trained on the Common Crawl corpus.1\nGoogle has released n-gram counts (Brants and Franz, 2006) trained on one trillion tokens of text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Google web 1T 5-gram corpus version 1.1. LDC2006T13"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/N-gram-Counts-and-Language-Models-from-the-Common-Buck-Heafield/8e4fb17fff38a7834af5b4eaafcbbde02bf00975?sort=total-citations"
}