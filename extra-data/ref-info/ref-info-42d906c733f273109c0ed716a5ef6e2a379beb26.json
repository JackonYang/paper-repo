{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 200
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 54
                            }
                        ],
                        "text": "One recent success along these lines was developed by Olshausen and Field (1996, 1997) from the viewpoint of learning sparse codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 107
                            }
                        ],
                        "text": "Some recent approaches have tried to approximate this integral by evaluating P(s)P(x|A, s) at its maximum (Olshausen & Field, 1996, 1997), but this ignores the volume information of the posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 87
                            }
                        ],
                        "text": "A is invertible, this integral can be solved and leads to the well known ICA algorithm (MacKay, 1996; Pearlmutter and Parra, 1997; Olshausen and Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": true,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 131
                            }
                        ],
                        "text": "For the learning rule, we replaced the term A in equation A.39 with an approximation to \u03bbAATAH\u22121 (see equation A.36) as suggested in Lewicki\nand Olshausen (1998, 1999):\n\u2212\u03bbATAH\u22121 \u2248 I\u2212 BQ diag\u22121[V+QTBQ]QT, (7.1)\nwhere B = \u2207s\u2207s log P(s\u0302) and Q and V are obtained from the singular value decomposition\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 179
                            }
                        ],
                        "text": "If \u03bb is large (low noise), then the Hessian is dominated by \u03bbATA and\n\u2212\u03bbAATAH\u22121 = \u2212A\u03bbATA(\u03bbATA+ B)\u22121 (A.35) \u2248 \u2212A. (A.36)\nIt is also possible to obtain more accurate approximations of this term (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 76
                            }
                        ],
                        "text": "This approach to denoising has been applied successfully to natural images (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6482128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87e0d75a8c17e464cf8e95a0466533e14b97c5e",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a Bayesian method for inferring an optimal basis to the problem of finding efficient image codes for natural scenes. The basis functions learned by the algorithm are oriented and localized in both space and frequency, bearing a resemblance to two-dimensional Gabor functions, and increasing the number of basis functions results in a greater sampling density in position, orientation, and scale. These properties also resemble the spatial receptive fields of neurons in the primary visual cortex of mammals, suggesting that the receptive-field structure of these neurons can be accounted for by a general efficient coding principle. The probabilistic framework provides a method for comparing the coding efficiency of different bases objectively by calculating their probability given the observed data or by measuring the entropy of the basis function coefficients. The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases. This framework also provides a Bayesian solution to the problems of image denoising and filling in of missing pixels. We demonstrate that the results obtained by applying the learned bases to these problems are improved over those obtained with traditional techniques."
            },
            "slug": "PROBABILISTIC-FRAMEWORK-FOR-THE-ADAPTATION-AND-OF-Lewicki-Olshausen",
            "title": {
                "fragments": [],
                "text": "PROBABILISTIC FRAMEWORK FOR THE ADAPTATION AND COMPARISON OF IMAGE CODES"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases and to provide a Bayesian solution to the problems of image denoising and filling in of missing pixels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 50
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 148
                            }
                        ],
                        "text": "\u2026ignored, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2344498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4547e5e6ac2e1fcb0ba51edbbe5d65b5765b4ca",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "In the context of both sensory coding and signal processing, building factorized codes has been shown to be an efficient strategy. In a wide variety of situations, the signal to be processed is a linear mixture of statistically independent sources. Building a factorized code is then equivalent to performing blind source separation. Thanks to the linear structure of the data, this can be done, in the language of signal processing, by finding an appropriate linear filter, or equivalently, in the language of neural modeling, by using a simple feedforward neural network. In this article, we discuss several aspects of the source separation problem. We give simple conditions on the network output that, if satisfied, guarantee that source separation has been obtained. Then we study adaptive approaches, in particular those based on redundancy reduction and maximization of mutual information. We show how the resulting updating rules are related to the BCM theory of synaptic plasticity. Eventually we briefly discuss extensions to the case of nonlinear mixtures. Through out this article, we take care to put into perspective our work with other studies on source separation and redundancy reduction. In particular we review algebraic solutions, pointing out their simplicity but also their drawbacks."
            },
            "slug": "Redundancy-Reduction-and-Independent-Component-on-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Redundancy Reduction and Independent Component Analysis: Conditions on Cumulants and Adaptive Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This article gives simple conditions on the network output that guarantee that source separation has been obtained and shows how the resulting updating rules are related to the BCM theory of synaptic plasticity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 54
                            }
                        ],
                        "text": "One recent success along these lines was developed by Olshausen and Field (1996, 1997) from the viewpoint of learning sparse codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Univ., 115 Mellon Inst., 4400 Fifth Ave., Pittsburgh, PA 15213"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 69
                            }
                        ],
                        "text": "This problem can be somewhat circumvented by adaptive normalization (Olshausen & Field, 1996), but setting the adaptation rates can be tricky in practice, and, more important,\nthere is no guarantee the desired objective function is the one being optimized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 107
                            }
                        ],
                        "text": "Some recent approaches have tried to approximate this integral by evaluating P(s)P(x|A, s) at its maximum (Olshausen & Field, 1996, 1997), but this ignores the volume information of the posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 181
                            }
                        ],
                        "text": "A general approach for optimizing s in the case of finite noise (\u00b2 > 0) and nongaussian P(s) is to use the gradient of the log posterior in an optimization algorithm (Daugman, 1988; Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 63
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 39
                            }
                        ],
                        "text": "This work generalizes the algorithm of Olshausen and Field (1996) by deriving a algorithm for learning overcomplete bases from a direct approximation to the data likelihood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": true,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3046011"
                        ],
                        "name": "S. O. Aase",
                        "slug": "S.-O.-Aase",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Aase",
                            "middleNames": [
                                "Ole"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O. Aase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31557075"
                        ],
                        "name": "J. H. Hus\u00f8y",
                        "slug": "J.-H.-Hus\u00f8y",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hus\u00f8y",
                            "middleNames": [
                                "H\u00e5kon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hus\u00f8y"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2754637"
                        ],
                        "name": "K. Skretting",
                        "slug": "K.-Skretting",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Skretting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Skretting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18439380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d16d7b4d28db1c0fb8356fb383f73d01a120734d",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional signal decompositions such as transforms, filterbanks, and wavelets generate signal expansions using the analysis-synthesis setting: the expansion coefficients are found by taking the inner product of the signal with the corresponding analysis vector. In this paper, we try to free ourselves from the analysis-synthesis paradigm by concentrating on the synthesis or reconstruction part of the signal expansion. Ignoring the analysis issue completely, we construct sets of synthesis vectors, which are denoted waveform dictionaries, for efficient signal representation. Within this framework, we present an algorithm for designing waveform dictionaries that allow sparse representations: the objective is to approximate a training signal using a small number of dictionary vectors. Our algorithm optimizes the dictionary vectors with respect to the average nonlinear approximation error, i.e., the error resulting when keeping a fixed number n of expansion coefficients but not necessarily the first n coefficients. Using signals from a Gaussian, autoregressive process with correlation factor 0.95, it is demonstrated that for established signal expansions like the Karhunen-Loeve transform, the lapped orthogonal transform, and the biorthogonal 7/9 wavelet, it is possible to improve the approximation capabilities by up to 30% by fine tuning of the expansion vectors."
            },
            "slug": "Optimized-signal-expansions-for-sparse-Aase-Hus\u00f8y",
            "title": {
                "fragments": [],
                "text": "Optimized signal expansions for sparse representation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that for established signal expansions like the Karhunen-Loeve transform, the lapped orthogonal transform, and the biorthogonal 7/9 wavelet, it is possible to improve the approximation capabilities by up to 30% by fine tuning of the expansion vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 403,
                                "start": 304
                            }
                        ],
                        "text": "If implementation-related issues such as synaptic noise are ignored, this is equivalent, to the methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal and Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961; Hinton and Sejnowski, 1986; Barlow, 1989; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 256
                            }
                        ],
                        "text": "\u2026methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20021288,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "da9cee8647743711c1f8b4f61185bfbece0ad284",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In biological visual systems, it is not obvious whether coding efficiency as measured by mutual information among the neurons is a factor that explains any of their properties. The center/surround receptive field profiles of neurons in the retina and geniculate are far from an orthogonal set, but a given neuron can still be regarded as a decorrelator of the incoming signal in the sense that it responds primarily to changes in the image. At the level of the brain's visual cortex, the introduction of the new variable of orientation selectivity can be regarded not only as a means for providing orientation labels for image structure, but also more basically as an effective decorrelator of the neural representation. The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes. Demonstrations of data compression to below 1 bit/pixel in cortically-based, quadrature self-similar wavelet image codes are also provided.<<ETX>>"
            },
            "slug": "Entropy-reduction-and-decorrelation-in-visual-by-Daugman",
            "title": {
                "fragments": [],
                "text": "Entropy reduction and decorrelation in visual coding by oriented neural receptive fields"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Biomedical Engineering"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079197069"
                        ],
                        "name": "AnalysisDavid",
                        "slug": "AnalysisDavid",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "AnalysisDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "AnalysisDavid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410246062"
                        ],
                        "name": "C. J.",
                        "slug": "C.-J.",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087372590"
                        ],
                        "name": "MacKayUniversity",
                        "slug": "MacKayUniversity",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "MacKayUniversity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MacKayUniversity"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Following MacKay (1996) we have\n\u2202 s\u030ck \u2202 a\u030cij = \u2202 \u2202 a\u030cij \u2211 l A\u030c\u22121kl (xl \u2212 \u00b2l), (A.13)\nUsing the identity \u2202A\u22121kl /\u2202aij = \u2212A\u22121ki A\u22121jl ,\n\u2202 s\u030ck \u2202 a\u030cij = \u2212 \u2211 l A\u030c\u22121ki A\u030c \u22121 jl (xl \u2212 \u00b2l)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 89
                            }
                        ],
                        "text": ", A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 603999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "074dc8903635bd463499a634f24d67638da80b46",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Bell and Sejnowski (1995) have derived a blind signal processing algorithm for a non-linear feedforward network from an information maximization viewpoint. This paper first shows that the same algorithm can be viewed as a maximum likelihood algorithm for the optimization of a linear generative model. Second, a covariant version of the algorithm is derived. This algorithm is simpler and somewhat more biologically plausible, involving no matrix inversions; and it converges in a smaller number of iterations. Third, this paper gives a partial proof of the \u2018folk-theorem\u2019 that any mixture of sources with high-kurtosis histograms is separable by the classic ICA algorithm. Fourth, a collection of formulae are given that may be useful for the adaptation of the non-linearity in the ICA algorithm. 1 Blind separation Algorithms for blind separation (Jutten and Herault 1991; Comon et al. 1991; Bell and Sejnowski 1995; Hendin et al. 1994) attempt to recover source signals s from observations x which are linear mixtures (with unknown coefficients V) of the source signals x = Vs. (1) The algorithms attempt to create the inverse of V (within a post-multiplicative factor) given only a set of examples {x}. Bell and Sejnowski (1995) have derived a blind separation algorithm from an information maximization viewpoint. The algorithm may be summarised as a linear mapping: a = Wx (2) followed by a non-linear map: zi = \u03c6i(ai), (3) where, for example, \u03c6 = \u2212 tanh(ai), with a learning rule: \u2206W \u221d [WT] + zxT. (4) 1 Another non-linear function of ai, yi = g(ai), is also mentioned by Bell and Sejnowski, but it will not be needed here. This paper has four parts. First it is shown that Bell and Sejnowski\u2019s (1995) algorithm may be derived as a maximum likelihood algorithm. This has been independently pointed out by Pearlmutter and Parra (1996) who also give an exciting generalization of the ICA algorithm. Second, it is pointed out that the algorithm (4) is not covariant , and a covariant algorithm is described which is simpler, faster, and somewhat more biologically plausible. This covariant algorithm has been independently suggested by Amari et al. (1996) and is used by Pearlmutter and Parra (1996). Third, this paper gives a partial proof of the \u2018folk-theorem\u2019 that any mixture of sources with high-kurtosis histograms is separable by the classic ICA algorithm. Fourth, a collection of formulae are given that may be useful for the adaptation of the nonlinearity in the ICA algorithm. 2 Maximum likelihood derivation of ICA 2.1 Latent variable models Many statistical models are generative models that make use of latent variables to describe a probability distribution over observables (Everitt 1984). Examples of latent variable models include mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (Hanson et al. 1991) (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang 1986); factor analysis; Helmholtz machines (Hinton et al. 1995; Dayan et al. 1995); and density networks (MacKay 1995; MacKay 1996). Note that it is usual for the latent variables to have a simple distribution, often a separable distribution. Thus when we learn a latent variable model, we are finding a description of the data in terms of independent components. One thus might expect that an \u2018independent component analysis\u2019 algorithm should have a description in terms of a generative latent variable model. And this is indeed the case. Independent component analysis is latent variable modelling. 2.2 The generative model Let us model the observable vector x = {xj}j=1 as being generated from latent variables s = {si}i=1 via a linear mapping V. The simplest derivation results if we assume I = J , i.e., the number of sources is equal to the number of observations. The data we obtain are a set of N observations D = {x}n=1. We assume that the latent variables are independently distributed, with marginal distributions P (si|H) \u2261 pi(si). Here H denotes the assumed form of this model and the assumed probability distributions pi of the latent variables. The probability of the observables and the hidden variables, given V and H, is: P ({x}n=1, {s}n=1|V,H) = N \u220f"
            },
            "slug": "Maximum-Likelihood-and-Covariant-Algorithms-for-AnalysisDavid-C.",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood and Covariant Algorithms for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that Bell and Sejnowski\u2019s (1995) algorithm can be viewed as a maximum likelihood algorithm for the optimization of a linear generative model and a covariant version of the algorithm is derived."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 180
                            }
                        ],
                        "text": "An overcomplete Fourier basis, with more than the minimum number of sinusoids, can compactly represent signals composed of small numbers of frequencies, achieving superresolution (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Univ., 115 Mellon Inst., 4400 Fifth Ave., Pittsburgh, PA 15213"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "LETTER Communicated by Joachim Buhmann"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "This can be solved efficiently and exactly with interior point linear programming methods (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 161
                            }
                        ],
                        "text": "In applications of overcomplete representations, it is common to assume the coefficients are independent and have Laplacian distributions (Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "One approach to removing the degeneracy in equation 1.1 is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), for example, by finding s satisfying equation 1.1 with minimum L1 norm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 178
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "An alternative method, which can be used when the prior is Laplacian and \u00b2 = 0, is to view the problem as a linear program (Chen et al., 1996):\nmin cT|s| subject to As = x. (3.1)\nLetting c = (1, . . . , 1), the objective function in the linear program, cT|s| =\u2211 m |sm|, corresponds to maximizing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 84
                            }
                        ],
                        "text": "The standard implementation handles only the noiseless case but can be generalized (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": true,
            "numCitedBy": 9740,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In the case where A is square, this form of the rule is exactly the natural gradient ICA learning rule for the basis matrix (Amari, Cichocki, & Yang, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "For the 2 \u00d7 2 matrices (the ICA solution), the entropy estimate yields approximately the same coding cost as the estimate based on P(x|A), which indicates that in the complete case, the computation of P(x|A) is accurate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "In the complete case, the model is equivalent to ICA, but with additive noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "If the density is more complicated, however, as in the case of the three-armed density, neither PCA nor ICA captures the underlying structure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 102
                            }
                        ],
                        "text": "ICA is highly e ective in several applications such as blind source separation of mixed audio signals (Jutten and Herault, 1991; Bell and Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 103
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995), allows the learning of nonorthogonal bases for data with nongaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 65
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1995), allows the learning of non-orthogonal bases for data with non-Gaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 70
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1995) and provides a method for the identi cation of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995) and provides a method for the identification of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ICA assumes the coefficients have nongaussian structure and allows the vectors to be nonorthogonal."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Unlike standard ICA, where the internal states are computed by inverting the basis function matrix, in this model the transformation from the data to the internal representation is nonlinear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "This algorithm generalizes ICA so that the model accounts for additive noise and allows the basis to be overcomplete."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 128
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The 2 \u00d7 2 basis matrix is\nequivalent to the ICA solution (under the assumption of a Laplacian prior)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "In the standard ICA learning algorithm (A square, zero noise), the coefficients are given by s =Wx, where W = A\u22121 is the filter matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28449499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bfbf789df18c8a6f825ef4f8777a0f145bf3b0f",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "An expectation-maximization algorithm for learning sparse and overcomplete data representations is presented. The proposed algorithm exploits a variational approximation to a range of heavy-tailed distributions whose limit is the Laplacian. A rigorous lower bound on the sparse prior distribution is derived, which enables the analytic marginalization of a lower bound on the data likelihood. This lower bound enables the development of an expectation-maximization algorithm for learning the overcomplete basis vectors and inferring the most probable basis coefficients."
            },
            "slug": "A-Variational-Method-for-Learning-Sparse-and-Girolami",
            "title": {
                "fragments": [],
                "text": "A Variational Method for Learning Sparse and Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An expectation-maximization algorithm for learning sparse and overcomplete data representations is presented, which exploits a variational approximation to a range of heavy-tailed distributions whose limit is the Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709398"
                        ],
                        "name": "M. Wickerhauser",
                        "slug": "M.-Wickerhauser",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wickerhauser",
                            "middleNames": [
                                "Victor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wickerhauser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 86
                            }
                        ],
                        "text": "A di erent approach is to iteratively construct a sparse representation of the signal (Coifman and Wickerhauser, 1992; Mallat and Zhang, 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 126
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 88
                            }
                        ],
                        "text": "A different approach is to construct iteratively a sparse representation of the signal (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 125
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman and Wickerhauser, 1992; Mallat and Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 546882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5478a91c183c3a460bd4098acb8927bfc671367c",
            "isKey": true,
            "numCitedBy": 3339,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals. It permits efficient compression of a variety of signals, such as sound and images. The predefined libraries of modulated waveforms include orthogonal wavelet-packets and localized trigonometric functions, and have reasonably well-controlled time-frequency localization properties. The idea is to build out of the library functions an orthonormal basis relative to which the given signal or collection of signals has the lowest information cost. The method relies heavily on the remarkable orthogonality properties of the new libraries: all expansions in a given library conserve energy and are thus comparable. Several cost functionals are useful; one of the most attractive is Shannon entropy, which has a geometric interpretation in this context. >"
            },
            "slug": "Entropy-based-algorithms-for-best-basis-selection-Coifman-Wickerhauser",
            "title": {
                "fragments": [],
                "text": "Entropy-based algorithms for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals, and relies heavily on the remarkable orthogonality properties of the new libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6298687,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b8da1aec8d5542abe815f58caaa99febef525aa7",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Empirical results were obtained for the blind source separation of more sources than mixtures using a previously proposed framework for learning overcomplete representations. This technique assumes a linear mixing model with additive noise and involves two steps: (1) learning an overcomplete representation for the observed data and (2) inferring sources given a sparse prior on the coefficients. We demonstrate that three speech signals can be separated with good fidelity given only two mixtures of the three signals. Similar results were obtained with mixtures of two speech signals and one music signal."
            },
            "slug": "Blind-source-separation-of-more-sources-than-using-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "Blind source separation of more sources than mixtures using overcomplete representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that three speech signals can be separated with good fidelity given only two mixtures of the three signals."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32052472"
                        ],
                        "name": "R. Buccigrossi",
                        "slug": "R.-Buccigrossi",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Buccigrossi",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Buccigrossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 76
                            }
                        ],
                        "text": "When this distribution is fitted to wavelet subband coefficients of images, Buccigrossi and Simoncelli (1997) found that p was in the range [0.5, 1.0]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1887438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "156ed6cd90506cd1111dffcd8e80e33ff62ca5ae",
            "isKey": true,
            "numCitedBy": 620,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature."
            },
            "slug": "Image-compression-via-joint-statistical-in-the-Buccigrossi-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Image compression via joint statistical characterization in the wavelet domain"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Markov model is proposed that explains dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and it is shown that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 487757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20cef0f008ae5c68795220389b2d83b38850dc23",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised classification algorithm is derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities. The algorithm estimates the data density in each class by using parametric nonlinear functions that fit to the non-Gaussian structure of the data. This improves classification accuracy compared with standard Gaussian mixture models. When applied to images, the algorithm can learn efficient codes (basis functions) for images that capture the statistically significant structure intrinsic in the images. We apply this technique to the problem of unsupervised classification, segmentation, and denoising of images. We demonstrate that this method was effective in classifying complex image textures such as natural scenes and text. It was also useful for denoising and filling in missing pixels in images with complex structures. The advantage of this model is that image codes can be learned with increasing numbers of classes thus providing greater flexibility in modeling structure and in finding more image features than in either Gaussian mixture models or standard independent component analysis (ICA) algorithms."
            },
            "slug": "Unsupervised-image-classification,-segmentation,-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "Unsupervised image classification, segmentation, and enhancement using ICA mixture models"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper demonstrates that the unsupervised classification method, derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities, was effective in classifying complex image textures such as natural scenes and text."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "LETTER Communicated by Joachim Buhmann"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 90
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995), allows the learning of nonorthogonal bases for data with nongaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995) and provides a method for the identification of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": true,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3240791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d2225253964183b14abcd168d024efe00422dd3",
            "isKey": false,
            "numCitedBy": 2145,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of current face recognition algorithms use face representations found by unsupervised statistical methods. Typically these methods find a set of basis images and represent faces as a linear combination of those images. Principal component analysis (PCA) is a popular example of such methods. The basis images found by PCA depend only on pairwise relationships between pixels in the image database. In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to these high-order statistics. Independent component analysis (ICA), a generalization of PCA, is one such method. We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons. ICA was performed on face images in the FERET database under two different architectures, one which treated the images as random variables and the pixels as outcomes, and a second which treated the pixels as random variables and the images as outcomes. The first architecture found spatially local basis images for the faces. The second architecture produced a factorial face code. Both ICA representations were superior to representations based on PCA for recognizing faces across days and changes in expression. A classifier that combined the two ICA representations gave the best performance."
            },
            "slug": "Face-recognition-by-independent-component-analysis-Bartlett-Movellan",
            "title": {
                "fragments": [],
                "text": "Face recognition by independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Independent component analysis (ICA), a generalization of PCA, was used, using a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons, which was superior to representations based on PCA for recognizing faces across days and changes in expression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "This can be alleviated, however, by multiplying the gradient by an appropriate positive definite matrix (Amari et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 137
                            }
                        ],
                        "text": "We note that in the case where A is square, this form of the rule is exactly the natural gradient ICA learning rule for the basis matrix (Amari et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 238
                            }
                        ],
                        "text": "\u2026uses so-called overcomplete bases (also called overcomplete dictionaries), which allow a greater number of basis functions (also called dictionary elements) than samples in the input signal (Simoncelli, Freeman, Adelson, & Heeger, 1992; Mallat & Zhang, 1993; Chen, Donoho, & Saunders, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "A different approach is to construct iteratively a sparse representation of the signal (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "LETTER Communicated by Joachim Buhmann"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 139
                            }
                        ],
                        "text": "In applications of overcomplete representations, it is common to assume the coefficients are independent and have Laplacian distributions (Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 156
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": true,
            "numCitedBy": 8852,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 173
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 87
                            }
                        ],
                        "text": "A is invertible, this integral can be solved and leads to the well known ICA algorithm (MacKay, 1996; Pearlmutter and Parra, 1997; Olshausen and Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9704838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cea9d59691410447bde0f39a028ffb3e21181a3",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result xi(t) of mixing n unknown independent sources si(t) through an unknown n \u00d7 n mixing matrix A(t) of causal linear filters: xi = \u03a3j aij * sj. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm \"Contextual ICA,\" after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "slug": "Maximum-Likelihood-Blind-Source-Separation:-A-of-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting algorithm is called cICA, after the (Bell and Sejnowski 1995) Infomax algorithm, which is able to separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713485"
                        ],
                        "name": "T. Wachtler",
                        "slug": "T.-Wachtler",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wachtler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wachtler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150321155"
                        ],
                        "name": "T. W. Lee",
                        "slug": "T.-W.-Lee",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Lee",
                            "middleNames": [
                                "Weng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8948394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "056cfc21c6677e4469516a87f6c3dcb704a70e86",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We applied independent component analysis (ICA) to hyperspectral images in order to learn an efficient representation of color in natural scenes. In the spectra of single pixels, the algorithm found basis functions that had broadband spectra and basis functions that were similar to natural reflectance spectra. When applied to small image patches, the algorithm found some basis functions that were achromatic and others with overall chromatic variation along lines in color space, indicating color opponency. The directions of opponency were not strictly orthogonal. Comparison with principal-component analysis on the basis of statistical measures such as average mutual information, kurtosis, and entropy, shows that the ICA transformation results in much sparser coefficients and gives higher coding efficiency. Our findings suggest that nonorthogonal opponent encoding of photoreceptor signals leads to higher coding efficiency and that ICA may be used to reveal the underlying statistical properties of color information in natural scenes."
            },
            "slug": "Chromatic-structure-of-natural-scenes.-Wachtler-Lee",
            "title": {
                "fragments": [],
                "text": "Chromatic structure of natural scenes."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The findings suggest that nonorthogonal opponent encoding of photoreceptor signals leads to higher coding efficiency and that ICA may be used to reveal the underlying statistical properties of color information in natural scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics, image science, and vision"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2383270"
                        ],
                        "name": "S. Malaroiu",
                        "slug": "S.-Malaroiu",
                        "structuredName": {
                            "firstName": "Simona",
                            "lastName": "Malaroiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Malaroiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41226710"
                        ],
                        "name": "Mika Ilmoniemi",
                        "slug": "Mika-Ilmoniemi",
                        "structuredName": {
                            "firstName": "Mika",
                            "lastName": "Ilmoniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mika Ilmoniemi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14830837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bddd49edd8b8b633e36f3bc9f688760d05e9d7c",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "In standard Independent Component Analysis (ICA), a linear data model is used for a global description of the data. Even though linear ICA yields meaningful results in many cases, it can provide a crude approximation only for general nonlinear data distributions. In this paper a new structure is proposed, where local ICA models are used in connection with a suitable grouping algorithm clustering the data. The clustering part is responsible for an overall coarse nonlinear representation of the data, while linear ICA models of each cluster are used for describing local features of the data. The goal is to represent the data better than in linear ICA while avoiding computational difficulties related with nonlinear ICA. Several data grouping methods are considered, including standard K-means clustering, self-organizing maps, and neural gas. Connections to existing methods are discussed, and experimental results are given for artificial data and natural images. Furthermore, a general theoretical framework encompassing a large number of methods for representing data is introduced. These range from global, dense representation methods to local, very sparse coding methods. The proposed local ICA methods lie between these two extremes."
            },
            "slug": "Local-Linear-Independent-Component-Analysis-Based-Karhunen-Malaroiu",
            "title": {
                "fragments": [],
                "text": "Local Linear Independent Component Analysis Based on Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new structure is proposed, where local ICA models are used in connection with a suitable grouping algorithm clustering the data, to represent the data better than in linear ICA while avoiding computational difficulties related with nonlinear ICA."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "Quadratic programming approaches to this type of problem have also recently been suggested (Osuna et al., 1997) for similar problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "LETTER Communicated by Joachim Buhmann"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "An additional advantage of some overcomplete representations is increased stability of the representation in response to small perturbations of the signal (Simoncelli et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1428,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 169671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ec3a3b0475b26c2d35d5ce644eea13440fed410",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised classification algorithm is derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities. The algorithm estimates the density of each class and is able to model class distributions with non-Gaussian structure. The new algorithm can improve classification accuracy compared with standard Gaussian mixture models. When applied to blind source separation in nonstationary environments, the method can switch automatically between classes, which correspond to contexts with different mixing properties. The algorithm can learn efficient codes for images containing both natural scenes and text. This method shows promise for modeling non-Gaussian structure in high-dimensional data and has many potential applications."
            },
            "slug": "ICA-Mixture-Models-for-Unsupervised-Classification-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "ICA Mixture Models for Unsupervised Classification of Non-Gaussian Classes and Automatic Context Switching in Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The algorithm estimates the density of each class and is able to model class distributions with non-Gaussian structure and can improve classification accuracy compared with standard Gaussian mixture models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 403,
                                "start": 304
                            }
                        ],
                        "text": "If implementation-related issues such as synaptic noise are ignored, this is equivalent, to the methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal and Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961; Hinton and Sejnowski, 1986; Barlow, 1989; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 271
                            }
                        ],
                        "text": "\u2026methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In the case where A is square, this form of the rule is exactly the natural gradient ICA learning rule for the basis matrix (Amari, Cichocki, & Yang, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "For the 2 \u00d7 2 matrices (the ICA solution), the entropy estimate yields approximately the same coding cost as the estimate based on P(x|A), which indicates that in the complete case, the computation of P(x|A) is accurate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "In the complete case, the model is equivalent to ICA, but with additive noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "If the density is more complicated, however, as in the case of the three-armed density, neither PCA nor ICA captures the underlying structure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995), allows the learning of nonorthogonal bases for data with nongaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 71
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995) and provides a method for the identification of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ICA assumes the coefficients have nongaussian structure and allows the vectors to be nonorthogonal."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Unlike standard ICA, where the internal states are computed by inverting the basis function matrix, in this model the transformation from the data to the internal representation is nonlinear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "This algorithm generalizes ICA so that the model accounts for additive noise and allows the basis to be overcomplete."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The 2 \u00d7 2 basis matrix is\nequivalent to the ICA solution (under the assumption of a Laplacian prior)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "In the standard ICA learning algorithm (A square, zero noise), the coefficients are given by s =Wx, where W = A\u22121 is the filter matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": true,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13960919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4093e53d0a6c0600d16adac5d9c545b01b90dce0",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel way of performing independent component analysis using a constrained version of the expectation-maximization (EM) algorithm. The source distributions are modeled as D one-dimensional mixtures of gaussians. The observed data are modeled as linear mixtures of the sources with additive, isotropic noise. This generative model is fit to the data using constrained EM. The simpler soft-switching approach is introduced, which uses only one parameter to decide on the sub- or supergaussian nature of the sources. We explain how our approach relates to independent factor analysis."
            },
            "slug": "A-Constrained-EM-Algorithm-for-Independent-Analysis-Welling-Weber",
            "title": {
                "fragments": [],
                "text": "A Constrained EM Algorithm for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel way of performing independent component analysis using a constrained version of the expectation-maximization (EM) algorithm and the simpler soft-switching approach is introduced, which uses only one parameter to decide on the sub- or supergaussian nature of the sources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88361182"
                        ],
                        "name": "Glen D. Brown",
                        "slug": "Glen-D.-Brown",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Brown",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glen D. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113570185"
                        ],
                        "name": "S. Yamada",
                        "slug": "S.-Yamada",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 511254,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4b7cc36c7af294305da8ec5ebdc1b350682ee53f",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis-at-the-neural-party-Brown-Yamada",
            "title": {
                "fragments": [],
                "text": "Independent component analysis at the neural cocktail party"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "A general approach for optimizing s in the case of finite noise (\u00b2 > 0) and nongaussian P(s) is to use the gradient of the log posterior in an optimization algorithm (Daugman, 1988; Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 196
                            }
                        ],
                        "text": "3 Inferring the internal state A general approach for optimizing s in the case of nite noise ( > 0) and non-Gaussian P (s) is to use the gradient of the log posterior in an optimization algorithm (Daugman, 1988; Olshausen and Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1984348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6513888c5ef473bdbb3167c7b52f0985be071f7a",
            "isKey": false,
            "numCitedBy": 1899,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D Gabor representations for image analysis, segmentation, and compression. These transforms are conjoint spatial/spectral representations, which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. In the present neural network approach, based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions. In wavelet expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20:1. Also demonstrated is image segmentation based on the clustering of coefficients in the complete 2-D Gabor transform. >"
            },
            "slug": "Complete-discrete-2-D-Gabor-transforms-by-neural-Daugman",
            "title": {
                "fragments": [],
                "text": "Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A three-layered neural network based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions for image analysis, segmentation, and compression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34212067"
                        ],
                        "name": "S. Abdallah",
                        "slug": "S.-Abdallah",
                        "structuredName": {
                            "firstName": "Samer",
                            "lastName": "Abdallah",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abdallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34894065"
                        ],
                        "name": "J. Bello",
                        "slug": "J.-Bello",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Bello",
                            "middleNames": [
                                "Pablo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113976"
                        ],
                        "name": "M. Davies",
                        "slug": "M.-Davies",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Davies",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34120006"
                        ],
                        "name": "Giuliano Monti",
                        "slug": "Giuliano-Monti",
                        "structuredName": {
                            "firstName": "Giuliano",
                            "lastName": "Monti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Giuliano Monti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40764812"
                        ],
                        "name": "M. Sandler",
                        "slug": "M.-Sandler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sandler",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sandler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6790124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d53dd78990385253ffd6793c0f7738721ec9247",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we give an overview of a range of approaches to the analysis and separation of musical audio. In particular, we consider the problems of automatic music transcription and audio source separation, which are of particular interest to our group. Monophonic music transcription, where a single note is present at one time, can be tackled using an autocorrelation-based method. For polyphonic music transcription, with several notes at any time, other approaches can be used, such as a blackboard model or a multiple-cause/sparse coding method. The latter is based on ideas and methods related to independent component analysis (ICA), a method for sound source separation."
            },
            "slug": "Automatic-Music-Transcription-and-Audio-Source-Plumbley-Abdallah",
            "title": {
                "fragments": [],
                "text": "Automatic Music Transcription and Audio Source Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An overview of a range of approaches to the analysis and separation of musical audio, including the problems of automatic music transcription and audio source separation, is given."
            },
            "venue": {
                "fragments": [],
                "text": "Cybern. Syst."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 208
                            }
                        ],
                        "text": "\u2026methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 568745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d62bcde418144411068d5b09952090962fbc05f6",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output."
            },
            "slug": "Unsupervised-Learning-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning and Data Mining"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737063"
                        ],
                        "name": "I. Daubechies",
                        "slug": "I.-Daubechies",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Daubechies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Daubechies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 77
                            }
                        ],
                        "text": "One approach to removing the degeneracy in (1) is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 87
                            }
                        ],
                        "text": "One approach to removing the degeneracy in equation 1.1 is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), for example, by finding s satisfying equation 1.1 with minimum L1 norm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15757500,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "2384a683444b8fc03774d5af6791d81f6c48b6b2",
            "isKey": true,
            "numCitedBy": 6213,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Two different procedures for effecting a frequency analysis of a time-dependent signal locally in time are studied. The first procedure is the short-time or windowed Fourier transform; the second is the wavelet transform, in which high-frequency components are studied with sharper time resolution than low-frequency components. The similarities and the differences between these two methods are discussed. For both schemes a detailed study is made of the reconstruction method and its stability as a function of the chosen time-frequency density. Finally, the notion of time-frequency localization is made precise, within this framework, by two localization theorems. >"
            },
            "slug": "The-wavelet-transform,-time-frequency-localization-Daubechies",
            "title": {
                "fragments": [],
                "text": "The wavelet transform, time-frequency localization and signal analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Two different procedures for effecting a frequency analysis of a time-dependent signal locally in time are studied and the notion of time-frequency localization is made precise, within this framework, by two localization theorems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952058"
                        ],
                        "name": "S. Makeig",
                        "slug": "S.-Makeig",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Makeig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Makeig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144437435"
                        ],
                        "name": "T. Jung",
                        "slug": "T.-Jung",
                        "structuredName": {
                            "firstName": "Tzyy-Ping",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737401"
                        ],
                        "name": "D. Ghahremani",
                        "slug": "D.-Ghahremani",
                        "structuredName": {
                            "firstName": "Dara",
                            "lastName": "Ghahremani",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ghahremani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141294562,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "797191cc3263fc370706da203a1e861a5b04438d",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Functional imaging of brain activity based on changes in blood flow does not supply information about the relative timing of brief bursts of neural activity in different brain areas. Multichannel electric or magnetic recordings from the scalp provide high temporal resolution, but are not easily decomposed into the separate activities of multiple brain networks. We report here a method for the blind separation of event-related brain responses into spatially stationary and temporally independent subcomponents using an Independent Component Analysis algorithm. Applied to electroencephalographic responses from an auditory detection task, each of the most active identified sources accounted for all or part of a previously identified response component. This spatiotemporal decomposition was robust to changes in sensors and input data length, and was stable within subjects. The method can be used to assess the timing, strength, and stability of event-related activity in brain networks during cognitive tasks, regardless of source location."
            },
            "slug": "Blind-Separation-of-Event-Related-Brain-Responses-Makeig-Jung",
            "title": {
                "fragments": [],
                "text": "Blind Separation of Event-Related Brain Responses into Independent Components"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 225
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026to the methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986; Daugman, 1989; Linsker, 1988;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 136
                            }
                        ],
                        "text": "Bases such as the Fourier or wavelet can provide a useful representation of some signals, but they are limited because they are not specialized for the signals under consideration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14149261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaf352dc0b8c02e22bf0e11dc7bbcbed90e4f16f",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for the blind separation of sources can be derived from several different principles. This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood. The application of the infomax principle to source separation consists of maximizing an output entropy."
            },
            "slug": "Infomax-and-maximum-likelihood-for-blind-source-Cardoso",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15208137,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "baf4491be1f4c1de7ecb03bf81325f6f09bda9c6",
            "isKey": false,
            "numCitedBy": 9488,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "IS B N 0-523108-5) C opright (C ) 19-1992 by C am bidge U nirsity P rss. P rogram s C opright (C ) 19-1992 by N um eical R eipes S ftw are. P rm ission is grnted or inrnet uers to m ke ne pper cpy or teir ow n peonal use. F uther repruction, or ny coying of m acineredable fles (inluding his one) to ny srver om pter, is sictly proibited. T o oder N um eical R eipes boks, disettes, or C D R O M s visit w esite hp://w w w .n.com or call 1-8072-7423 (N orth A m erica oly), or snd em il to trde@ cu.cam .ac.uk (otside N orth A m eca). Numerical Recipes in C"
            },
            "slug": "Numerical-recipes-in-C++:-the-art-of-scientific-2nd-Press",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C++: the art of scientific computing, 2nd Edition (C++ ed., print. is corrected to software version 2.10)"
            },
            "tldr": {
                "abstractSimilarityScore": 32,
                "text": "F uther repruction, or ny coying of m acineredable fles (inluding his one) to ny srver om pter, is sictly proibited."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711848"
                        ],
                        "name": "M. McKeown",
                        "slug": "M.-McKeown",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "McKeown",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144437435"
                        ],
                        "name": "T. Jung",
                        "slug": "T.-Jung",
                        "structuredName": {
                            "firstName": "Tzyy-Ping",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952058"
                        ],
                        "name": "S. Makeig",
                        "slug": "S.-Makeig",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Makeig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Makeig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107744315"
                        ],
                        "name": "G. Brown",
                        "slug": "G.-Brown",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Brown",
                            "middleNames": [
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32656389"
                        ],
                        "name": "S. Kindermann",
                        "slug": "S.-Kindermann",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Kindermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kindermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "78006509"
                        ],
                        "name": "T. Lee",
                        "slug": "T.-Lee",
                        "structuredName": {
                            "firstName": "Tsz-wing",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 277
                            }
                        ],
                        "text": "\u2026blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2040214,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "7f0b6a0f2c1e1587828a7572287d4485e33a147b",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is given for determining the time course and spatial extent of consistently and transiently task-related activations from other physiological and artifactual components that contribute to functional MRI (fMRI) recordings. Independent component analysis (ICA) was used to analyze two fMRI data sets from a subject performing 6-min trials composed of alternating 40-sec Stroop color-naming and control task blocks. Each component consisted of a fixed three-dimensional spatial distribution of brain voxel values (a \"map\") and an associated time course of activation. For each trial, the algorithm detected, without a priori knowledge of their spatial or temporal structure, one consistently task-related component activated during each Stroop task block, plus several transiently task-related components activated at the onset of one or two of the Stroop task blocks only. Activation patterns occurring during only part of the fMRI trial are not observed with other techniques, because their time courses cannot easily be known in advance. Other ICA components were related to physiological pulsations, head movements, or machine noise. By using higher-order statistics to specify stricter criteria for spatial independence between component maps, ICA produced improved estimates of the temporal and spatial extent of task-related activation in our data compared with principal component analysis (PCA). ICA appears to be a promising tool for exploratory analysis of fMRI data, particularly when the time courses of activation are not known in advance."
            },
            "slug": "Spatially-independent-activity-patterns-in-MRI-data-McKeown-Jung",
            "title": {
                "fragments": [],
                "text": "Spatially independent activity patterns in functional MRI data during the stroop color-naming task."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Independent component analysis was used to analyze two fMRI data sets from a subject performing 6-min trials composed of alternating 40-sec Stroop color-naming and control task blocks, and produced improved estimates of the temporal and spatial extent of task-related activation in the authors' data compared with principal component analysis (PCA)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 228
                            }
                        ],
                        "text": "\u2026methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986; Daugman, 1989; Linsker, 1988; Atick, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 58
                            }
                        ],
                        "text": "The function f (s) by applying kernel density estimation (Silverman, 1986) to the distribution of coefficients fit to a training data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 56
                            }
                        ],
                        "text": "The function f(s) by applying kernel density estimation (Silverman, 1986) to the distribution of coe cients t to a training data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67073029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "729cb7a620b4e81b63b281627474020cdfbadd39",
            "isKey": false,
            "numCitedBy": 7456,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. Survey of Existing Methods. The Kernel Method for Univariate Data. The Kernel Method for Multivariate Data. Three Important Methods. Density Estimation in Action."
            },
            "slug": "Density-Estimation-for-Statistics-and-Data-Analysis-Silverman",
            "title": {
                "fragments": [],
                "text": "Density Estimation for Statistics and Data Analysis."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The Kernel Method for Multivariate Data: Three Important Methods and Density Estimation in Action."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 180
                            }
                        ],
                        "text": "An overcomplete Fourier basis, with more than the minimum number of sinusoids, can compactly represent signals composed of small numbers of frequencies, achieving superresolution (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 267
                            }
                        ],
                        "text": "An alternative and potentially more general method of signal representation uses socalled \\overcomplete\" bases (also called overcomplete dictionaries), which allow a greater number of basis functions (also called dictionary elements) than samples in the input signal (Simoncelli et al., 1992; Mallat and Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 136
                            }
                        ],
                        "text": "In applications of overcomplete representations, it is common to assume the coe cients are independent and have Laplacian distributions (Mallat and Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 77
                            }
                        ],
                        "text": "One approach to removing the degeneracy in (1) is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "This is a nonlinear operation that essentially selects a subset of basis vectors to represent the data (Chen et al., 1996), so that the resulting representation is sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "This can be solved efficiently and exactly with interior point linear programming methods (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 121
                            }
                        ],
                        "text": "An alternative method, which can be used when the prior is Laplacian and = 0, is to view the problem as a linear program (Chen et al., 1996): mincT jsj subject to As = x: (5) Letting c = (1; : : : ; 1), the objective function in the linear program, cT jsj = Pm jsmj, corresponds to maximizing the log posterior likelihood under a Laplacian prior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 161
                            }
                        ],
                        "text": "In applications of overcomplete representations, it is common to assume the coefficients are independent and have Laplacian distributions (Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 125
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman and Wickerhauser, 1992; Mallat and Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "One approach to removing the degeneracy in equation 1.1 is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), for example, by finding s satisfying equation 1.1 with minimum L1 norm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 178
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "An alternative method, which can be used when the prior is Laplacian and \u00b2 = 0, is to view the problem as a linear program (Chen et al., 1996):\nmin cT|s| subject to As = x. (3.1)\nLetting c = (1, . . . , 1), the objective function in the linear program, cT|s| =\u2211 m |sm|, corresponds to maximizing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 180
                            }
                        ],
                        "text": "An overcomplete Fourier basis, with more than the minimum number of sinusoids, can compactly represent signals composed of small numbers of frequencies, achieving super-resolution (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 84
                            }
                        ],
                        "text": "The standard implementation handles only the noiseless case but can be generalized (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 49
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen and Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "This can solved e ciently and exactly with interior point linear programming methods (Chen et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 83
                            }
                        ],
                        "text": "The standard implementation only handles the noiseless case but can be generalized (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atomic decomposition by basis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 180
                            }
                        ],
                        "text": "An overcomplete Fourier basis, with more than the minimum number of sinusoids, can compactly represent signals composed of small numbers of frequencies, achieving superresolution (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "An alternative method, which can be used when the prior is Laplacian and 2 = 0, is to view the problem as a linear program (Chen et al., 1996): min cT|s| subject to As = x."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "This is a nonlinear operation that essentially selects a subset of basis vectors to represent the data (Chen et al., 1996), so that the resulting representation is sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "This can be solved efficiently and exactly with interior point linear programming methods (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 32
                            }
                        ],
                        "text": "1 is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), for example, by finding s satisfying equation 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 161
                            }
                        ],
                        "text": "In applications of overcomplete representations, it is common to assume the coefficients are independent and have Laplacian distributions (Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "One approach to removing the degeneracy in equation 1.1 is to place a constraint on s (Daubechies, 1990; Chen et al., 1996), for example, by finding s satisfying equation 1.1 with minimum L1 norm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 178
                            }
                        ],
                        "text": "Combining both of these bases into a single overcomplete basis would allow compact representations for both types of signals (Coifman & Wickerhauser, 1992; Mallat & Zhang, 1993; Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "An alternative method, which can be used when the prior is Laplacian and \u00b2 = 0, is to view the problem as a linear program (Chen et al., 1996):\nmin cT|s| subject to As = x. (3.1)\nLetting c = (1, . . . , 1), the objective function in the linear program, cT|s| =\u2211 m |sm|, corresponds to maximizing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 84
                            }
                        ],
                        "text": "The standard implementation handles only the noiseless case but can be generalized (Chen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atomic decomposition by basis pursuit (Technical Rep.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 131
                            }
                        ],
                        "text": "For the learning rule, we replaced the term A in equation A.39 with an approximation to \u03bbAATAH\u22121 (see equation A.36) as suggested in Lewicki\nand Olshausen (1998, 1999):\n\u2212\u03bbATAH\u22121 \u2248 I\u2212 BQ diag\u22121[V+QTBQ]QT, (7.1)\nwhere B = \u2207s\u2207s log P(s\u0302) and Q and V are obtained from the singular value decomposition\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 179
                            }
                        ],
                        "text": "If \u03bb is large (low noise), then the Hessian is dominated by \u03bbATA and\n\u2212\u03bbAATAH\u22121 = \u2212A\u03bbATA(\u03bbATA+ B)\u22121 (A.35) \u2248 \u2212A. (A.36)\nIt is also possible to obtain more accurate approximations of this term (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 75
                            }
                        ],
                        "text": "This approach to denoising has been applied successfully to natural images (Lewicki and Olshausen, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 76
                            }
                        ],
                        "text": "This approach to denoising has been applied successfully to natural images (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inferring sparse, overcomplete image codes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 54
                            }
                        ],
                        "text": "One recent success along these lines was developed by Olshausen and Field (1996, 1997) from the viewpoint of learning sparse codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 196
                            }
                        ],
                        "text": "3 Inferring the internal state A general approach for optimizing s in the case of nite noise ( > 0) and non-Gaussian P (s) is to use the gradient of the log posterior in an optimization algorithm (Daugman, 1988; Olshausen and Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 68
                            }
                        ],
                        "text": "This problem can be somewhat circumvented by adaptive normalization (Olshausen and Field, 1996), but setting the adaptation rates can be tricky in practice, and more importantly, there is no guarantee the desired objective function is the one being optimized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 69
                            }
                        ],
                        "text": "This problem can be somewhat circumvented by adaptive normalization (Olshausen & Field, 1996), but setting the adaptation rates can be tricky in practice, and, more important,\nthere is no guarantee the desired objective function is the one being optimized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 107
                            }
                        ],
                        "text": "Some recent approaches have tried to approximate this integral by evaluating P(s)P(x|A, s) at its maximum (Olshausen & Field, 1996, 1997), but this ignores the volume information of the posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 49
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen and Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 181
                            }
                        ],
                        "text": "A general approach for optimizing s in the case of finite noise (\u00b2 > 0) and nongaussian P(s) is to use the gradient of the log posterior in an optimization algorithm (Daugman, 1988; Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 63
                            }
                        ],
                        "text": "An alternative choice, advocated by some authors (Field, 1994; Olshausen & Field, 1996; Chen et al., 1996), is to use priors that assume sparse representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 39
                            }
                        ],
                        "text": "This work generalizes the algorithm of Olshausen and Field (1996) by deriving a algorithm for learning overcomplete bases from a direct approximation to the data likelihood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive- eld properties"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 102
                            }
                        ],
                        "text": "ICA is highly e ective in several applications such as blind source separation of mixed audio signals (Jutten and Herault, 1991; Bell and Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995), allows the learning of nonorthogonal bases for data with nongaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 65
                            }
                        ],
                        "text": "An extension of PCA, called independent component analysis (ICA) (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1995), allows the learning of non-orthogonal bases for data with non-Gaussian distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 70
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1995) and provides a method for the identi cation of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 71
                            }
                        ],
                        "text": "This also generalizes the technique of independent component analysis (Jutten & He\u0301rault, 1991; Comon, 1994; Bell & Sejnowski, 1995) and provides a method for the identification of more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources .1. an adaptive algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 76
                            }
                        ],
                        "text": "When this distribution is fitted to wavelet subband coefficients of images, Buccigrossi and Simoncelli (1997) found that p was in the range [0.5, 1.0]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image compression via joint statistical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 131
                            }
                        ],
                        "text": "For the learning rule, we replaced the term A in equation A.39 with an approximation to \u03bbAATAH\u22121 (see equation A.36) as suggested in Lewicki\nand Olshausen (1998, 1999):\n\u2212\u03bbATAH\u22121 \u2248 I\u2212 BQ diag\u22121[V+QTBQ]QT, (7.1)\nwhere B = \u2207s\u2207s log P(s\u0302) and Q and V are obtained from the singular value decomposition\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 179
                            }
                        ],
                        "text": "If \u03bb is large (low noise), then the Hessian is dominated by \u03bbATA and\n\u2212\u03bbAATAH\u22121 = \u2212A\u03bbATA(\u03bbATA+ B)\u22121 (A.35) \u2248 \u2212A. (A.36)\nIt is also possible to obtain more accurate approximations of this term (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 76
                            }
                        ],
                        "text": "This approach to denoising has been applied successfully to natural images (Lewicki & Olshausen, 1998, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10493570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8474f73c16e283d61794465b0ac0b904d91fefba",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inferring-Sparse,-Overcomplete-Image-Codes-Using-an-Lewicki-Olshausen",
            "title": {
                "fragments": [],
                "text": "Inferring Sparse, Overcomplete Image Codes Using an Efficient Coding Framework"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Following MacKay (1996) we have\n\u2202 s\u030ck \u2202 a\u030cij = \u2202 \u2202 a\u030cij \u2211 l A\u030c\u22121kl (xl \u2212 \u00b2l), (A.13)\nUsing the identity \u2202A\u22121kl /\u2202aij = \u2212A\u22121ki A\u22121jl ,\n\u2202 s\u030ck \u2202 a\u030cij = \u2212 \u2211 l A\u030c\u22121ki A\u030c \u22121 jl (xl \u2212 \u00b2l)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 87
                            }
                        ],
                        "text": "A is invertible, this integral can be solved and leads to the well known ICA algorithm (MacKay, 1996; Pearlmutter and Parra, 1997; Olshausen and Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for indepen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 353,
                                "start": 331
                            }
                        ],
                        "text": "ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten & H\u00e9rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 277
                            }
                        ],
                        "text": "\u2026blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatially independent activity patterns in functional magnetic resonance imaging data during the stroop color-naming"
            },
            "venue": {
                "fragments": [],
                "text": "task. Proc. Natl. Acad. Sci. USA,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 112
                            }
                        ],
                        "text": "This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of event-related brain response components"
            },
            "venue": {
                "fragments": [],
                "text": "Psychophysiology"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: A a factorial"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 277
                            }
                        ],
                        "text": "\u2026blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatially independent activity patterns in functional magnetic resonance imaging data during the stroop colornaming task"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Natl . Acad . Sci . USA"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 112
                            }
                        ],
                        "text": "The most probable coe cients were obtained using a publicly available interior point linear programming package (Meszaros, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 115
                            }
                        ],
                        "text": "The most probable coefficients were obtained using a publicly available interior point\nlinear programming package (Meszaros, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BPMPD: An interior point linear programming solver"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Following MacKay (1996) we have\n\u2202 s\u030ck \u2202 a\u030cij = \u2202 \u2202 a\u030cij \u2211 l A\u030c\u22121kl (xl \u2212 \u00b2l), (A.13)\nUsing the identity \u2202A\u22121kl /\u2202aij = \u2212A\u22121ki A\u22121jl ,\n\u2202 s\u030ck \u2202 a\u030cij = \u2212 \u2211 l A\u030c\u22121ki A\u030c \u22121 jl (xl \u2212 \u00b2l)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent component analysis. Unpublished manuscript. Cambridge: University of Cambridge, Cavendish Laboratory Available online at: ftp://wol"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent component analysis. Unpublished manuscript. Cambridge: University of Cambridge, Cavendish Laboratory Available online at: ftp://wol"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received February"
            },
            "venue": {
                "fragments": [],
                "text": "Received February"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C: The art of scientific programming"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical recipes in C: The art of scientific programming"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 205
                            }
                        ],
                        "text": "This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 277
                            }
                        ],
                        "text": "\u2026blind source separation of mixed audio signals (Jutten & He\u0301rault, 1991; Bell & Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig, Jung, Bell, Ghahremani, & Sejnowski, 1996), and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatially independent activity patterns in functional magnetic resonance imaging data during the stroop color-naming task"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Natl. Acad. Sci. USA"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C: The art of scientific programming (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 115
                            }
                        ],
                        "text": "The most probable coefficients were obtained using a publicly available interior point\nlinear programming package (Meszaros, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BPMPD: An interior point linear programming solver. Code available at ftp"
            },
            "venue": {
                "fragments": [],
                "text": "BPMPD: An interior point linear programming solver. Code available at ftp"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientiic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientiic Programming"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction and independent component"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatially independent activity patterns in functional magnetic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 115
                            }
                        ],
                        "text": "The most probable coefficients were obtained using a publicly available interior point\nlinear programming package (Meszaros, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BPMPD: An interior point linear programming solver. Code available online at: ftp://ftp"
            },
            "venue": {
                "fragments": [],
                "text": "BPMPD: An interior point linear programming solver. Code available online at: ftp://ftp"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 148
                            }
                        ],
                        "text": "\u2026ignored, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation (Nadal & Parga, 1994a, 1994b; Cardoso, 1997), which have been advocated by several researchers (Barlow, 1961, 1989; Hinton and Sejnowski, 1986;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: A a factorial code maximizes information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Following MacKay (1996) we have\n\u2202 s\u030ck \u2202 a\u030cij = \u2202 \u2202 a\u030cij \u2211 l A\u030c\u22121kl (xl \u2212 \u00b2l), (A.13)\nUsing the identity \u2202A\u22121kl /\u2202aij = \u2212A\u22121ki A\u22121jl ,\n\u2202 s\u030ck \u2202 a\u030cij = \u2212 \u2211 l A\u030c\u22121ki A\u030c \u22121 jl (xl \u2212 \u00b2l)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "In the special case of zero noise and a complete representation (i.e., A is invertible) this integral can be solved and leads to the well-known ICA algorithm (MacKay, 1996; Pearlmutter & Parra, 1997; Olshausen & Field, 1997; Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Bases such as the Fourier or wavelet can provide a useful representation of some signals, but they are limited because they are not specialized for the signals under consideration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent component analysis. University of Cambridge, Cavendish Laboratory. Available at ftp"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent component analysis. University of Cambridge, Cavendish Laboratory. Available at ftp"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 28,
            "methodology": 30
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Overcomplete-Representations-Lewicki-Sejnowski/42d906c733f273109c0ed716a5ef6e2a379beb26?sort=total-citations"
}