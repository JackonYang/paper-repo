{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "On the other hand, whitening can make separation of sources more difficult or even impossible if the mixing matrix is ill-conditioned or if some of the sources are weak compared to the others [9], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Generally, it is impossible to separate the possible noise in the input data from the source signals [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "The EASI learning algorithm (and its generalization) usually perform rather similarly than the other learning rules, but it can separate the sources even though the mixing matrix is ill-conditioned provided that there is no noise [9], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "We refer to the tutorial paper [31], where various neural approaches are reviewed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18298402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b1f523824650ff694fd36b59c24392825d1de63",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent Component Analysis (ICA) is a recently developed technique that in many cases characterizes the data in a natural way. The main application area of the linear ICA model is blind source separation. Here, unknown source signals are estimated from their unknown linear mixtures using the strong assumption that the sources are mutually independent. In practice, separation can be achieved by using suitable higher-order statistics or nonlinearities. Various neural approaches have recently been proposed for blind source separation and ICA. In this paper , these approaches and the respective learning algorithms are brieey reviewed, and some extensions of the basic ICA model are discussed."
            },
            "slug": "Neural-approaches-to-independent-component-analysis-Karhunen",
            "title": {
                "fragments": [],
                "text": "Neural approaches to independent component analysis and source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Various neural approaches have recently been proposed for blind source separation and ICA and the respective learning algorithms are reviewed, and some extensions of the basic ICA model are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "paper can be found in [21]\u2013[23], [32], [33],and [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "and [23] computationally efficient, accurate fixed-point algo-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The condition on the signs of the kurtosis has been recently removed in one-unit learning algorithms [22], [23] that are able to find one source signal at a time, and have computationally efficient fixed-point variants."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6349934,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0b5c2f71251759b39296cd1656d7d2fa178aa1ad",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent Component Analysis (ICA) is a statistical signal processing technique whose main applications are blind source separation, blind deconvolution, and feature extraction. Estimation of ICA is usually performed by optimizing a 'contrast' function based on higher-order cumulants. In this paper, it is shown how almost any error function can be used to construct a contrast function to perform the ICA estimation. In particular, this means that one can use contrast functions that are robust against outliers. As a practical method for nding the relevant extrema of such contrast functions, a xed-point iteration scheme is then introduced. The resulting algorithms are quite simple and converge fast and reliably. These algorithms also enable estimation of the independent components one-by-one, using a simple de ation scheme."
            },
            "slug": "A-family-of-fixed-point-algorithms-for-independent-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "A family of fixed-point algorithms for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how almost any error function can be used to construct a contrast function to perform the ICA estimation, and this means that one can use contrast functions that are robust against outliers."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "In nonlinear PCA approaches, a closed-form solution is usually not possible, which makes neural learning algorithms computationally attractive [27], [28], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "Quite recently, we have introduced in [33] adaptive leastsquares type algorithms for minimizing the same criterion function from which the nonlinear PCA subspace rule (17) has been derived in [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "signals somehow [27], but usually not other type of sources."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "However, recently Fyfe and Baddeley [17] have applied a nonlinear (robust) PCA algorithm suggested and derived by us earlier in [27], [41] to finding projection pursuit directions from prewhitened data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Some hints on how to do this are given in [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Instead of (16), we can use the nonlinear PCA subspace rule introduced by one of the authors in [41]; see also [27] and [30]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In our previous papers [27], [28], [30], [52], [53], we have derived several robust and nonlinear extensions of PCA starting either from maximization of the output variances or from minimization of the mean-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [27], we have shown that (16) is a stochastic gradient algorithm which tries to maximize the criterion"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31727807,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "4c9e13d6d34da1cefa1be368d7fa7bf95cf4ad74",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Representation-and-separation-of-signals-using-PCA-Karhunen-Joutsensalo",
            "title": {
                "fragments": [],
                "text": "Representation and separation of signals using nonlinear PCA type learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346683"
                        ],
                        "name": "J. Hurri",
                        "slug": "J.-Hurri",
                        "structuredName": {
                            "firstName": "Jarmo",
                            "lastName": "Hurri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hurri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "Such blind techniques are needed for example in various applications of array processing, communications, medical signal processing, and speech processing [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "paper can be found in [21]\u2013[23], [32], [33],and [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "The basis vectors of ICA should be especially useful in linear projection pursuit and in extracting characteristic features from natural data [5], [6], [21], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8879512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79cf6e5696ed280396c853d37f97c14933ac875b",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In blind source separation one tries to separate statistically independent unknown source signals from their linear mixtures without knowing the mixing coefficients. Such techniques are currently studied actively both in statistical signal processing and unsupervised neural learning. We apply neural blind separation techniques developed in our laboratory to the extraction of features from natural images and to the separation of medical EEG signals. The new analysis method yields features that describe the underlying data better than for example classical principal component analysis. We discuss difficulties related with real-world applications of blind signal processing, too."
            },
            "slug": "Applications-of-neural-blind-separation-to-signal-Karhunen-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Applications of neural blind separation to signal and image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Neural blind separation techniques developed in the laboratory are applied to the extraction of features from natural images and to the separation of medical EEG signals, which yields features that describe the underlying data better than for example classical principal component analysis."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some attempts to extend blind source separation and ICA into these directions have already been made for example in [ 7 ], [14], [36], [44], and [50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this purpose, many well-established neural learning algorithms based on the single-unit PCA rule [38] are available [3], [10], [15], [18], [ 42 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Several unsupervised learning algorithms of such networks are neural realizations [10], [15], [18], [ 42 ] of the widely used statistical technique principal component analysis (PCA)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40000333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de86c61defbb8c259583074f3cf63afe13571ce1",
            "isKey": false,
            "numCitedBy": 856,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principal-components,-minor-components,-and-linear-Oja",
            "title": {
                "fragments": [],
                "text": "Principal components, minor components, and linear neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "We are ready to state the main result of this section, which is a simplified version of a more general theorem originally presented by one of the authors in [43] Theorem: In the matrix differential equation (27), assume the following."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "The inequalities (30) and (32) are sufficient for asymptotic stability, as shown in [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "It turns out that condition 5) holds for (for details, see [43]), and condition 4) is always satisfied."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14234254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f010d4fb56fe96127388ef982e71cf3e30e6c6d",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been veriied experimentally that nonlinear versions of the PCA network learning rules for the weights of a neural layer produce neurons that have signal separation capabilities. One of the learning rules earlier proposed by the author is studied here mathematically to analyze why and how the algorithm works in this application. It is shown that for input vectors whose density is symmetrical around the origin and has equal variances for each element, the weight matrix obtained as the asymptotic solution of the nonlinear PCA learning rule is in some cases a rotation of the input vector to statistically independent directions. This explains why it can be used for image and speech signal separation. Suucient conditions are formulated, depending on the nonlinear neuron activation function and on the probability densities of the original signal components. It is shown that a sigmoidal nonlinearity as the activation function is feasible for at sub-Gaussian densities of the original signals, while polynomial activation functions are feasible for sharp super-Gaussian densities. An example with uniform densities and a hyperbolic tangent activation function is analyzed in depth both by mathematical analysis and by simulations on the learning rule. Aapo Hyvrinen for helpful comments on the preliminary version of the manuscript."
            },
            "slug": "The-Nonlinear-PCA-Learning-Rule-and-Signal-Analysis-Oja",
            "title": {
                "fragments": [],
                "text": "The Nonlinear PCA Learning Rule and Signal Separation - Mathematical Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "One of the learning rules earlier proposed by the author is studied here mathematically to analyze why and how the algorithm works in this application of PCA network learning rules for the weights of a neural layer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "The condition on the signs of the kurtosis has been recently removed in one-unit learning algorithms [22], [23] that are able to find one source signal at a time, and have computationally efficient fixed-point variants."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8612415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce6fc28a27903d262142e1c0f7cebfb044717dde",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural one-unit learning rules for the problem of Independent Component Analysis (ICA) and blind source separation are introduced. In these new algorithms, every ICA neuron develops into a separator that finds one of the independent components. The learning rules use very simple constrained Hebbian/anti-Hebbian learning in which decorrelating feedback may be added. To speed up the convergence of these stochastic gradient descent rules, a novel computationally efficient fixed-point algorithm is introduced."
            },
            "slug": "One-unit-Learning-Rules-for-Independent-Component-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "One-unit Learning Rules for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Neural one-unit learning rules for the problem of Independent Component Analysis and blind source separation are introduced and a novel computationally efficient fixed-point algorithm is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693795"
                        ],
                        "name": "K. Matsuoka",
                        "slug": "K.-Matsuoka",
                        "structuredName": {
                            "firstName": "Kiyotoshi",
                            "lastName": "Matsuoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Matsuoka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49249521"
                        ],
                        "name": "Masahiro Ohoya",
                        "slug": "Masahiro-Ohoya",
                        "structuredName": {
                            "firstName": "Masahiro",
                            "lastName": "Ohoya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masahiro Ohoya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307492"
                        ],
                        "name": "M. Kawamoto",
                        "slug": "M.-Kawamoto",
                        "structuredName": {
                            "firstName": "Mitsuru",
                            "lastName": "Kawamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "example in [7], [14], [36], [44], and [50]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20151003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c87539aaf225c0311d18331af402e951d58997f4",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-neural-net-for-blind-separation-of-nonstationary-Matsuoka-Ohoya",
            "title": {
                "fragments": [],
                "text": "A neural net for blind separation of nonstationary signals"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23408621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d60694bb4f0f8736aff49f0513a4dca1303526ae",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources:-A-nonlinear-neural-Burel",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources: A nonlinear neural algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "A few new neural separating algorithms [1], [4], [14] have been derived from information theoretic concepts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "A simple example of a sub-Gaussian density is the uniform density on the interval [1,1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [1] the authors define a performance index which measures the difference of the matrix from a permutation matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "Instead of (16), we can use the nonlinear PCA subspace rule introduced by one of the authors in [41]; see also [27] and [30]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "However, the nonlinear PCA subspace rule (17) is only indirectly related to an optimization criterion [30], and so a convergence analysis should be given."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "In our previous papers [27], [28], [30], [52], [53], we have derived several robust and nonlinear extensions of PCA starting either from maximization of the output variances or from minimization of the mean-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "In nonlinear PCA approaches, a closed-form solution is usually not possible, which makes neural learning algorithms computationally attractive [27], [28], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1578694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "993ebaefaa0bc8cc201d9e2f5cfef346cb8881b9",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalizations-of-principal-component-analysis,-Karhunen-Joutsensalo",
            "title": {
                "fragments": [],
                "text": "Generalizations of principal component analysis, optimization problems, and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21476173"
                        ],
                        "name": "P. Pajunen",
                        "slug": "P.-Pajunen",
                        "structuredName": {
                            "firstName": "Petteri",
                            "lastName": "Pajunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pajunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "paper can be found in [21]\u2013[23], [32], [33],and [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Quite recently, we have introduced in [33] adaptive leastsquares type algorithms for minimizing the same criterion function from which the nonlinear PCA subspace rule (17) has been derived in [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Their great advantage is that the learning parameter is determined automatically from the input data so that it is roughly optimal, resulting in a fast convergence [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11832229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f459c8f4e668d45438762ff01bd98d039bfbe65",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive least-squares type algorithms are introduced for blind source separation. They are based on minimizing a criterion used in context with nonlinear PCA (principal component analysis) networks. The new algorithms converge clearly faster and provide more accurate results than typical current adaptive blind separation algorithms based on instantaneous gradients. They are also applicable to the difficult case of nonstationary mixtures. The proposed algorithms have a close relationship to a nonlinear extension of Oja's (see Computational Intelligence-a Dynamic System Perspective, p.83-97, IEEE Press, 1995) PCA learning rule. A batch algorithm based on the same criterion is also presented."
            },
            "slug": "Blind-source-separation-using-least-squares-type-Karhunen-Pajunen",
            "title": {
                "fragments": [],
                "text": "Blind source separation using least-squares type adaptive algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Adaptive least-squares type algorithms are introduced for blind source separation based on minimizing a criterion used in context with nonlinear PCA (principal component analysis) networks, based on a close relationship to Oja's PCA learning rule."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120692531"
                        ],
                        "name": "Liuyue Wang",
                        "slug": "Liuyue-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liuyue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The bigradient algorithm (18) [as well as the robust PCA subspace rule (16)] has been derived by optimizing the criterion under orthonormality constraints, and so it can be expected to converge to an orthogonal matrix that will minimize or maximize the criterion (depending on the sign of the learning parameter ). We have presented mathematical analysis supporting this in [ 52 ], showing that in the standard PCA/MCA case in which , the ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In our previous papers [27], [28], [30], [ 52 ], [53], we have derived several robust and nonlinear extensions of PCA starting either from maximization of the output variances or from minimization of the meansquare representation error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The algorithm (18) is derived and discussed in more detail in [ 52 ] and [53]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have recently developed another so-called bigradient algorithm [ 52 ], [53], which is applied for learning the orthonormal separating matrix as follows:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207108089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f2dae3923c0bcdf53c52fc40a45892587ff1e82",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new instantaneous-gradient search algorithm for computing a principal component or minor component type solution is proposed. The algorithm can use normalized Hebbian or anti-Hebbian learning in a unified formula. Starting from one-unit rule, a multi-unit algorithm is developed which can simultaneously extract several robust counterparts of the principal or minor eigenvectors of the data covariance matrix. Standard principal or minor components emerge as special cases from the general non-quadratic criterion. The learning rule is analyzed mathematically, and the theoretical results are verified by simulations. The proposed bigradient approach can be applied to blind separation of independent source signals from their linear mixtures."
            },
            "slug": "A-Unified-Neural-Bigradient-Algorithm-for-robust-Wang-Karhunen",
            "title": {
                "fragments": [],
                "text": "A Unified Neural Bigradient Algorithm for robust PCA and MCA"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A new instantaneous-gradient search algorithm for computing a principal component or minor component type solution is proposed which can use normalized Hebbian or anti-Hebbian learning in a unified formula."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this subsection, we try to further clarify the basic ideas behind the introduced ICA networks by studying their connections to the linear autoassociative network [ 2 ] which realizes standard PCA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is well known [ 2 ] that the optimal solution is given by any matrix of the form , where the columns of span the -dimensional PCA subspace of the input vectors . This subspace is defined by the principal eigenvectors of the data covariance matrix E (assuming that has zero mean)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14333248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9552ac39a57daacf3d75865a268935b5a0df9bbb",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-principal-component-analysis:-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal component analysis: Learning from examples without local minima"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72677066"
                        ],
                        "name": "N. Rondel",
                        "slug": "N.-Rondel",
                        "structuredName": {
                            "firstName": "Nadine",
                            "lastName": "Rondel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Rondel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62455003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "216581c4158f04e44d657cf98b7d21e24012b602",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In many signal processing applications, signals are received on an array of sensors, and the problem consists in estimating the directions of arrival (DOA) of the signals, and/or in estimating the sources. Basically, the techniques proposed for its solution use either information about the geometry of the array, or information about the statistics of the sources. Efficient neural-based approaches for both kinds of situations are proposed in this paper. When geometrical knowledge is available, the weights and structure of the neural networks are constrained according to the geometry of the array. When statistical information is available, neural networks which optimize a statistical criterion (namely the measure of dependence) are developed. Furthermore, neural networks provide the opportunity to fuse both approaches in a unified framework, and to take profit simultaneously of both kind of information.<<ETX>>"
            },
            "slug": "Neural-networks-for-array-processing:-from-DOA-to-Burel-Rondel",
            "title": {
                "fragments": [],
                "text": "Neural networks for array processing: from DOA estimation to blind separation of sources"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "In many signal processing applications, signals are received on an array of sensors, and the problem consists in estimating the directions of arrival (DOA) of the signals, and/or in estimates the sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Systems Man and Cybernetics Conference - SMC"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "A few new neural separating algorithms [1], [4], [14] have been derived from information theoretic concepts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The speech signals are typically super-Gaussian [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Sources with a positive kurtosis (super-Gaussian sources) have usually a distribution which has longer tails and a sharper peak than standard Gaussian distribution [4], [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770190"
                        ],
                        "name": "K. Torkkola",
                        "slug": "K.-Torkkola",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Torkkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Torkkola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "example in [7], [14], [36], [44], and [50]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5749256,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "17318d51a624933fbf7c6dbada7da9e3b850bc00",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind separation of independent sources from their convolutive mixtures is a problem in many real world multi-sensor applications. In this paper we present a solution to this problem based on the information maximization principle, which was proposed by Bell and Sejnowski (1995) for the case of blind separation of instantaneous mixtures. We present a feedback network architecture capable of coping with convolutive mixtures, and we derive the adaptation equations for the adaptive filters in the network by maximizing the information transferred through the network. Examples using speech signals are presented to illustrate the algorithm."
            },
            "slug": "Blind-separation-of-convolved-sources-based-on-Torkkola",
            "title": {
                "fragments": [],
                "text": "Blind separation of convolved sources based on information maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a feedback network architecture capable of coping with convolutive mixtures, and derives the adaptation equations for the adaptive filters in the network by maximizing the information transferred through the network."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "In our previous papers [27], [28], [30], [52], [53], we have derived several robust and nonlinear extensions of PCA starting either from maximization of the output variances or from minimization of the mean-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "In nonlinear PCA approaches, a closed-form solution is usually not possible, which makes neural learning algorithms computationally attractive [27], [28], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 108300504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37af736e5eb419f18b7c10d856bae8e0ab224bb0",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the relationships of various nonlinear extensions of principal component analysis (PCA) to optimization are considered. Standard PCA arises as an optimal solution to several different information representation problems. It is claimed that basically this follows from the fact that PCA solution utilizes second-order statistics only. If the optimization problems are generalized for nonquadratic criteria so that higher order statistics are taken into account, their solutions will in general be different. The solutions define, in a natural way, several nonlinear extensions of PCA and give a solid foundation to them. The respective gradient type neural algorithms are discussed.<<ETX>>"
            },
            "slug": "Optimization-criteria-and-nonlinear-PCA-neural-Karhunen",
            "title": {
                "fragments": [],
                "text": "Optimization criteria and nonlinear PCA neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The relationships of various nonlinear extensions of principal component analysis (PCA) to optimization are considered and the respective gradient type neural algorithms are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "This stochastic approximation algorithm has been independently proposed in [35] and [47], and it is used as a part of the EASI (PFS) separation algorithm [9], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "In adaptive source separation [9], [26], [35], an separating matrix is updated so that the -vector"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The learning rule (19) is introduced in [9] and [35] as an adaptive source separation algorithm without any reference to neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "PFS) algorithm[9], [35], where the total separating matrix is computed from the formula"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "In the following, we present the basic data model used in defining both ICA [13], [26] and the source separation problem [26], [35] for linear memoryless channels, and discuss the necessary assumptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "The assumptions typically made in ICA and source separation on the model (3) can be listed more precisely as follows [9], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "In [9] and [35], the EASI algorithm (19) is actually derived by first whitening the data vectors; this yields the \u201clinear\u201d whitening part in (19)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "Also some adaptive blind separation algorithms proposed in the field of signal processing, such as the equivalent adaptive separation via independence (EASI) (or PFS) algorithm [9], [35], can be interpreted as learning algorithms of a neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Instead of normalizing the basis vectors , in source separation it is often assumed [9], [35] that each source signal has unit variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16820523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37f498b18fb1407fa1db905625a3802986b3016b",
            "isKey": true,
            "numCitedBy": 59,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a family of adaptive algorithms for the blind separation of independent signals. Source separation consists in recovering a set of independent signals from some linear mixtures of them, the coeecients of the mixtures being unknown. In the noiseless case, th\u00e8hardness' of the blind source separation problem does not depend on the mixing matrix (see the companion paper 1]). It is then reasonable to expect adaptive algorithms to exhibit convergence and stability properties that would also be independent of the mixing matrix. We show that this desirable uniform performance feature is simply achieved by considering`serial updating' of the separating matrix. Next, generalizing from the gradient of a standard cumulant-based contrast function, we present a family of adaptive algorithms called`PFS', based on the idea of serial updating. The stability condition and the theoretical asymptotic separation levels are given in closed form and, as expected, depend only on the distributions of the sources. Performance is also illustrated by some numerical experiments. 1. Blind source separation Consider an array of m sensors receiving signals emitted by n statistically independent sources. The array output at time t is a m 1 vector x(t) modelled as x(t) = A s(t); (1) where the mn matrix A is called th\u00e8mixing' matrix and where the n source signals are collected in a n 1 vector denoted s(t). In the complex case, this is the familiar linear model used in narrow band array processing. Adaptive source separation consists in updating a nm matrix B(t), called the separating matrix, such that y(t) def = B(t) x(t) (2) is an estimate of the source signals, hence the terminology`source separation'. s(t)-A m n x(t)-B(t) n m-y(t) = ^ s(t) Figure 1. Blind source separation model We emphasize that B(t) should be updated without resorting to any information about the spatial mixing matrix A. This is in sharp contrast t\u00f2standard' array processing and beamforming techniques where the columns of A or their dependence on the location of the sources is assumed to be known. Matrix A is supposed to be a xed matrix with full column rank but no other assumptions are made. The crucial property source separation relies on is the mutual statistical independence of the source signals. Note that under these assumptions, the n n global system matrix C(t) def = B(t) A (3) may only be identiied up to the product of a permutation and a diagonal \u2026"
            },
            "slug": "Adaptive-Source-Separation-With-Uniform-Performance-Laheld-Cardoso",
            "title": {
                "fragments": [],
                "text": "Adaptive Source Separation With Uniform Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A family of adaptive algorithms called`PFS', based on the idea of serial updating of the separating matrix, is presented, which exhibits convergence and stability properties that would also be independent of the mixing matrix."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3270307"
                        ],
                        "name": "L. Wang",
                        "slug": "L.-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 53 ], an example similar to Comon\u2019s data is presented, where (18) successfully separates three artificially constructed super-Gaussian sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The algorithm (18) is derived and discussed in more detail in [52] and [ 53 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In our previous papers [27], [28], [30], [52], [ 53 ], we have derived several robust and nonlinear extensions of PCA starting either from maximization of the output variances or from minimization of the meansquare representation error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have recently developed another so-called bigradient algorithm [52], [ 53 ], which is applied for learning the orthonormal separating matrix as follows:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62599156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7d2583c8119e8a1fdca9ad9399611f78a3b8223",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors earlier derived neural principal or minor component learning algorithms and their robust extensions by optimizing a generalized variance criterion under orthonormality constraints. In this paper, the authors propose an alternative approach, where the stochastic learning algorithm is derived by optimizing two criteria simultaneously. This yields a new bigradient algorithm, which can be used in slightly different forms for PCA, MCA, and their robust extensions in either symmetric (subspace) or hierarchic modes. The algorithm is successfully applied to separation of independent sources from their linear mixture."
            },
            "slug": "A-bigradient-optimization-approach-for-robust-PCA,-Wang-Karhunen",
            "title": {
                "fragments": [],
                "text": "A bigradient optimization approach for robust PCA, MCA, and source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes an alternative approach, where the stochastic learning algorithm is derived by optimizing two criteria simultaneously, which yields a new bigradient algorithm, which can be used in slightly different forms for PCA, MCA, and their robust extensions in either symmetric (subspace) or hierarchic modes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICNN'95 - International Conference on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346683"
                        ],
                        "name": "J. Hurri",
                        "slug": "J.-Hurri",
                        "structuredName": {
                            "firstName": "Jarmo",
                            "lastName": "Hurri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hurri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "have been applied to large-scale practical problems in [21],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "paper can be found in [21]\u2013[23], [32], [33],and [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Quite recently, the basis vectors of ICA have been estimated for real-world image and sound data in [5], [6], and [21], showing their relevance in characterizing natural data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "The basis vectors of ICA should be especially useful in linear projection pursuit and in extracting characteristic features from natural data [5], [6], [21], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14775827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7ae07f0b04a40326c19029306c1939fc3b8cbf",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In Independent Component Analysis, one tries to model the underlying data so that in the linear expansion of the data vectors the coeecients are as independent as possible. This often leads to natural features characterizing well the data. In this paper , we present some results on applying Independent Component Analysis to image data. This has become possible by using a recently developed, computation-ally highly eecient xed-point learning rule. The resulting feature masks are sensitive either to lines and edges of varying thickness or to local spatial features and frequencies."
            },
            "slug": "Image-Feature-Extraction-Using-Independent-Analysis-Hurri-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Image Feature Extraction Using Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents some results on applying Independent Component Analysis to image data by using a recently developed, computation-ally highly eecient xed-point learning rule."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87c4eca6aceb29557f693fdc4efc1fbff003e02a",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Networks of linear units are the simplest kind of networks, where the basic questions related to learning, generalization, and self-organization can sometimes be answered analytically. We survey most of the known results on linear networks, including: 1) backpropagation learning and the structure of the error function landscape, 2) the temporal evolution of generalization, and 3) unsupervised learning algorithms and their properties. The connections to classical statistical ideas, such as principal component analysis (PCA), are emphasized as well as several simple but challenging open questions. A few new results are also spread across the paper, including an analysis of the effect of noise on backpropagation networks and a unified view of all unsupervised algorithms."
            },
            "slug": "Learning-in-linear-neural-networks:-a-survey-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Learning in linear neural networks: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Most of the known results on linear networks, including backpropagation learning and the structure of the error function landscape, the temporal evolution of generalization, and unsupervised learning algorithms and their properties are surveyed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 154
                            }
                        ],
                        "text": "This stochastic approximation algorithm has been independently proposed in [35] and [47], and it is used as a part of the EASI (PFS) separation algorithm [9], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "In adaptive source separation [9], [26], [35], an separating matrix is updated so that the -vector"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "The learning rule (19) is introduced in [9] and [35] as an adaptive source separation algorithm without any reference to neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "PFS) algorithm[9], [35], where the total separating matrix is computed from the formula"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The assumptions typically made in ICA and source separation on the model (3) can be listed more precisely as follows [9], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [9] and [35], the EASI algorithm (19) is actually derived by first whitening the data vectors; this yields the \u201clinear\u201d whitening part in (19)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "Also some adaptive blind separation algorithms proposed in the field of signal processing, such as the equivalent adaptive separation via independence (EASI) (or PFS) algorithm [9], [35], can be interpreted as learning algorithms of a neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Instead of normalizing the basis vectors , in source separation it is often assumed [9], [35] that each source signal has unit variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 192
                            }
                        ],
                        "text": "On the other hand, whitening can make separation of sources more difficult or even impossible if the mixing matrix is ill-conditioned or if some of the sources are weak compared to the others [9], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 230
                            }
                        ],
                        "text": "The EASI learning algorithm (and its generalization) usually perform rather similarly than the other learning rules, but it can separate the sources even though the mixing matrix is ill-conditioned provided that there is no noise [9], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": true,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188132"
                        ],
                        "name": "E. Moreau",
                        "slug": "E.-Moreau",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Moreau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Moreau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952331"
                        ],
                        "name": "O. Macchi",
                        "slug": "O.-Macchi",
                        "structuredName": {
                            "firstName": "Odile",
                            "lastName": "Macchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Macchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61904308,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fc9d24a6a955a707a7b62389a8e600c216380c48",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduces self-adaptive algorithms for source separation based on a generalized criterion with the introduction of cross-cumulants. By adequate adaptive preprocessing it can be supposed that the observed source mixture x is 'white'. Then a separating matrix H (such that y=Hx has independent components) can be assumed unitary. A new contrast function is defined whose maximum occurs when H is separating. Its (simple) form admits an associated adaptive algorithm. Two different algorithms are proposed to estimate H, either directly or through its equivalent product of Givens rotations. Computer simulations illustrate the contribution of the cross-cumulants on the convergence of the algorithms. In the three-sources case, they show that the performances are improved substantially.<<ETX>>"
            },
            "slug": "New-self-adaptative-algorithms-for-source-based-on-Moreau-Macchi",
            "title": {
                "fragments": [],
                "text": "New self-adaptative algorithms for source separation based on contrast functions"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Self-adaptive algorithms for source separation based on a generalized criterion with the introduction of cross-cumulants and a new contrast function is defined whose maximum occurs when H is separating."
            },
            "venue": {
                "fragments": [],
                "text": "[1993 Proceedings] IEEE Signal Processing Workshop on Higher-Order Statistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1267924,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "59325886fafd153f0a8dc2009310bbe725682d1e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Field (1994) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and Barlow (1989) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that non-linear 'infomax', when applied to an ensemble of natural scenes, produces sets of visual filters that are localised and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximisation network of Olshausen & Field (1996). In addition, the outputs of these filters are as independent as possible, since the infomax network is able to perform Independent Components Analysis (ICA). We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form an information-theoretic co-ordinate system for images."
            },
            "slug": "Edges-are-the-Independent-Components-of-Natural-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Edges are the Independent Components of Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown here that non-linear 'infomax', when applied to an ensemble of natural scenes, produces sets of visual filters that are localised and oriented and resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form an information-theoretic co-ordinate system for images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400361607"
                        ],
                        "name": "John G. Taylor",
                        "slug": "John-G.-Taylor",
                        "structuredName": {
                            "firstName": "John G.",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John G. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see [14], [18], [ 48 ] for further information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53747549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea0ac9e6583c254284c0d572e64edb293a59a9b0",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-Theory-and-Neural-Networks-Taylor-Plumbley",
            "title": {
                "fragments": [],
                "text": "Information Theory and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059566454"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144687300"
                        ],
                        "name": "D. Obradovic",
                        "slug": "D.-Obradovic",
                        "structuredName": {
                            "firstName": "Dragan",
                            "lastName": "Obradovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Obradovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "A few new neural separating algorithms [1], [4], [14] have been derived from information theoretic concepts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "example in [7], [14], [36], [44], and [50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "information-theoretic criterion as their starting point; see [14],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35682003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034f59dd5c0af2ced15aa1770bd4ac593cea273a",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nNeural networks provide a powerful new technology to model and control nonlinear and complex systems. In this book, the authors present a detailed formulation of neural networks from the information-theoretic viewpoint. They show how this perspective provides new insights into the design theory of neural networks. In particular, they show how these methods may be applied to the topics of supervised and unsupervised learning, including feature extraction, linear and nonlinear independent component analysis, and Boltzmann machines. Readers are assumed to have a basic understanding of neural networks, but all of the relevant concepts from information theory are carefully introduced and explained. Consequently, readers from several different scientific disciplines - notably, cognitive scientists, engineers, physicists, statisticians, and computer scientists - will find this book to be a very valuable contribution to this topic."
            },
            "slug": "An-Information-Theoretic-Approach-to-Neural-Deco-Obradovic",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Approach to Neural Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This book presents a detailed formulation of neural networks from the information-theoretic viewpoint, and shows how these methods may be applied to the topics of supervised and unsupervised learning, including feature extraction, linear and nonlinear independent component analysis, and Boltzmann machines."
            },
            "venue": {
                "fragments": [],
                "text": "Perspectives in Neural Computing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067720375"
                        ],
                        "name": "A. Bell",
                        "slug": "A.-Bell",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Bell",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14707799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5241dd7d74602186dd65fe05435fc65eae797e4",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning algorithms paying attention only to second-order statistics ignore the phase structure (higher-order statistics) of signals, which contains all the informative temporal and spatial coincidences which we think of as 'features'. Here we discuss how an Independent Component Analysis (ICA) algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions. This is illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth. The resulting independent basis functions look like the sounds themselves, having similar temporal envelopes and the same musical pitches. Thus they reflect both the phase and frequency information inherent in the data."
            },
            "slug": "Learning-the-higher-order-structure-of-a-natural-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning the higher-order structure of a natural sound."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How an Independent Component Analysis algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions, illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732878"
                        ],
                        "name": "R. Unbehauen",
                        "slug": "R.-Unbehauen",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Unbehauen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Unbehauen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several unsupervised learning algorithms of such networks are neural realizations [ 10 ], [15], [18], [42] of the widely used statistical technique principal component analysis (PCA)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this purpose, many well-established neural learning algorithms based on the single-unit PCA rule [38] are available [3], [ 10 ], [15], [18], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Roughly speaking, neural blind source separation algorithms are often some modifications of the seminal Herault\u2010Jutten (HJ) algorithm [26], [ 10 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8362494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e763665c612250f05705293cd62ce82f8c2f2a6",
            "isKey": true,
            "numCitedBy": 1581,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nArtificial neural networks can be employed to solve a wide spectrum of problems in optimization, parallel computing, matrix algebra and signal processing. Taking a computational approach, this book explains how ANNs provide solutions in real time, and allow the visualization and development of new techniques and architectures. Features include a guide to the fundamental mathematics of neurocomputing, a review of neural network models and an analysis of their associated algorithms, and state-of-the-art procedures to solve optimization problems. Computer simulation programs MATLAB, TUTSIM and SPICE illustrate the validity and performance of the algorithms and architectures described. The authors encourage the reader to be creative in visualizing new approaches and detail how other specialized computer programs can evaluate performance. Each chapter concludes with a short bibliography. Illustrative worked examples, questions and problems assist self study. The authors' self-contained approach will appeal to a wide range of readers, including professional engineers working in computing, optimization, operational research, systems identification and control theory. Undergraduate and postgraduate students in computer science, electrical and electronic engineering will also find this text invaluable. In particular, the text will be ideal to supplement courses in circuit analysis and design, adaptive systems, control systems, signal processing and parallel computing."
            },
            "slug": "Neural-networks-for-optimization-and-signal-Cichocki-Unbehauen",
            "title": {
                "fragments": [],
                "text": "Neural networks for optimization and signal processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A guide to the fundamental mathematics of neurocomputing, a review of neural network models and an analysis of their associated algorithms, and state-of-the-art procedures to solve optimization problems are explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "[48] J. Taylor and M. Plumbley, \u201cInformation theory and neural networks,\u201d in Mathematical Approaches to Neural Networks, J. Taylor, Ed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 8
                            }
                        ],
                        "text": "[45] M. Plumbley, \u201cA Hebbian/anti-Hebbian network which optimizes information capacity by orthonormalizing the principal subspace,\u201d inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Recently, Plumbley [45], [46] has introduced neural algorithms that simultaneously whiten the input data and compress them into the PCA subspace."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14543399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f1dac2fc96245240c5f210d6e2c1085d94414d8",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent papers have used the approach of maximising information capacity or mutual information (MI) to examine unsupervised neural networks. In particular, for a linear 'compressing' N-input M-output network (N>M), with noise on the input only, the author (1991) maximised MI when the output represents the principal subspace (or top M principal components) of the input. On the other hand, for a linear 'straight-through' M-input M-output network with noise on the output only, he maximised MI (for a fixed output power) when the outputs are orthonormalised, i.e. decorrelated and of equal variance. A number of algorithms exist to achieve both of these optimal arrangements. In this paper, the author extends this work to develop an algorithm for the case of both input and output noise, with an output power constraint. He finds that it is possible to simplify the obvious algorithm obtained by concatenating the two previous solutions. >"
            },
            "slug": "A-Hebbian/anti-Hebbian-network-which-optimizes-by-Plumbley",
            "title": {
                "fragments": [],
                "text": "A Hebbian/anti-Hebbian network which optimizes information capacity by orthonormalizing the principal subspace"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author extends work to develop an algorithm for the case of both input and output noise, with an output power constraint, and finds that it is possible to simplify the obvious algorithm obtained by concatenating the two previous solutions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "In [53], an example similar to Comon\u2019s data is presented, where (18) successfully separates three artificially constructed super-Gaussian sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "A precise mathematical discussion of ICA is given in Comon\u2019s recent fundamental paper [13]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Comon\u2019s Data: Consider first a test example used earlier by Comon [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "[12] P. Comon, \u201cSeparation of stochastic processes,\u201d inProc."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120993591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ba7798612e74189d1307237a4c264241d6f87c4",
            "isKey": true,
            "numCitedBy": 101,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Assume we observe an unknown linear combination of p stochastic processes. The problem we are addressing here is to recover original source processes by applying an inverse transform, F, based solely upon their statistical independence. The conditions under which this problem is solvable are first pointed out. Then, by imposing the cancellation of output crosscumulants, we obtain a polynomial system of equations that the entries of the linear transform F must cancel. In the ring of polynomials with real or complex coefficients, obtaining a greatest common divisor is not easy; for instance Euclid's algorithm is unstable. Our approach is rather based on a detailed analysis of properties of input cumulants, and provides when p=2 a direct solution, which can be also implemented in an adaptive fashion. The extension to p>2 sources raises several specific problems; the solution we propose in this more general case becomes iterative."
            },
            "slug": "Separation-Of-Stochastic-Processes-Comon",
            "title": {
                "fragments": [],
                "text": "Separation Of Stochastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Higher-Order Spectral Analysis"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797605"
                        ],
                        "name": "F. Faggin",
                        "slug": "F.-Faggin",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Faggin",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Faggin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "example in [7], [14], [36], [44], and [50]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14561566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aefa67521edb6de90ccc70e26355bb572ef92da",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have created new networks to unmix signals which have been mixed either with time delays or via filtering. We first show that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power. We then apply this principle to extensions of the Herault-Jutten network which have delays in the feedback path. Our networks perform well on real speech and music signals that have been mixed using time delays or filtering."
            },
            "slug": "Networks-for-the-Separation-of-Sources-that-Are-and-Platt-Faggin",
            "title": {
                "fragments": [],
                "text": "Networks for the Separation of Sources that Are Superimposed and Delayed"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "New networks to unmix signals which have been mixed either with time delays or via filtering are created and it is shown that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68985595"
                        ],
                        "name": "P. Kotilainen",
                        "slug": "P.-Kotilainen",
                        "structuredName": {
                            "firstName": "Petri",
                            "lastName": "Kotilainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kotilainen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144031722"
                        ],
                        "name": "J. Saarinen",
                        "slug": "J.-Saarinen",
                        "structuredName": {
                            "firstName": "Jukka",
                            "lastName": "Saarinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Saarinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670814"
                        ],
                        "name": "K. Kaski",
                        "slug": "K.-Kaski",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Kaski",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kaski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "can be realized fairly easily using hardware, for example by modifying slightly the implementation presented in [34] for the standard PCA subspace rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59791817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8269349dbc8eed4754f4231b24306af2581eafb6",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hardware-Implementations-of-PCA-Neural-Networks-Kotilainen-Saarinen",
            "title": {
                "fragments": [],
                "text": "Hardware Implementations of PCA Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1644387,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9dbd339097e8e4971fc5785364dbbec1107ac147",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-information-transfer-and-anti-Hebbian-Plumbley",
            "title": {
                "fragments": [],
                "text": "Efficient information transfer and anti-Hebbian neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705272"
                        ],
                        "name": "K. Diamantaras",
                        "slug": "K.-Diamantaras",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Diamantaras",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Diamantaras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53883702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f1a9350fd8141bcda3068aec33aef385d5c02eb",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Review of Linear Algebra. Principal Component Analysis. PCA Neural Networks. Channel Noise and Hidden Units. Heteroassociative Models. Signal Enhancement Against Noise. VLSI Implementation. Appendices. Bibliography. Index."
            },
            "slug": "Principal-Component-Neural-Networks:-Theory-and-Diamantaras-Kung",
            "title": {
                "fragments": [],
                "text": "Principal Component Neural Networks: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A review of Linear Algebra, Principal Component Analysis, and VLSI Implementation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "For the robust PCA subspace rule (16), the first author has derived a bound ensuring the stability of the algorithm in [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32776368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfc30f24c637981a5ccbf40eec73968c17df611a",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with stability of Oja's symmetric algorithm for estimating the principal component subspace of the input data. Exact conditions are derived for the gain parameter on which the discrete algorithm remains bounded. The result is extended for a nonlinear version of Oja's algorithm."
            },
            "slug": "Stability-of-Oja's-PCA-Subspace-Rule-Karhunen",
            "title": {
                "fragments": [],
                "text": "Stability of Oja's PCA Subspace Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Stability of Oja's symmetric algorithm for estimating the principal component subspace of the input data is dealt with and exact conditions are derived for the gain parameter on which the discrete algorithm remains bounded."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "Assuming that theth weight vector of a PCA network at step is a roughly normalized estimate of , the respective eigenvalue can be adaptively estimated using the simple algorithm [40]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122159626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "320dd7814f28ae2cb5a01f514dd4af860f600fd7",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-stochastic-approximation-of-the-eigenvectors-and-Oja-Karhunen",
            "title": {
                "fragments": [],
                "text": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40574103"
                        ],
                        "name": "S. Klinke",
                        "slug": "S.-Klinke",
                        "structuredName": {
                            "firstName": "Sigbert",
                            "lastName": "Klinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Klinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549124"
                        ],
                        "name": "J. Polzehl",
                        "slug": "J.-Polzehl",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Polzehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Polzehl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "In projection pursuit [16], [20], one tries to describe the structure of high-dimensional data by projecting them onto a low-dimensional subspace and looking for the structure of the projection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60628897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43d7cce568c4d6ec9cc2d95fe54dd5fa8f51e936",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques. This class contains methods designed for analyzing high dimensional data using low-dimensional projections. The main idea is to describe \u201cinteresting\u201d projections by maximizing an objective function or projection pursuit index."
            },
            "slug": "Exploratory-Projection-Pursuit-Klinke-Polzehl",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques that contains methods designed for analyzing high dimensional data using low-dimensional projections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "For this purpose, many well-established neural learning algorithms based on the single-unit PCA rule [38] are available [3], [10], [15], [18], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Several unsupervised learning algorithms of such networks are neural realizations [10], [15], [18], [42] of the widely used statistical technique principal component analysis (PCA)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9900,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149907"
                        ],
                        "name": "C. Therrien",
                        "slug": "C.-Therrien",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Therrien",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Therrien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This can be done by estimating all the eigenvalues of the covariance matrix E . From the theoretical expression (12) of the covariance matrix it is easy to see [ 49 ] that the largest eigenvalues of E are some linear combinations of the source signal powers E added to the noise power . The remaining eigenvalues correspond to noise only, and are all theoretically equal to . If the signal-to-noise ratio is good enough, the largest \u201csignal\u201d ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "From the structure of the data covariance matrix (12) one can deduce that the basis vectors ,o f ICA theoretically lie in its -dimensional PCA subspace [ 49 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Assume now that the noise term in (3) is standard zeromean white noise with covariance matrix E , where is the common variance of the components of the vector , and that is uncorrelated with the sources . Under these assumptions, it is easy to show (see, for example, [ 49 ]) that the covariance matrix of the data vectors (3) is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "If the structure of the matrix were known from the problem statement excluding some parameters, more efficient subspace or maximum likelihood type methods [ 49 ] are available for estimating the unknown parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118564438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11b0040672683b63c48e02d54e8edfbc50a2c596",
            "isKey": true,
            "numCitedBy": 1171,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Random vectors random processes moment analysis linear transformations estimation optimal filtering linear prediction linear models spectrum estimation."
            },
            "slug": "Discrete-Random-Signals-and-Statistical-Signal-Therrien",
            "title": {
                "fragments": [],
                "text": "Discrete Random Signals and Statistical Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Random vectors random processes moment analysis linear transformations estimation optimal filtering linear prediction linear models spectrum estimation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92394848"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Ch.",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 172992793,
            "fieldsOfStudy": [],
            "id": "d61b5d685808356c6b72e8d3e15eb51f3c4f1a98",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Analyse rigoureuse sur le traitement de signaux multidimensionnels par analogie au systeme nerveux central"
            },
            "slug": "Calcul-neuromim\u00e9tique-et-traitement-du-signal-:-en-Jutten",
            "title": {
                "fragments": [],
                "text": "Calcul neuromim\u00e9tique et traitement du signal : analyse en composantes ind\u00e9pendantes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Under these assumptions, it is easy to show (see, for example, [49]) that the covariance matrix of the data vectors (3) is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "From the structure of the data covariance matrix (12) one c deduce that the basis vectors , of ICA theoretically lie in its -dimensional PCA subspace [49]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "From the theoretical expression (12) of the covariance matrix it is easy to see [49] that the largest eigenvalues of E are some linear combinations of the source signal powers E added to the noise power ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "If the structure of the matrix were known from the problem statement excluding some parameters, more efficient subspace or maximum likelihood type methods [49] are available for estimating the unknown parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random Signals and Statistical Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, NJ: Prentice-Hall,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "In projection pursuit [16], [20], one tries to describe the structure of high-dimensional data by projecting them onto a low-dimensional subspace and looking for the structure of the projection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2357067"
                        ],
                        "name": "B. Ans",
                        "slug": "B.-Ans",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Ans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60606081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66f00998a1b8319bb247112e89541759e528f7f7",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-de-grandeurs-primitives-dans-un-message-H\u00e9rault-Jutten",
            "title": {
                "fragments": [],
                "text": "Detection de grandeurs primitives dans un message composite par une architecture de calcul neuromime"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "examples on comparing PCA with ICA are given in [25] and [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59878735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ced99023a86140d81489e2b1f15efcc2191c72ce",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-components-analysis-versus-principal-Jutten",
            "title": {
                "fragments": [],
                "text": "Independent components analysis versus principal components analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "paper can be found in [21]\u2013[23], [32], [33],and [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICA fixed-point algorithm in extraction of artifacts from EEG"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 1996 IEEE Nordic Signal Processing Symp.  , Espoo, Finland, Sept. 1996, pp. 383\u2013386."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "ICA is a useful extension of PCA that has been developed in context with blind separation of independent sources from their linear mixtures [13], [19], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "D  \u00e9tection de grandeur primitives dans un message composite par une architecture de calcul neuromim  \u00e9tique en apprentissage nonsupervis  \u00e9, "
            },
            "venue": {
                "fragments": [],
                "text": "in Proc. GRETSI Conference"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "This stochastic approximation algorithm has been independently proposed in [35] and [47], and it is used as a part of the EASI (PFS) separation algorithm [9], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A distributed solution for data orthonormalization"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Neural Networks Proc. ICANN-91 , T. Kohonen et al., Eds. Amsterdam, The Netherlands: North-Holland, 1991, pp. 943\u2013948."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multilayer neural networks with a local adaptive learning rule for blind separation of source signals"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1995 Int. Symp. Nonlinear Theory Applicat., NOLTA-95  , Las Vegas, NV, vol. 1, Dec. 1995, pp. 61\u201366."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "For this purpose, many well-established neural learning algorithms based on the single-unit PCA rule [38] are available [3], [10], [15], [18], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Several unsupervised learning algorithms of such networks are neural realizations [10], [15], [18], [42] of the widely used statistical technique principal component analysis (PCA)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Component Networks\u2014Theory and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind source separation using least - squares type adaptive algorithms , \u201d to appear in"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . 1997 IEEE Int . Conf . Acoust . , Speech , Signal Processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Instead of (16), we can use the nonlinear PCA subspace rule introduced by one of the authors in [41]; see also [27] and [30]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [41], one of the authors proposed two nonlinear extensions of his PCA subspace learning rule which can be applied to learning the orthogonal separating matrix ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "However, recently Fyfe and Baddeley [17] have applied a nonlinear (robust) PCA algorithm suggested and derived by us earlier in [27], [41] to finding projection pursuit directions from prewhitened data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning in nonlinear constrained Hebbian networks"
            },
            "venue": {
                "fragments": [],
                "text": " Artificial Neural Networks Proc. ICANN-91, T. Kohonen et al., Eds. Amsterdam, The Netherlands: North-Holland, 1991, pp. 385\u2013390."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "ICA is a useful extension of PCA that has been developed in context with blind separation of independent sources from their linear mixtures [13], [19], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Calcul neuromi\u1e3f etique et traitement du signal, analyze en composantes ind  \u00e9pendantes"
            },
            "venue": {
                "fragments": [],
                "text": "Th\u0301 ese d\u2019Etat, INPG, Univ. Grenoble, France, 1987 (in French)."
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 23,
            "methodology": 26,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 57,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/A-class-of-neural-networks-for-independent-analysis-Karhunen-Oja/1c93c897fd80f5246b839a2044798780cf2c5a77?sort=total-citations"
}