{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376916"
                        ],
                        "name": "G. Flammia",
                        "slug": "G.-Flammia",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Flammia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Flammia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688499"
                        ],
                        "name": "R. Kompe",
                        "slug": "R.-Kompe",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Kompe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kompe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 167
                            }
                        ],
                        "text": "However, it has been shown experimentally with the above ANN/HMM hybrid how training the ANN jointly with the HMM improves performance on a speech recognition problem [101, 2], bringing down the error rate on a plosive recognition task from 19% to 14%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "Another approach [2, 101, 32] uses the ANN to transform the observation sequence into a form that is easier to model for an HMM that has simple (but continuous) emission models (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "This model was introduced in [101] for phoneme recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 894840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) is proposed. ANNs are suitable for performing phonetic classification, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported.<<ETX>>"
            },
            "slug": "Global-optimization-of-a-neural-network-hidden-Bengio-Mori",
            "title": {
                "fragments": [],
                "text": "Global optimization of a neural network-hidden Markov model hybrid"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) with results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [87, 88, 1, 100]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61826819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd0568b4faa03910ae3c07d00c627666f404305d",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A phoneme based, speaker-dependent continuous-speech recognition system embedding a multilayer perceptron (MLP) (i.e. a feedforward artificial neural network) into a hidden Markov model (HMM) approach is described. Contextual information from a sliding window on the input frames is used to improve frame or phoneme classification performance over the corresponding performance for simple maximum-likelihood probabilities, or even maximum a posteriori (MAP) probabilities which are estimated without the benefit of context. Performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the probabilities.<<ETX>>"
            },
            "slug": "Continuous-speech-recognition-using-multilayer-with-Morgan-Bourlard",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition using multilayer perceptrons with hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A phoneme based, speaker-dependent continuous-speech recognition system embedding a multilayer perceptron (MLP) into a hidden Markov model (HMM) approach is described, which appears to be somewhat better when MLP methods are used to estimate the probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 519313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e510627d3f28601e370212bf063bbfa539ebed",
            "isKey": false,
            "numCitedBy": 1200,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot."
            },
            "slug": "Factorial-Hidden-Markov-Models-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Factorial Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner, and a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2798755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "slug": "Hidden-Markov-Model}-Induction-by-Bayesian-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Model} Induction by Bayesian Model Merging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is compared with the Baum-Welch method of estimating fixed-size models, and it is found that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "See for example [74, 75] and [76] for polynomial-time algorithms that constructively learn a probabilistic structure for the language by merging states, and [45] on the learnability of acyclic probabilistic nite automata."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1985596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204f6148bc6aba37eb5a7c5686d80547a99425b1",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "slug": "Best-first-Model-Merging-for-Hidden-Markov-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Best-first Model Merging for Hidden Markov Model Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy, and how the algorithm was incorporated in an operational speech understanding system, where it was combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "See also [46, 47] for relations between HMMs and other graphical models such as Markov random elds and Kalman lters, and see [48] for an application of these ideas to Turbo-decoding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10043879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0999dc17b35c0d893974f03d98293f71f27698b",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach."
            },
            "slug": "Probabilistic-Independence-Networks-for-Hidden-Smyth-Heckerman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Independence Networks for Hidden Markov Probability Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the well-known forward-backward and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs and the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354787"
                        ],
                        "name": "G. Zavaliagkos",
                        "slug": "G.-Zavaliagkos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zavaliagkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zavaliagkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143811539"
                        ],
                        "name": "S. Austin",
                        "slug": "S.-Austin",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Austin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Austin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8785536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce0c63c0f64ae323599b850e7a02d1b1136a7803",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Up till recently, state-of-the-art, large vocabulary, continuous speech recognition (CSR) had employed hidden Markov modeling (HMM) to model speech sounds. In an attempt to improve over HMM we developed a hybrid system that integrates HMM technology with neural networks. We present the concept of a Segmental Neural Net (SNN) for phonetic modeling in CSR. By taking into account all the frames of a phonetic segment simultaneously, the SNN overcomes the well-known conditional-independence limitation of HMMs. We have developed a novel hybrid SNN/HMM system that combines the advantages of SNNs and HMMs using a multiple hypothesis (or N-best) paradigm. In this system, we generate likely phonetic segmentations from the HMM N-best list of word sequences, which are scored by the SNN. The HMM and SNN scores are then combined to optimize performance. In several speaker-independent, 1000-word CSR tests, the error rate for the hybrid system dropped 20% from that of a state-of-the-art HMM system alone."
            },
            "slug": "A-Hybrid-Continuous-Speech-Recognition-System-Using-Zavaliagkos-Austin",
            "title": {
                "fragments": [],
                "text": "A Hybrid Continuous Speech Recognition System Using Segmental Neural Nets with Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors have developed a system that combines the SNN with a hidden Markov model (HMM) system and believe that this is the first system incorporating a neural network for which the performance has exceeded the state of the art in large-vocabulary, continuous speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2308463"
                        ],
                        "name": "Salah El Hihi",
                        "slug": "Salah-El-Hihi",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Hihi",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salah El Hihi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2843869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs."
            },
            "slug": "Hierarchical-Recurrent-Neural-Networks-for-Hihi-Bengio",
            "title": {
                "fragments": [],
                "text": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically, which implies that long-term dependencies are represented by variables with a long time scale."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906655"
                        ],
                        "name": "T. Hunkapiller",
                        "slug": "T.-Hunkapiller",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hunkapiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hunkapiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35016244"
                        ],
                        "name": "M. A. McClure",
                        "slug": "M.-A.-McClure",
                        "structuredName": {
                            "firstName": "Marcella",
                            "lastName": "McClure",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. McClure"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 196
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [24, 25, 26, 27, 28, 29, 30], pattern recognition in molecular biology [31, 32], and fault-detection [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44553028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a68a57a90611629ee21f4cb3b8a2ddadbb6b41b",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov model (HMM) techniques are used to model families of biological sequences. A smooth and convergent algorithm is introduced to iteratively adapt the transition and emission parameters of the models from the examples in a given family. The HMM approach is applied to three protein families: globins, immunoglobulins, and kinases. In all cases, the models derived capture the important statistical characteristics of the family and can be used for a number of tasks, including multiple alignments, motif detection, and classification. For K sequences of average length N, this approach yields an effective multiple-alignment algorithm which requires O(KN2) operations, linear in the number of sequences."
            },
            "slug": "Hidden-Markov-models-of-biological-primary-sequence-Baldi-Chauvin",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models of biological primary sequence information."
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A smooth and convergent algorithm is introduced to iteratively adapt the transition and emission parameters of the models from the examples in a given family, yielding an effective multiple-alignment algorithm which requires O(KN2) operations, linear in the number of sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9028097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f3876251cc2f03a06190ef0a47544042da416aa",
            "isKey": false,
            "numCitedBy": 472,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a probabilistic model-based approach to clustering sequences, using hidden Markov models (HMMs). The problem can be framed as a generalization of the standard mixture model approach to clustering in feature space. Two primary issues are addressed. First, a novel parameter initialization procedure is proposed, and second, the more difficult problem of determining the number of clusters K, from the data, is investigated. Experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences."
            },
            "slug": "Clustering-Sequences-with-Hidden-Markov-Models-Smyth",
            "title": {
                "fragments": [],
                "text": "Clustering Sequences with Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "A variant of the continuous HMM with Gaussian mixtures is the so-called semi-continuous HMM [59, 60], in which the Gaussians are shared and the parameters speci c to each state are only the mixture weights:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17743203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df682aa90fbbbf665a8b273a57ca87d6cea9ff99",
            "isKey": false,
            "numCitedBy": 1561,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of hidden Markov models for speech recognition has become predominant in the last several years, as evidenced by the number of published papers and talks at major speech conferences. The reasons this method has become so popular are the inherent statistical (mathematically precise) framework; the ease and availability of training algorithms for cstimating the parameters of the models from finite training sets of speech data; the flexibility of the resulting recognition system in which one can easily change the size, type, or architecture of the models to suit particular words, sounds, and so forth; and the ease of implementation of the overall recognition system. In this expository article, we address the role of statistical methods in this powerful technology as applied to speech recognition and discuss a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations."
            },
            "slug": "Hidden-Markov-Models-for-Speech-Recognition-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The role of statistical methods in this powerful technology as applied to speech recognition is addressed and a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18865663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "830ccb44084d9d6cdcb70d623df5012ae4835142",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'."
            },
            "slug": "Training-Stochastic-Model-Recognition-Algorithms-as-Bridle",
            "title": {
                "fragments": [],
                "text": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that once the output layer of a multilayer perceptron is modified to provide mathematically correct probability distributions, and the usual squared error criterion is replaced with a probability-based score, the result is equivalent to Maximum Mutual Information training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "See [71] for a recent survey of statistical methods for speech recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12495425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "231f6de83cfa4d641da1681e97a11b689a48e3aa",
            "isKey": false,
            "numCitedBy": 2251,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method."
            },
            "slug": "Statistical-methods-for-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Statistical methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110051082"
                        ],
                        "name": "K. Lee",
                        "slug": "K.-Lee",
                        "structuredName": {
                            "firstName": "K.-F.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145058181"
                        ],
                        "name": "H. Hon",
                        "slug": "H.-Hon",
                        "structuredName": {
                            "firstName": "Hsiao-Wuen",
                            "lastName": "Hon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "A variant of the continuous HMM with Gaussian mixtures is the so-called semi-continuous HMM [45, 46], in which the Gaussians are shared and the parameters speci c to each state are only the mixture weights: P (ytjqt = i) =Xj wjiN(yt; j ; j): where the mixture weights play a role that is similar to the multinomial coe cients of the discrete emission HMMs described above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123145665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9fee0d8c9d7f8c697b96718cb40aec820590313",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The semicontinuous hidden Markov model is used in a 1000-word speaker-independent continuous speech recognition system and compared with the continuous mixture model and the discrete model. When the acoustic parameter is not well modeled by the continuous probability density, it is observed that the model assumption problems may cause the recognition accuracy of the semicontinuous model to be inferior to the discrete model. A simple method based on the semicontinuous model is investigated, to re-estimate the vector quantization codebook without continuous probability density function assumptions. Preliminary experiments show that such reestimation methods are as effective as the semicontinuous model, especially when the continuous probability density function assumption is inappropriate.<<ETX>>"
            },
            "slug": "On-semi-continuous-hidden-Markov-modeling-Huang-Lee",
            "title": {
                "fragments": [],
                "text": "On semi-continuous hidden Markov modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A simple method is investigated, to re-estimate the vector quantization codebook without continuous probability density function assumptions, and preliminary experiments show that such reestimation methods are as effective as the semicontinuous model, especially when the continuous probabilitydensity function assumption is inappropriate."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Instead, in [16], a function that is a lower bound on the log likelihood is maximized with a tractable algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "This idea was applied to hybrids of continuous and discrete state variables [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 355,
                                "start": 347
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "See [137] and [16] for a review of such models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16365840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad4963fdb01b7e55fc08fe8914a6dab15205ba7c",
            "isKey": true,
            "numCitedBy": 92,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a statistical model for times series data with nonlinear dynamics which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of those regimes. This model combines and generalizes two of the most widely used stochastic time series models|the hidden Markov model and the linear dynamical system|and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward{backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems."
            },
            "slug": "Switching-State-Space-Models-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "Switching State-Space Models"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A statistical model for times series data with nonlinear dynamics which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of those regimes, and presents a variational approximation which maximizes a lower bound on the log likelihood."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60753901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c41868f69d265783b7540094946ee902571c5cd",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The acoustic-modelling problem in automatic speech recognition is examined from an information theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is factored into two steps: a signal-processing step which converts a speech waveform into a sequence of informative acoustic feature vectors, and a step which models such a sequence. The authors are primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space. They explore the trade-off between packing information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous-parameter sequences is addressed by investigating a method of parameter estimation which is designed to cope with inaccurate modeling assumptions.<<ETX>>"
            },
            "slug": "Speech-recognition-with-continuous-parameter-hidden-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "Speech recognition with continuous-parameter hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors explore the trade-off between packing information into sequences of feature vectors and being able to model them accurately and investigate a method of parameter estimation which is designed to cope with inaccurate modeling assumptions."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076736465"
                        ],
                        "name": "R. Nag",
                        "slug": "R.-Nag",
                        "structuredName": {
                            "firstName": "Ronjon",
                            "lastName": "Nag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144893100"
                        ],
                        "name": "K. Wong",
                        "slug": "K.-Wong",
                        "structuredName": {
                            "firstName": "Kin",
                            "lastName": "Wong",
                            "middleNames": [
                                "Hong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58277721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc3e1988d5f7bdd0c755d873914090b416570bc7",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A handwritten script recognition system is presented which uses Hidden Markov Models (HMM), a technique widely used in speech recognition. The script is encoded as templates in the form of a sequence of quantised inclination angles of short equal length vectors together with some additional features. A HMM is created for each written word from a set of training data. Incoming templates are recognised by calculating which model has the highest probability for producing that template. The task chosen to test the system is that of handwritten word recognition, where the words are digits written by one person. Results are given which show that HMMs provide a versatile pattern matching tool suitable for some image processing tasks as well as speech processing problems."
            },
            "slug": "Script-recognition-using-hidden-Markov-models-Nag-Wong",
            "title": {
                "fragments": [],
                "text": "Script recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Results are given which show that HMMs provide a versatile pattern matching tool suitable for some image processing tasks as well as speech processing problems."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "An HMM can also be viewed as a particular kind of recurrent [85] ANN [98, 99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41994273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c91c2ac02e33caff601b2e4d62a6841b33ca3929",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Alpha-nets:-A-recurrent-'neural'-network-with-a-Bridle",
            "title": {
                "fragments": [],
                "text": "Alpha-nets: A recurrent 'neural' network architecture with a hidden Markov model interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16693657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "730068cb367489f91aa8a99bc83fb43ef3b2466c",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In learning tasks in which input sequences are mapped to output sequences it is often the case that the input and output sequences are not synchronous For example in speech recognition acoustic sequences are longer than phoneme sequences Input Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length We extend here this model to the case of asynchronous sequences and show an Expectation Maximization algorithm for training such models Introduction Supervised learning algorithms for sequential data minimize a training criterion that depends on pairs of input and output sequences It is often assumed that input and output sequences are synchronized i e that each input sequence has the same length as the corresponding output sequence For instance recurrent networks Rumelhart et al can be used to map input sequences to output sequences for example minimizing at each time step the squared di erence between the actual output and the desired output Another example is a recently proposed recurrent mixture of experts connectionist ar chitecture which has an interpretation as a probabilistic model called Input Output Hidden Markov Model IOHMM Bengio and Frasconi Bengio and Frasconi This model represents the distribution of an output sequence when given an input sequence of the same length using a hidden state variable and a Markovian independence assumption as in Hidden Markov Models HMMs Levinson et al Rabiner in order to simplify the distribution IOHMMs are a form of probabilistic transducers Pereira et al Singer with input and output variables which can be discrete as well as continuous valued However in many sequential problems where one tries to map an input sequence to an output sequence the length of the input and output sequences may not be equal Input and output sequences could behave at di erent time scales For example in a speech recognition problem where one wants to map an acoustic signal to a phoneme sequence each phoneme approximately corresponds to a subsequence of the acoustic signal therefore the input acoustic sequence is generally longer than the output phoneme sequence and the alignment between inputs and outputs is often not available In comparison with HMMs emission and transition probabilities in IOHMMs vary with time in function of an input sequence Unlike HMMs IOHMMs with discrete outputs are discriminant models Furthermore the transition probabilities and emission probabilities are generally better matched which reduces a problem observed in speech recognition HMMs because outputs are in a much higher dimensional space than transitions in HMMs the dynamic range of transition probabilities is much less than that of emission probabilities Therefore the choice between di erent paths during recognition is mostly in uenced by emission rather than transition probabilities In this paper we present an extension of IOHMMs to the asynchronous case We rst present the proba bilistic model then derive an exact Expectation Maximization EM algorithm for training asynchronous IOHMMs For complex distributions e g using arti cial neural networks to represent transition and emission distributions a Generalized EM algorithm or gradient ascent in likelihood can be used Finally a recognition algorithm similar to the Viterbi algorithm is presented to map given input sequences to likely output sequences The Model Let us note u for input sequences u u uT and similarly y S for output sequences y y yS In this paper we consider the case in which the output sequences are shorter than the input sequences The more general case is a straightforward extension of this model using empty transitions that do not take any time and will be discussed elsewhere As in HMMs and IOHMMs we introduce a discrete hidden state variable xt which will allow us to simplify the distribution P y ju T by using Markovian independence assumptions The state sequence x is taken to be synchronous with the input sequence u In order to produce output sequences shorter than input sequences we will have states that do not emit an output as well as states that do emit an output When at time t the system is in a non emitting state no output can be produced Therefore there exists many sequences of states corresponding to di erent shorter length output sequences When conceived as a generative model of the output given the input an asynchronous IOHMM works as follows At time t an initial state x is chosen according to the distribution P x and the length of the output sequence s is initialized to At other time steps t a state xt is rst picked according to the transition distribution P xtjxt ut using the state at the previous time step xt and the current input ut If xt is an emitting state then the length of the output sequence is increased from s to s and the sth output ys is sampled from the emission distribution P ysjxt ut The parameters of the model are thus the initial state probabilities i P x i and the parameters of the output and transition conditional distribution models P ysjxt ut and P xtjxt ut Since the input and output sequences are of di erent lengths we will introduce another hidden variable t speci cally to represent the alignment between inputs and outputs with t s meaning that s outputs have been emitted at time t Let us rst formalize the independence assumptions and the form of the conditional distribution rep resented by the model The conditional probability P y ju T can be written as a sum of terms P y x T T ju T over all possible state sequences x T such that the number of emitting states in each of these sequences is S the length of the output sequence P y ju T X x T S P y x T T ju T All S outputs must have been emitted by time T so T S The hidden state xt takes discrete values in a nite set Each of the terms P y x T T ju T corresponds to a particular sequence of states and a corresponding alignment this probability can be written as the initial state probabilities P x times a product of factors over all the time steps t if state xt i is an emitting state that factor is P xtjxt ut P ysjxt ut otherwise that factor is simply P xtjxt ut where s is the position in the output sequence of the output emitted at time t when an output is emitted at time t We summarize in table the notation we have introduced and de ne additional notation used in this paper Table Notation used in the paper S size of the output sequence T size of the input sequence N number of states in the IOHMM a i j t output of the module that computes P xt ijxt j ut b i s t output of the module that computes P ysjxt i ut i P x i initial probability of state i zi t if xt i zi t otherwise These indicator variables give the state sequence ms t if the system emits the s th output at time t ms t otherwise These indicator variables give the input output alignment ei is true if state i emits so P ei ei is false otherwise t s means that the rst s rst outputs have been emitted at time t t k if the t th input symbol is k t k otherwise s k if the s th output symbol is k s k otherwise pred i is the set of all the predecessors states of state i succ i is the set of all the successors states of state i The Markovian conditional independence assumptions in this model mean that the state variable xt summarizes su ciently the past of the sequence so P xtjx t u t P xtjxt ut and P ysjx t u t P ysjxt ut These assumptions are analogous to the Markovian independence assumptions used in HMMs and are the same as in synchronous IOHMMs Based on these two assumptions the conditional probability can be e ciently represented and computed recursively using an intermediate variable i s t def P xt i t s y s ju t The conditional probability of an output sequence can be expressed in terms of this variable L def P y ju T X"
            },
            "slug": "An-EM-Algorithm-for-Asynchronous-Input/Output-Bengio-Bengio",
            "title": {
                "fragments": [],
                "text": "An EM Algorithm for Asynchronous Input/Output Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper considers the case in which the output sequences are shorter than the input sequences, and presents an Expectation Maximization algorithm for training asynchronous IOHMMs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46254718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call \u201cleft-to-right\u201d models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper."
            },
            "slug": "An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner",
            "title": {
                "fragments": [],
                "text": "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain, and focuses on a particular class of Markov models, which are especially appropriate for isolated word recognition."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10494385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21339e43ba099c5729f936031c24cfaf38a95ccd",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 159,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic Bayesian networks (DBNs) are a powerful and flexible methodology for representing and computing with probabilistic models of stochastic processes. In the past decade, there has been increasing interest in applying them to practical problems, and this thesis shows that they can be used effectively in the field of automatic speech recognition. \nA principle characteristic of dynamic Bayesian networks is that they can model an arbitrary set of variables as they evolve over time. Moreover, an arbitrary set of conditional independence assumptions can be specified, and this allows the joint distribution to be represented in a highly factored way. Factorization allows for models with relatively few parameters, and computational efficiency. Standardized inference and learning routines allow a variety of model structures to be tested without deriving new formulae, or writing new code. \nThe contribution of this thesis is to show how DBNs can be used in automatic speech recognition. This involves solving problems related to both representation and inference. Representationally, the thesis shows how to encode stochastic finite-state word models as DBNs, and how to construct DBNs that explicitly model the speech-articulators, accent, gender, speaking-rate, and other important phenomena. Technically, the thesis presents inference routines that are especially tailored to the requirements of speech recognition: efficient inference with deterministic constraints, variable-length utterances, and online inference. Finally, the thesis presents experimental results that indicate that real systems can be built, and that modeling important phenomena with DBNs results in higher recognition accuracy."
            },
            "slug": "Speech-Recognition-with-Dynamic-Bayesian-Networks-Zweig-Russell",
            "title": {
                "fragments": [],
                "text": "Speech Recognition with Dynamic Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This thesis shows that dynamic Bayesian networks can be used effectively in the field of automatic speech recognition, and presents inference routines that are especially tailored to the requirements of speech recognition: efficient inference with deterministic constraints, variable-length utterances, and online inference."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [87, 88, 1, 100]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "This procedure has been found to converge and yield speech recognition performance at the level of state-of-the-art systems [1, 100]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61058350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d82e058a5c40954b8f5db170a298a889a254c37",
            "isKey": false,
            "numCitedBy": 1409,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing."
            },
            "slug": "Connectionist-Speech-Recognition:-A-Hybrid-Approach-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist Speech Recognition: A Hybrid Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115311"
                        ],
                        "name": "R. Pieraccini",
                        "slug": "R.-Pieraccini",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Pieraccini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Pieraccini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606242"
                        ],
                        "name": "E. Bocchieri",
                        "slug": "E.-Bocchieri",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Bocchieri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bocchieri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "In some cases the dynamic programming algorithm is embedded in the ANN itself [73, 76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18864457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3673a263d6bc1af93f2b00fdc5e002df501b0f26",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, much interest has been generated regarding speech recognition systems based on Hidden Markov Models (HMMs) and neural network (NN) hybrids. Such systems attempt to combine the best features of both models: the temporal structure of HMMs and the discriminative power of neural networks. In this work we define a time-warping (TW) neuron that extends the operation of the formal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights. We show that a single-layer network of TW neurons is equivalent to a Gaussian density HMM-based recognition system, and we propose to improve the discriminative power of this system by using back-propagation discriminative training, and/or by generalizing the structure of the recognizer to a multi-layered net. The performance of the proposed network was evaluated on a highly confusable, isolated word, multi speaker recognition task. The results indicate that not only does the recognition performance improve, but the separation between classes is enhanced also, allowing us to set up a rejection criterion to improve the confidence of the system."
            },
            "slug": "Time-Warping-Network:-A-Hybrid-Framework-for-Speech-Levin-Pieraccini",
            "title": {
                "fragments": [],
                "text": "Time-Warping Network: A Hybrid Framework for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A time-warping neuron is defined that extends the operation of the formal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47749181"
                        ],
                        "name": "M. Franzini",
                        "slug": "M.-Franzini",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Franzini",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Franzini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110051082"
                        ],
                        "name": "K. Lee",
                        "slug": "K.-Lee",
                        "structuredName": {
                            "firstName": "K.-F.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12292955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f866ac085771f5676800db2d9b102975b2a1b2d7",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A hybrid method for continuous-speech recognition which combines hidden Markov models (HMMs) and a connectionist technique called connectionist Viterbi training (CVT) is presented. CVT can be run iteratively and can be applied to large-vocabulary recognition tasks. Successful completion of training the connectionist component of the system, despite the large network size and volume of training data, depends largely on several measures taken to reduce learning time. The system is trained and tested on the TI/NBS speaker-independent continuous-digits database. Performance on test data for unknown-length strings is 98.5% word accuracy and 95.0% string accuracy. Several improvements to the current system are expected to increase these accuracies significantly.<<ETX>>"
            },
            "slug": "Connectionist-Viterbi-training:-a-new-hybrid-method-Franzini-Lee",
            "title": {
                "fragments": [],
                "text": "Connectionist Viterbi training: a new hybrid method for continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A hybrid method for continuous-speech recognition which combines hidden Markov models (HMMs) and a connectionist technique called connectionist Viterbi training (CVT) is presented and can be run iteratively and applied to large-vocabulary recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [26, 27, 28, 29, 30, 31, 32], pattern recognition in molecular biology [33, 34, 35, 36, 3], and fault-detection [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2160404,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5d28fc1a4027d23cc9e4ad8555361d48940e9be8",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both."
            },
            "slug": "Hidden-Markov-models-in-computational-biology.-to-Krogh-Brown",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models in computational biology. Applications to protein modeling."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "These algorithms were used in language modeling [34, 11] and handwritten character recognition [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [34, 35, 11], econometrics [12, 13, 36], time series, and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12714484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef68578b40aed36f8b117a64b9b21db11cb6de56",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a linguistic postprocessor for character recognizers. The central module of our system is a trainable variable memory length Markov model (VLMM) that predicts the next character given a variable length window of past characters. The overall system is composed of several finite state automata, including the main VLMM and a proper noun VLMM. The best model reported in the literature (Brown et al., 1992) achieves 1.75 bits per character on the Brown corpus. On that same corpus, our model, trained on 10 times less data, reaches 2.19 bits per character and is 200 times smaller (/spl sime/160,000 parameters). The model was designed for handwriting recognition applications but could also be used for other OCR problems and speech recognition."
            },
            "slug": "Design-of-a-linguistic-postprocessor-using-variable-Guyon-Pereira",
            "title": {
                "fragments": [],
                "text": "Design of a linguistic postprocessor using variable memory length Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A linguistic postprocessor for character recognizers that predicts the next character given a variable length window of past characters that was designed for handwriting recognition applications but could also be used for other OCR problems and speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34320269"
                        ],
                        "name": "A. Kundu",
                        "slug": "A.-Kundu",
                        "structuredName": {
                            "firstName": "Amlan",
                            "lastName": "Kundu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kundu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066057960"
                        ],
                        "name": "P. Bahl",
                        "slug": "P.-Bahl",
                        "structuredName": {
                            "firstName": "Paramvir",
                            "lastName": "Bahl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [26, 27, 28, 29, 30, 31, 32], pattern recognition in molecular biology [33, 34, 35, 36, 3], and fault-detection [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61492014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8401323b907eb81f4c50e9959863809af46fe5c",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The handwritten script recognition problem is modeled in the framework of the hidden Markov model. For English text, which is the focus of the present research, the states can be identified with the letters of the alphabet, and the optimum symbols can be generated. In order to do so, a quantitative definition of symbols, in terms of features, is required. Fourteen features (some old, some new) are proposed for this task. Using the existing statistical knowledge about the English language, the calculation of the model parameters is immensely simplified. Once the model is established, the Viterbi algorithm is proposed to recognize the single best optimal state sequence, i.e. sequence of letters comprising the word. The modification of the recognition algorithm to accommodate context information is also discussed. Some experimental results are provided indicating the success of the new scheme.<<ETX>>"
            },
            "slug": "Recognition-of-handwritten-script:-a-hidden-Markov-Kundu-Bahl",
            "title": {
                "fragments": [],
                "text": "Recognition of handwritten script: a hidden Markov model based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The handwritten script recognition problem is modeled in the framework of the hidden Markov model, and the Viterbi algorithm is proposed to recognize the single best optimal state sequence, i.e. sequence of letters comprising the word."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1758609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "914fa99500eb779e33d22609f8a0fdf3fd2799e1",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm."
            },
            "slug": "Diffusion-of-Context-and-Credit-Information-in-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "Diffusion of Context and Credit Information in Markovian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper shows that the problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107710"
                        ],
                        "name": "Vincent-Philippe Lauzon",
                        "slug": "Vincent-Philippe-Lauzon",
                        "structuredName": {
                            "firstName": "Vincent-Philippe",
                            "lastName": "Lauzon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent-Philippe Lauzon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [38, 39, 13], econometrics [14, 15, 40], time series [41], and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": ", using a neural network, as in the applications of IOHMMs to modeling the distribution of future returns described in [41] (which shows that these non-linear models can yield to improved modeling of the distribution of stock market indices future returns)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6417702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "139118b34a043c3b47d7682bb409de8663f068fa",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Input-output hidden Markov models (IOHMM) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditioned on an input sequence. For example, these conditional distributions can be linear, logistic, or nonlinear (using for example multilayer neural networks). We compare the generalization performance of several models which are special cases of input-output hidden Markov models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs. The experiments compare these models on predicting the conditional density of returns of market and sector indices. Note that the unconditional Gaussian estimates the first moment with the historical average. The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood."
            },
            "slug": "Experiments-on-the-application-of-IOHMMs-to-model-Bengio-Lauzon",
            "title": {
                "fragments": [],
                "text": "Experiments on the application of IOHMMs to model financial returns series"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "See also [46, 47] for relations between HMMs and other graphical models such as Markov random elds and Kalman lters, and see [48] for an application of these ideas to Turbo-decoding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17026089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2792ccda37a3818b8e33b20f0492c2e6a097fa1f",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Belief-networks,-hidden-Markov-models,-and-Markov-A-Smyth",
            "title": {
                "fragments": [],
                "text": "Belief networks, hidden Markov models, and Markov random fields: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149620"
                        ],
                        "name": "D. Mergel",
                        "slug": "D.-Mergel",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Mergel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mergel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801257"
                        ],
                        "name": "A. Noll",
                        "slug": "A.-Noll",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Noll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Noll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1968677"
                        ],
                        "name": "A. Paeseler",
                        "slug": "A.-Paeseler",
                        "structuredName": {
                            "firstName": "Annedore",
                            "lastName": "Paeseler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Paeseler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29795200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee61f8cd6a75e1a87f3e5d1aae49b9dbe95e97db",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe an architecture and search organization for continuous speech recognition. The recognition module is part of the Siemens-Philips-Ipo project on continuous speech recognition and understanding (SPICOS) system for the understanding of database queries spoken in natural language. The goal of this project is a man-machine dialogue system that is able to understand fluently spoken German sentences and thus to provide voice access to a database. The recognition strategy is based on Bayes decision rule and attempts to find the best interpretation of the input speech data in terms of knowledge sources such as a language model, pronunciation lexicon, and inventory of subword units. The implementation of the search has been tested on a continuous speech database comprising up to 4000 words for each of several speakers. The efficiency and robustness of the search organization have been checked and evaluated along many dimensions, such as different speakers, phoneme models, and language models. >"
            },
            "slug": "Data-driven-search-organization-for-continuous-Ney-Mergel",
            "title": {
                "fragments": [],
                "text": "Data driven search organization for continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "An architecture and search organization for continuous speech recognition based on Bayes decision rule and attempts to find the best interpretation of the input speech data in terms of knowledge sources such as a language model, pronunciation lexicon, and inventory of subword units."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 237
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [26, 27, 28, 29, 30, 31, 32], pattern recognition in molecular biology [33, 34, 35, 36, 3], and fault-detection [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17048772,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "3262f18a26d6d2d300138097b1d15b269197bba1",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hidden-Markov-models-for-fault-detection-in-dynamic-Smyth",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models for fault detection in dynamic systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 130
                            }
                        ],
                        "text": "The most common are the maximum a posteriori criterion, to maximize P (w 1 jy T 1 ), and the maximum mutual information criterion [77, 78, 79], to maximize log P (y 1 jw L 1 ) P (y 1 ) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60769407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This thesis examines the acoustic-modeling problem in automatic speech recognition from an information-theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is broken down into two steps: a signal processing step which converts a speech waveform into a sequence of information bearing acoustic feature vectors, and a step which models such a sequence. This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N. It explores the trade-off between packing a lot of information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous parameter sequences is addressed by investigating a method of parameter estimation which is specifically designed to cope with inaccurate modeling assumptions."
            },
            "slug": "The-acoustic-modeling-problem-in-automatic-speech-Brown",
            "title": {
                "fragments": [],
                "text": "The acoustic-modeling problem in automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N and explores the trade-off between packing a lot of information into such sequences and being able to model them accurately."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61818881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "758eae04fc9f4331b0ceab797387c8fc9f00db58",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-recurrent-error-propagation-network-speech-system-Robinson-Fallside",
            "title": {
                "fragments": [],
                "text": "A recurrent error propagation network speech recognition system"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735952"
                        ],
                        "name": "D. Ron",
                        "slug": "D.-Ron",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8202564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11376222763d16f39a5be1a5151d55556997236e",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long memory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text. Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states. We also show how the algorithm can be applied to intergenic E. coli DNA base prediction with results comparable to HMM based methods."
            },
            "slug": "The-Power-of-Amnesia-Ron-Singer",
            "title": {
                "fragments": [],
                "text": "The Power of Amnesia"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The algorithm is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small and using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "78659204"
                        ],
                        "name": "M. Mohri",
                        "slug": "M.-Mohri",
                        "structuredName": {
                            "firstName": "Mehryar",
                            "lastName": "Mohri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 32
                            }
                        ],
                        "text": "A generic composition operation [8, 9, 10, 11] allows to combine a cascade of transducers and acceptors, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "More generally, weighted acceptors and transducers [8, 9, 10, 11] can be used to assign a weight to a sequence (or a pair of input/output sequences)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 229
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5548799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4cc5563c694355ddcf746ff9a55ccdb22d86a98",
            "isKey": false,
            "numCitedBy": 1040,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Finite-machines have been used in various domains of natural language processing. We consider here the use of a type of transducer that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-to-string transducers. Transducers that outpur weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minizizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated."
            },
            "slug": "Finite-State-Transducers-in-Language-and-Speech-Mohri",
            "title": {
                "fragments": [],
                "text": "Finite-State Transducers in Language and Speech Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work recalls classical theorems and gives new ones characterizing sequential string-to-string transducers, including algorithms for determinizing and minizizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 130
                            }
                        ],
                        "text": "The most common are the maximum a posteriori criterion, to maximize P (wL 1 jyT 1 ), and the maximum mutual information criterion [66, 67, 68], to maximize log P (yT 1 jwL 1 ) P (yT 1 ) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33275295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "039900eaeeddd13752aa8d6c61759f0b0e54f0de",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Training methods for designing better decoders are compared. The training problem is considered as a statistical parameter estimation problem. In particular, the conditional maximum likelihood estimate (CMLE), which estimates the parameter values that maximize the conditional probability of words given acoustics during training, is compared to the maximum-likelihood estimate, which is obtained by maximizing the joint probability of the words and acoustics. For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect. In this sense, the CMLE/MMIE appears more robust than the MLE. >"
            },
            "slug": "On-a-model-robust-training-method-for-speech-N\u00e1das-Nahamoo",
            "title": {
                "fragments": [],
                "text": "On a model-robust training method for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11097170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc27f210ea7adc6de88a9efdbe99fabbbc1feeb9",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, and analyze the complexity of the training problem as a computational problem. The PA training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by a PA. We investigate the following question about this important, well-studied problem: Does there exist anefficient training algorithm such that the trained PAsprovably converge to a model close to an optimum one with high confidence, after only a feasibly small set of training data? We model this problem in the framework of computational learning theory and analyze the sample as well as computational complexity. We show that the number of examples required for training PAs is moderate\u2014except for some log factors the number of examples is linear in the number of transition probabilities to be trained and a low-degree polynomial in the example length and parameters quantifying the accuracy and confidence. Computationally, however, training PAs is quite demanding: Fixed state size PAs are trainable in time polynomial in the accuracy and confidence parameters and example length, butnot in the alphabet size unlessRP=NP. The latter result is shown via a strong non-approximability result for the single string maximum likelihood model probem for 2-state PAs, which is of independent interest."
            },
            "slug": "On-the-computational-complexity-of-approximating-by-Abe-Warmuth",
            "title": {
                "fragments": [],
                "text": "On the computational complexity of approximating distributions by probabilistic automata"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, is introduced and the complexity of the training problem as a computational problem is analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29085cdffb3277c1c8fd10ac09e0d89452c8db83",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation."
            },
            "slug": "An-Input-Output-HMM-Architecture-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "An Input Output HMM Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A recurrent architecture having a modular structure that has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47749181"
                        ],
                        "name": "M. Franzini",
                        "slug": "M.-Franzini",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Franzini",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Franzini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61727617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24ca5231df7cbd31e11a154c0c63ad48295f0398",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe two systems in which neural network classifiers are merged with dynamic programming (DP) time alignment methods to produce high-performance continuous speech recognizers. One system uses the connectionist Viterbi-training (CVT) procedure, in which a neural network with frame-level outputs is trained using guidance from a time alignment procedure. The other system uses multi-state time-delay neural networks (MS-TDNNs), in which embedded DP time alignment allows network training with only word-level external supervision. The CVT results on the, TI Digits are 99.1% word accuracy and 98.0% string accuracy. The MS-TDNNs are described in detail, with attention focused on their architecture, the training procedure, and results of applying the MS-TDNNs to continuous speaker-dependent alphabet recognition: on two speakers, word accuracy is respectively 97.5% and 89.7%.<<ETX>>"
            },
            "slug": "Integrating-time-alignment-and-neural-networks-for-Haffner-Franzini",
            "title": {
                "fragments": [],
                "text": "Integrating time alignment and neural networks for high performance continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors describe two systems in which neural network classifiers are merged with dynamic programming (DP) time alignment methods to produce high-performance continuous speech recognizers."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48046324"
                        ],
                        "name": "D. Gillman",
                        "slug": "D.-Gillman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gillman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gillman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367457"
                        ],
                        "name": "M. Sipser",
                        "slug": "M.-Sipser",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17646500,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "88e08bd20b2ffc201eb92f3d073e06df92bb9fad",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A hidden Markov chain (hmc) is a finite ergodic Markov chain in which each of the states is labelled 0 or 1. As the Markov chain moves through a random trajectory, the hmc emits a 0 or a 1 at each times step according to the label of the state just entered.\nThe inference problem is to construct a mechanism which will emit 0's and 1's and which is equivalent to a given hmc in the sense of having identical long-term behavior. We define the inference problem in a learning setting in which an algorithm can query an oracle for the long-term probability of any binary string. We prove that inference is hard: any algorithm for inference must make exponentially many oracle calls. Our method is information-theoretic and does not depend on separation assumptions for any complexity classes. We show that the related discrimination problem is also hard, but that on a nontrivial subclass of hmc's there is a randomized algorithm for discrimination. Finally, we give a polynomial-time algorithm for reducing a hidden Markov chain to its minimal form, and from this there follows a new algorithm for equivalence of hmc's."
            },
            "slug": "Inference-and-minimization-of-hidden-Markov-chains-Gillman-Sipser",
            "title": {
                "fragments": [],
                "text": "Inference and minimization of hidden Markov chains"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is proved that inference is hard: any algorithm for inference must make exponentially many oracle calls, and from this there follows a new algorithm for equivalence of hmc's."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 60
                            }
                        ],
                        "text": "For o -line applications, the EM algorithm can also be used [130], with a backward pass that is equivalent to the Rauch equations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12534912,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2e3170f91e1d8037f8ba03286fa5ddd347a0b88e",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Sto er, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models. Introduction The goal of this note is to introduce the EM algorithm for estimating the parameters of linear dynamical systems (LDS). Such linear systems can be used both for supervised and unsupervised modeling of time series. We rst describe the model and then brie y point out its relation to factor analysis and other data modeling techniques. The Model Linear time-invariant dynamical systems, also known as linear Gaussian state-space models, can be described by the following two equations: xt+1 = Axt +wt (1) yt = Cxt+ vt: (2) Time is indexed by the discrete index t. The output yt is a linear function of the state, xt, and the state at one time step depends linearly on the previous state. Both state and output noise, wt and vt, are zero-mean normally distributed random variables with covariance matrices Q and R, respectively. Only the output of the system is observed, the state and all the noise variables are hidden. Rather than regarding the state as a deterministic value corrupted by random noise, we combine the state variable and the state noise variable into a single Gaussian random"
            },
            "slug": "Parameter-estimation-for-linear-dynamical-systems-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "Parameter estimation for linear dynamical systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (LDS) is introduced and its relation to factor analysis and other data modeling techniques is pointed out."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [66, 67, 68, 69, 70, 71, 72, 73, 74, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62025798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a55d31784aca11871985096644a025f036633569",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speech-pattern-discrimination-and-multilayer-Bourlard-Wellekens",
            "title": {
                "fragments": [],
                "text": "Speech pattern discrimination and multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476651"
                        ],
                        "name": "K. Karplus",
                        "slug": "K.-Karplus",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Karplus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karplus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50787569"
                        ],
                        "name": "C. Barrett",
                        "slug": "C.-Barrett",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Barrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32813672"
                        ],
                        "name": "M. Cline",
                        "slug": "M.-Cline",
                        "structuredName": {
                            "firstName": "Melissa",
                            "lastName": "Cline",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cline"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144364614"
                        ],
                        "name": "R. Hughey",
                        "slug": "R.-Hughey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hughey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hughey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144907031"
                        ],
                        "name": "L. Holm",
                        "slug": "L.-Holm",
                        "structuredName": {
                            "firstName": "Liisa",
                            "lastName": "Holm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Holm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144882390"
                        ],
                        "name": "C. Sander",
                        "slug": "C.-Sander",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Sander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1183199,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a4bc19811ac9d235df0ebdd5420cc4936c3337c9",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss how methods based on hidden Markov models performed in the fold\u2010recognition section of the CASP2 experiment. Hidden Markov models were built for a representative set of just over 1,000 structures from the Protein Data Bank (PDB). Each CASP2 target sequence was scored against this library of HMMs. In addition, an HMM was built for each of the target sequences and all of the sequences in PDB were scored against that target model, with a good score on both methods indicating a high probability that the target sequence is homologous to the structure. The method worked well in comparison to other methods used at CASP2 for targets of moderate difficulty, where the closest structure in PDB could be aligned to the target with at least 15% residue identity. Proteins, Suppl. 1:134\u2013139, 1997. \u00a9 1998 Wiley\u2010Liss, Inc."
            },
            "slug": "Predicting-protein-structure-using-hidden-Markov-Karplus-Sj\u00f6lander",
            "title": {
                "fragments": [],
                "text": "Predicting protein structure using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "How methods based on hidden Markov models performed in the fold\u2010recognition section of the CASP2 experiment is discussed, with a good score on both methods indicating a high probability that the target sequence is homologous to the structure."
            },
            "venue": {
                "fragments": [],
                "text": "Proteins"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735952"
                        ],
                        "name": "D. Ron",
                        "slug": "D.-Ron",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "A Variable Length Markov Model [12] is a probability model over strings in which the state variable is not hidden: its value is a deterministic function of the past observation sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A constructive, on-line (one-pass), learning algorithm was proposed to adaptively grow this tree [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 275
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9839775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e5e82a07998ce355b65fe5d43b5c3138fd767d9",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA."
            },
            "slug": "The-power-of-amnesia:-Learning-probabilistic-with-Ron-Singer",
            "title": {
                "fragments": [],
                "text": "The power of amnesia: Learning probabilistic automata with variable memory length"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that the algorithm presented can efficiently learn distributions generated by PSAs, and it is shown that for any target PSA, the KL-divergence between the distributiongenerated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428168"
                        ],
                        "name": "M. Riley",
                        "slug": "M.-Riley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Riley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1320875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39e5fbed9b4928f4e88483e1f52c69d01396fb8a",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing. This generality allows us to represent uniformly such information sources as pronunciation dictionaries, language models and lattices, and to use uniform algorithms for building decoding stages and for optimizing and combining them. In particular, a single automata join algorithm can be used either to combine information sources such as a pronunciation dictionary and a context-dependency model during the construction of a decoder, or dynamically during the operation of the decoder. Applications to speech recognition and to Chinese text segmentation will be discussed."
            },
            "slug": "Weighted-Rational-Transductions-and-their-to-Human-Pereira-Riley",
            "title": {
                "fragments": [],
                "text": "Weighted Rational Transductions and their Application to Human Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The concepts of weighted language, transduction and automaton from algebraic automata theory are presented as a general framework for describing and implementing decoding cascades in speech and language processing and applications to speech recognition and Chinese text segmentation are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2975061"
                        ],
                        "name": "C. Nohl",
                        "slug": "C.-Nohl",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Nohl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nohl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3179635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a4192fd6efb5661eca197cce24289776a4fbcc2",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors."
            },
            "slug": "LeRec:-A-NN/HMM-Hybrid-for-On-Line-Handwriting-Bengio-LeCun",
            "title": {
                "fragments": [],
                "text": "LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new approach for on-line recognition of handwritten words written in unconstrained mixed style by fitting a model of the word structure using the EM algorithm to minimize word-level errors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122690"
                        ],
                        "name": "X. Aubert",
                        "slug": "X.-Aubert",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Aubert",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Aubert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32703822"
                        ],
                        "name": "C. Dugast",
                        "slug": "C.-Dugast",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Dugast",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dugast"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214660"
                        ],
                        "name": "Volker Steinbiss",
                        "slug": "Volker-Steinbiss",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Steinbiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Steinbiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2526767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11d74704fe3bb078f81e32e61857e082794c829d",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on recent developments of the Philips large vocabulary speech recognition system and on our experiments with the Wall Street Journal (WSJ) corpus. A two-pass decoding has been devised that allows an easy integration of more complex language models. First, a word lattice is produced using a time synchronous beam search with a bigram language model. Next, a higher-order language model is applied to the lattice at the phrase level. The conditions insuring the validity of this approach are explained and practical results for trigram demonstrate its usefulness. The main system development stages on WSJ data are presented and our final recognizers are evaluated on Nov. '92 and Nov. '93 test-data for both 5 K and 20 K vocabularies.<<ETX>>"
            },
            "slug": "Large-vocabulary-continuous-speech-recognition-of-Aubert-Dugast",
            "title": {
                "fragments": [],
                "text": "Large vocabulary continuous speech recognition of Wall Street Journal data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A two-pass decoding has been devised that allows an easy integration of more complex language models and practical results for trigram demonstrate its usefulness."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [26, 27, 28, 29, 30, 31, 32], pattern recognition in molecular biology [33, 34, 35, 36, 3], and fault-detection [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10783156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbb63cd4e20abdeaa1d0a4f631abf819967cab3e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Model techniques are used to derive a new model of the G-protein-coupled receptor family. The transition and emission parameters of the model are adjusted using a training set comprising 142 sequences. The resulting model is shown to perform well on a number of tasks, including multiple alignments, discrimination, large data base searches, classification, and fragment detection. General analytical results on the expectation and standard deviation of the likelihood of random sequences are also presented."
            },
            "slug": "Hidden-Markov-Models-of-the-G-Protein-Coupled-Baldi-Chauvin",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models of the G-Protein-Coupled Receptor Family"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The Hidden Markov Model techniques are used to derive a new model of the G-protein-coupled receptor family that performs well on a number of tasks, including multiple alignments, discrimination, large data base searches, classification, and fragment detection."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Biol."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46880385"
                        ],
                        "name": "C. Carter",
                        "slug": "C.-Carter",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Carter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Carter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144402257"
                        ],
                        "name": "R. Kohn",
                        "slug": "R.-Kohn",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kohn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122959783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "917d8472fc08b50ed13f1af9ff1b684310d12fef",
            "isKey": false,
            "numCitedBy": 2077,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We show how to use the Gibbs sampler to carry out Bayesian inference on a linear state space model with errors that are a mixture of normals and coefficients that can switch over time. Our approach simultaneously generates the whole of the state vector given the mixture and coefficient indicator variables and simultaneously generates all the indicator variables conditional on the state vectors. The states are generated efficiently using the Kalman filter. We illustrate our approach by several examples and empirically compare its performance to another Gibbs sampler where the states are generated one at a time. The empirical results suggest that our approach is both practical to implement and dominates the Gibbs sampler that generates the states one at a time."
            },
            "slug": "On-Gibbs-sampling-for-state-space-models-Carter-Kohn",
            "title": {
                "fragments": [],
                "text": "On Gibbs sampling for state space models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work shows how to use the Gibbs sampler to carry out Bayesian inference on a linear state space model with errors that are a mixture of normals and coefficients that can switch over time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47429521"
                        ],
                        "name": "F. Alleva",
                        "slug": "F.-Alleva",
                        "structuredName": {
                            "firstName": "Fil",
                            "lastName": "Alleva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Alleva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144091892"
                        ],
                        "name": "M. Hwang",
                        "slug": "M.-Hwang",
                        "structuredName": {
                            "firstName": "Mei-Yuh",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hwang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57374189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ee844d3d9c5b9f8ddfac228e99f87bc782d8247",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A search algorithm that incrementally makes effective use of detailed sources of knowledge is proposed. The algorithm incrementally applies all available acoustic and linguistic information in three search phases. Phase one is a left-to-right Viterbi beam search that produces word end times and scores using right context between-word models with a bigram language model. Phase two, guided by results from phase one, is a right-to-left Viterbi beam search that produces word begin times and scores based on left context between-word models. Phase three is an A* search that combines the results of phases one and two with a long-distance language model. The objective is to maximize the recognition accuracy with a minimal increase in computational cost. With the decomposed, incremental, search algorithm, it is shown that early use of detailed acoustic models can significantly reduce the recognition error rate with a negligible increase in computational cost. It is demonstrated that the early use of detailed knowledge can improve the word error bound by at least 22% for large-vocabulary, speaker-independent, continuous speech recognition.<<ETX>>"
            },
            "slug": "An-improved-search-algorithm-using-incremental-for-Alleva-Huang",
            "title": {
                "fragments": [],
                "text": "An improved search algorithm using incremental knowledge for continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that the early use of detailed knowledge can improve the word error bound by at least 22% for large-vocabulary, speaker-independent, continuous speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166695544"
                        ],
                        "name": "Chang\u2010Jin Kim",
                        "slug": "Chang\u2010Jin-Kim",
                        "structuredName": {
                            "firstName": "Chang\u2010Jin",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang\u2010Jin Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The parameters of Markov switching models can generally be estimated using the EM algorithm [121, 109, 111, 122] to maximize the likelihood P (y 1 j ) (see next section)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121219057,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2c417753849f01f12f3f788460c3858dc77b4e0",
            "isKey": false,
            "numCitedBy": 1379,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-linear-models-with-Markov-switching-Kim",
            "title": {
                "fragments": [],
                "text": "Dynamic linear models with Markov-switching"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443457"
                        ],
                        "name": "M. Schenkel",
                        "slug": "M.-Schenkel",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Schenkel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schenkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [24, 25, 26, 27, 28, 29, 30], pattern recognition in molecular biology [31, 32], and fault-detection [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31949019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09bdbdf186e24db2cf11c4c1006c718478c10b3f",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a writer-independent system for online handwriting recognition that can handle a variety of writing styles including cursive script and handprinting. The input to our system contains the pen trajectory information, encoded as a time-ordered sequence of feature vectors. A time-delay neural network is used to estimate a posteriori probabilities for characters in a word. A hidden Markov model segments the word in a way that optimizes the global word score, using a dictionary in the process. A geometrical normalization scheme and a fast but efficient dictionary search are also presented. Trained on 20 k words from 59 writers, using a 25 k-word dictionary, our system reached recognition rates of 89% for characters and 80% for words on test data from a disjoint set of writers."
            },
            "slug": "On-line-cursive-script-recognition-using-time-delay-Schenkel-Guyon",
            "title": {
                "fragments": [],
                "text": "On-line cursive script recognition using time-delay neural networks and hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A writer-independent system for online handwriting recognition that can handle a variety of writing styles including cursive script and handprinting is presented and a geometrical normalization scheme and a fast but efficient dictionary search are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "See [48, 49, 50, 51] for more formal de nitions, and pointers to related literature on graphical probabilistic models and inference algorithms for them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11672931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, andthe manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximizationalgorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "slug": "Operations-for-Learning-with-Graphical-Models-Buntine",
            "title": {
                "fragments": [],
                "text": "Operations for Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "These algorithms were used in language modeling [38, 13] and handwritten character recognition [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "More recently, an extension of this idea to probabilistic but synchronous transducers was proposed [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Using these posteriors, a mixture over a very large family of such trees can be formed, whose generalization performance tracks that of the best tree in that family [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [38, 39, 13], econometrics [14, 15, 40], time series [41], and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 275
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11904338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02c760a002b5dfb8fbf637a07bcb18608a9b9d0a",
            "isKey": true,
            "numCitedBy": 27,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and analyze a mixture model for supervised learning of probabilistic transducers. We devise an online learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture. Theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best transducer from an arbitrarily large (possibly infinite) pool of models. We also present an application of the model for inducing a noun phrase recognizer."
            },
            "slug": "Adaptive-Mixtures-of-Probabilistic-Transducers-Singer",
            "title": {
                "fragments": [],
                "text": "Adaptive Mixtures of Probabilistic Transducers"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An online learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture is devised and an application of the model for inducing a noun phrase recognizer is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072278622"
                        ],
                        "name": "P. Blanchet",
                        "slug": "P.-Blanchet",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Blanchet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blanchet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145268518"
                        ],
                        "name": "J. Li\u00e9nard",
                        "slug": "J.-Li\u00e9nard",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Li\u00e9nard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Li\u00e9nard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [95, 92, 2, 93, 94], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41065888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "830a0c617b99a2cd39517f699fe376442c662816",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speaker-independent-isolated-digit-recognition:-vs.-Bottou-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Speaker-independent isolated digit recognition: Multilayer perceptrons vs. Dynamic time warping"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781290"
                        ],
                        "name": "H. Murveit",
                        "slug": "H.-Murveit",
                        "structuredName": {
                            "firstName": "Hy",
                            "lastName": "Murveit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murveit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148249"
                        ],
                        "name": "J. Butzberger",
                        "slug": "J.-Butzberger",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Butzberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Butzberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744182"
                        ],
                        "name": "M. Weintraub",
                        "slug": "M.-Weintraub",
                        "structuredName": {
                            "firstName": "Mitch",
                            "lastName": "Weintraub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weintraub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57374243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3492842d6ec502cd9d314ccf1080b0defdb7955f",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a technique called progressive search which is useful for developing and implementing speech recognition systems with high computational requirements. The scheme iteratively uses more and more complex recognition schemes, where each iteration constrains the speech space of the next. An algorithm called the forward-backward word-life algorithm is described. It can generate a word lattice in a progressive search that would be used as a language model embedded in a succeeding recognition pass to reduce computation requirements. It is shown that speed-ups of more than an order of magnitude are achievable with only minor costs in accuracy.<<ETX>>"
            },
            "slug": "Large-vocabulary-dictation-using-SRI's-DECIPHER-Murveit-Butzberger",
            "title": {
                "fragments": [],
                "text": "Large-vocabulary dictation using SRI's DECIPHER speech recognition system: progressive search techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The authors describe a technique called progressive search which is useful for developing and implementing speech recognition systems with high computational requirements and shows that speed-ups of more than an order of magnitude are achievable with only minor costs in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13863771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa426fc1d6bed08eb8f0d44d5e574dcbcd3f4678",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Compliant control is a standard method for performing fine manipulation tasks, like grasping and assembly, but it requires estimation of the state of contact between the robot arm and the objects involved. Here we present a method to learn a model of the movement from measured data. The method requires little or no prior knowledge and the resulting model explicitly estimates the state of contact. The current state of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition probabilities between states are modeled as parametrized functions of the measurement We show that their parameters can be estimated from measurements concurrently with the estimation of the parameters of the movement in each state of contact. The learning algorithm is a variant of the EM procedure. The E step is computed exactly; solving the M step exactly would require solving a set of coupled nonlinear algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood."
            },
            "slug": "Learning-Fine-Motion-by-Markov-Mixtures-of-Experts-Meil\u0103-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning Fine Motion by Markov Mixtures of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A method to learn a model of the movement from measured data that requires little or no prior knowledge and the resulting model explicitly estimates the state of contact."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735952"
                        ],
                        "name": "D. Ron",
                        "slug": "D.-Ron",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 890211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba7d7e7591b937b166373646e8b0f81d2a23a727",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and analyze a distribution learning algorithm for a subclass ofacyclic probalistic finite automata(APFA). This subclass is characterized by a certain distinguishability property of the automata's states. Though hardness results are known for learning distributions generated by general APFAs, we prove that our algorithm can efficiently learn distributions generated by the subclass of APFAs we consider. In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made arbitrarily small with high confidence in polynomial time. We present two applications of our algorithm. In the first, we show how to model cursively written letters. The resulting models are part of a complete cursive handwriting recognition system. In the second application we demonstrate how APFAs can be used to build multiple-pronunciation models for spoken words. We evaluate the APFA-based pronunciation models on labeled speech data. The good performance (in terms of the log-likelihood obtained on test data) achieved by the APFAs and the little time needed for learning suggests that the learning algorithm of APFAs might be a powerful alternative to commonly used probabilistic models."
            },
            "slug": "On-the-learnability-and-usage-of-acyclic-finite-Ron-Singer",
            "title": {
                "fragments": [],
                "text": "On the learnability and usage of acyclic probabilistic finite automata"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is proved that the algorithm proposed can efficiently learn distributions generated by the subclass of APFAs it considers, and it is shown that the KL-divergence between the distributiongenerated by the target source and the distribution generated by the authors' hypothesis can be made arbitrarily small with high confidence in polynomial time."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785148"
                        ],
                        "name": "F. Kubala",
                        "slug": "F.-Kubala",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Kubala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kubala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2925865"
                        ],
                        "name": "T. Anastasakos",
                        "slug": "T.-Anastasakos",
                        "structuredName": {
                            "firstName": "Tasos",
                            "lastName": "Anastasakos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Anastasakos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150603996"
                        ],
                        "name": "L. Nguyen",
                        "slug": "L.-Nguyen",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354787"
                        ],
                        "name": "G. Zavaliagkos",
                        "slug": "G.-Zavaliagkos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zavaliagkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zavaliagkos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13267522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e57ad806481e5670d46a882309ac5140825e245",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe recent changes to the BYBLOS system's training and recognition algorithms and report on numerous experiments in large vocabulary speech recognition. In earlier work, we performed five key experiments that were designed to answer questions related to different training scenarios. We investigated (1) the effect of varying the number of training speakers if the total amount of training data remains constant, (2) data pooling versus model averaging for generating speaker-independent (SI) HMMs, (3) the benefit of doubling the acoustic training data, (4) SI versus SD performance when the SI training data is twelve times greater, (5) the effect of cross-domain training for both the acoustic and language models. Our recent work was focused on four specific problem areas sharing the common thread that the test condition exposes the recognizer to phenomena not observed in the training data. Here we investigated (1) words outside the vocabulary, (2) spoken language effects due to subject variability and spontaneous dictation, (3) non-native dialects of the language, and (4) new microphones not used in training.<<ETX>>"
            },
            "slug": "Comparative-Experiments-on-Large-Vocabulary-Speech-Kubala-Anastasakos",
            "title": {
                "fragments": [],
                "text": "Comparative Experiments on Large Vocabulary Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Four specific problem areas sharing the common thread that the test condition exposes the recognizer to phenomena not observed in the training data are focused on, including words outside the vocabulary, spoken language effects due to subject variability and spontaneous dictation, and new microphones not used in training."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780040"
                        ],
                        "name": "R. Shumway",
                        "slug": "R.-Shumway",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Shumway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shumway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069324"
                        ],
                        "name": "D. Stoffer",
                        "slug": "D.-Stoffer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stoffer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stoffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122714036,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4ad99f4e843cd866879e0ce5309ec34f771e4f13",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The problem of modeling change in a vector time series is studied using a dynamic linear model with measurement matrices that switch according to a time-varying independent random process. We derive filtered estimators for the usual state vectors and also for the state occupancy probabilities of the underlying nonstationary measurement process. A maximum likelihood estimation procedure is given that uses a pseudo-expectation-maximization algorithm in the initial stages and nonlinear optimization. We relate the models to those considered previously in the literature and give an application involving the tracking of multiple targets."
            },
            "slug": "Dynamic-linear-models-with-switching-Shumway-Stoffer",
            "title": {
                "fragments": [],
                "text": "Dynamic linear models with switching"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152184249"
                        ],
                        "name": "Robert Roth",
                        "slug": "Robert-Roth",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107569404"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721830"
                        ],
                        "name": "L. Gillick",
                        "slug": "L.-Gillick",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Gillick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gillick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34644387"
                        ],
                        "name": "M. Hunt",
                        "slug": "M.-Hunt",
                        "structuredName": {
                            "firstName": "Melvyn",
                            "lastName": "Hunt",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152776656"
                        ],
                        "name": "Y. Ito",
                        "slug": "Y.-Ito",
                        "structuredName": {
                            "firstName": "Yoshiko",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38332678"
                        ],
                        "name": "S. Lowe",
                        "slug": "S.-Lowe",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33433401"
                        ],
                        "name": "J. Orloff",
                        "slug": "J.-Orloff",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Orloff",
                            "middleNames": [
                                "N"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Orloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780994"
                        ],
                        "name": "B. Peskin",
                        "slug": "B.-Peskin",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Peskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Peskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217454"
                        ],
                        "name": "F. Scattone",
                        "slug": "F.-Scattone",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Scattone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scattone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57375182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7bfe02531933ee3b78643a3bde46fea65736f48",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report on the progress that has been made at Dragon Systems in speaker-independent large-vocabulary speech recognition using speech from DARPA's Wall Street Journal corpus. First they present an overview of the recognition and training algorithms. Then, they describe experiments involving two improvements to these algorithms, moving to higher-dimensional streams and using an IMELDA transformation. They also present some results showing the reduction in error rates.<<ETX>>"
            },
            "slug": "Large-vocabulary-continuous-speech-recognition-of-Roth-Baker",
            "title": {
                "fragments": [],
                "text": "Large vocabulary continuous speech recognition of Wall Street Journal data"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The authors report on the progress that has been made at Dragon Systems in speaker-independent large-vocabulary speech recognition using speech from DARPA's Wall Street Journal corpus, and describe experiments involving two improvements to these algorithms, moving to higher-dimensional streams and using an IMELDA transformation."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2779846"
                        ],
                        "name": "H. Sakoe",
                        "slug": "H.-Sakoe",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Sakoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sakoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35805230"
                        ],
                        "name": "S. Chiba",
                        "slug": "S.-Chiba",
                        "structuredName": {
                            "firstName": "Seibi",
                            "lastName": "Chiba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chiba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "In the extreme case, if the numerical value of non-zero transition probabilities are completely ignored, the Viterbi algorithm only does a \\dynamic time-warping\" match [84] between the observation sequence and the sequence of probabilistic prototypes associated (through the emission distributions) with a sequence of state values in the HMM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17900407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18f355d7ef4aa9f82bf5c00f84e46714efa5fd77",
            "isKey": false,
            "numCitedBy": 5370,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm."
            },
            "slug": "Dynamic-programming-algorithm-optimization-for-word-Sakoe-Chiba",
            "title": {
                "fragments": [],
                "text": "Dynamic programming algorithm optimization for spoken word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition, in which the warping function slope is restricted so as to improve discrimination between words in different categories."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11369016,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c51ec71b5bec9b90fb1cbf1c037c0f27d714cac4",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years it has been shown that recurrent neural networks are adequate for processing general data structures like trees and graphs, which opens the doors to a number of new interesting applications previously unexplored. In this paper, we analyze the efficiency of learning the membership of DO AGs (Directed Ordered Acyclic Graphs) in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning. We give sufficient conditions under which the error surface is local minima free. Specifically, we define a topological index associated wi th a collection of DOAGs that makes it possible to design the architecture so as to avoid local minima."
            },
            "slug": "On-the-Efficient-Classification-of-Data-Structures-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "On the Efficient Classification of Data Structures by Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper analyzes the efficiency of learning the membership of DO AGs (Directed Ordered Acyclic Graphs) in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "The application of HMMs to speech was independently proposed by [21] and [22], and popularized by [23], [24], and [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31408841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods."
            },
            "slug": "Continuous-speech-recognition-by-statistical-Jelinek",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition by statistical methods"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Experimental results are presented that indicate the power of the methods and concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1172,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443457"
                        ],
                        "name": "M. Schenkel",
                        "slug": "M.-Schenkel",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Schenkel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schenkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10331982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f80e7b5ca45cbaf0823e4cd75ba63336a0e32ae",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a writer independent system for on-line handwriting recognition which can handle both cursive script and hand-print. The pen trajectory is recorded by a touch sensitive pad, such as those used by note-pad computers. The input to the system contains the pen trajectory information, encoded as a time-ordered sequence of feature vectors. Features include X and Y coordinates, pen-lifts, speed, direction and curvature of the pen trajectory. A time delay neural network with local connections and shared weights is used to estimate a posteriori probabilities for characters in a word. A hidden Markov model segments the word into characters in a way which optimizes the global word score, taking a dictionary into account. A geometrical normalization scheme and a fast but efficient dictionary search are also presented. Trained on 20000 unconstrained cursive words from 59 writers and using a 25000 word dictionary the authors reached a 89% character and 80% word recognition rate on test data from a disjoint set of writers.<<ETX>>"
            },
            "slug": "On-line-cursive-script-recognition-using-time-delay-Schenkel-Guyon",
            "title": {
                "fragments": [],
                "text": "On-line cursive script recognition using time delay neural networks and hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Presents a writer independent system for on-line handwriting recognition which can handle both cursive script and hand-print, and reached a 89% character and 80% word recognition rate on test data from a disjoint set of writers."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "See [48, 49, 50, 51] for more formal de nitions, and pointers to related literature on graphical probabilistic models and inference algorithms for them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 86367536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6acd45949c2b167928211c562c8d9c445651f1ef",
            "isKey": false,
            "numCitedBy": 777,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "We review recent developments in applying Bayesian probabilistic and statistical ideas to expert systems. Using a real, moderately complex, medical example we illustrate how qualitative and quantitative knowledge can be represented within a directed graphical model, generally known as a belief network in this context. Exact probabilistic inference on individual cases is possible using a general propagation procedure. When data on a series of cases are available, Bayesian statistical techniques can be used for updating the original subjective quantitative inputs, and we present a sets of diagnostics for identifying conflicts between the data and the prior specification. A model comparison procedure is explored, and a number of links made with mainstream statistical methods. Details are given on the use of Dirichlet prior distributions for learning about parameters and the process of transforming the original graphical model to a junction tree as the basis for efficient computation."
            },
            "slug": "Bayesian-analysis-in-expert-systems-Spiegelhalter-Dawid",
            "title": {
                "fragments": [],
                "text": "Bayesian analysis in expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Using a real, moderately complex, medical example, it is illustrated how qualitative and quantitative knowledge can be represented within a directed graphical model, generally known as a belief network in this context."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 233
                            }
                        ],
                        "text": "Models with such a factorial (or distributed) state are very appealing for their expressive power, and there has recently been a lot of research on trying to make computationally e cient learning algorithms for them (see for example [132, 133, 134, 135, 136, 131])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35241,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3395263"
                        ],
                        "name": "E. Sondik",
                        "slug": "E.-Sondik",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Sondik",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sondik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40615976,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f7a30ab04aaf80174fe527e51133e8068a6b8509",
            "isKey": false,
            "numCitedBy": 603,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper treats the discounted cost, optimal control problem for Markov processes with incomplete state information. The optimization approach for these partially observable Markov processes is a generalization of the well-known policy iteration technique for finding optimal stationary policies for completely observable Markov processes. The state space for the problem is the space of state occupancy probability distributions the unit simplex. The development of the algorithm introduces several new ideas, including the class of finitely transient policies, which are shown to possess piecewise linear cost functions. The paper develops easily implemented approximations to stationary policies based on these finitely transient policies and shows that the concave hull of an approximation can be included in the well-known Howard policy improvement algorithm with subsequent convergence. The paper closes with a detailed example illustrating the application of the algorithm to the two-state partially observable Markov process."
            },
            "slug": "The-Optimal-Control-of-Partially-Observable-Markov-Sondik",
            "title": {
                "fragments": [],
                "text": "The Optimal Control of Partially Observable Markov Processes over the Infinite Horizon: Discounted Costs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The paper develops easily implemented approximations to stationary policies based on finitely transient policies and shows that the concave hull of an approximation can be included in the well-known Howard policy improvement algorithm with subsequent convergence."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47753224"
                        ],
                        "name": "R. Smallwood",
                        "slug": "R.-Smallwood",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Smallwood",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smallwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3395263"
                        ],
                        "name": "E. Sondik",
                        "slug": "E.-Sondik",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Sondik",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sondik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 133
                            }
                        ],
                        "text": "In the control and reinforcement learning literature, similar models have been called Partially Observable Markov Decision Processes [104, 105, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43604344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "450b0047c7314cb47b1b1bf1d081d9472acaa0e4",
            "isKey": false,
            "numCitedBy": 1613,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper formulates the optimal control problem for a class of mathematical models in which the system to be controlled is characterized by a finite-state discrete-time Markov process. The states of this internal process are not directly observable by the controller; rather, he has available a set of observable outputs that are only probabilistically related to the internal state of the system. The formulation is illustrated by a simple machine-maintenance example, and other specific application areas are also discussed. The paper demonstrates that, if there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process. In addition, an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined. These results are illustrated by a numerical example for the machine-maintenance problem."
            },
            "slug": "The-Optimal-Control-of-Partially-Observable-Markov-Smallwood-Sondik",
            "title": {
                "fragments": [],
                "text": "The Optimal Control of Partially Observable Markov Processes over a Finite Horizon"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "If there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process, and an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780040"
                        ],
                        "name": "R. Shumway",
                        "slug": "R.-Shumway",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Shumway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shumway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069324"
                        ],
                        "name": "D. Stoffer",
                        "slug": "D.-Stoffer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stoffer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stoffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120411572,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "658ee89b35cde8dae323452f01146b6176b2ece8",
            "isKey": false,
            "numCitedBy": 1444,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters."
            },
            "slug": "AN-APPROACH-TO-TIME-SERIES-SMOOTHING-AND-USING-THE-Shumway-Stoffer",
            "title": {
                "fragments": [],
                "text": "AN APPROACH TO TIME SERIES SMOOTHING AND FORECASTING USING THE EM ALGORITHM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62138892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cf661487d8708a3e9a74e9cc83ce290aa5355b8",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-modeling-for-automatic-speech-Baker",
            "title": {
                "fragments": [],
                "text": "Stochastic modeling for automatic speech understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3145585"
                        ],
                        "name": "O. Matan",
                        "slug": "O.-Matan",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Matan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Matan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [25, 26, 27, 28, 29, 30, 31], pattern recognition in molecular biology [32, 33, 34, 35, 3], and fault-detection [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3260890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "464e8d981df7f326c3af6e9d7bd627f83e438816",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a feed-forward network architecture for recognizing an unconstrained handwritten multi-digit string. This is an extension of previous work on recognizing isolated digits. In this architecture a single digit recognizer is replicated over the input. The output layer of the network is coupled to a Viterbi alignment module that chooses the best interpretation of the input. Training errors are propagated through the Viterbi module. The novelty in this procedure is that segmentation is done on the feature maps developed in the Space Displacement Neural Network (SDNN) rather than the input (pixel) space."
            },
            "slug": "Multi-Digit-Recognition-Using-a-Space-Displacement-Matan-Burges",
            "title": {
                "fragments": [],
                "text": "Multi-Digit Recognition Using a Space Displacement Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A feed-forward network architecture for recognizing an unconstrained handwritten multi-digit string with segmentation done on the feature maps developed in the Space Displacement Neural Network rather than the input (pixel) space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055898769"
                        ],
                        "name": "G. Neumann",
                        "slug": "G.-Neumann",
                        "structuredName": {
                            "firstName": "Guenter",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Neumann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "It makes an error in less than 5% of the calls and processes around one billion calls per year [83]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "For example, on the ATIS benchmark (where the task is to provide airline information to users, and the vocabulary has around 2000 words), laboratory experiments yielded around 5% of incorrectly answered queries [83]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63120779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "412454658ef0cc810013ca0be6f298ed15978016",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 155,
            "paperAbstract": {
                "fragments": [],
                "text": "Spoken language interfaces to computers is a topic that has lured and fascinated engineers and speech scientists alike for over ve decades. For many, the ability to converse freely with a machine represents the ultimate challenge to our understanding of the production and perception processes involved in human speech communication. In addition to being a provocative topic, spoken language interfaces are fast becoming a necessity. In the near future, interactive networks will provide easy access to a wealth of information and services that will fundamentally aaect how people work, play and conduct their daily aaairs. Today, such networks are limited to people who can read and have access to computers|a relatively small part of the population even in the most developed countries. Advances in human language technology are needed for the average citizen to communicate with networks using natural communication skills using everyday devices, such as telephones and televisions. Without fundamental advances in user-centered interfaces, a large portion of society will be prevented from participating in the age of information, resulting in further stratiication of society and tragic loss in human potential. The rst chapter in this survey deals with spoken language input technologies. A speech interface, in a user's own language, is ideal because it is the most natural, exible, eecient, and economical form of human communication. The following sections summarize spoken input technologies that will facilitate such an interface. 1 2 Chapter 1: Spoken Language Input Spoken input to computers embodies many diierent technologies and applications, as shown in Figure 1.1. In some cases, as shown at the bottom of the gure, one is interested not in the underlying linguistic content, but the identity of the speaker, or the language being spoken. Speaker recognition can involve identifying a speciic speaker out of a known population, which has forensic implications, or verifying the claimed identity of a user, thus enabling controlled access to locales (e.g., a computer room) and services (e.g., voice banking). Speaker recognition technologies are addressed in section 1.7. Language identiication also has important applications, and techniques applied to this area are summarized in section 8.7. When one thinks about speaking to computers, the rst image is usually speech recognition, the conversion of an acoustic signal to a stream of words. After many years of research, speech recognition technology is beginning to pass the threshold of practicality. The last decade has witnessed dramatic improvement in speech recognition \u2026"
            },
            "slug": "Survey-of-the-state-of-the-art-in-human-language-Neumann",
            "title": {
                "fragments": [],
                "text": "Survey of the state of the art in human language technology"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A speech interface, in a user's own language, is ideal because it is the most natural, exible, eecient, and economical form of human communication."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15116562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9a54374aec5c92296c7b24436f08934643829ae",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory."
            },
            "slug": "Exploiting-Tractable-Substructures-in-Intractable-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Exploiting Tractable Substructures in Intractable Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A refined mean field approximation for inference and learning in probabilistic neural networks is developed, and it is shown how to incorporate weak higher order interactions into a first-order hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66408701"
                        ],
                        "name": "X. Driancourt",
                        "slug": "X.-Driancourt",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Driancourt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Driancourt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 169
                            }
                        ],
                        "text": "The idea of training a set of modules together (rather than separately) with respect to a global criterion with gradient-based algorithms was proposed several years ago [102, 103, 92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [95, 92, 2, 93, 94], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61944624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843ae92ba254ed99bcfe88e9fe1f36d86f388023",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors compare dynamic programming, or DP, multilayer perceptron, time-delay neural network, or TDNN, shift-tolerant learning vector quantization, and K-means on a multispeaker isolated-word small vocabulary problem. A suboptimal cooperation between TDNN and other algorithms is proposed and successfully tested on the problem. The combination of TDNN and DP performs especially well. An optimal cooperation method between DP and some other algorithms is proposed.<<ETX>>"
            },
            "slug": "Learning-vector-quantization,-multi-layer-and-and-Driancourt-Bottou",
            "title": {
                "fragments": [],
                "text": "Learning vector quantization, multi layer perceptron and dynamic programming: comparison and cooperation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The authors compare dynamic programming, or DP, multilayer perceptron, time-delay neural network, or TDNN, shift-tolerant learning vector quantization, and K-means on a multispeaker isolated-word small vocabulary problem and propose an optimal cooperation method between DP and some other algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4765016"
                        ],
                        "name": "James D. Hamilton",
                        "slug": "James-D.-Hamilton",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James D. Hamilton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13322867,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a747ef8ec26d5312846429717d85ee814d68b369",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "State-space-models-Hamilton",
            "title": {
                "fragments": [],
                "text": "State-space models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2359255"
                        ],
                        "name": "T. Cacciatore",
                        "slug": "T.-Cacciatore",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Cacciatore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cacciatore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "In the literature on learning algorithms [4, 5, 6], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17567961,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7077a8802ac5f65991bf1086b07d0d6f17e9d894",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior."
            },
            "slug": "Mixtures-of-Controllers-for-Jump-Linear-and-Plants-Cacciatore-Nowlan",
            "title": {
                "fragments": [],
                "text": "Mixtures of Controllers for Jump Linear and Non-Linear Plants"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior is described, based on a Markov process model, and a recurrent network for gating a set of linear or non-linear controllers is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624386"
                        ],
                        "name": "L. Chrisman",
                        "slug": "L.-Chrisman",
                        "structuredName": {
                            "firstName": "Lonnie",
                            "lastName": "Chrisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chrisman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "In the control and reinforcement learning literature, similar models have been called Partially Observable Markov Decision Processes [84, 85, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 86
                            }
                        ],
                        "text": "In the control and reinforcement learning literature, similar models have been called Partially Observable Markov Decision Processes [104, 105, 4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1963904,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a1b055577a86141df13f13a3203c76a32bffdc3a",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. \n \nThis paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "slug": "Reinforcement-Learning-with-Perceptual-Aliasing:-Chrisman",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world is introduced and Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102429215"
                        ],
                        "name": "Kai-Fu Lee",
                        "slug": "Kai-Fu-Lee",
                        "structuredName": {
                            "firstName": "Kai-Fu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Fu Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "An early review of alternative methods based on HMMs or related to HMMs, also for speech recognition, can be found in the collection of papers [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57420724,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0071c960f49d8279e7a5503214a3567cb2237505",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Readings-in-speech-recognition-Waibel-Lee",
            "title": {
                "fragments": [],
                "text": "Readings in speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443457"
                        ],
                        "name": "M. Schenkel",
                        "slug": "M.-Schenkel",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Schenkel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schenkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073469360"
                        ],
                        "name": "H. Weissman",
                        "slug": "H.-Weissman",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Weissman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weissman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2975061"
                        ],
                        "name": "C. Nohl",
                        "slug": "C.-Nohl",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Nohl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nohl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14432769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4d2207b9b9ee71da7b24c7172e5107492dd421d",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on the performance of two methods for recognition-based segmentation of strings of on-line handprinted capital Latin characters. The input strings consist of a time-ordered sequence of X-Y coordinates, punctuated by pen-lifts. The methods were designed to work in \"run-on mode\" where there is no constraint on the spacing between characters. While both methods use a neural network recognition engine and a graph-algorithmic post-processor, their approaches to segmentation are quite different. The first method, which we call INSEG (for input segmentation), uses a combination of heuristics to identify particular penlifts as tentative segmentation points. The second method, which we call OUTSEG (for output segmentation), relies on the empirically trained recognition engine for both recognizing characters and identifying relevant segmentation points."
            },
            "slug": "Recognition-Based-Segmentation-of-On-Line-Words-Schenkel-Weissman",
            "title": {
                "fragments": [],
                "text": "Recognition-Based Segmentation of On-Line Hand-Printed Words"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper reports on the performance of two methods for recognition-based segmentation of strings of on-line handprinted capital Latin characters, using a neural network recognition engine and a graph-algorithmic post-processor."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Perform a vector quantization [57] in order to map each vector-valued yt to a discrete value quantize(yt), and use P (quantize(yt)jqt) as emission probability, or more generally."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14754287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a5952532cdd48eec5e3dc326907c36a70e0a24",
            "isKey": false,
            "numCitedBy": 2921,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications."
            },
            "slug": "Vector-quantization-Gray",
            "title": {
                "fragments": [],
                "text": "Vector quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157745208"
                        ],
                        "name": "Jung-Fu Cheng",
                        "slug": "Jung-Fu-Cheng",
                        "structuredName": {
                            "firstName": "Jung-Fu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Fu Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14553992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d953005dd08a863c157b528bbabdf5671d18b6",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the close connection between the now celebrated iterative turbo decoding algorithm of Berrou et al. (1993) and an algorithm that has been well known in the artificial intelligence community for a decade, but which is relatively unknown to information theorists: Pearl's (1982) belief propagation algorithm. We see that if Pearl's algorithm is applied to the \"belief network\" of a parallel concatenation of two or more codes, the turbo decoding algorithm immediately results. Unfortunately, however, this belief diagram has loops, and Pearl only proved that his algorithm works when there are no loops, so an explanation of the experimental performance of turbo decoding is still lacking. However, we also show that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's (1962) low-density parity-check codes, serially concatenated codes, and product codes. Thus, belief propagation provides a very attractive general methodology for devising low-complexity iterative decoding algorithms for hybrid coded systems."
            },
            "slug": "Turbo-Decoding-as-an-Instance-of-Pearl's-\"Belief-McEliece-Mackay",
            "title": {
                "fragments": [],
                "text": "Turbo Decoding as an Instance of Pearl's \"Belief Propagation\" Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's low-density parity-check codes, serially concatenated codes, and product codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51916854"
                        ],
                        "name": "Eric Ghysels",
                        "slug": "Eric-Ghysels",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ghysels",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Ghysels"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 256
                            }
                        ],
                        "text": "In most of the cases described in the econometrics literature, this distribution is assumed to be time-invariant, and it is speci ed by a matrix of transition probabilities (as in ordinary HMMs), although more complicated speci cations have been suggested [116, 117, 118, 119, 120]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123814678,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "6ea8911e0728066c5089d277e200d33244567ffd",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a historical chronology of economic activity in 16th- and 17th-century France that is based on wheat price series in Paris and Toulouse. A stochastic regime-switching model enables us to benchmark eras and summarize the salient features of a development difficult to appraise in all its complexity. A new class of Markov regime-switching time-series models is introduced to allow for nontrivial interdependencies between different types of cycles that make the economy grow at an unsteady rate. With a predominantly agricultural cycle, we uncover a strongly periodic Markov switching scheme for recorded wheat prices from the grain markets of Paris and Toulouse. Besides the periodic nature of the Markov chain, we also study whether a common factor determined the state of the economy in Paris and Toulouse or whether each series moved independently."
            },
            "slug": "TIME-SERIES-MODEL-WITH-PERIODIC-STOCHASTIC-REGIME-Ghysels",
            "title": {
                "fragments": [],
                "text": "TIME-SERIES MODEL WITH PERIODIC STOCHASTIC REGIME SWITCHING"
            },
            "venue": {
                "fragments": [],
                "text": "Macroeconomic Dynamics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4765016"
                        ],
                        "name": "James D. Hamilton",
                        "slug": "James-D.-Hamilton",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James D. Hamilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103050083"
                        ],
                        "name": "Raul Susmel",
                        "slug": "Raul-Susmel",
                        "structuredName": {
                            "firstName": "Raul",
                            "lastName": "Susmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raul Susmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122751719,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "1feb9454aac0df525d8073a2304b8cba8176f949",
            "isKey": false,
            "numCitedBy": 1888,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Autoregressive-conditional-heteroskedasticity-and-Hamilton-Susmel",
            "title": {
                "fragments": [],
                "text": "Autoregressive conditional heteroskedasticity and changes in regime"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The EM algorithm can also used for some graphical models (Bayesian networks) [62]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122985977,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2e67c0312b81b698834315ea33f8a23be6eed6eb",
            "isKey": false,
            "numCitedBy": 765,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-EM-algorithm-for-graphical-association-models-Lauritzen",
            "title": {
                "fragments": [],
                "text": "The EM algorithm for graphical association models with missing data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "A Bayesian network [48] is a graphical representation of conditional independencies between random variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "See [48, 49, 50, 51] for more formal de nitions, and pointers to related literature on graphical probabilistic models and inference algorithms for them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18213,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4765016"
                        ],
                        "name": "James D. Hamilton",
                        "slug": "James-D.-Hamilton",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James D. Hamilton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12579598,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "de6046f58a05a769b5aa526d95a09c5fa5e5b42c",
            "isKey": false,
            "numCitedBy": 8602,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper models occasional, discrete shifts in the growth rate of a nonstationary series. Algorithms for inferring these unobserved shifts are presented, a byproduct of which permits estimation of parameters by maximum likelihood. An empirical application of this technique suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3 percent permanent drop in the level of GNP. Copyright 1989 by The Econometric Society."
            },
            "slug": "A-New-Approach-to-the-Economic-Analysis-of-Time-and-Hamilton",
            "title": {
                "fragments": [],
                "text": "A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 233
                            }
                        ],
                        "text": "Models with such a factorial (or distributed) state are very appealing for their expressive power, and there has recently been a lot of research on trying to make computationally e cient learning algorithms for them (see for example [132, 133, 134, 135, 136, 131])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7424318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79433b5feacd9e8feeafa629dae5a85f362fef",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-Networks-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The utility of a mean field theory for sigmoid belief networks based on ideas from statistical mechanics is demonstrated on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4765016"
                        ],
                        "name": "James D. Hamilton",
                        "slug": "James-D.-Hamilton",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James D. Hamilton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The parameters of Markov switching models can generally be estimated using the EM algorithm [121, 109, 111, 122] to maximize the likelihood P (y 1 j ) (see next section)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 179
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121492404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b443a6e9ab95f00b9024a3bab2a7bb6655e0791",
            "isKey": false,
            "numCitedBy": 1959,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-time-series-subject-to-changes-in-Hamilton",
            "title": {
                "fragments": [],
                "text": "Analysis of time series subject to changes in regime"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5207,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144269637"
                        ],
                        "name": "Ren\u00e9 Garcia",
                        "slug": "Ren\u00e9-Garcia",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9 Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33759516"
                        ],
                        "name": "P. Perron",
                        "slug": "P.-Perron",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Perron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [38, 39, 13], econometrics [14, 15, 40], time series [41], and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 179
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16691663,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "4307b1f30a91b1f6133eab0f789e2973d13eec6f",
            "isKey": false,
            "numCitedBy": 812,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This study considers the time series behavior of the U.S. real interest rate from 1961 to 1986. We provide a statistical characterization of the series using the methodology of Hamilton (1989), by allowing three possible regimes affecting both the mean and variance of the series. The results suggest that the ex-post real interest rate is essentially random around a mean that is different for the periods 1961-1973, 1973-1980 and 1980-1986. The variance of the process is also different in these episodes being higher in both the 1973-1980 and 1980-1986 sub-periods. The inflation rate series is also analyzed using a three regime framework and again our results show interesting patterns with shifts in both mean and variance. Various model selection tests are run and both an ex-ante real interest rate and an expected inflation series are constructed. Finally, we make clear how our results can explain some recent findings in the literature. Cette etude s'interesse au comportement des series du taux d'interet reel americain de 1961 a 1986. En utilisant la methodologie d'Hamilton (1989), la modelisation statistique des series se fait en postulant trois regimes possibles affectant la moyenne et la variance de celles-ci. Les resultats suggerent que le taux d'interet reel ex-post est essentiellement un processus non correle et centre sur une moyenne qui differe sur les periodes 1961-1973, 1973-1980 et 1980-1986. La variance du processus est aussi differente pour chacune de ces periodes, etant plus elevee dans les sous periodes 1973-1980 et 1980-1986. Les series du taux d'inflation sont aussi analysees a la lumiere de ce modele a trois regimes et les resultats traduisent encore un comportement interessant de celles-ci, avec des changements dans la moyenne et la variance. Differents tests de specification sont utilises et des series, a la fois du taux d'interet reel ex-ante et de l'inflation anticipee, sont construites. Enfin, il est montre comment ces resultats peuvent expliquer certaines conclusion recentes de la litterature."
            },
            "slug": "An-Analysis-of-the-Real-Interest-rate-Under-Regime-Garcia-Perron",
            "title": {
                "fragments": [],
                "text": "An Analysis of the Real Interest rate Under Regime Shifts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4184,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144306503"
                        ],
                        "name": "M. Sol\u00e1",
                        "slug": "M.-Sol\u00e1",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Sol\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sol\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398786211"
                        ],
                        "name": "J. Driff\u00edll",
                        "slug": "J.-Driff\u00edll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Driff\u00edll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Driff\u00edll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 202
                            }
                        ],
                        "text": "1 Markov Switching Models Markov Switching Models have been introduced in the econometrics literature [95, 96, 97, 13, 14] for modeling non-stationarities due to abrupt changes of regime in the economy [98, 99, 100, 101, 102, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 154645840,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "545bbdeae4dc15e051c88ad6c033546714c81b0e",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Testing-the-term-structure-of-interest-rates-using-Sol\u00e1-Driff\u00edll",
            "title": {
                "fragments": [],
                "text": "Testing the term structure of interest rates using a stationary vector autoregression with regime switching"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "These assumptions, however, have been found very useful in practice, in order to build the current state-of-the-art speech recognition systems [16] and applications to bioinformatics [35, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "A good tutorial on HMMs in the context of speech recognition is [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "3 The Viterbi Algorithm In several applications of HMMs (as in speech recognition [16] and molecular biology [3] applications, for example), the hidden state variable is associated with a particular meaning (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "The application of HMMs to speech was independently proposed by [20] and [21], and popularized by [22], [23], and [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": true,
            "numCitedBy": 24803,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66177368"
                        ],
                        "name": "Daniel Sichel",
                        "slug": "Daniel-Sichel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sichel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Sichel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 256
                            }
                        ],
                        "text": "In most of the cases described in the econometrics literature, this distribution is assumed to be time-invariant, and it is speci ed by a matrix of transition probabilities (as in ordinary HMMs), although more complicated speci cations have been suggested [116, 117, 118, 119, 120]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 153768063,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "3ebbda2628c48b86cafcd51e476a514880d57f07",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reexamines duration dependence in U.S. business cycles using parametric hazard models. Positive duration dependence would indicate that expansions or contractions are more likely to end as they become \"older.\" This paper provides statistically significant evidence of positive duration dependence for expansions before World War II and contractions after World War II. The evidence is stronger than in earlier research utilizing nonparametric techniques, because certain nonparametric techniques have low statistical power against the type of duration dependence found in this paper. Evidence is also presented suggesting that expansions became longer, on average, after World War II, while contractions became shorter. Copyright 1991 by MIT Press."
            },
            "slug": "Business-cycle-duration-dependence:-a-parametric-Sichel",
            "title": {
                "fragments": [],
                "text": "Business cycle duration dependence: a parametric approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144269637"
                        ],
                        "name": "Ren\u00e9 Garcia",
                        "slug": "Ren\u00e9-Garcia",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9 Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46397635"
                        ],
                        "name": "Huntley Schaller",
                        "slug": "Huntley-Schaller",
                        "structuredName": {
                            "firstName": "Huntley",
                            "lastName": "Schaller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huntley Schaller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18081814,
            "fieldsOfStudy": [
                "Sociology",
                "Economics"
            ],
            "id": "e42812697a08ec8f81c53b853e13528143e2219c",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on whether monetary policy has asymmetric effects. By building on the Markov switching model introduced by Hamilton (1989), we examine questions like: Does monetary policy have the same effect regardless of the current phase of economic fluctuations? Given that the economy is currently in a recession, does a fall in interest rates increase the probability of an expansion? Does monetary policy have an incremental effect on the growth rate within a given state, or does it only affect the economy if it is sufficiently strong to induce a state change (e.g., from recession to expansion)? We find economically and statistically significant evidence of asymmetry. As suggested by models with sticky prices or finance constraints, interest rate changes have larger effects during recessions. Interest rates also have substantial effects on the probability of a state switch. Le present article etudie si la politique monetaire a des effets asymetriques. En developpant le modele a changements de regime markoviens introduit par Hamilton (1989), nous examinons des questions du genre : La politique monetaire a-t-elle les memes effets selon les differentes phases du cycle economique ? Etant donne que l'economie est actuellement en recession, une baisse des taux d'interet accroit-elle la probabilite d'une expansion ? La politique monetaire a-t-elle un effet sur le taux de croissance de l'economie au sein d'une phase donnee, ou n'affecte-t-elle l'economie que si elle est suffisamment soutenue pour entrainer un changement de phase (p. ex. d'une recession a une expansion) ? Nous trouvons des effets asymetriques importants economiquement et statistiquement significatifs. Comme le suggerent les modeles supposant un ajustement lent des prix ou l'existence de contraintes financieres, les changements de taux d'interet ont des effets plus importants durant les recessions. Les taux d'interet ont egalement des effets substantiels sur la probabilite d'un changement d'etat de l'ec (This abstract was borrowed from another version of this item.)"
            },
            "slug": "Are-the-Effects-of-Monetary-Policy-Asymmetric-Garcia-Schaller",
            "title": {
                "fragments": [],
                "text": "Are the Effects of Monetary Policy Asymmetric"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145806397"
                        ],
                        "name": "Marco Bonomo",
                        "slug": "Marco-Bonomo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Bonomo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Bonomo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144269637"
                        ],
                        "name": "Ren\u00e9 Garcia",
                        "slug": "Ren\u00e9-Garcia",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9 Garcia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 179
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 152549607,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "494d24f585ce387998edadc1e1baca522ef19b6b",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent papers, Cecchetti et al (1990) and Kandel and Stambaugh (1990) showed that negative serial correlation in long horizon returns was consistent with an equilibrium model of asset pricing. In this paper we show that their results rely on misspecified Markov switching models for the endowment process. Once the proper Markov specification is chosen for the endowment process, the model does not produce mean reversion of the magnitude detected in the data. Furthermore, the small amount of mean reversion produced by the model is due only to small sample bias. We also show that this model is unable to predict negative excess returns, contrary to empirical evidence. Copyright 1994 by John Wiley & Sons, Ltd.(This abstract was borrowed from another version of this item.)"
            },
            "slug": "Can-a-well-fitted-equilibrium-asset-pricing-model-Bonomo-Garcia",
            "title": {
                "fragments": [],
                "text": "Can a well-fitted equilibrium asset pricing model produce mean reversion?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "For all the above distributions, the EM (Expectation-Maximization) algorithm [61, 18, 19, 20] can be used to estimate the parameters of the HMM in order to maximize the likelihood function l( ) = P (Dj ) = Q p P (y Tp 1 (p)j ) over the set D of training sequences y Tp 1 (p) of length Tp (indiced by the letter p)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "It can be shown [61] that an increase of F brings an increase of the likelihood, and this algorithm converges to a local maximum of the likelihood, P (YjX ; )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "present a special case of the EM algorithm applied to discrete emissions HMMs, but were written before the general version of the EM algorithm was described [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "In this section we will sketch the application of the EM (Expectation-Maximization) algorithm [61] to HMMs [18, 19, 20] and IOHMMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": true,
            "numCitedBy": 48399,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "For a general mathematical analysis of the learning theory behind learning algorithms such as those discussed here, see for example [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "This procedure has been found to converge and yield speech recognition performance at the level of state-of-theart systems [1, 80]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [67, 68, 1, 80]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 165
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60349410,
            "fieldsOfStudy": [
                "Education",
                "Economics"
            ],
            "id": "09b99be3591c423fee4937477534f1eec22dcbc2",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: speech Reference EPFL-CONF-82487 Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "Connectionist-speech-recognition-Bourlard",
            "title": {
                "fragments": [],
                "text": "Connectionist speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Speech Reference EPFL-CONF-82487 describes the \u201cpolitics of language\u201d in the developing world and some of the challenges faced by speech interpreters and interpreters in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "In the literature on learning algorithms [5, 6, 7], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "An HMM can also be viewed as a particular kind of recurrent [85] ANN [98, 99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Arti cial Neural Networks (ANNs) or connectionist models have been successfully used in several pattern recognition and sequential data processing problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "The local models (emission and transitions) can be represented by complex models such as ANNs, which are exible non-linear models more powerful and yet more parsimonious than the Gaussian mixtures often used in HMMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Therefore, the gradients @C @yt can be used to train the parameters of the ANN: gradient descent using the chain rule for derivatives (also called back-propagation [85]) yields the parameter gradients @C @ = X"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Another way to integrate ANNs with HMMs in a mathematically clear way is based on the idea of\nInput-Output HMMs described in the next section."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Multi-layered ANNs [85] can represent a non-linear regression or classi cation model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Furthermore, these ANNs can take into account a wide context (not just the observations at time t but also neighboring observations in the sequence), without violating the Markov assumptions, because there are no independence assumptions on the conditioning input variable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "The ANN can even be recurrent [85, 2] (to take into account arbitrarily distant past contexts)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Di erent researchers have proposed di erent ways to combine ANNs with HMMs, in particular for automatic speech recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 238440002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eccfb47fa72551b2951ab927eadc8358b2609027",
            "isKey": true,
            "numCitedBy": 295,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Internal-Representations-by-Error-Parallel-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations by Error Propagation, Parallel Distributed Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16491491"
                        ],
                        "name": "G. Grant",
                        "slug": "G.-Grant",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grant",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 183
                            }
                        ],
                        "text": "These assumptions, however, have been found very useful in practice, in order to build the current state-of-the-art speech recognition systems [17] and applications to bioinformatics [36, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "See also this book [3], which focusses on the applications to bioinformatics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [26, 27, 28, 29, 30, 31, 32], pattern recognition in molecular biology [33, 34, 35, 36, 3], and fault-detection [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In several applications of HMMs (as in speech recognition [17] and molecular biology [3] applications, for example), the hidden state variable is associated with a particular meaning (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22799724,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "cf171d57f8232ba90a0696f8cb46144b39380d0b",
            "isKey": true,
            "numCitedBy": 514,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bioinformatics-The-Machine-Learning-Approach-Grant",
            "title": {
                "fragments": [],
                "text": "Bioinformatics - The Machine Learning Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Chem."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "In the literature on learning algorithms [5, 6, 7], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "In section 5 we discuss the recently proposed asynchronous Input-Output HMMs, which could signi cantly alleviate this problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 194
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10], variable-length Markov models [11, 12], Markov switching models [13] and switching state-space models [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "The remaining sections describe extensions of HMMs and Markovian models related to HMMs, i.e., hybrids with Arti cial Neural Networks in section 4, Input-Output HMMs in section 5 (including Markov switching models in section 5.1, asynchronous Input-Output HMMs in section 5.3), generalizations of HMMs called weighted transducers in section 6 (useful to combine many Markovian models), and nally, state space models (Markovian models with continuous state) in section 7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "Some models (such as the Input-Output HMMs described in the next section) are also designed to learn long-term dependencies better and to eliminate the problem of imbalance between emission and transition probabilities, therefore yielding more e ective models of duration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "5 Input-Output HMMs Input-Output Hidden Markov Models (IOHMMs) [6] (or Conditional HMMs) are simply HMMs for which the emission and transition distributions are conditional on another sequence, called the input sequence, and noted xL1 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 94
                            }
                        ],
                        "text": "Another way to integrate ANNs with HMMs in a mathematically clear way is based on the idea of\nInput-Output HMMs described in the next section."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 176
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An input/output HMM architecture,\" in Advances in Neural Informa-  tion Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "2 EM for HMMs and IOHMMs In this section we will sketch the application of the EM (Expectation-Maximization) algorithm [53] to HMMs [17, 18, 19] and IOHMMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "Algorithms for estimating the parameters of HMMs have been developed in the 60's and 70's [17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 437,
                                "start": 425
                            }
                        ],
                        "text": "These weights are the state posterior probabilities P (qt=ijxT1 ; yT 1 ; k) = Eq [zi;tjxT1 ; yT 1 ; k]; and the transition posterior probabilities P (qt=i; qt 1=jjxT1 ; yT 1 ; k) = Eq [zi;t; zj;t 1jxT1 ; yT 1 ; k]: Let us now see how these posterior probabilities, which we will note P (qtjxT1 ; yT 1 ) and P (qt; qt 1jxT1 ; yT 1 ) to lighten the notation, can be computed with the Baum-Welch forward and backward recursions [17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 100
                            }
                        ],
                        "text": "4 Parameter Estimation For all the above distributions, the EM (Expectation-Maximization) algorithm [53, 17, 18, 19] can be used to estimate the parameters of the HMM in order to maximize the likelihood function l( ) = P (Dj ) = Qp P (yTp 1 (p)j ) over the set training sequences D (indiced by the letter p)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation for prob-  abilistic functions of a Markov process,\" Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "In the literature on learning algorithms [5, 6, 7], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "In section 5 we discuss the recently proposed asynchronous Input-Output HMMs, which could signi cantly alleviate this problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Input-Output Hidden Markov Models (IOHMMs) [6] (or Conditional HMMs) are simply HMMs for which the emission and transition distributions are conditional on another sequence, called the input sequence, and noted x1 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "The remaining sections describe extensions of HMMs and Markovian models related to HMMs, i.e., hybrids with Arti cial Neural Networks in section 4, Input-Output HMMs in section 5 (including Markov switching models in section 5.1, asynchronous Input-Output HMMs in section 5.3), generalizations of HMMs called weighted transducers in section 6 (useful to combine many Markovian models), and nally, state space models (Markovian models with continuous state) in section 7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "Some models (such as the Input-Output HMMs described in the next section) are also designed to learn long-term dependencies better and to eliminate the problem of imbalance between emission and transition probabilities, therefore yielding more e ective models of duration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 94
                            }
                        ],
                        "text": "Another way to integrate ANNs with HMMs in a mathematically clear way is based on the idea of\nInput-Output HMMs described in the next section."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 194
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An input/output HMM architecture,\" in Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "In the literature on learning algorithms [5, 6, 7], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "In section 5 we discuss the recently proposed asynchronous Input-Output HMMs, which could signi cantly alleviate this problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "The remaining sections describe extensions of HMMs and Markovian models related to HMMs, i.e., hybrids with Arti cial Neural Networks in section 4, Input-Output HMMs in section 5 (including Markov switching models in section 5.1, asynchronous Input-Output HMMs in section 5.3), generalizations of HMMs called weighted transducers in section 6 (useful to combine many Markovian models), and nally, state space models (Markovian models with continuous state) in section 7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "Some models (such as the Input-Output HMMs described in the next section) are also designed to learn long-term dependencies better and to eliminate the problem of imbalance between emission and transition probabilities, therefore yielding more e ective models of duration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 94
                            }
                        ],
                        "text": "Another way to integrate ANNs with HMMs in a mathematically clear way is based on the idea of\nInput-Output HMMs described in the next section."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 194
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning ne motion by markov mixtures of experts,\" in Advances in Neural Information Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102329511"
                        ],
                        "name": "George W. Soules",
                        "slug": "George-W.-Soules",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Soules",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George W. Soules"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063108982"
                        ],
                        "name": "Norman Weiss",
                        "slug": "Norman-Weiss",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norman Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "For all the above distributions, the EM (Expectation-Maximization) algorithm [61, 18, 19, 20] can be used to estimate the parameters of the HMM in order to maximize the likelihood function l( ) = P (Dj ) = Q p P (y Tp 1 (p)j ) over the set D of training sequences y Tp 1 (p) of length Tp (indiced by the letter p)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 216
                            }
                        ],
                        "text": "Let us now see how these posterior probabilities, which we will note P (qtjx T 1 ; y T 1 ) and P (qt; qt 1jx T 1 ; y T 1 ) to lighten the notation, can be computed with the Baum-Welch forward and backward recursions [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "Algorithms for estimating the parameters of HMMs have been developed in the 60's and 70's [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "In this section we will sketch the application of the EM (Expectation-Maximization) algorithm [61] to HMMs [18, 19, 20] and IOHMMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122568650,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4",
            "isKey": true,
            "numCitedBy": 4551,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Maximization-Technique-Occurring-in-the-Analysis-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "See also related work using Bayesian networks to factor the state variable to represent di erent types of contexts [55, 56, 131]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 128
                            }
                        ],
                        "text": "One promising direction that was proposed to manage this problem is to split the state variable in multiple sub-state variables [131], which may operate at di erent time scales [141], since the \\slow\" variables can more easily represent longer-term context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 233
                            }
                        ],
                        "text": "Models with such a factorial (or distributed) state are very appealing for their expressive power, and there has recently been a lot of research on trying to make computationally e cient learning algorithms for them (see for example [132, 133, 134, 135, 136, 131])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "Unfortunally, the types of algorithms presented in this paper would also require exponentially more computation, but so-called factorial HMMs [131] have been proposed with such properties, and approximations to the EM algorithm whose cost does not grow exponentially."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factorial hidden markov models,\" in Advances in Neural Information Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A constructive, on-line (one-pass), learning algorithm was proposed to adaptively grow this tree [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 2
                            }
                        ],
                        "text": "A Variable Length Markov Model [12] is a probability model over strings in which the state variable is not hidden: its value is a deterministic function of the past observation sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 271
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10], variable-length Markov models [11, 12], Markov switching models [13] and switching state-space models [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "A Variable Length Markov Model [11] is a probability model over strings in which the state variable is not hidden: its value is a deterministic function of the past observation sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The power of amnesia,\" in Advances in Neural Information Pro-  cessing Systems 6 (J"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 74
                            }
                        ],
                        "text": "This approach was successfully used as part of a document analysis system [125, 126] that reads amounts from check images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 85
                            }
                        ],
                        "text": "This feature is used pro tably in the document recognition applications described in [125, 126]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 54
                            }
                        ],
                        "text": "A way to generalize transducers was recently proposed [125, 126] which allows any kind of data structure to be used as \\labels\" (instead of discrete symbols) in the sequences to be processed, generalizes the transducers to parameterized operations on weighted graphs, and allows a cascade of these generalized transducers and acceptors to be jointly trained with respect to a global criterion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document analysis with generalized transduction,"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. HA6156000-960701-01TM,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "It is also described in [2], and was extended to character recognition in [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [95, 92, 2, 93, 94], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 167
                            }
                        ],
                        "text": "However, it has been shown experimentally with the above ANN/HMM hybrid how training the ANN jointly with the HMM improves performance on a speech recognition problem [101, 2], bringing down the error rate on a plosive recognition task from 19% to 14%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "Another approach [2, 101, 32] uses the ANN to transform the observation sequence into a form that is easier to model for an HMM that has simple (but continuous) emission models (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "The ANN can even be recurrent [85, 2] (to take into account arbitrarily distant past contexts)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Speech and Sequence Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 2
                            }
                        ],
                        "text": "A Variable Length Markov Model [12] is a probability model over strings in which the state variable is not hidden: its value is a deterministic function of the past observation sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "A Variable Length Markov Model [10] is a probability model over strings in which the state variable is not hidden: its value is a deterministic function of the past observation sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A constructive, on-line (one-pass), learning algorithm was proposed to adaptively grow this tree [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The power of amnesia,\" in Advances in Neural Infor-  mation Processing Systems 6 (J"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "For all the above distributions, the EM (Expectation-Maximization) algorithm [61, 18, 19, 20] can be used to estimate the parameters of the HMM in order to maximize the likelihood function l( ) = P (Dj ) = Q p P (y Tp 1 (p)j ) over the set D of training sequences y Tp 1 (p) of length Tp (indiced by the letter p)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 216
                            }
                        ],
                        "text": "Let us now see how these posterior probabilities, which we will note P (qtjx T 1 ; y T 1 ) and P (qt; qt 1jx T 1 ; y T 1 ) to lighten the notation, can be computed with the Baum-Welch forward and backward recursions [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "Algorithms for estimating the parameters of HMMs have been developed in the 60's and 70's [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "In this section we will sketch the application of the EM (Expectation-Maximization) algorithm [61] to HMMs [18, 19, 20] and IOHMMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process,\" Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [38, 39, 13], econometrics [14, 15, 40], time series [41], and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 355,
                                "start": 347
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Many early models assume that some of the parameters of the distribution are known a-priori, and others [15] approximate the EM algorithm with a heuristic, because the E-step would require exponential computations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sto er, \\Dynamic linear models with switching,"
            },
            "venue": {
                "fragments": [],
                "text": "J. Amer. Stat. Assoc.,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "These algorithms were used in language modeling [37, 12] and handwritten character recognition [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 271
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10], variable-length Markov models [11, 12], Markov switching models [13] and switching state-space models [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [37, 38, 12], econometrics [13, 14, 39], time series, and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Using these posteriors, a mixture over a very large family of such trees can be formed, whose generalization performance tracks that of the best tree in that family [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "More recently, an extension of this idea to probabilistic but synchronous transducers was proposed [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive mixtures of probabilistic transducers,\" in Advances in Neural Information Pro-  cessing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "More recently, an extension of this idea to probabilistic but synchronous transducers was proposed [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [34, 35, 11], econometrics [12, 13, 36], time series, and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "These algorithms were used in language modeling [34, 11] and handwritten character recognition [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Using these posteriors, a mixture over a very large family of such trees can be formed, whose generalization performance tracks that of the best tree in that family [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive mixtures of probabilistic transducers,\" in Advances in Neural Infor-  mation Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "For all the above distributions, the EM (Expectation-Maximization) algorithm [61, 18, 19, 20] can be used to estimate the parameters of the HMM in order to maximize the likelihood function l( ) = P (Dj ) = Q p P (y Tp 1 (p)j ) over the set D of training sequences y Tp 1 (p) of length Tp (indiced by the letter p)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 216
                            }
                        ],
                        "text": "Let us now see how these posterior probabilities, which we will note P (qtjx T 1 ; y T 1 ) and P (qt; qt 1jx T 1 ; y T 1 ) to lighten the notation, can be computed with the Baum-Welch forward and backward recursions [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "Algorithms for estimating the parameters of HMMs have been developed in the 60's and 70's [18, 19, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "In this section we will sketch the application of the EM (Expectation-Maximization) algorithm [61] to HMMs [18, 19, 20] and IOHMMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eagon, \\An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology,"
            },
            "venue": {
                "fragments": [],
                "text": "Bull. Amer. Math. Soc.,"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52091989"
                        ],
                        "name": "N. Kiefer",
                        "slug": "N.-Kiefer",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kiefer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kiefer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The parameters of Markov switching models can generally be estimated using the EM algorithm [121, 109, 111, 122] to maximize the likelihood P (y 1 j ) (see next section)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 154648988,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "94658d6fe536bfa9ad412fd13e676df810fe00bf",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Note-on-Switching-Regressions-and-Logistic-Kiefer",
            "title": {
                "fragments": [],
                "text": "A Note on Switching Regressions and Logistic Discrimination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064569"
                        ],
                        "name": "F. Diebold",
                        "slug": "F.-Diebold",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Diebold",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Diebold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65729671"
                        ],
                        "name": "Glenn D. Rudebusch",
                        "slug": "Glenn-D.-Rudebusch",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Rudebusch",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn D. Rudebusch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66177368"
                        ],
                        "name": "Daniel Sichel",
                        "slug": "Daniel-Sichel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sichel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Sichel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 153396345,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b80a83e049de812b6f52e1c9c77ee8bc247928c6",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Further-Evidence-on-Business-Cycle-Duration-Diebold-Rudebusch",
            "title": {
                "fragments": [],
                "text": "Further Evidence on Business Cycle Duration Dependence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064569"
                        ],
                        "name": "F. Diebold",
                        "slug": "F.-Diebold",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Diebold",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Diebold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87343029"
                        ],
                        "name": "Joon-Haeng Lee",
                        "slug": "Joon-Haeng-Lee",
                        "structuredName": {
                            "firstName": "Joon-Haeng",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joon-Haeng Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119300289"
                        ],
                        "name": "G. Weinbach",
                        "slug": "G.-Weinbach",
                        "structuredName": {
                            "firstName": "Gretchen",
                            "lastName": "Weinbach",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weinbach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 152590964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "86f9c0ce294d5d15c616847872efcc6efba6568f",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Regime-Switching-with-Time-Varying-Transition-Diebold-Lee",
            "title": {
                "fragments": [],
                "text": "Regime Switching with Time-Varying Transition Probabilities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2528580"
                        ],
                        "name": "M. Kolawole",
                        "slug": "M.-Kolawole",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kolawole",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kolawole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "See [137] and [16] for a review of such models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125080990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ada8e5d8d72fd1c0dbbd5c42e8d972ea6b7756fa",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-and-tracking-Kolawole",
            "title": {
                "fragments": [],
                "text": "Estimation and tracking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102976691"
                        ],
                        "name": "S. Goldfeld",
                        "slug": "S.-Goldfeld",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Goldfeld",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goldfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831694"
                        ],
                        "name": "R. Quandt",
                        "slug": "R.-Quandt",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Quandt",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Quandt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121443596,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42b49d659beb3005b9c1ce7905c6c348d7dbdd3c",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Markov-model-for-switching-regressions-Goldfeld-Quandt",
            "title": {
                "fragments": [],
                "text": "A Markov model for switching regressions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2357050"
                        ],
                        "name": "H. Rauch",
                        "slug": "H.-Rauch",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Rauch",
                            "middleNames": [
                                "Ernest"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rauch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "models, a backward recursion (the Rauch equations [128]) allows to compute the posterior probabilities P (qtjx T 1 ; y T 1 ) for T > t (thus solving the smoothing problem)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120993486,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "64d437c00e09c179e0f4f727a0b8e66cb9e601d2",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-to-the-linear-smoothing-problem-Rauch",
            "title": {
                "fragments": [],
                "text": "Solutions to the linear smoothing problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112438342"
                        ],
                        "name": "Jin-Young Ha",
                        "slug": "Jin-Young-Ha",
                        "structuredName": {
                            "firstName": "Jin-Young",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin-Young Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66851516"
                        ],
                        "name": "\ud558\uc9c4\uc601",
                        "slug": "\ud558\uc9c4\uc601",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\ud558\uc9c4\uc601",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\ud558\uc9c4\uc601"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62348918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e97812d007551efbd5ee2744e44fe6e546de072",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unconstrained-handwritten-word-recognition-with-=-Ha-\ud558\uc9c4\uc601",
            "title": {
                "fragments": [],
                "text": "Unconstrained handwritten word recognition with interconnected hidden markov models = \uc0c1\ud638 \uc5f0\uacb0\ub41c \uc740\ub2c9 \ub9c8\ub974\ucf54\ud504 \ubaa8\ub378\uc744 \uc774\uc6a9\ud55c \ubb34\uc81c\uc57d \ud544\uae30 \ub2e8\uc5b4 \uc778\uc2dd"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": false,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095625734"
                        ],
                        "name": "\u53e4\u4e95 \u8c9e\u7155",
                        "slug": "\u53e4\u4e95-\u8c9e\u7155",
                        "structuredName": {
                            "firstName": "\u53e4\u4e95",
                            "lastName": "\u8c9e\u7155",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u53e4\u4e95 \u8c9e\u7155"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60280916,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "5c8cfdb9c2a50853e95314de89f5eba063a10e60",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "1994-ARPA-Human-Language-Technology-Workshop\u53c2\u52a0\u5831\u544a-\u53e4\u4e95",
            "title": {
                "fragments": [],
                "text": "1994 ARPA Human Language Technology Workshop\u53c2\u52a0\u5831\u544a"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59810176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cde2937cb41cf461f624ded2012743ac4624eca8",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-neural-networks-and-their-application-to-Bengio",
            "title": {
                "fragments": [],
                "text": "Artificial neural networks and their application to sequence recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108011249"
                        ],
                        "name": "Markus Schenke",
                        "slug": "Markus-Schenke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Schenke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Schenke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59841867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6cd1f4ff52485e4bc154033e7b139b640ca9e12",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "OVERVIEW-AND-SYNTHESIS-OF-ON-LINE-CURSIVE-Guyon-Schenke",
            "title": {
                "fragments": [],
                "text": "OVERVIEW AND SYNTHESIS OF ON-LINE CURSIVE HANDWRITING RECOGNITION TECHNIQUES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [73, 74, 75, 76, 77, 78, 79, 80, 81, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59812007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d33c7c733a5960827fe6abe841ef1faa68cef6f4",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Phoneme-Recognition-from-the-TIMIT-database-using-Robinson-Fallside",
            "title": {
                "fragments": [],
                "text": "Phoneme Recognition from the TIMIT database using Recurrent Error Propa-gation Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1958203"
                        ],
                        "name": "Hugues Leprieur",
                        "slug": "Hugues-Leprieur",
                        "structuredName": {
                            "firstName": "Hugues",
                            "lastName": "Leprieur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hugues Leprieur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 104
                            }
                        ],
                        "text": "Other criteria have been proposed to approximate the minimization of the number of classi cation errors [69, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33615981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aad73b2d815e773424f7fe5ef0df9e747294eeb",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Discriminant-learning-with-minimum-memory-loss-for-Leprieur-Haffner",
            "title": {
                "fragments": [],
                "text": "Discriminant learning with minimum memory loss for improved non-vocabulary rejection"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 16
                            }
                        ],
                        "text": "See for example [114, 115] for Markov-switching ARCH models applied to analyzing respectively the changes in variance of stock returns and interest rates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 179
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A markov model of unconditional variance in ARCH,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Business and Economic Statistics,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Zavaliagkos, \\Comparative experiments on large vocabulary speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Neural classiiers useful for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. First Intl. Conf. on Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 32
                            }
                        ],
                        "text": "A generic composition operation [8, 9, 10, 11] allows to combine a cascade of transducers and acceptors, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "More generally, weighted acceptors and transducers [8, 9, 10, 11] can be used to assign a weight to a sequence (or a pair of input/output sequences)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 229
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weighted- nite-automata tools with applications to speech and language processing,"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. Technical Memorandum 11222-931130-28TM, AT&T Bell Laboratories,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wellekens, \\Links between hidden Markov models and multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "In the literature on learning algorithms [4, 5, 6], IOHMMs have been proposed for sequence processing tasks, with complex emission and transition models based on ANNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning ne motion by markov mixtures of experts,\" in Ad-  vances in Neural Information Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Transition-based statistical training for asr"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Automatic Speech Recognition Workshop (Snowbird)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Alphanets: a recurrentrecurrent`neural' network architecture with a hidden Markov model interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Communication"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural classi ers useful for speech"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. First Intl. Conf. on Neural Networks,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 256
                            }
                        ],
                        "text": "In most of the cases described in the econometrics literature, this distribution is assumed to be time-invariant, and it is speci ed by a matrix of transition probabilities (as in ordinary HMMs), although more complicated speci cations have been suggested [95, 96, 97, 98, 99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weinbach, \\Regime switching with time-varying transition  probabilities,\" in Nonstationary Time Series Analysis and Cointegration (C. Hargreaves,  ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory and Practice o f r ecursive identiication Parameter estimation for linear dynamical systems"
            },
            "venue": {
                "fragments": [],
                "text": "Theory and Practice o f r ecursive identiication Parameter estimation for linear dynamical systems"
            },
            "year": 1300
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 7
                            }
                        ],
                        "text": "Others [138, 139] used expensive Monte-Carlo simulations to address this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Likelihood estimation and state estimation for nonlinear state space models"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Graduate Group in Managerial Science and Applied Economics,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 65
                            }
                        ],
                        "text": "Other inference algorithms are used in econometrics applications [123], for ltering, smoothing, and prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "State-space models,\" in Handbook of Econometrics (R"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computing Surveys"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning regular grammars by means of a state merging method The Acoustic-Modeling problem in Automatic Speech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Grammatical Inference and Applications Proc. of the 2nd International Colloquium on Grammatical Inference ICGI94"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 32
                            }
                        ],
                        "text": "A generic composition operation [8, 9, 10, 11] allows to combine a cascade of transducers and acceptors, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "More generally, weighted acceptors and transducers [8, 9, 10, 11] can be used to assign a weight to a sequence (or a pair of input/output sequences)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 229
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10, 11], variable-length Markov models [12, 13], Markov switching models [14] and switching state-space models [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition by composition of weighted nite automata,\" in Finite- State Language Processing (E"
            },
            "venue": {
                "fragments": [],
                "text": "Roche and Y. Schabes, eds.),"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 256
                            }
                        ],
                        "text": "In most of the cases described in the econometrics literature, this distribution is assumed to be time-invariant, and it is speci ed by a matrix of transition probabilities (as in ordinary HMMs), although more complicated speci cations have been suggested [116, 117, 118, 119, 120]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weinbach, \\Regime switching with time-varying transition probabilities,\" in Nonstationary Time Series Analysis and Cointegration (C"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fallside, \\Script recognition using hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "The idea of training a set of modules together (rather than separately) with respect to a global criterion with gradient-based algorithms was proposed several years ago [82, 83, 72]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une approche th eorique de l'apprentissage connexioniste; applications a la re-  connaissance de la parole"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Universit e de Paris XI,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition by composition of weighted nite automata,\" in Finite- State Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Speech recognition by composition of weighted nite automata,\" in Finite- State Language Processing"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Serial correlation in discrete variable models,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Econometrics,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 104
                            }
                        ],
                        "text": "Other criteria have been proposed to approximate the minimization of the number of classi cation errors [80, 81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminant learning with minimum memory loss for improved nonvocabulary rejection,\" in EUROSPEECH'95"
            },
            "venue": {
                "fragments": [],
                "text": "(Madrid, Spain),"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "An HMM can also be viewed as a particular kind of recurrent [72] ANN [85, 86]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Alphanets: a recurrent `neural' network architecture with a hidden Markov model interpre-  tation,"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Communication,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Paesler, \\Data driven search organization for contin-  uous speech recognition,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [73, 74, 75, 76, 77, 78, 79, 80, 81, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [74, 75, 1, 87]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition using multilayer perceptrons with hid-  den Markov models,\" in International Conference on Acoustics, Speech and Signal Processing, (Albu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inputtoutput HMM architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Le Cun, \\Document analysis with generalized transduction"
            },
            "venue": {
                "fragments": [],
                "text": "Le Cun, \\Document analysis with generalized transduction"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 104
                            }
                        ],
                        "text": "Other criteria have been proposed to approximate the minimization of the number of classi cation errors [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminant learning with minimum memory loss for im-  proved non-vocabulary rejection,\" in EUROSPEECH'95"
            },
            "venue": {
                "fragments": [],
                "text": "(Madrid, Spain),"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique occuring in the statistical analysis of probabilistic functions of Markov c hains"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Math. Statistic"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "An HMM can also be viewed as a particular kind of recurrent [65] ANN [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training stochastic model recognition algorithms as networks can lead to max-  imum mutual information estimation of parameters,\" in Advances in Neural Information  Processing Systems 2 (D"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [87, 88, 1, 100]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Links between hidden Markov models and multilayer perceptrons,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [66, 67, 68, 69, 70, 71, 72, 73, 74, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition from the TIMIT database using  recurrent error propagation networks,\" Technical Report CUED/F-INFENG/TR.42, Cam-  bridge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\New results in linear ltering and prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Basic Engineering (ASME)"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 202
                            }
                        ],
                        "text": "1 Markov Switching Models Markov Switching Models have been introduced in the econometrics literature [95, 96, 97, 13, 14] for modeling non-stationarities due to abrupt changes of regime in the economy [98, 99, 100, 101, 102, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Can a well- tted equilibrium asset-pricing model produce mean rever-  sion?,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Applied Econometrics,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "A Markov model [38] of order k is a probability distribution over a sequence of variables qt 1 = fq1; q2; : : : ; qtg with the following conditional independence property: P (qtjqt 1 1 ) = P (qtjqt 1 t k): Since qt 1 t k summarizes all the relevant past information, qt is generally called a state variable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of statistical investigation in the text of `eugene onyegin' illustrat-  ing coupling of `tests' in chains,"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Academy of Science, St. Petersburg,  vol"
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Links between hidden Markov models and multilayer perceptrons Continuous speech recognition using multilayer perceptrons with hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference o n A coustics, Speech and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative learning for minimum error classiication Discriminant learning with minimum memory loss for improved nonvocabulary rejection"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH'95 822 Advanced Research Projects Agency, Proceedings of the 1994 ARPA Human Language Technology Workshop"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 183
                            }
                        ],
                        "text": "These assumptions, however, have been found very useful in practice, in order to build the current state-of-the-art speech recognition systems [16] and applications to bioinformatics [35, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition [25, 26, 27, 28, 29, 30, 31], pattern recognition in molecular biology [32, 33, 34, 35, 3], and fault-detection [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pre-  dicting protein structure using hidden markov models,\" Proteins: Structure, Function and Genetics,  vol"
            },
            "venue": {
                "fragments": [],
                "text": "Supp. 1,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "This approach was successfully used as part of a document analysis system [104] that reads amounts from check images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "1 Generalized Transducers A way to generalize transducers was recently proposed [104] which allows any kind of data structure to be used as \\labels\" (instead of discrete symbols) in the sequences to be processed, and allows the transducers and acceptors to have parameters that are learned with respect to a global criterion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document analysis with generalized transduc-  tion,"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. HA6156000-960701-01TM, AT&T Laboratories,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\An example of statistical investigation in the text of\u00e8ugene onyegin' illustrating coupling ofof`tests' in chains"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Academy of Science, St. Petersburg"
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "An HMM can also be viewed as a particular kind of recurrent [72] ANN [85, 86]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training stochastic model recognition algorithms as networks can lead to maximum mu-  tual information estimation of parameters,\" in Advances in Neural Information Processing Systems 2  (D"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Continuous speech recognition using linked predictive networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The parameters of Markov switching models can generally be estimated using the EM algorithm [121, 109, 111, 122] to maximize the likelihood P (y 1 j ) (see next section)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "Markov Switching Models have been introduced in the econometrics literature [108, 109, 110, 14, 15] for modeling non-stationarities due to abrupt changes of regime in the economy [111, 112, 113, 114, 115, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sto er, \\An approach to time series smoothing and forecasting using the EM algorithm,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Time Series Analysis,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory and Practice of recursive identiication"
            },
            "venue": {
                "fragments": [],
                "text": "Theory and Practice of recursive identiication"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 346,
                                "start": 338
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2], InputOutput HMMs [3, 4, 5, 6], weighted transducers [7, 8, 9], variable-length Markov models [10, 11], Markov switching models [12] and switching state-space models [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Switching state-space models,\" Tech. Rep. Technical Re-  port CRG-TR-91-3"
            },
            "venue": {
                "fragments": [],
                "text": "University of Toronto,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "The application of HMMs to speech was independently proposed by [20] and [21], and popularized by [22], [23], and [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to the application of the theory of proba-  bilistic functions of a Markov process to automatic speech recognition,\" Bell System"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Journal,  vol. 64,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 83
                            }
                        ],
                        "text": "As also exploited in the review presented here, the graphical model formulation of [55, 56] lends itself naturally to extensions such as the introduction of context variables or factoring of the state variable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "See also related work using Bayesian networks to factor the state variable to represent di erent types of contexts [55, 56, 131]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 113
                            }
                        ],
                        "text": "Other formulations of HMMs as graphical models have been proposed and applied successfully to speech recognition [55, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic modeling with Bayesian networks for ASR,"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Statistical Language Processing, (Sidney,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "AMarkovmodel [47] of order k is a probability distribution over a sequence of variables q 1 = fq1; q2; : : : ; qtg"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of statistical investigation in the text of `eugene onyegin' illustrating coupling of `tests' in chains,"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Academy of Science, St. Petersburg,"
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical recurrent neural networks for long-term dependen-  cies,\" in Advances in Neural Information Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "See also [44] for an analysis of the case of hidden Markov chains with deterministic emissions, which shows that some classes of Markovian learning problems are hard while others are polynomial in the number of samples required."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 179
                            }
                        ],
                        "text": "Similar trade-o s (between generality of the model and intractability of the learning algorithm) have been described for variants HMMs and other nite-state learning algorithms in [42, 44, 45]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inference and minimization of hidden marko chains,\" in proceedings of the 7th annual ACM conference on Computational learning theory (COLT'94)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "See for example [74, 75] and [76] for polynomial-time algorithms that constructively learn a probabilistic structure for the language by merging states, and [45] on the learnability of acyclic probabilistic nite automata."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning regular grammars by means of a state merging method,\" in Grammatical Inference and Applications Proc. of the 2nd International Colloquium on Grammatical Inference ICGI94, (Alicante (Spain))"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Arti cial Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "A variant of the continuous HMM with Gaussian mixtures is the so-called semi-continuous HMM [45, 46], in which the Gaussians are shared and the parameters speci c to each state are only the mixture weights: P (ytjqt = i) =Xj wjiN(yt; j ; j): where the mixture weights play a role that is similar to the multinomial coe cients of the discrete emission HMMs described above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition. Edin-  burgh"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "These algorithms were used in language modeling [37, 12] and handwritten character recognition [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [37, 38, 12], econometrics [13, 14, 39], time series, and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overview and synthesis of on-line cursive handwriting recog-  nition techniques,\" in Handbook on Optical Character Recognition and Document Image Analysis  (P"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 89
                            }
                        ],
                        "text": "In general such models would be very expensive to maintain, but so-called factorial HMMs [117] have been proposed with such properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factorial hidden markov models,\" in Advances in Neural Informa-  tion Processing Systems 8 (M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [86, 87, 88, 89, 90, 91, 92, 93, 94, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [95, 92, 2, 93, 94], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schmidbauer, \\Continuous speech recognition using linked predictive networks,\" in Advances in Neural Information Processing Systems 3 (R"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [73, 74, 75, 76, 77, 78, 79, 80, 81, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist Viterbi training: a new hybrid method for con-  tinuous speech"
            },
            "venue": {
                "fragments": [],
                "text": "recognition,\" in International Conference on Acoustics, Speech and Signal Processing,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 58
                            }
                        ],
                        "text": "rely on a probabilistic interpretation of the ANN outputs [67, 68, 1, 80]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [66, 67, 68, 69, 70, 71, 72, 73, 74, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition using multilayer perceptrons  with hidden Markov models,\" in International Conference on Acoustics, Speech and Signal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Paesler, \\Data driven search organization for continuous speech recognition,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 169
                            }
                        ],
                        "text": "The idea of training a set of modules together (rather than separately) with respect to a global criterion with gradient-based algorithms was proposed several years ago [102, 103, 92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une approche th eorique de l'apprentissage connexioniste; applications a la reconnaissance de la parole"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Universit e de Paris XI,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "The Kalman lter [127] is in fact such a model, and the associated algorithms allow to compute P (qtjx t 1; y t 1) in a forward recursion (thus solving the ltering problem)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bucy, \\New results in linear ltering and prediction,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Basic Engineering (ASME), vol. 83D,"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Computing Surveys 2"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computing Surveys 2"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "See for example [74, 75] and [76] for polynomial-time algorithms that constructively learn a probabilistic structure for the language by merging states, and [45] on the learnability of acyclic probabilistic nite automata."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction by bayesian model merging,\" in Advances in Neural Information Processing Systems 5 (S"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 218
                            }
                        ],
                        "text": "In the context of real-time control and other applications where learning must be on-line, numerical maximization of the likelihood can be performed recursively with a second-order method which requires only gradients [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soderstrom, Theory and Practice of recursive identi cation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [75, 72, 2, 73, 74], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the Viterbi algorithm to perform the alignment and segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 263
                            }
                        ],
                        "text": "Since ANNs were successful at classifying individual phonemes, initial research focused on using the dynamic programming tools of HMMs in order to go from the recognition of individual phonemes (or other local classi cation) to the recognition of whole sequences [66, 67, 68, 69, 70, 71, 72, 73, 74, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "In some cases the dynamic programming algorithm is embedded in the ANN itself [73, 76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Integrating time alignment and neural net-  works for high performance continuous speech"
            },
            "venue": {
                "fragments": [],
                "text": "recognition,\" in International Conference  on Acoustics, Speech and Signal Processing,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "In some cases [75, 72, 2, 73, 74], the ANN outputs are not interpreted as probabilities, but are rather used as scores and generally combined with a dynamic programming algorithm akin to the Viterbi algorithm to perform the alignment and segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speaker independent iso-  lated digit recognition: multilayer perceptrons vs dynamic time warping,"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,  vol"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Instead, in [15], a function that is a lower bound on the log likelihood is maximized with a tractable algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "See [118] and [15] for a review of such models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 351,
                                "start": 343
                            }
                        ],
                        "text": "The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HMMs with arti cial neural networks [1, 2, 3], Input-Output HMMs [4, 5, 6, 7], weighted transducers [8, 9, 10], variable-length Markov models [11, 12], Markov switching models [13] and switching state-space models [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Switching state-space models,\" Tech. Rep. Technical Report CRG-  TR-91-3"
            },
            "venue": {
                "fragments": [],
                "text": "University of Toronto,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "These algorithms were used in language modeling [34, 11] and handwritten character recognition [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The variants and extensions of HMMs discussed here also include language models [34, 35, 11], econometrics [12, 13, 36], time series, and signal processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overview and synthesis of on-line cursive handwrit-  ing recognition techniques,\" in Hankbook on Optical Character Recognition and Document  Image Analysis (P"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology"
            },
            "venue": {
                "fragments": [],
                "text": "Bull. Amer. Math. Soc"
            },
            "year": 1967
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 74,
            "methodology": 77
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 203,
        "totalPages": 21
    },
    "page_url": "https://www.semanticscholar.org/paper/Markovian-Models-for-Sequential-Data-Bengio/f316d350c85fa7198878a1149c327218f7d4ea30?sort=total-citations"
}