{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2289537"
                        ],
                        "name": "Y. Caspi",
                        "slug": "Y.-Caspi",
                        "structuredName": {
                            "firstName": "Yaron",
                            "lastName": "Caspi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Caspi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "Some of these methods further require a prior learning phase with many examples [21, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14497206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a63e20ed4634790ec163ebabb79e66b305babd56",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major problems in modeling images for vision tasks is that images with very similar structure may locally have completely different appearance, e.g., images taken under different illumination conditions, or the images of pedestrians with different clothing. While there have been many successful attempts to address these problems in application-specific settings, we believe that underlying a large set of problems in vision is a representational deficiency of intensity-derived local measurements that are the basis of most efficient models. We argue that interesting structure in images is better captured when the image is defined as a matrix whose entries are discrete indices to a separate palette of possible intensities, colors or other features, much like the image representation often used to save on storage. In order to model the variability in images, we define an image class not by a single index map, but by a probability distribution over the index maps, which can be automatically estimated from the data, and which we call probabilistic index maps. The existing algorithms can be adapted to work with this representation, as we illustrate in this paper on the example of transformation-invariant clustering and background subtraction. Furthermore, the probabilistic index map representation leads to algorithms with computational costs proportional to either the size of the palette or the log of the size of the palette, making the cost of significantly increased invariance to non-structural changes quite bearable."
            },
            "slug": "Capturing-image-structure-with-probabilistic-index-Jojic-Caspi",
            "title": {
                "fragments": [],
                "text": "Capturing image structure with probabilistic index maps"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that interesting structure in images is better captured when the image is defined as a matrix whose entries are discrete indices to a separate palette of possible intensities, colors or other features, much like the image representation often used to save on storage."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785317"
                        ],
                        "name": "W. Wells",
                        "slug": "W.-Wells",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Wells",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wells"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Self-similarity is closely related to the notion of statistical co-occurrence of pixel intensities across images, captured by Mutual Information (MI) [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2499926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae466aae94b7fb9944056249d7c5be2e8cdef280",
            "isKey": false,
            "numCitedBy": 3426,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "A new information-theoretic approach is presented for finding the pose of an object in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and may foreseeably be used in a wide variety of imaging situations.Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images, aligning a complex 3D object model to real scenes including clutter and occlusion, tracking a human head in a video sequence and aligning a view-based 2D object model to real images.The method is based on a formulation of the mutual information between the model and the image. As applied here the technique is intensity-based, rather than feature-based. It works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than traditional correlation. Additionally, it has an efficient implementation that is based on stochastic approximation."
            },
            "slug": "Alignment-by-Maximization-of-Mutual-Information-Viola-Wells",
            "title": {
                "fragments": [],
                "text": "Alignment by Maximization of Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new information-theoretic approach is presented for finding the pose of an object in an image that works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than traditional correlation."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "9 shows results of applying our action detection algorithm for detecting a ballet turn in a video clip of [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Furthermore, our method can match both stationary and moving objects (as opposed to only moving objects in [20])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Action detection and comparison to [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "These sequences contain very strong temporal aliasing and therefore cannot be handled well by methods like [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "However, unlike [20], our new method can handle highly aliased video sequences with non-instantaneous motions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "(c) In the results obtained by [20] there are"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Our approach is probably most closely related to the capabilities presented in our previous work [20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6891864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4811a11937448ea5d40a0af36c974e08ab12c08c",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a behavior-based similarity measure which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field. This is done directly from the intensity information, without explicitly computing the underlying motions. Such a measure allows us to detect similarity between video segments of differently dressed people performing the same type of activity. It requires no foreground/background segmentation, no prior learning of activities, and no motion estimation or tracking. Using this behavior-based similarity measure, we extend the notion of 2-dimensional image correlation into the 3-dimensional space-time volume, thus allowing to correlate dynamic behaviors and actions. Small space-time video segments (small video clips) are \"correlated\" against entire video sequences in all three dimensions (x,y, and t). Peak correlation values correspond to video locations with similar dynamic behaviors. Our approach can detect very complex behaviors in video sequences (e.g., ballet movements, pool dives, running water), even when multiple complex activities occur simultaneously within the field-of-view of the camera."
            },
            "slug": "Space-time-behavior-based-correlation-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Space-time behavior based correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A behavior-based similarity measure is introduced which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field, thus allowing to correlate dynamic behaviors and actions."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "For a comprehensive comparison of many region descriptors for image matching see [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Note that the remaining descriptors still form a dense collection (much denser than sparse interest points [13, 16, 12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "We further compared the matching performance of our descriptors with some state-of-the-art local descriptors evaluated in [16]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "We selected a subset of local descriptors which ranked highest (and used the implementation in [16])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "These included: gradient location-orientation histogram (GLOH) [16] \u2013 a log-polar extension of SIFT [13] that was shown to be more robust and distinctive, local Shape Context [1] (an extended version with orientations [16]), and four other descriptors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2572455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69401bfdafab7cde00bb8e5b2f6c28e9d72d8cfb",
            "isKey": true,
            "numCitedBy": 3663,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "slug": "A-performance-evaluation-of-local-descriptors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "A performance evaluation of local descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is observed that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best and Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1910299"
                        ],
                        "name": "Oren Boiman",
                        "slug": "Oren-Boiman",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Boiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Boiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "To find a good match for the \u201censemble of descriptors\u201d of F within G, we use a modified version of the efficient \u201censemble matching\u201d algorithm of [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 100
                            }
                        ],
                        "text": "Dense properties include raw pixel intensity or color values (of the entire image, of small patches [25, 3] or fragments [22]), texture filters [15] or other filter responses [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 463427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86078efdd0d1bbfaf4b7f821e973f607429751fc",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of detecting irregularities in visual data, e.g., detecting suspicious behaviors in video sequences, or identifying salient patterns in images. The term \u201cirregular\u201d depends on the context in which the \u201cregular\u201d or \u201cvalid\u201d are defined. Yet, it is not realistic to expect explicit definition of all possible valid configurations for a given context. We pose the problem of determining the validity of visual data as a process of constructing a puzzle: We try to compose a new observed image region or a new video segment (\u201cthe query\u201d) using chunks of data (\u201cpieces of puzzle\u201d) extracted from previous visual examples (\u201cthe database\u201d). Regions in the observed data which can be composed using large contiguous chunks of data from the database are considered very likely, whereas regions in the observed data which cannot be composed from the database (or can be composed, but only using small fragmented pieces) are regarded as unlikely/suspicious. The problem is posed as an inference process in a probabilistic graphical model. We show applications of this approach to identifying saliency in images and video, for detecting suspicious behaviors and for automatic visual inspection for quality assurance."
            },
            "slug": "Detecting-Irregularities-in-Images-and-in-Video-Boiman-Irani",
            "title": {
                "fragments": [],
                "text": "Detecting Irregularities in Images and in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This work addresses the problem of detecting irregularities in visual data, e.g., detecting suspicious behaviors in video sequences, or identifying salient patterns in images, using a probabilistic graphical model."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10251113"
                        ],
                        "name": "C. Jacobs",
                        "slug": "C.-Jacobs",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37737599"
                        ],
                        "name": "A. Finkelstein",
                        "slug": "A.-Finkelstein",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745260"
                        ],
                        "name": "D. Salesin",
                        "slug": "D.-Salesin",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Salesin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Salesin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 2
                            }
                        ],
                        "text": ", [10, 7]) assume that the sketched query image and the database image share similar low-resolution photometric properties (colors, textures, low-level wavelet coefficients, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7884491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e9bd47e9fa53e8719aba15e4367096317d45f74",
            "isKey": false,
            "numCitedBy": 835,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for searching in an image database using a query image that is similar to the intended target. The query image may be a hand-drawn sketch or a (potentially low-quality) scan of the image to be retrieved. Our searching algorithm makes use of multiresolution wavelet decompositions of the query and database images. The coefficients of these decompositions are distilled into small \u201csignatures\u201d for each image. We introduce an \u201cimage querying metric\u201d that operates on these signatures. This metric essentially compares how many significant wavelet coefficients the query has in common with potential targets. The metric includes parameters that can be tuned, using a statistical analysis, to accommodate the kinds of image distortions found in different types of image queries. The resulting algorithm is simple, requires very little storage overhead for the database of signatures, and is fast enough to be performed on a database of 20,000 images at interactive rates (on standard desktop machines) as a query is sketched. Our experiments with hundreds of queries in databases of 1000 and 20,000 images show dramatic improvement, in both speed and success rate, over using a conventional L1, L2, or color histogram norm. CR"
            },
            "slug": "Fast-multiresolution-image-querying-Jacobs-Finkelstein",
            "title": {
                "fragments": [],
                "text": "Fast multiresolution image querying"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An \u201cimage querying metric\u201d is introduced that operates on how many significant wavelet coefficients the query has in common with potential targets, and includes parameters that can be tuned, using a statistical analysis, to accommodate the kinds of image distortions found in different types of image queries."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34712076"
                        ],
                        "name": "C. Stauffer",
                        "slug": "C.-Stauffer",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stauffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stauffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719838"
                        ],
                        "name": "W. Grimson",
                        "slug": "W.-Grimson",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimson",
                            "middleNames": [
                                "Eric",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grimson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "Some of these methods further require a prior learning phase with many examples [21, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5902518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75ceea673e1d6651a032dc7b0294e5d5698fe1c3",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates applications of a new representation for images, the similarity template. A similarity template is a probabilistic representation of the similarity of pixels in an image patch. It has application to detection of a class of objects, because it is reasonably invariant to the color of a particular object. Further, it enables the decomposition of a class of objects into component parts over which robust statistics of color can be approximated. These regions can be used to create a factored color model that is useful for recognition. Detection results are shown on a system that learns to detect a class of objects (pedestrians) in static scenes based on examples of the object provided automatically by a tracking system. Applications of the factored color model to image indexing and anomaly detection are pursued on a database of images of pedestrians."
            },
            "slug": "Similarity-templates-for-detection-and-recognition-Stauffer-Grimson",
            "title": {
                "fragments": [],
                "text": "Similarity templates for detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper investigates applications of a new representation for images, the similarity template, a probabilistic representation of the similarity of pixels in an image patch that enables the decomposition of a class of objects into component parts over which robust statistics of color can be approximated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10617783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b3dbc36ae4d8171f9a4179001e37299554f1652",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper makes two contributions: it provides (1) an operational definition of textons, the putative elementary units of texture perception, and (2) an algorithm for partitioning the image into disjoint regions of coherent brightness and texture, where boundaries of regions are defined by peaks in contour orientation energy and differences in texton densities across the contour. B. Julesz (1981) introduced the term texton, analogous to a phoneme in speech recognition, but did not provide an operational definition for gray-level images. We re-invent textons as frequently co-occurring combinations of oriented linear filter outputs. These can be learned using a K-means approach. By mapping each pixel to its nearest texton, the image can be analyzed into texton channels, each of which is a point set where discrete techniques such as Voronoi diagrams become applicable. Local histograms of texton frequencies can be used with a /spl chi//sup 2/ test for significant differences to find texture boundaries. Natural images contain both textured and untextured regions, so we combine this cue with that of the presence of peaks of contour energy derived from outputs of odd- and even-symmetric oriented Gaussian derivative filters. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on a statistical test for isotropy of Delaunay neighbors. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown."
            },
            "slug": "Textons,-contours-and-regions:-cue-integration-in-Malik-Belongie",
            "title": {
                "fragments": [],
                "text": "Textons, contours and regions: cue integration in image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An operational definition of textons, the putative elementary units of texture perception, and an algorithm for partitioning the image into disjoint regions of coherent brightness and texture, where boundaries of regions are defined by peaks in contour orientation energy and differences in texton densities across the contour."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "Our approach can be applied to complex video sequences without requiring any foreground background segmentation [26], nor any motion estimation [5] or tracking."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304025"
                        ],
                        "name": "Chiraz BenAbdelkader",
                        "slug": "Chiraz-BenAbdelkader",
                        "structuredName": {
                            "firstName": "Chiraz",
                            "lastName": "BenAbdelkader",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chiraz BenAbdelkader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145952419"
                        ],
                        "name": "Ross Cutler",
                        "slug": "Ross-Cutler",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Cutler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Cutler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "The use of global self-similarity (between entire pre-aligned video frames) has also been proposed in video [2] for gait recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1892124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "848fc7cdf31989522830df7cf8e59bbcbbb99adc",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Gait is one of the few biometrics that can be measured at a distance, and is hence useful for passive surveillance as well as biometric applications. Gait recognition research is still at its infancy, however, and we have yet to solve the fundamental issue of finding gait features which at once have sufficient discrimination power and can be extracted robustly and accurately from low-resolution video. This paper describes a novel gait recognition technique based on the image self-similarity of a walking person. We contend that the similarity plot encodes a projection of gait dynamics. It is also correspondence-free, robust to segmentation noise, and works well with low-resolution video. The method is tested on multiple data sets of varying sizes and degrees of difficulty. Performance is best for fronto-parallel viewpoints, whereby a recognition rate of 98% is achieved for a data set of 6 people, and 70% for a data set of 54 people."
            },
            "slug": "Gait-Recognition-Using-Image-Self-Similarity-BenAbdelkader-Cutler",
            "title": {
                "fragments": [],
                "text": "Gait Recognition Using Image Self-Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the similarity plot encodes a projection of gait dynamics, which is also correspondence-free, robust to segmentation noise, and works well with low-resolution video."
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP J. Adv. Signal Process."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": ", local derivatives [12]), shape-based descriptors using extracted edges (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [4, 17, 12], our method requires no prior learning (nor multiple examples of each action), nor assumes existence of common space-time features across sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Note that the remaining descriptors still form a dense collection (much denser than sparse interest points [13, 16, 12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2619278,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f90d79809325d2b78e35a79ecb372407f81b3993",
            "isKey": false,
            "numCitedBy": 2380,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds."
            },
            "slug": "Space-time-interest-points-Laptev-Lindeberg",
            "title": {
                "fragments": [],
                "text": "Space-time interest points"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work builds on the idea of the Harris and Forstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time to detect spatio-temporal events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35195550"
                        ],
                        "name": "E. Sali",
                        "slug": "E.-Sali",
                        "structuredName": {
                            "firstName": "Erez",
                            "lastName": "Sali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398995671"
                        ],
                        "name": "Michel Vidal-Naquet",
                        "slug": "Michel-Vidal-Naquet",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Vidal-Naquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Vidal-Naquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Dense properties include raw pixel intensity or color values (of the entire image, of small patches [25, 3] or fragments [22]), texture filters [15] or other filter responses [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12923066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2809b20aea8d2a15ac655e17b47fd5dfb304aa4c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of visual classification is thce recognition of an object in the image as belonging to a general class of similar objects, such as a face, a car, a dog, and the like. This is a fundamental and natural task for biological visual systems, but it has proven difficult to perform visual classification by artificial computer vision systems. The main reason for this difficulty is the variability of shape within a class: different objects vary widely in appearance, and it is difficult to capture the essential shape features that characterize the members of one category and distinguish them from another, such as dogs from cats. \n \nIn this paper we describe an approach to classification using a fragment-based representation. In this approach, objects within a class are represented in terms of common image fragments that are used as building blocks for representing a large variety of different objects that belong to a common class. The fragments are selected from a training set of images based on a criterion of maximizing the mutual information of the fragments and the class they represent. For the purpose of classification the fragments are also organized into types, where each type is a collection of alternative fragments, such as different hairline or eye regions for face classification. During classification, the algorithm detects fragments of the different types, and then combines the evidence for the detected fragments to reach a final decision. Experiments indicate that it is possible to trade off the complexity of fragments with the complexity of the combination and decision stage, and this tradeoff is discussed. \n \nThe method is different from previous part-based methods in using class-specific object fragments of varying complexity, the method of selecting fragments, and the organization into fragment types. Experimental results of detecting face and car views show that the fragment-based approach can generalize well to a variety of novel image views within a class while maintaining low mis-classification error rates. We briefly discuss relationships between the proposed method and properties of parts of the primate visual system involved in object perception"
            },
            "slug": "A-Fragment-Based-Approach-to-Object-Representation-Ullman-Sali",
            "title": {
                "fragments": [],
                "text": "A Fragment-Based Approach to Object Representation and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experimental results of detecting face and car views show that the fragment-based approach can generalize well to a variety of novel image views within a class while maintaining low mis-classification error rates."
            },
            "venue": {
                "fragments": [],
                "text": "IWVF"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "7)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Dense properties include raw pixel intensity or color values (of the entire image, of small patches [25, 3] or fragments [22]), texture filters [15] or other filter responses [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2551159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd232cf2ab28cc0ba06942875f14206f04ebbae0",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "The appearance of an object is composed of local structure. This local structure can be described and characterized by a vector of local features measured by local operators such as Gaussian derivatives or Gabor filters. This article presents a technique where appearances of objects are represented by the joint statistics of such local neighborhood operators. As such, this represents a new class of appearance based techniques for computer vision. Based on joint statistics, the paper develops techniques for the identification of multiple objects at arbitrary positions and orientations in a cluttered scene. Experiments show that these techniques can identify over 100 objects in the presence of major occlusions. Most remarkably, the techniques have low complexity and therefore run in real-time."
            },
            "slug": "Recognition-without-Correspondence-using-Receptive-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Recognition without Correspondence using Multidimensional Receptive Field Histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article presents a technique where appearances of objects are represented by the joint statistics of such local neighborhood operators, which represents a new class of appearance based techniques for computer vision."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144171"
                        ],
                        "name": "K. Shanmugam",
                        "slug": "K.-Shanmugam",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Shanmugam",
                            "middleNames": [
                                "Sam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shanmugam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686153"
                        ],
                        "name": "I. Dinstein",
                        "slug": "I.-Dinstein",
                        "structuredName": {
                            "firstName": "Its'hak",
                            "lastName": "Dinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We compare our measure to commonly used image-based and video-based similarity measures, and demonstrate its applicability to object detection, retrieval, and action detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206786900,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1fdb62555eb650662dbe2a6f3985d390861597c2",
            "isKey": false,
            "numCitedBy": 19243,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications."
            },
            "slug": "Textural-Features-for-Image-Classification-Haralick-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Textural Features for Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These results indicate that the easily computable textural features based on gray-tone spatial dependancies probably have a general applicability for a wide variety of image-classification applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Our approach can be applied to complex video sequences without requiring any foreground background segmentation [26], nor any motion estimation [5] or tracking."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18724425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "487c37dce9b93d48f753ab2ec3fc997edb5639ce",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose to model an action based on both the shape and the motion of the performing object. When the object performs an action in 3D, the points on the outer boundary of the object are projected as 2D (x, y) contour in the image plane. A sequence of such 2D contours with respect to time generates a spatiotemporal volume (STV) in (x, y, t), which can be treated as 3D object in the (x, y, t) space. We analyze STV by using the differential geometric surface properties to identify action descriptors capturing both spatial and temporal properties. A set of action descriptors is called an action sketch. The first step in our approach is to generate STV by solving the point correspondence problem between consecutive frames. The correspondences are determined using a two-step graph theoretical approach. After the STV is generated, actions descriptors are computed by analyzing the differential geometric properties of STV. Finally, using these descriptors, we perform action recognition, which is also formulated as graph theoretical problem. Several experimental results are presented to demonstrate our approach."
            },
            "slug": "Actions-sketch:-a-novel-action-representation-Yilmaz-Shah",
            "title": {
                "fragments": [],
                "text": "Actions sketch: a novel action representation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper proposes to model an action based on both the shape and the motion of the performing object, and generates STV by solving the point correspondence problem between consecutive frames using a two-step graph theoretical approach."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [6]) or proximity of corresponding extracted edges (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7149126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "339c13bfe3371a71ab486381721dbb689ff415ab",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for object detection in cluttered real images, given a single hand-drawn example as model. The image edges are partitioned into contour segments and organized in an image representation which encodes their interconnections: the Contour Segment Network. The object detection problem is formulated as finding paths through the network resembling the model outlines, and a computationally efficient detection technique is presented. An extensive experimental evaluation on detecting five diverse object classes over hundreds of images demonstrates that our method works in very cluttered images, allows for scale changes and considerable intra-class shape variation, is robust to interrupted contours, and is computationally efficient."
            },
            "slug": "Object-Detection-by-Contour-Segment-Networks-Ferrari-Tuytelaars",
            "title": {
                "fragments": [],
                "text": "Object Detection by Contour Segment Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An extensive experimental evaluation on detecting five diverse object classes over hundreds of images demonstrates that the proposed method works in very cluttered images, allows for scale changes and considerable intra-class shape variation, is robust to interrupted contours, and is computationally efficient."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762649"
                        ],
                        "name": "V. Rabaud",
                        "slug": "V.-Rabaud",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Rabaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rabaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524603"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unlike [4, 17, 12], our method requires no prior learning (nor multiple examples of each action), nor assumes existence of common space-time features across sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1956774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f1707caad72573633c2307fa26ec093e8f4bb03",
            "isKey": false,
            "numCitedBy": 2716,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior."
            },
            "slug": "Behavior-recognition-via-sparse-spatio-temporal-Doll\u00e1r-Rabaud",
            "title": {
                "fragments": [],
                "text": "Behavior recognition via sparse spatio-temporal features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and an alternative is proposed, and a recognition algorithm based on spatio-temporally windowed data is devised."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143891655"
                        ],
                        "name": "Hao Jiang",
                        "slug": "Hao-Jiang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680726"
                        ],
                        "name": "M. S. Drew",
                        "slug": "M.-S.-Drew",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Drew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Drew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051053953"
                        ],
                        "name": "Ze-Nian Li",
                        "slug": "Ze-Nian-Li",
                        "structuredName": {
                            "firstName": "Ze-Nian",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ze-Nian Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This assumption is not valid for the database of Fig. 8. Other methods assume similarity of the contour of objects (e.g., [6]) or proximity of corresponding extracted edges (e.g., [ 24 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1878790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3bf24beedff1fd3128e37d17a2777b93ec56a5e",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider the problem of describing the action being performed by human figures in still images. We will attack this problem using an unsupervised learning approach, attempting to discover the set of action classes present in a large collection of training images. These action classes will then be used to label test images. Our approach uses the coarse shape of the human figures to match pairs of images. The distance between a pair of images is computed using a linear programming relaxation technique. This is a computationally expensive process, and we employ a fast pruning method to enable its use on a large collection of images. Spectral clustering is then performed using the resulting distances. We present clustering and image labeling results on a variety of datasets."
            },
            "slug": "Unsupervised-Discovery-of-Action-Classes-Wang-Jiang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Action Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper will attack the problem of describing the action being performed by human figures in still images using an unsupervised learning approach, attempting to discover the set of action classes present in a large collection of training images."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254319"
                        ],
                        "name": "Hongcheng Wang",
                        "slug": "Hongcheng-Wang",
                        "structuredName": {
                            "firstName": "Hongcheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongcheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [17] (who also showed results on ice-skating video sequences), our approach requires no prior learning from multiple examples of each action, and does not rely on having common extractable features across action sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [4, 17, 12], our method requires no prior learning (nor multiple examples of each action), nor assumes existence of common space-time features across sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40046466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e25d2a9aa691e63657fef30f5850799d757f69e6",
            "isKey": false,
            "numCitedBy": 981,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel unsupervised learning method for human action categories. A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories. This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach can handle noisy feature points arisen from dynamic background and moving cameras due to the application of the probabilistic models. Given a novel video sequence, the algorithm can categorize and localize the human action(s) contained in the video. We test our algorithm on three challenging datasets: the KTH human motion dataset, the Weizmann human action dataset, and a recent dataset of figure skating actions. Our results reflect the promise of such a simple approach. In addition, our algorithm can recognize and localize multiple actions in long and complex video sequences containing multiple motions."
            },
            "slug": "Unsupervised-Learning-of-Human-Action-Categories-Niebles-Wang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel unsupervised learning method for human action categories that can recognize and localize multiple actions in long and complex video sequences containing multiple motions."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143713756"
                        ],
                        "name": "Xiaolei Huang",
                        "slug": "Xiaolei-Huang",
                        "structuredName": {
                            "firstName": "Xiaolei",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolei Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052824938"
                        ],
                        "name": "Ian Martin",
                        "slug": "Ian-Martin",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Local self-similarities of image patterns have also been employed for the purpose of texture edge detection [25], for detecting symmetries [14], and for other applications."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 100
                            }
                        ],
                        "text": "Dense properties include raw pixel intensity or color values (of the entire image, of small patches [25, 3] or fragments [22]), texture filters [15] or other filter responses [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14292752,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "00c065d1837e99b455101ab7ddcc8f2088cbfa08",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel technique for extracting texture edges is introduced. It is based on the combination of two ideas: the patch-based approach, and non-parametric tests of distributions. \n \nOur method can reliably detect texture edges using only local information. Therefore, it can be computed as a preprocessing step prior to segmentation, and can be very easily combined with parametric deformable models. These models furnish our system with smooth boundaries and globally salient structures."
            },
            "slug": "Patch-Based-Texture-Edges-and-Segmentation-Wolf-Huang",
            "title": {
                "fragments": [],
                "text": "Patch-Based Texture Edges and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A novel technique for extracting texture edges based on the combination of two ideas: the patch-based approach, and non-parametric tests of distributions, which can reliably detect texture edges using only local information and can be combined with parametric deformable models."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414443"
                        ],
                        "name": "G. Loy",
                        "slug": "G.-Loy",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Loy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Loy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270435"
                        ],
                        "name": "J. Eklundh",
                        "slug": "J.-Eklundh",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Eklundh",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eklundh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Local self-similarities of image patterns have also been employed for the purpose of texture edge detection [25], for detecting symmetries [14], and for other applications."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15706128,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "cd504d07b1b7205f7c960da1d2c70cfda4126027",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel and efficient method is presented for grouping feature points on the basis of their underlying symmetry and characterising the symmetries present in an image. We show how symmetric pairs of features can be efficiently detected, how the symmetry bonding each pair is extracted and evaluated, and how these can be grouped into symmetric constellations that specify the dominant symmetries present in the image. Symmetries over all orientations and radii are considered simultaneously, and the method is able to detect local or global symmetries, locate symmetric figures in complex backgrounds, detect bilateral or rotational symmetry, and detect multiple incidences of symmetry."
            },
            "slug": "Detecting-Symmetry-and-Symmetric-Constellations-of-Loy-Eklundh",
            "title": {
                "fragments": [],
                "text": "Detecting Symmetry and Symmetric Constellations of Features"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how symmetric pairs of features can be efficiently detected, how the symmetry bonding each pair is extracted and evaluated, and how these can be grouped into symmetric constellations that specify the dominant symmetries present in the image."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": ", SIFT [13]), differential descriptors (e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "These included: gradient location-orientation histogram (GLOH) [16] \u2013 a log-polar extension of SIFT [13] that was shown to be more robust and distinctive, local Shape Context [1] (an extended version with orientations [16]), and four other descriptors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Common compact region descriptors include distribution based descriptors (e.g., SIFT [13]), differential descriptors (e.g., local derivatives [12]), shape-based descriptors using extracted edges (e.g. Shape Context [1]), and others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Note that the remaining descriptors still form a dense collection (much denser than sparse interest points [13, 16, 12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40179523"
                        ],
                        "name": "J. L. Hafner",
                        "slug": "J.-L.-Hafner",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hafner",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712308"
                        ],
                        "name": "W. Equitz",
                        "slug": "W.-Equitz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Equitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Equitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141915"
                        ],
                        "name": "W. Niblack",
                        "slug": "W.-Niblack",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Niblack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Niblack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33312054,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e1dec5c4aaff8e79ebdcca5feec2640aebce323a",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "An improved shipping container having novel locking features in the end panel and corner flaps. The novel locking features improved bulge resistance at the end panels from sideward bulge of the product. The improved container comprises a pair of corner flaps being hinged from the side panels and folded inwardly against an end panel with the two corner flaps and end panel on each side of the container having a quadruple lock. The lock is formed by providing a locking tab on each corner flap as well as a pair of locking tabs on the end panel with the end panel and corner flaps locking tabs being designed to be swung and locked in the opening formed by the aligned mating locking tab."
            },
            "slug": "Efficient-Color-Histogram-Indexing-for-Quadratic-Hafner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Efficient Color Histogram Indexing for Quadratic Form Distance Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An improved shipping container having novel locking features in the end panel and corner flaps and improved bulge resistance at the end panels from sideward bulge of the product."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The latter type of non-informative descriptors (homogeneity) are detected by employing the sparseness measure of [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The normalized loglikelihood surfaces are then combined by a weighted average with weights corresponding to the degree of sparseness [9] of these log-likelihood surfaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12009862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ede3af3637977988b8cbb330c294183e919b7d5a",
            "isKey": false,
            "numCitedBy": 2697,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of 'sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems."
            },
            "slug": "Non-negative-Matrix-Factorization-with-Sparseness-Hoyer",
            "title": {
                "fragments": [],
                "text": "Non-negative Matrix Factorization with Sparseness Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper shows how explicitly incorporating the notion of 'sparseness' improves the found decompositions, and provides complete MATLAB code both for standard NMF and for an extension of this technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "The latter type of non-informative descriptors (homogeneity) are detected by employing the sparseness measure of [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "The normalized loglikelihood surfaces are then combined by a weighted average with weights corresponding to the degree of sparseness [9] of these log-likelihood surfaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-negative matrix factorization with sparseness"
            },
            "venue": {
                "fragments": [],
                "text": "constraints. JMLR,"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 13,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Matching-Local-Self-Similarities-across-Images-and-Shechtman-Irani/5feb23e3c01c12c797b271fa5cd2a1f2f096130f?sort=total-citations"
}