{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720698"
                        ],
                        "name": "R. Kadobayashi",
                        "slug": "R.-Kadobayashi",
                        "structuredName": {
                            "firstName": "Rieko",
                            "lastName": "Kadobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kadobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109387168"
                        ],
                        "name": "Katsumi Tanaka",
                        "slug": "Katsumi-Tanaka",
                        "structuredName": {
                            "firstName": "Katsumi",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katsumi Tanaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Realityflythrough [McCurdy and Griswold 2005] uses interface ideas similar to ours for exploring video from camcorders instrumented with GPS and tilt sensors, and Kadobayashi and Tanaka [2005] present an interface for retrieving images using proximity to a virtual camera."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17328700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "618a051c12ba717a8611cfec3b7f6ea31c96af12",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new photo search method that uses three-dimensional (3D) viewpoints as queries. 3D viewpoint-based image retrieval is especially useful for searching collections of archaeological photographs,which contain many different images of the same object. Our method is designed to enable users to retrieve images that contain the same object but show a different view, and to browse groups of images taken from a similar viewpoint. We also propose using 3D scenes to query by example, which means that users do not have the problem of trying to formulate appropriate queries. This combination gives users an easy way of accessing not only photographs but also archived information."
            },
            "slug": "3D-viewpoint-based-photo-search-and-information-Kadobayashi-Tanaka",
            "title": {
                "fragments": [],
                "text": "3D viewpoint-based photo search and information browsing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new photo search method that uses three-dimensional (3D) viewpoints as queries to enable users to retrieve images that contain the same object but show a different view, and to browse groups of images taken from a similar viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698910"
                        ],
                        "name": "D. Aliaga",
                        "slug": "D.-Aliaga",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Aliaga",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Aliaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440142"
                        ],
                        "name": "Dimah Yanovsky",
                        "slug": "Dimah-Yanovsky",
                        "structuredName": {
                            "firstName": "Dimah",
                            "lastName": "Yanovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimah Yanovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716726"
                        ],
                        "name": "I. Carlbom",
                        "slug": "I.-Carlbom",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Carlbom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Carlbom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 195
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "\u2026et al. s [2003a] Sea of Im\u00adages work is perhaps \nclosest to ours in its use of a large collectionof images taken throughout an architectural space; the \nsame authors address the problem of computing consistent feature matches acrossmultiple images for the \npurposes of IBR [Aliaga et al. 2003b]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5738490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b5e6ca829def1c159c34e626351020d704af987",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-based rendering (IBR) systems enable virtual walkthroughs of photorealistic environments by warping and combining reference images to novel viewpoints under interactive user control. A significant challenge in such systems is to automatically compute image correspondences that enable accurate image warping.In this paper, we describe a new algorithm for computing a globally consistent set of image feature correspondences across a wide range of viewpoints suitable for IBR walkthroughs. We first detect point features in a dense set of omnidirectional images captured on an eye-height plane. Then, we track these features from image to image, identifying potential correspondences when two features track to the same position in the same image. Among the potential correspondences, we select the maximal consistent set using a greedy graph-labeling algorithm.A key feature of our approach is that it exploits the multiple paths that can be followed between images in order to increase the number of feature correspondences between distant images. We demonstrate the benefits of this approach in a real-time IBR walkthrough system where novel images are reconstructed as the user moves interactively."
            },
            "slug": "Interactive-image-based-rendering-using-feature-Aliaga-Yanovsky",
            "title": {
                "fragments": [],
                "text": "Interactive image-based rendering using feature globalization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new algorithm for computing a globally consistent set of image feature correspondences across a wide range of viewpoints suitable for IBR walkthroughs is described, exploiting the multiple paths that can be followed between images in order to increase the number of feature correspondence between distant images."
            },
            "venue": {
                "fragments": [],
                "text": "I3D '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787364"
                        ],
                        "name": "A. Rom\u00e1n",
                        "slug": "A.-Rom\u00e1n",
                        "structuredName": {
                            "firstName": "Augusto",
                            "lastName": "Rom\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rom\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750885"
                        ],
                        "name": "Gaurav Garg",
                        "slug": "Gaurav-Garg",
                        "structuredName": {
                            "firstName": "Gaurav",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gaurav Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801789"
                        ],
                        "name": "M. Levoy",
                        "slug": "M.-Levoy",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Levoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levoy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 164
                            }
                        ],
                        "text": "These efforts in\u00adclude the \n4D Cities project (www.cc.gatech.edu/4d-cities), whichaims to create a spatial-temporal model of Atlanta \nfrom histor\u00adical photographs; the Stanford CityBlock Project [Rom\u00b4an et al. 2004], which uses video of \ncity blocks to create multi-perspectivestrip images; and the UrbanScape project of Pollefeys and Nist\u00b4er \n(www.cs.unc.edu/Research/urbanscape/)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026(www.cc.gatech.edu/4d-cities), whichaims to create a spatial-temporal model of Atlanta \nfrom histor\u00adical photographs; the Stanford CityBlock Project [Rom\u00b4an et al. 2004], which uses video of \ncity blocks to create multi-perspectivestrip images; and the UrbanScape project of Pollefeys and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1861327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad4de98f0a91e9c9e5799e0ac8d34d89afc6449d",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiperspective images are a useful way to visualize extended, roughly planar scenes such as landscapes or city blocks. However, constructing effective multiperspective images is something of an art. We describe an interactive system for creating multiperspective images composed of serially blended cross-slits images. Beginning with a sideways-looking video of the scene as might be captured from a moving vehicle, we allow the user to interactively specify a set of cross-slits cameras, possibly with gaps between them. In each camera, one of the slits is defined to be the camera path, which is typically horizontal, and the user is left to choose the second slit, which is typically vertical. The system then generates intermediate views between these cameras using a novel interpolation scheme, thereby producing a multiperspective image with no seams. The user can also choose the picture surface in space onto which viewing rays are projected, thereby establishing a parameterization for the image. We show how the choice of this surface can be used to create interesting visual effects. We demonstrate our system by constructing multiperspective images that summarize city blocks, including corners, blocks with deep plazas and other challenging urban situations."
            },
            "slug": "Interactive-design-of-multi-perspective-images-for-Rom\u00e1n-Garg",
            "title": {
                "fragments": [],
                "text": "Interactive design of multi-perspective images for visualizing urban landscapes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes an interactive system for creating multiperspective images composed of serially blended cross-slits images, and demonstrates how the choice of this surface in space can be used to create interesting visual effects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Visualization 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778676"
                        ],
                        "name": "P. Debevec",
                        "slug": "P.-Debevec",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Debevec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Debevec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31589308"
                        ],
                        "name": "C. J. Taylor",
                        "slug": "C.-J.-Taylor",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Taylor",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2609415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37b0da92da8796835383f85c65cc81a386052a99",
            "isKey": false,
            "numCitedBy": 1992,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The first component is a photogrammetricmodeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach\u2019s ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs. CR Descriptors: I.2.10 [Artificial Intelligence]: Vision and Scene Understanding Modeling and recovery of physical attributes; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, shadowing, and texture I.4.8 [Image Processing]: Scene Analysis Stereo; J.6 [Computer-Aided Engineering]: Computer-aided design (CAD)."
            },
            "slug": "Modeling-and-rendering-architecture-from-a-hybrid-Debevec-Taylor",
            "title": {
                "fragments": [],
                "text": "Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs, which combines both geometry-based and imagebased techniques, and presents view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574494"
                        ],
                        "name": "D. Robertson",
                        "slug": "D.-Robertson",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Robertson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "Alternatively, a user can manually specify correspondences between points or cameras and locations in an image or map, as in [Robertson and Cipolla 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2456699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6971651a26532a942df4102113687c84859e94e",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an interactive system for creating geometric models from many uncalibrated images of architectural scenes. In this context, we must solve the structure from motion problem given only few and noisy feature correspondences in non-sequential views. By exploiting the strong constraints obtained by modelling a map as a single affine view of the scene, we are able to compute all 3D points and camera positions simultaneously as the solution of a set of linear equations. Reconstruction is achieved without making restrictive assumptions about the scene (such as that reference points or planes are visible in all views). We have implemented a practical interactive system, which has been used to make large-scale models of a variety of architectural scenes. We present quantitative and qualitative results obtained by this system."
            },
            "slug": "Building-Architectural-Models-from-Many-Views-Using-Robertson-Cipolla",
            "title": {
                "fragments": [],
                "text": "Building Architectural Models from Many Views Using Map Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An interactive system for creating geometric models from many uncalibrated images of architectural scenes is described, able to compute all 3D points and camera positions simultaneously as the solution of a set of linear equations."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687465"
                        ],
                        "name": "Mor Naaman",
                        "slug": "Mor-Naaman",
                        "structuredName": {
                            "firstName": "Mor",
                            "lastName": "Naaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mor Naaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750481"
                        ],
                        "name": "A. Paepcke",
                        "slug": "A.-Paepcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Paepcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Paepcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 19
                            }
                        ],
                        "text": "The LOCALE system \n[Naaman et al. 2003] uses prox\u00adimity to transfer labels between geo-referenced photographs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 18
                            }
                        ],
                        "text": "The LOCALE system [Naaman et al. 2003] uses proximity to transfer labels between geo-referenced photographs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17304150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f675fc7f6c837f373817feb4df17506f0ed54e59",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe LOCALE, a system that allows cooperating information systems to share labels for photographs. Participating photographs are enhanced with a geographic location stamp -- the latitude and longitude where the photograph was taken. For a photograph with no label, LOCALE can use the shared information to assign a label based on other photographs that were taken in the same area. LOCALE thus allows (i) text search over unlabeled sets of photos, and (ii) automated label suggestions for unlabeled photos. We have implemented a LOCALE prototype where users cooperate in submitting labels and locations, enhancing search quality for all users in the system. We ran an experiment to test the system in centralized and distributed settings. The results show that the system performs search tasks with surprising accuracy, even when searching for specific landmarks."
            },
            "slug": "From-Where-to-What:-Metadata-Sharing-for-Digital-Naaman-Paepcke",
            "title": {
                "fragments": [],
                "text": "From Where to What: Metadata Sharing for Digital Photographs with Geographic Coordinates"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results show that the LOCALE system performs search tasks with surprising accuracy, even when searching for specific landmarks."
            },
            "venue": {
                "fragments": [],
                "text": "OTM"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808594"
                        ],
                        "name": "A. Lippman",
                        "slug": "A.-Lippman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Lippman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lippman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "A forerunner to this .eld was the groundbreaking AspenMovieMap project [Lippman 1980], in which thousands \nof imagesof Aspen Colorado were captured from a moving car, registered toa street map of the city, and \nstored on laserdisc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "A forerunner to this field was the groundbreaking Aspen MovieMap project [Lippman 1980], in which thousands of images of Aspen Colorado were captured from a moving car, registered to a street map of the city, and stored on laserdisc."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16418443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fca2b4e8fbdde423747c6f48bd628f3f65628e7",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An interactive, dynamic map has been built using videodisc technology to engage the user in a simulated \u201cdrive\u201d through an unfamiliar space. The driver, or map reader, is presented with either sparsely sampled sequences of images taken by single frame cameras that replicate actual imagery from a space, or with computer synthesized replicas of those images. The reader may control the speed, route, angle of view and mode of presentation of this information and may thus tour the area. In addition, he may access spatially stored ancillary data stored in the buildings or in locales in the environment. This basic map is being enhanced to provide topographic views, and to incorporate optical and electronic image processing to provide a more responsive, visually complete representation of an environment."
            },
            "slug": "Movie-maps:-An-application-of-the-optical-videodisc-Lippman",
            "title": {
                "fragments": [],
                "text": "Movie-maps: An application of the optical videodisc to computer graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An interactive, dynamic map has been built using videodisc technology to engage the user in a simulated \u201cdrive\u201d through an unfamiliar space, and to incorporate optical and electronic image processing to provide a more responsive, visually complete representation of an environment."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '80"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31790073"
                        ],
                        "name": "Chris Buehler",
                        "slug": "Chris-Buehler",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145649511"
                        ],
                        "name": "M. Bosse",
                        "slug": "M.-Bosse",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bosse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145170879"
                        ],
                        "name": "L. McMillan",
                        "slug": "L.-McMillan",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "McMillan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. McMillan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415843"
                        ],
                        "name": "S. Gortler",
                        "slug": "S.-Gortler",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gortler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gortler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400248273"
                        ],
                        "name": "Michael F. Cohen",
                        "slug": "Michael-F.-Cohen",
                        "structuredName": {
                            "firstName": "Michael F.",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael F. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215780580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0112657f62ccbd036bae4c84cb41557ee2c8c80d",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an image based rendering approach that generalizes many current image based rendering algorithms, including light field rendering and view-dependent texture mapping. In particular, it allows for lumigraph-style rendering from a set of input cameras in arbitrary configurations (i.e., not restricted to a plane or to any specific manifold). In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. When presented with fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. The algorithm achieves this flexibility because it is designed to meet a set of specific goals that we describe. We demonstrate this flexibility with a variety of examples."
            },
            "slug": "Unstructured-lumigraph-rendering-Buehler-Bosse",
            "title": {
                "fragments": [],
                "text": "Unstructured lumigraph rendering"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An image based rendering approach that generalizes many current imagebased rendering algorithms, including light field rendering and view-dependent texture mapping, that allows for lumigraph-style rendering from a set of input cameras in arbitrary configurations."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724754"
                        ],
                        "name": "C. Dyer",
                        "slug": "C.-Dyer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dyer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 70
                            }
                        ],
                        "text": "1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6405929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7fe406304cfc2d12fd478a0dddeedcf853c6a3",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Image morphing techniques can generate compelling 2D transitions between images. However, differences in object pose or viewpoint often cause unnatural distortions in image morphs that are difficult to correct manually. Using basic principles of projective geometry, this paper introduces a simple extension to image morphing that correctly handles 3D projective camera and scene transformations. The technique, called view morphing, works by prewarping two images prior to computing a morph and then postwarping the interpolated images. Because no knowledge of 3D shape is required, the technique may be applied to photographs and drawings, as well as rendered scenes. The ability to synthesize changes both in viewpoint and image structure affords a wide variety of interesting 3D effects via simple image transformations. CR"
            },
            "slug": "View-morphing-Seitz-Dyer",
            "title": {
                "fragments": [],
                "text": "View morphing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces a simple extension to image morphing that correctly handles 3D projective camera and scene transformations and works by prewarping two images prior to computing a morph and then postwarped the interpolated images."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698910"
                        ],
                        "name": "D. Aliaga",
                        "slug": "D.-Aliaga",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Aliaga",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Aliaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440142"
                        ],
                        "name": "Dimah Yanovsky",
                        "slug": "Dimah-Yanovsky",
                        "structuredName": {
                            "firstName": "Dimah",
                            "lastName": "Yanovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimah Yanovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716726"
                        ],
                        "name": "I. Carlbom",
                        "slug": "I.-Carlbom",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Carlbom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Carlbom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 195
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1996; Levoy and Hanrahan 1996; Seitz and Dyer 1996; Aliaga et al. 2003a; Zitnick et al. 2004; Buehler et al. 2001]. In terms of applications, Aliaga et al.\u2019s [2003a] Sea of Images work is perhaps closest to ours in its use of a large collection of images taken throughout an architectural space; the same authors"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "\u2026et al. s [2003a] Sea of Im\u00adages work is perhaps \nclosest to ours in its use of a large collectionof images taken throughout an architectural space; the \nsame authors address the problem of computing consistent feature matches acrossmultiple images for the \npurposes of IBR [Aliaga et al. 2003b]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8978008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84d5b43e8691cd2bf830d1ff772c38cd0d2c5b9a",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "A long-standing research problem in computer graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively. On one hand, traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment. On the other hand, image-based rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects. In this paper, we present a \"sea of images,\" a practical approach to dense sampling, storage, and reconstruction of the plenoptic function in large, complex indoor environments. We use a motorized cart to capture omnidirectional images every few inches on a eye-height plane throughout an environment. The captured images are compressed and stored in a multiresolution hierarchy suitable for real-time prefetching during an interactive walkthrough. Later, novel images are reconstructed for a simulated observer by resampling nearby captured images. Our system acquires 15,254 images over 1,050 square feet at an average image spacing of 1.5 inches. The average capture and processing time is 7 hours. We demonstrate realistic walkthroughs of real-world environments reproducing specular reflections and occlusion effects while rendering 15-25 frames per second."
            },
            "slug": "Sea-of-images-Aliaga-Funkhouser",
            "title": {
                "fragments": [],
                "text": "Sea of images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A \"sea of images,\" a practical approach to dense sampling, storage, and reconstruction of the plenoptic function in large, complex indoor environments, using a motorized cart to capture omnidirectional images every few inches on a eye-height plane throughout an environment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Visualization, 2002. VIS 2002."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410019001"
                        ],
                        "name": "Shenchang Eric Chen",
                        "slug": "Shenchang-Eric-Chen",
                        "structuredName": {
                            "firstName": "Shenchang Eric",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenchang Eric Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2955375"
                        ],
                        "name": "L. Williams",
                        "slug": "L.-Williams",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Williams",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 70
                            }
                        ],
                        "text": "1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 204
                            }
                        ],
                        "text": "\u2026more chal\u00adlenging \nproblems of reconstructing full surface models [Debevecet al. 1996; Teller et al. 2003], light .elds \n[Gortler et al. 1996;Levoy and Hanrahan 1996], or pixel-accurate view interpolations[Chen and Williams \n1993; McMillan and Bishop 1995; Seitz andDyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7680709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f68cff502414a4ea054e154c880be150b2ca74eb",
            "isKey": false,
            "numCitedBy": 1271,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today\u2019s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism. Additional Keywords: image morphing, interpolation, virtual reality, motion blur, shadow, incremental rendering, real-time display, virtual holography, motion compensation."
            },
            "slug": "View-interpolation-for-image-synthesis-Chen-Williams",
            "title": {
                "fragments": [],
                "text": "View interpolation for image synthesis"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738740"
                        ],
                        "name": "S. B. Kang",
                        "slug": "S.-B.-Kang",
                        "structuredName": {
                            "firstName": "Sing",
                            "lastName": "Kang",
                            "middleNames": [
                                "Bing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. B. Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143711233"
                        ],
                        "name": "M. Uyttendaele",
                        "slug": "M.-Uyttendaele",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Uyttendaele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Uyttendaele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818882"
                        ],
                        "name": "S. Winder",
                        "slug": "S.-Winder",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Winder",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Winder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 216
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 70
                            }
                        ],
                        "text": "1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 274
                            }
                        ],
                        "text": "\u2026more chal\u00adlenging \nproblems of reconstructing full surface models [Debevecet al. 1996; Teller et al. 2003], light .elds \n[Gortler et al. 1996;Levoy and Hanrahan 1996], or pixel-accurate view interpolations[Chen and Williams \n1993; McMillan and Bishop 1995; Seitz andDyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 261631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a285a442317f24068f7cc262531976e82a5c41d6",
            "isKey": false,
            "numCitedBy": 1515,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates."
            },
            "slug": "High-quality-video-view-interpolation-using-a-Zitnick-Kang",
            "title": {
                "fragments": [],
                "text": "High-quality video view interpolation using a layered representation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms, and develops a novel temporal two-layer compressed representation that handles matting."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801789"
                        ],
                        "name": "M. Levoy",
                        "slug": "M.-Levoy",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Levoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144872229"
                        ],
                        "name": "P. Hanrahan",
                        "slug": "P.-Hanrahan",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Hanrahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanrahan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 150
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 20
                            }
                        ],
                        "text": "2003], light fields [Gortler et al. 1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "\u2026the more chal\u00adlenging \nproblems of reconstructing full surface models [Debevecet al. 1996; Teller et al. 2003], light .elds \n[Gortler et al. 1996;Levoy and Hanrahan 1996], or pixel-accurate view interpolations[Chen and Williams \n1993; McMillan and Bishop 1995; Seitz andDyer 1996; Zitnick et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1363510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bad341086b9f4fd66f4102c10f11f433f76a621f",
            "isKey": false,
            "numCitedBy": 3992,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views. We hav e created light fields from large arrays of both rendered and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. Once a light field has been created, new views may be constructed in real time by extracting slices in appropriate directions. Since the success of the method depends on having a high sample rate, we describe a compression system that is able to compress the light fields we have generated by more than a factor of 100:1 with very little loss of fidelity. We also address the issues of antialiasing during creation, and resampling during slice extraction. CR Categories: I.3.2 [Computer Graphics]: Picture/Image Generation \u2014 Digitizing and scanning, Viewing algorithms; I.4.2 [Computer Graphics]: Compression \u2014 Approximate methods Additional keywords: image-based rendering, light field, holographic stereogram, vector quantization, epipolar analysis"
            },
            "slug": "Light-field-rendering-Levoy-Hanrahan",
            "title": {
                "fragments": [],
                "text": "Light field rendering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views, and describes a compression system that is able to compress the light fields generated by more than a factor of 100:1 with very little loss of fidelity."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769685"
                        ],
                        "name": "K. Toyama",
                        "slug": "K.-Toyama",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Toyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Toyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40566669"
                        ],
                        "name": "R. Logan",
                        "slug": "R.-Logan",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Logan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Logan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057918"
                        ],
                        "name": "A. Roseway",
                        "slug": "A.-Roseway",
                        "structuredName": {
                            "firstName": "Asta",
                            "lastName": "Roseway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roseway"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 45
                            }
                        ],
                        "text": "In particular, the World-WideMedia Exchange [Toyama \net al. 2003] arranges images on an in\u00adteractive 2D map."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 45
                            }
                        ],
                        "text": "In particular, the World-Wide Media Exchange [Toyama et al. 2003] arranges images on an interactive 2D map."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16886868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "353fd4036c169c69649d753104de1bb8a4cbc534",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an end-to-end system that capitalizes on geographic location tags for digital photographs. The World Wide Media eXchange (WWMX) database indexes large collections of image media by several pieces of metadata including timestamp, owner, and critically, location stamp. The location where a photo was shot is important because it says much about its semantic content, while being relatively easy to acquire, index, and search.The process of building, browsing, and writing applications for such a database raises issues that have heretofore been un- addressed in either the multimedia or the GIS community. This paper brings all of these issues together, explores different options, and offers novel solutions where necessary. Topics include acquisition of location tags for image media, data structures for location tags on photos, database optimization for location-tagged image media, and an intuitive UI for browsing a massive location-tagged image database. We end by describing an application built on top of the WWMX, a lightweight travelogue-authoring tool that automatically creates appropriate context maps for a slideshow of location-tagged photographs."
            },
            "slug": "Geographic-location-tags-on-digital-images-Toyama-Logan",
            "title": {
                "fragments": [],
                "text": "Geographic location tags on digital images"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "An end-to-end system that capitalizes on geographic location tags for digital photographs, a lightweight travelogue-authoring tool that automatically creates appropriate context maps for a slideshow of location- tagged photographs, and an intuitive UI for browsing a massive location-tagged image database are described."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687465"
                        ],
                        "name": "Mor Naaman",
                        "slug": "Mor-Naaman",
                        "structuredName": {
                            "firstName": "Mor",
                            "lastName": "Naaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mor Naaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121610651"
                        ],
                        "name": "YeeJiun Song",
                        "slug": "YeeJiun-Song",
                        "structuredName": {
                            "firstName": "YeeJiun",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "YeeJiun Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750481"
                        ],
                        "name": "A. Paepcke",
                        "slug": "A.-Paepcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Paepcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Paepcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 12
                            }
                        ],
                        "text": "PhotoCompas [Naaman et al. 2004] clusters images based on time and location."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 13
                            }
                        ],
                        "text": "PhotoCompas [Naaman et al. 2004] clusters im\u00adages \nbased on time and location."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 564335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e6fad1ef3ba488578c01fc7ad36ac56a38c5906",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe PhotoCompas, a system that utilizes the time and location information embedded in digital photographs to automatically organize a personal photo collection. PhotoCompas produces browseable location and event hierarchies for the collection. These hierarchies are created using algorithms that interleave time and location to produce an organization that mimics the way people think about their photo collections. In addition, the algorithm annotates the generated hierarchy with geographical names. We tested our approach in case studies of three real-world collections and verified that the results are meaningful and useful for the collection owners."
            },
            "slug": "Automatic-organization-for-digital-photographs-with-Naaman-Song",
            "title": {
                "fragments": [],
                "text": "Automatic organization for digital photographs with geographic coordinates"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742208"
                        ],
                        "name": "M. Pollefeys",
                        "slug": "M.-Pollefeys",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pollefeys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollefeys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2989123"
                        ],
                        "name": "M. Vergauwen",
                        "slug": "M.-Vergauwen",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Vergauwen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vergauwen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2450841"
                        ],
                        "name": "F. Verbiest",
                        "slug": "F.-Verbiest",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Verbiest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Verbiest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804147"
                        ],
                        "name": "K. Cornelis",
                        "slug": "K.-Cornelis",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Cornelis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cornelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072921"
                        ],
                        "name": "Jan Tops",
                        "slug": "Jan-Tops",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Tops",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Tops"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144839904"
                        ],
                        "name": "R. Koch",
                        "slug": "R.-Koch",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 112
                            }
                        ],
                        "text": "Image-based modeling (IBM) is the process of creating threedimensional models from a collection of input images [Debevec et al. 1996; Grzeszczuk 2002; Pollefeys et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 176
                            }
                        ],
                        "text": "3.1 Image-based modeling \nImage-based modeling (IBM) is the process of creating three\u00addimensional models from a collection of input \nimages [Debevec et al. 1996; Grzeszczuk 2002; Pollefeys et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1219093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82ebff86c9f862522d5a78ecac3717243daddd43",
            "isKey": true,
            "numCitedBy": 1036,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a complete system to build visual models from camera images is presented. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through self-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data different types of visual models are constructed. Besides the traditional geometry- and image-based approaches, a combined approach with view-dependent geometry and texture is presented. As an application fusion of real and virtual scenes is also shown."
            },
            "slug": "Visual-Modeling-with-a-Hand-Held-Camera-Pollefeys-Gool",
            "title": {
                "fragments": [],
                "text": "Visual Modeling with a Hand-Held Camera"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A complete system to build visual models from camera images is presented and a combined approach with view-dependent geometry and texture is presented, as an application fusion of real and virtual scenes is also shown."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415843"
                        ],
                        "name": "S. Gortler",
                        "slug": "S.-Gortler",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gortler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gortler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400248273"
                        ],
                        "name": "Michael F. Cohen",
                        "slug": "Michael-F.-Cohen",
                        "structuredName": {
                            "firstName": "Michael F.",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael F. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 129
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 20
                            }
                        ],
                        "text": "2003], light fields [Gortler et al. 1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "As such, we side-step the more chal\u00adlenging \nproblems of reconstructing full surface models [Debevecet al. 1996; Teller et al. 2003], light .elds \n[Gortler et al. 1996;Levoy and Hanrahan 1996], or pixel-accurate view interpolations[Chen and Williams \n1993; McMillan and Bishop 1995; Seitz andDyer\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2036193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a737fbad8dd29730313c89ae1123efeab48786d",
            "isKey": false,
            "numCitedBy": 2700,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a new method for capturing the complete appearanceof both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subsetof the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation."
            },
            "slug": "The-lumigraph-Gortler-Grzeszczuk",
            "title": {
                "fragments": [],
                "text": "The lumigraph"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 58
                            }
                        ],
                        "text": "One particular system related to our work is Video Google [Sivic and Zisserman 2003] (not to be confused with Google\u2019s own video search), which allows a user to select a query object in one frame of video and efficiently find that object in other frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": "One particular system related to our work isVideo Google [Sivic and Zisserman \n2003] (not to be confused withGoogle s own video search), which allows a user to select a queryobject \nin one frame of video and ef.ciently .nd that object in otherframes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 41
                            }
                        ],
                        "text": "This type of search, applied \ntovideo in [Sivic and Zisserman 2003] is complementary to, and hascertain advantages over, keyword search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Video Google: A textretrieval approach to object matching \nin videos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 41
                            }
                        ],
                        "text": "This type of search, applied to video in [Sivic and Zisserman 2003] is complementary to, and has certain advantages over, keyword search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14457153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642e328cae81c5adb30069b680cf60ba6b475153",
            "isKey": true,
            "numCitedBy": 6760,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films."
            },
            "slug": "Video-Google:-a-text-retrieval-approach-to-object-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video Google: a text retrieval approach to object matching in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video, represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637486"
                        ],
                        "name": "M. Cooper",
                        "slug": "M.-Cooper",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Cooper",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774480"
                        ],
                        "name": "J. Foote",
                        "slug": "J.-Foote",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Foote",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Foote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195286"
                        ],
                        "name": "Andreas Girgensohn",
                        "slug": "Andreas-Girgensohn",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Girgensohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Girgensohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144115507"
                        ],
                        "name": "L. Wilcox",
                        "slug": "L.-Wilcox",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Wilcox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wilcox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 112
                            }
                        ],
                        "text": "Many of thesetechniques use metadata, such as keywords, photographer, or \ntime,as a basis of photo organization [Cooper et al. 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "Many of these techniques use metadata, such as keywords, photographer, or time, as a basis of photo organization [Cooper et al. 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5624190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "624d82c69c24e7d2dc84cf21afb9cbe008d4cb74",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according to one of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections."
            },
            "slug": "Temporal-event-clustering-for-digital-photo-Cooper-Foote",
            "title": {
                "fragments": [],
                "text": "Temporal event clustering for digital photo collections"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity."
            },
            "venue": {
                "fragments": [],
                "text": "TOMCCAP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 32
                            }
                        ],
                        "text": "Tools such as the ESP Game [von Ahn and Dab\u00adbish 2004] and LabelMe [Russell et al. 2005] encourage users \ntolabel images on the web, and have accumulated a database of an\u00adnotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720894"
                        ],
                        "name": "S. Teller",
                        "slug": "S.-Teller",
                        "structuredName": {
                            "firstName": "Seth",
                            "lastName": "Teller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809823"
                        ],
                        "name": "Matthew E. Antone",
                        "slug": "Matthew-E.-Antone",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Antone",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Antone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46857814"
                        ],
                        "name": "Z. Bodnar",
                        "slug": "Z.-Bodnar",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Bodnar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bodnar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145649511"
                        ],
                        "name": "M. Bosse",
                        "slug": "M.-Bosse",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bosse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913134"
                        ],
                        "name": "S. Coorg",
                        "slug": "S.-Coorg",
                        "structuredName": {
                            "firstName": "Satyan",
                            "lastName": "Coorg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Coorg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100853"
                        ],
                        "name": "M. Jethwa",
                        "slug": "M.-Jethwa",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Jethwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jethwa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30963470"
                        ],
                        "name": "N. Master",
                        "slug": "N.-Master",
                        "structuredName": {
                            "firstName": "Neel",
                            "lastName": "Master",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Master"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9448972,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "f68f580e176ad817f26525e682f10229bb8966ae",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a dataset of several thousand calibrated, time-stamped, geo-referenced, high dynamic range color images, acquired under uncontrolled, variable illumination conditions in an outdoor region spanning several hundred meters. The image data is grouped into several regions which have little mutual inter-visibility. For each group, the calibration data is globally consistent on average to roughly five centimeters and 0 1\u00b0, or about four pixels of epipolar registration. All image, feature and calibration data is available for interactive inspection and downloading at http://city.lcs.mit.edu/data.Calibrated imagery is of fundamental interest in a variety of applications. We have made this data available in the belief that researchers in computer graphics, computer vision, photogrammetry and digital cartography will find it of value as a test set for their own image registration algorithms, as a calibrated image set for applications such as image-based rendering, metric 3D reconstruction, and appearance recovery, and as input for existing GIS applications."
            },
            "slug": "Calibrated,-Registered-Images-of-an-Extended-Urban-Teller-Antone",
            "title": {
                "fragments": [],
                "text": "Calibrated, Registered Images of an Extended Urban Area"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A dataset of several thousand calibrated, time-stamped, geo-referenced, high dynamic range color images, acquired under uncontrolled, variable illumination conditions in an outdoor region spanning several hundred meters, which is of fundamental interest in a variety of applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054362383"
                        ],
                        "name": "B. Johansson",
                        "slug": "B.-Johansson",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Johansson and Cipolla [2002] have developed a system where a user can take a photograph, upload it to a server where it is compared to an image database, and receive location information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9754605,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f8ba2797f7663851610b4dd77e2f94521cdc01bb",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an automatic system for pose-estimation from a single image in a city scene. Each building has a model consisting of a number of parallel planes associated with it. The homographies for the best match of the planes to the image is estimated automatically for each of the possible buildings. We show how the estimation of the homographies can be done effectively by reducing the search space and using fast convolution. The model having the best match is then used to determine the position and orientation of the camera. The results of a number of experiments of the system in realistic circumstances is also presented"
            },
            "slug": "A-system-for-automatic-pose-estimation-from-a-image-Johansson-Cipolla",
            "title": {
                "fragments": [],
                "text": "A system for automatic pose-estimation from a single image in a city scene"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An automatic system for pose-estimation from a single image in a city scene where each building has a model consisting of a number of parallel planes associated with it and the model having the best match is used to determine the position and orientation of the camera."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "Tools such as the ESP Game [von Ahn and Dab\u00adbish 2004] and LabelMe [Russell et al. 2005] encourage users \ntolabel images on the web, and have accumulated a database of an\u00adnotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 66
                            }
                        ],
                        "text": "Tools such as the ESP Game [von Ahn and Dabbish 2004] and LabelMe [Russell et al. 2005] encourage users to label images on the web, and have accumulated a database of annotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": ") Annotation transfer has been also explored for video sequences [Irani and Anandan 1998]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "Annotation transfer hasbeen \nalso explored for video sequences [Irani and Anandan 1998]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14510147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc41ea3fb76475eff0f9d6e3cda34bf0406ce039",
            "isKey": false,
            "numCitedBy": 323,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Video is a rich source of information. It provides visual information about scenes. This information is implicitly buried inside the raw video data, however, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a movie mode, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data are first transformed from their sequential and redundant frame-based representation, in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports nonlinear browsing and efficient indexing to provide rapid access directly to information of interest. This paper describes a new set of methods for indexing into the video sequence based on the scene-based representation. These indexing methods are based on geometric and dynamic information contained in the video. These methods complement the more traditional content-based indexing methods, which utilize image appearance information (namely, color and texture properties) but are considerably simpler to achieve and are highly computationally efficient."
            },
            "slug": "Video-indexing-based-on-mosaic-representations-Irani-Anandan",
            "title": {
                "fragments": [],
                "text": "Video indexing based on mosaic representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new set of methods for indexing into the video sequence based on the scene-based representation, based on geometric and dynamic information contained in the video, complement the more traditional content-based indexing methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145170879"
                        ],
                        "name": "L. McMillan",
                        "slug": "L.-McMillan",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "McMillan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. McMillan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144052443"
                        ],
                        "name": "G. Bishop",
                        "slug": "G.-Bishop",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Bishop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 104
                            }
                        ],
                        "text": "More recent work in IBR \nhas focused on techniques for new viewsynthesis, e.g., [Chen and Williams 1993; McMillan and Bishop1995; \nGortler et al. 1996; Levoy and Hanrahan 1996; Seitz andDyer 1996; Aliaga et al. 2003a; Zitnick et al. \n2004; Buehler et al.2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 70
                            }
                        ],
                        "text": "1996; Levoy and Hanrahan 1996], or pixel-accurate view interpolations [Chen and Williams 1993; McMillan and Bishop 1995; Seitz and Dyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 228
                            }
                        ],
                        "text": "\u2026more chal\u00adlenging \nproblems of reconstructing full surface models [Debevecet al. 1996; Teller et al. 2003], light .elds \n[Gortler et al. 1996;Levoy and Hanrahan 1996], or pixel-accurate view interpolations[Chen and Williams \n1993; McMillan and Bishop 1995; Seitz andDyer 1996; Zitnick et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10224615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49cb11a6c98a610efb7bdadae0a0e0fd716710c7",
            "isKey": false,
            "numCitedBy": 1579,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the \u201cplenoptic function\u201d of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections."
            },
            "slug": "Plenoptic-modeling:-an-image-based-rendering-system-McMillan-Bishop",
            "title": {
                "fragments": [],
                "text": "Plenoptic modeling: an image-based rendering system"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function is presented and a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145246674"
                        ],
                        "name": "A. Heyden",
                        "slug": "A.-Heyden",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Heyden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Heyden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742208"
                        ],
                        "name": "M. Pollefeys",
                        "slug": "M.-Pollefeys",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pollefeys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollefeys"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 104
                            }
                        ],
                        "text": "During each RANSAC iteration, we compute a candidate fundamental matrix using the eight-point algorithm [Hartley and Zisserman 2004], followed by non-linear refinement."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 195
                            }
                        ],
                        "text": "Our image\u00adbased modeling system \nis based on recent work in structure from motion (SfM), which aims to recover camera parameters, pose \nesti\u00admates, and sparse 3D scene geometry from image sequences [Hart\u00adley and Zisserman 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 214
                            }
                        ],
                        "text": "We select thecamera that observes \nthe largest number of tracks whose 3D loca\u00adtions have already been estimated, and initialize the new \ncamera sextrinsic parameters using the direct linear transform (DLT) tech\u00adnique [Hartley and Zisserman \n2004] inside a RANSAC procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 215
                            }
                        ],
                        "text": "We select the camera that observes the largest number of tracks whose 3D locations have already been estimated, and initialize the new camera\u2019s extrinsic parameters using the direct linear transform (DLT) technique [Hartley and Zisserman 2004] inside a RANSAC procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 194
                            }
                        ],
                        "text": "Our imagebased modeling system is based on recent work in structure from motion (SfM), which aims to recover camera parameters, pose estimates, and sparse 3D scene geometry from image sequences [Hartley and Zisserman 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 104
                            }
                        ],
                        "text": "During each RANSAC \niteration, wecompute a candidate fundamental matrix using the eight-point al\u00adgorithm [Hartley and Zisserman \n2004], followed by non-linear re\u00ad.nement."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16679566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee84ae7d9a2822709da1554f712736263c9db63",
            "isKey": true,
            "numCitedBy": 506,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "There exist intricate geometric relations between multiple views of a 3D scene. These relations are related to the camera motion and calibration as well as to the scene structure. In this chapter we introduce these concepts and discuss how they can be applied to recover 3D models from images. In Section 3.2 a rather thorough description of projective geometry is given. Section 3.3 gives a short introduction to tensor calculus and Section 3.4 describes in detail the camera model used. In Section 3.5 a modern approach to multiple view geometry is presented and in Section 3.6 simple structure and motion algorithms are presented. In Section 3.7 more advanced algorithms are presented that are suited for automatic processing on real image data. Section 3.8 discusses the possibility of calibrating the camera from images. Section 3.9 describes how the depth can be computed for most image pixels and Section 3.10 presents how the results of the previous sections can be combined to yield 3D models, render novel views or combine real and virtual elements in video. 45"
            },
            "slug": "Multiple-view-geometry-Heyden-Pollefeys",
            "title": {
                "fragments": [],
                "text": "Multiple view geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This chapter introduces the concepts of projective geometry and tensor calculus and discusses how they can be applied to recover 3D models from images to yield models suited for automatic processing on real image data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735785"
                        ],
                        "name": "Matthew A. Brown",
                        "slug": "Matthew-A.-Brown",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brown",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew A. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, our SfM approach is similar to that of Brown and Lowe [2005], with several modifications to improve scalability and robustness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our approach is similar to that of Brown and Lowe [2005], with several motifications to improve scalability and robustness, plus additional geo-registration capabilities."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14196251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e30dcbc6998e1710c342fedffd440babaabdfd01",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a system for fully automatic recognition and reconstruction of 3D objects in image databases. We pose the object recognition problem as one of finding consistent matches between all images, subject to the constraint that the images were taken from a perspective camera. We assume that the objects or scenes are rigid. For each image, we associate a camera matrix, which is parameterised by rotation, translation and focal length. We use invariant local features to find matches between all images, and the RANSAC algorithm to find those that are consistent with the fundamental matrix. Objects are recognised as subsets of matching images. We then solve for the structure and motion of each object, using a sparse bundle adjustment algorithm. Our results demonstrate that it is possible to recognise and reconstruct 3D objects from an unordered image database with no user input at all."
            },
            "slug": "Unsupervised-3D-object-recognition-and-in-unordered-Brown-Lowe",
            "title": {
                "fragments": [],
                "text": "Unsupervised 3D object recognition and reconstruction in unordered datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper presents a system for fully automatic recognition and reconstruction of 3D objects in image databases, using invariant local features to find matches between all images, and the RANSAC algorithm to find those that are consistent with the fundamental matrix."
            },
            "venue": {
                "fragments": [],
                "text": "Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809403"
                        ],
                        "name": "S. Feiner",
                        "slug": "S.-Feiner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Feiner",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Feiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145931052"
                        ],
                        "name": "B. MacIntyre",
                        "slug": "B.-MacIntyre",
                        "structuredName": {
                            "firstName": "Blair",
                            "lastName": "MacIntyre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. MacIntyre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743721"
                        ],
                        "name": "Tobias H\u00f6llerer",
                        "slug": "Tobias-H\u00f6llerer",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "H\u00f6llerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias H\u00f6llerer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49215948"
                        ],
                        "name": "A. Webster",
                        "slug": "A.-Webster",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Webster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Webster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "In combination \nwith a head-mounted display, such a capability couldoffer a highly portable, computer-vision-based augmented \nrealitysystem [Feiner et al. 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 74
                            }
                        ],
                        "text": "This goal is similar \nto that of aug\u00admented reality (AR) approaches (e.g., [Feiner et al. 1997]), whichalso seek to annotate \nimages."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 140
                            }
                        ],
                        "text": "In combination with a head-mounted display, such a capability could offer a highly portable, computer-vision-based augmented reality system [Feiner et al. 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": ", [Feiner et al. 1997]), which also seek to annotate images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2108280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc9404cb021d7f859271de89aec145dd8b0fee87",
            "isKey": true,
            "numCitedBy": 914,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a prototype system that combines the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university's campus, using a head-tracked, see-through, head-worn, 3D display, and an untracked, opaque, hand-held, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs."
            },
            "slug": "A-touring-machine:-Prototyping-3D-mobile-augmented-Feiner-MacIntyre",
            "title": {
                "fragments": [],
                "text": "A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A prototype system that combines the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing is described, to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world."
            },
            "venue": {
                "fragments": [],
                "text": "Digest of Papers. First International Symposium on Wearable Computers"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064190"
                        ],
                        "name": "K. Rodden",
                        "slug": "K.-Rodden",
                        "structuredName": {
                            "firstName": "Kerry",
                            "lastName": "Rodden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rodden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053320673"
                        ],
                        "name": "Kenneth R. Wood",
                        "slug": "Kenneth-R.-Wood",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Wood",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth R. Wood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3202587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9e77e4cd53762ef4212807770a699aefd50e411",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present and discuss the findings of a study that investigated how people manage their collections of digital photographs. The six-month, 13-participant study included interviews, questionnaires, and analysis of usage statistics gathered from an instrumented digital photograph management tool called Shoebox. Alongside simple browsing features such as folders, thumbnails and timelines, Shoebox has some advanced multimedia features: content-based image retrieval and speech recognition applied to voice annotations. Our results suggest that participants found their digital photos much easier to manage than their non-digital ones, but that this advantage was almost entirely due to the simple browsing features. The advanced features were not used very often and their perceived utility was low. These results should help to inform the design of improved tools for managing personal digital photographs."
            },
            "slug": "How-do-people-manage-their-digital-photographs-Rodden-Wood",
            "title": {
                "fragments": [],
                "text": "How do people manage their digital photographs?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results suggest that participants found their digital photos much easier to manage than their non-digital ones, but that this advantage was almost entirely due to the simple browsing features of Shoebox."
            },
            "venue": {
                "fragments": [],
                "text": "CHI '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775242"
                        ],
                        "name": "Frederik Schaffalitzky",
                        "slug": "Frederik-Schaffalitzky",
                        "structuredName": {
                            "firstName": "Frederik",
                            "lastName": "Schaffalitzky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederik Schaffalitzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1699616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e81996384b030b580a0e02c0dc367d59c0c15ba",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been considerable success in automated reconstruction for image sequences where small baseline algorithms can be used to establish matches across a number of images. In contrast in the case of widely separated views, methods have generally been restricted to two or three views.In this paper we investigate the problem of establishing relative viewpoints given a large number of images where no ordering information is provided. A typical application would be where images are obtained from different sources or at different times: both the viewpoint (position, orientation, scale) and lighting conditions may vary significantly over the data set.Such a problem is not fundamentally amenable to exhaustive pair wise and triplet wide baseline matching because this would be prohibitively expensive as the number of views increases. Instead, we investiate how a combination of image invariants, covariants, and multiple view relations can be used in concord to enable efficient multiple view matching. The result is a matching algorithm which is linear in the number of views.The methods are illustrated on several real image data sets. The output enables an image based technique for navigating in a 3D scene, moving from one image to whichever image is the next most appropriate."
            },
            "slug": "Multi-view-Matching-for-Unordered-Image-Sets,-or-Do-Schaffalitzky-Zisserman",
            "title": {
                "fragments": [],
                "text": "Multi-view Matching for Unordered Image Sets, or \"How Do I Organize My Holiday Snaps?\""
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper invests how a combination of image invariants, covariants, and multiple view relations can be used in concord to enable efficient multiple view matching and produces a matching algorithm which is linear in the number of views."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 191
                            }
                        ],
                        "text": "After reconstructing a scene, we optionally run a postprocess step in which we detect 3D line segments in the scene, for use in rendering, using a line segment reconstruction technique as in [Schmid and Zisserman 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7625742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3b2c316a2d89d2e38263de87bdb0738404d2214",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a new method for matching individual line segments between images. The method uses both grey-level information and the multiple view geometric relations between the images. For image pairs epipolar geometry facilitates the computation of a cross-correlation based matching score for putative line correspondences. For image triplets cross-correlation matching scores are used in conjunction with line transfer based on the trifocal geometry. Algorithms are developed for both short and long range motion. In the case of long range motion the algorithm involves evaluating a one parameter family of plane induced homographies. The algorithms are robust to deficiencies in the line segment extraction and partial occlusion. Experimental results are given for image pairs and triplets, for varying motions between views, and for different scene types. The three view algorithm eliminates all mismatches."
            },
            "slug": "Automatic-line-matching-across-views-Schmid-Zisserman",
            "title": {
                "fragments": [],
                "text": "Automatic line matching across views"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The paper presents a new method for matching individual line segments between images that uses both grey-level information and the multiple view geometric relations between the images and eliminates all mismatches."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 251
                            }
                        ],
                        "text": "We select thecamera that observes \nthe largest number of tracks whose 3D loca\u00adtions have already been estimated, and initialize the new \ncamera sextrinsic parameters using the direct linear transform (DLT) tech\u00adnique [Hartley and Zisserman \n2004] inside a RANSAC procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 78
                            }
                        ],
                        "text": "[1998], then robustly estimate a fundamental matrix for the pair using RANSAC [Fischler and Bolles 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 70
                            }
                        ],
                        "text": "For each camera Cj , we .rst robustly .t a plane to Points(Cj ) using \nRANSAC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "During each RANSAC \niteration, wecompute a candidate fundamental matrix using the eight-point al\u00adgorithm [Hartley and Zisserman \n2004], followed by non-linear re\u00ad.nement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 217
                            }
                        ],
                        "text": "Next, for each pair ofimages, we match keypoint descriptors \nbetween the pair, using theapproximate nearest neighbors package of Arya, et al.[1998], thenrobustly \nestimate a fundamental matrix for the pair using RANSAC[Fischler and Bolles 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 146
                            }
                        ],
                        "text": "Next, to compute Eangle, we .rst attempt to .nd a dominant plane in Pinliers by .tting a plane to the \npoints using orthogonal re\u00adgression inside a RANSAC loop."
                    },
                    "intents": []
                }
            ],
            "corpusId": 972888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278c9a78d4505cfaf6b709df364dbd1206a017c1",
            "isKey": true,
            "numCitedBy": 15955,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing"
            },
            "slug": "Random-sample-consensus:-a-paradigm-for-model-with-Fischler-Bolles",
            "title": {
                "fragments": [],
                "text": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "New results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form that provide the basis for an automatic system that can solve the Location Determination Problem under difficult viewing."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2699095"
                        ],
                        "name": "A. Dick",
                        "slug": "A.-Dick",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dick",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15419087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ce9055afce0fe60e6094070e1404f57ef6e578",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the automatic acquisition of three dimensional architectural models from short image sequences. The approach is Bayesian and model based. Bayesian methods necessitate the formulation of a prior distribution; however designing a generative model for buildings is a difficult task. In order to overcome this a building is described as a set of walls together with a \u2018Lego\u2019 kit of parameterised primitives, such as doors or windows. A prior on wall layout, and a prior on the parameters of each primitive can then be defined. Part of this prior is learnt from training data and part comes from expert architects. The validity of the prior is tested by generating example buildings using MCMC and verifying that plausible buildings are generated under varying conditions. The same MCMC machinery can also be used for optimising the structure recovery, this time generating a range of possible solutions from the posterior. The fact that a range of solutions can be presented allows the user to select the best when the structure recovery is ambiguous."
            },
            "slug": "Modelling-and-Interpretation-of-Architecture-from-Dick-Torr",
            "title": {
                "fragments": [],
                "text": "Modelling and Interpretation of Architecture from Several Images"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes the automatic acquisition of three dimensional architectural models from short image sequences using Bayesian and model based methods and proves the validity of the prior by verifying that plausible buildings are generated under varying conditions."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573256"
                        ],
                        "name": "Neil J. McCurdy",
                        "slug": "Neil-J.-McCurdy",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "McCurdy",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil J. McCurdy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733025"
                        ],
                        "name": "W. Griswold",
                        "slug": "W.-Griswold",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Griswold",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Griswold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 18
                            }
                        ],
                        "text": "Realityflythrough [McCurdy and Griswold 2005] uses interface ideas similar to ours for exploring video from camcorders instrumented with GPS and tilt sensors, and Kadobayashi and Tanaka [2005] present an interface for retrieving images using proximity to a virtual camera."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5536619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00686a31db24cb6f96b06c4aeefadc8572ee4a13",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Realityflythrough is a telepresence/tele-reality system that works in the dynamic, uncalibrated environments typically associated with ubiquitous computing. By harnessing networked mobile video cameras, it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements."
            },
            "slug": "A-systems-architecture-for-ubiquitous-video-McCurdy-Griswold",
            "title": {
                "fragments": [],
                "text": "A systems architecture for ubiquitous video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: its support of the abstractions for infinite camera coverage, its scalability, and its robustness to changing user requirements."
            },
            "venue": {
                "fragments": [],
                "text": "MobiSys '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543680"
                        ],
                        "name": "L. Chew",
                        "slug": "L.-Chew",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Chew",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 91
                            }
                        ],
                        "text": "The projections of Lines(Cj ) into Ij are \nimposed as edge constraintson the triangulation [Chew 1987]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 90
                            }
                        ],
                        "text": "The projections of Lines(Cj) into Ij are imposed as edge constraints on the triangulation [Chew 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6317590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "351947b7bd97febfbc66001db9c6f72b6e6d2ff5",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a set of n vertices in the plane together with a set of noncrossing edges, the constrained Delaunay triangulation (CDT) is the triangulation of the vertices with the following properties: (1) the prespecified edges are included in the triangulation, and (2) it is as close as possible to the Delaunay triangulation. We show that the CDT can be built in optimal &Ogr;(n log n) time using a divide-and-conquer technique. This matches the time required to build an arbitrary (unconstrained) Delaunay triangulation and the time required to build an arbitrary constrained (nonDelaunay) triangulation. CDTs, because of their relationship with Delaunay triangulations, have a number of properties that should make them useful for the finite-element method. Applications also include motion planning in the presence of polygonal obstacles in the plane and constrained Euclidean minimum spanning trees, spanning trees subject to the restriction that some edges are prespecified."
            },
            "slug": "Constrained-Delaunay-triangulations-Chew",
            "title": {
                "fragments": [],
                "text": "Constrained Delaunay triangulations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is shown that the constrained Delaunay triangulation (CDT) can be built in optimal &Ogr;(n log n) time using a divide-and-conquer technique, which matches the time required to build an arbitrary (unconstrained) Delaunays and an arbitrary constrained (nonDelaunay) triangulations."
            },
            "venue": {
                "fragments": [],
                "text": "SCG '87"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775242"
                        ],
                        "name": "Frederik Schaffalitzky",
                        "slug": "Frederik-Schaffalitzky",
                        "structuredName": {
                            "firstName": "Frederik",
                            "lastName": "Schaffalitzky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederik Schaffalitzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264779"
                        ],
                        "name": "T. Kadir",
                        "slug": "T.-Kadir",
                        "structuredName": {
                            "firstName": "Timor",
                            "lastName": "Kadir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kadir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6794491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "514b8c50a5b427e2aae75f877454ec9ab3cb4e99",
            "isKey": false,
            "numCitedBy": 3358,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris\u00a0 (Mikolajczyk and \u00a0Schmid, 2002; Schaffalitzky and \u00a0Zisserman, 2002) and Hessian points\u00a0 (Mikolajczyk and \u00a0Schmid, 2002), a detector of \u2018maximally stable extremal regions', proposed by Matas et al.\u00a0(2002); an edge-based region detector\u00a0 (Tuytelaars and Van\u00a0Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van\u00a0Gool, 2000), and a detector of \u2018salient regions', proposed by Kadir, Zisserman and Brady\u00a0(2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression.The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework."
            },
            "slug": "A-Comparison-of-Affine-Region-Detectors-Mikolajczyk-Tuytelaars",
            "title": {
                "fragments": [],
                "text": "A Comparison of Affine Region Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions to establish a reference test set of images and performance software so that future detectors can be evaluated in the same framework."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833354"
                        ],
                        "name": "Drew Steedly",
                        "slug": "Drew-Steedly",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Steedly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Drew Steedly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038264"
                        ],
                        "name": "F. Dellaert",
                        "slug": "F.-Dellaert",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dellaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dellaert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "Additionally, we can improve ef.ciency \nusing partition\u00ading methods [Steedly et al. 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 67
                            }
                        ],
                        "text": "Additionally, we can improve efficiency using partitioning methods [Steedly et al. 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2543530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3704ce14a227d2aebd48bf99965194a7befa8fc2",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvector. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms."
            },
            "slug": "Spectral-partitioning-for-structure-from-motion-Steedly-Essa",
            "title": {
                "fragments": [],
                "text": "Spectral partitioning for structure from motion"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized, and takes advantage of more information than generic matrix partitioning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724709"
                        ],
                        "name": "S. Arya",
                        "slug": "S.-Arya",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Arya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Arya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709509"
                        ],
                        "name": "D. Mount",
                        "slug": "D.-Mount",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mount",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mount"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712586"
                        ],
                        "name": "N. Netanyahu",
                        "slug": "N.-Netanyahu",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Netanyahu",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Netanyahu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37746341"
                        ],
                        "name": "R. Silverman",
                        "slug": "R.-Silverman",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "Silverman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Silverman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736712"
                        ],
                        "name": "A. Wu",
                        "slug": "A.-Wu",
                        "structuredName": {
                            "firstName": "Angela",
                            "lastName": "Wu",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "We match keypoint descriptors between each pair of images, using the approximate nearest neighbors package of [Arya et al. 1998]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8193729,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "219101fe724232acc330ff0910152931538f85c7",
            "isKey": false,
            "numCitedBy": 2723,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a set of <italic>S</italic> of <italic>n</italic> data points  in real <italic>d</italic>-dimensional space, R<supscrpt>d</supscrpt>, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess <italic>S</italic> into a data structure, so that given any query point <italic>q</italic><inline-equation> <f>\u2208</f></inline-equation> R<supscrpt>d</supscrpt>, is the closest point of S to <italic>q</italic> can be reported quickly. Given any positive real \u03b5, data point <italic>p</italic> is a (1 +\u03b5)-<italic>approximate nearest neighbor</italic> of <italic>q</italic> if its distance from <italic>q</italic> is within a factor of (1 + \u03b5) of the distance to the true nearest neighbor. We show that it is possible to preprocess a    set of <italic>n</italic> points in     R<supscrpt>d</supscrpt> in <italic>O(dn</italic> log <italic>n</italic>) time and <italic>O(dn)</italic> space, so that given a query point <italic> q</italic> <inline-equation> <f>\u2208</f></inline-equation> R<supscrpt>d</supscrpt>, and \u03b5 > 0, a (1 + \u03b5)-approximate nearest neighbor of <italic>q</italic> can be computed in <italic>O</italic>(<italic>c</italic><subscrpt><italic>d</italic>, \u03b5</subscrpt> log <italic>n</italic>) time, where <italic>c<subscrpt>d,\u03b5</subscrpt></italic>\u2264<italic>d</italic> <inline-equation> <f><fen lp=\"ceil\">1 + 6d/<g>e</g><rp post=\"ceil\"></fen></f></inline-equation>;<supscrpt>d</supscrpt> is a factor depending only on dimension and \u03b5. In general, we show that given an integer <italic>k</italic> \u2265 1, (1 + \u03b5)-approximations  to the  <italic>k</italic> nearest neighbors of <italic>q</italic> can  be computed in additional <italic>O(kd</italic> log <italic>n</italic>) time."
            },
            "slug": "An-optimal-algorithm-for-approximate-nearest-fixed-Arya-Mount",
            "title": {
                "fragments": [],
                "text": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that it is possible to preprocess a set of data points in real D-dimensional space in O(kd) time and in additional space, so that given a query point q, the closest point of S to S to q can be reported quickly."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 34
                            }
                        ],
                        "text": "We use theSIFT \nkeypoint detector [Lowe 2004], because of its invariance toimage transformations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 34
                            }
                        ],
                        "text": "We use the SIFT keypoint detector [Lowe 2004], because of its invariance to image transformations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25501,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157892296"
                        ],
                        "name": "D. K. Smith",
                        "slug": "D.-K.-Smith",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Smith",
                            "middleNames": [
                                "K.",
                                "Skip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 149
                            }
                        ],
                        "text": "This minimization \nproblem is formulated as a non-linearleast squares problem (see Appendix A) and solved with algorithmssuch \nas Levenberg-Marquardt [Nocedal and Wright 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "This minimization problem is formulated as a non-linear least squares problem (see Appendix A) and solved with algorithms such as Levenberg-Marquardt [Nocedal and Wright 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 189864167,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bf86896c23300a46b7fc76298e365984c0b05105",
            "isKey": false,
            "numCitedBy": 10988,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "no exception. MRP II and JIT=TQC in purchasing and supplier education are covered in Chapter 15. Without proper education MRP II and JIT=TQC will not be successful and will not generate their true bene\u00aets. Suppliers are key to the success of MRP II and JIT=TQC. They therefore need to understand these disciplines. Purchasing in the 21st century is going to be marked by continuous changes, by who can gain the competitive edge \u00aerst, who will be the most \u0304exible and who will build the best supplier relationships. This will only be achieved by following the process as described in Schorr in a step by step fashion. An organization must however be willing to, as Schorr states in Chapter 16, `create the spark, ignite change'! Only then can it happen! If you really want to know something about purchasing then this is the book to read. It is most de\u00aenitely relevant and more importantly up to date. It will certainly be a handy reference book for a course on purchasing."
            },
            "slug": "Numerical-Optimization-Smith",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "J. Oper. Res. Soc."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58259115,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c9ddd155770b3f79f6fdd537eb877b32a5c35815",
            "isKey": false,
            "numCitedBy": 2211,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimization is an important tool used in decision science and for the analysis of physical systems used in This space but at the book requires substantial background. Numerical optimization presents a graduate text, in continuous presents. Mmor mathematical optimization in understanding of initiative. I acknowledge the necessary to use, optimization it talks extensively about algorithmic performance and thinking. The optimization they delve into, the new ideas progressing through more thoroughly updated throughout."
            },
            "slug": "Numerical-Optimization-(Springer-Series-in-Research-Nocedal-Wright",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization (Springer Series in Operations Research and Financial Engineering)"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Numerical optimization presents a graduate text, in continuous presents, that talks extensively about algorithmic performance and thinking, and about mathematical optimization in understanding of initiative."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 112
                            }
                        ],
                        "text": "Image-based modeling (IBM) is the process of creating threedimensional models from a collection of input images [Debevec et al. 1996; Grzeszczuk 2002; Pollefeys et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 159
                            }
                        ],
                        "text": "3.1 Image-based modeling \nImage-based modeling (IBM) is the process of creating three\u00addimensional models from a collection of input \nimages [Debevec et al. 1996; Grzeszczuk 2002; Pollefeys et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Course 44: Image-based modeling"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2002."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 83
                            }
                        ],
                        "text": "To assist the user, we estimate the \u201cup\u201d or gravity vector by either the method of [Szeliski 2005] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image alignment and stitching: A tutorial"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. MSR-TR-2004-92, Microsoft Research."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiple view geometry in computer vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "photographs copyright 2005"
            },
            "venue": {
                "fragments": [],
                "text": "Daryoush Mansouri Paul Meidinger Laurete de Albuquerque Mouazan"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 102
                            }
                        ],
                        "text": "To minimize the objective function at every iteration, we use the sparse bundle adjustment library of [Lourakis and Argyros 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design and implementation of a generic sparse bundle adjustment software package based on the levenberg-marquardt algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. 340, Inst. of Computer Science-FORTH, Heraklion, Crete, Greece. Available from www.ics.forth.gr/ \u0303lourakis/sba."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 112
                            }
                        ],
                        "text": "Image-based modeling (IBM) is the process of creating threedimensional models from a collection of input images [Debevec et al. 1996; Grzeszczuk 2002; Pollefeys 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Course 54: Obtaining 3d models with a hand-held camera"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2002."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Course 44: Image-based modeling"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 30,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 48,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Photo-tourism:-exploring-photo-collections-in-3D-Snavely-Seitz/b5ebf37ce170f13a905f7feba9fb7096b49fb8b3?sort=total-citations"
}