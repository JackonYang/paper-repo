{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ar X\niv :c\nm p-\nlg /9\n70 60\n03 v2\n7 J\nun 1\n99 7"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "In this paper, we present a flexible probabilistic parser that simultaneously assigns both part-ofspeech tags and a bare-bones dependency structure (illustrated in Figure 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5598810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "slug": "Towards-History-based-Grammars:-Using-Richer-Models-Black-Jelinek",
            "title": {
                "fragments": [],
                "text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way and significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3265631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4c0e02d99de82149efd719260e5a5549a13854a",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper, with details of how the results were obtained. Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependencygrammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n 3 ) parsing algorithm. The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences. As reported at the talk, the more extensive training yields greatly improved performance, cutting in half the error rate of Eisner (1996). Nearly half the sentences are parsed with no misattachments; two-thirds of sentences are parsed with at most one misattachment. Of the models described in the original paper, the best score is obtained with the generative \u201cmodel C,\u201d which attaches 87\u201388% of all words to the correct parent. However, better models are also explored, in particular, two simple variants on the comprehension \u201cmodel B.\u201d The better of these has an attachment accuracy of 90%, and (unlike model C) tags words more accurately than the comparable trigram tagger. If tags are roughly known in advance, search error is all but eliminated and the new model attains an attachment accuracy of 93%. We find that the parser of Collins (1996), when combined with a highlytrained tagger, also achieves 93% when trained and tested on the same sentences. We briefly discuss the similarities and differences between Collins\u2019s model and ours, pointing out the strengths of each and noting that these strengths could be combined for either dependency parsing or phrase-structure parsing."
            },
            "slug": "An-Empirical-Comparison-of-Probability-Models-for-Eisner",
            "title": {
                "fragments": [],
                "text": "An Empirical Comparison of Probability Models for Dependency Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences, finding that the parser of Collins (1996), when combined with a highlytrained tagger, also achieves 93% when trained and tested on the same sentences."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "But model C says that speakers\u2019 primary goal is to flesh out the syntactic and conceptual structure for each word they utter, surrounding it with arguments, modifiers, and function words as appropriate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ar X\niv :c\nm p-\nlg /9\n70 60\n03 v2\n7 J\nun 1\n99 7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length."
            },
            "slug": "Statistical-Decision-Tree-Models-for-Parsing-Magerman",
            "title": {
                "fragments": [],
                "text": "Statistical Decision-Tree Models for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SPATTER is described, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5429505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d607ed3aa8a1762e06988329aeb0c05b997023db",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars."
            },
            "slug": "Stochastic-Lexicalized-Tree-adjoining-Grammars-Schabes",
            "title": {
                "fragments": [],
                "text": "Stochastic Lexicalized Tree-adjoining Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars are reported and an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SL TAG are reported."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62304080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "617241818e8ddd6edcb4ee7682992673c18c6f3d",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In automatic speech recognition, language models can be represented by Probabilistic Context Free Grammars (PCFGs). In this lecture we review some known algorithms which handle PCFGs; in particular an algorithm for the computation of the total probability that a PCFG generates a given sentence (Inside), an algorithm for finding the most probable parse tree (Viterbi), and an algorithm for the estimation of the probabilities of the rewriting rules of a PCFG given a corpus (Inside-Outside). Moreover, we introduce the Left-to-Right Inside algorithm, which computes the probability that successive applications of the grammar rewriting rules (beginning with the sentence start symbol s) produce a word string whose initial substring is a given one."
            },
            "slug": "Basic-Methods-of-Probabilistic-Context-Free-Jelinek-Lafferty",
            "title": {
                "fragments": [],
                "text": "Basic Methods of Probabilistic Context Free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Left-to-Right Inside algorithm is introduced, which computes the probability that successive applications of the grammar rewriting rules produce a word string whose initial substring is a given one."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335864"
                        ],
                        "name": "D. Temperley",
                        "slug": "D.-Temperley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Temperley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Temperley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 26
                            }
                        ],
                        "text": "This may be the case for (Lafferty et al., 1992)\u2019s model for link grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "Indeed, abstractly, the algorithm resembles a cleaner, bottom-up version of the top-down link grammar parser developed independently by (Lafferty et al., 1992).\nprobabilistic behavior depends on its headword\u2014 the lexicalist hypothesis\u2014then differently headed analyses need different signatures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 136
                            }
                        ],
                        "text": "Indeed, abstractly, the algorithm resembles a cleaner, bottom-up version of the top-down link grammar parser developed independently by (Lafferty et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 141
                            }
                        ],
                        "text": "The final model we propose is a generation model, as opposed to the comprehension models A and B (and to other comprehension models such as (Lafferty et al., 1992; Magerman, 1995; Collins, 1996))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Lafferty et al., 1992, p. 3)\nin any number of sensible or perverse ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6208015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d3a863b71f093e1cbc9304a3287c1ddc48c6f31",
            "isKey": true,
            "numCitedBy": 185,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new class of language models. This class derives from link grammar, a context-free formalism for the description of natural language. We describe an algorithm for determining maximum-likelihood estimates of the parameters of these models. The language models which we present differ from previous models based on stochastic context-free grammars in that they are highly lexical. In particular, they include the familiar $n$-gram models as a natural subclass. The motivation for considering this class is to estimate the contribution which grammar can make to reducing the relative entropy of natural language."
            },
            "slug": "Grammatical-Trigrams:-A-Probabilistic-Model-of-Link-Lafferty-Sleator",
            "title": {
                "fragments": [],
                "text": "Grammatical Trigrams: A Probabilistic Model of Link Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An algorithm for determining maximum-likelihood estimates of the parameters of these language models, which include the familiar $n$-gram models as a natural subclass, are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335864"
                        ],
                        "name": "D. Temperley",
                        "slug": "D.-Temperley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Temperley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Temperley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 15
                            }
                        ],
                        "text": "Link grammars (Sleator and Temperley, 1991) and lexicalized tree-adjoining grammars (Schabes, 1992) have now received stochastic treatments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5118729,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "5752b8dcec5856b7ad6289bbe1177acce535fba4",
            "isKey": false,
            "numCitedBy": 1030,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a new formal grammatical system called a link grammar. A sequence of words is in the language of a link grammar if there is a way to draw links between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. We have encoded English grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. The formalism is lexical and makes no explicit use of constituents and categories. The breadth of English phenomena that our system handles is quite large. A number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. Our program is written in C, and the entire system may be obtained via anonymous ftp. Several other researchers have begun to use link grammars in their own research."
            },
            "slug": "Parsing-English-with-a-Link-Grammar-Sleator-Temperley",
            "title": {
                "fragments": [],
                "text": "Parsing English with a Link Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work has encoded English grammar into a new formal grammatical system called a link grammar, and written a program (based on new algorithms) for efficiently parsing with this very complex grammar."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "For example, we follow standard practice (Church, 1988) in n-gram tagging by using (3) to approximate the first term in (2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "For example, we follow standard practice (Church, 1988) in n-gram tagging by using (3) to approximate the rst term in (2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "In a legal dependency parse, every word except for the head of the sentence (the EOS mark) has\nPr(words, tags, links) = Pr(words, tags) \u00b7 Pr(link presences and absences | words, tags) (1)\n\u2248 \u220f\n1\u2264i\u2264n\nPr(tword(i) | tword(i + 1), tword(i + 2)) \u00b7 \u220f\n1\u2264i,j\u2264n\nPr(Lij | tword(i), tword(j)) (2)\nPr(tword(i) |\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 61014458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a9b6828c5e4339025bb78af6b025d21b4830800",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiments on the use of a probabilistic model to tag English text, that is, to assign to each word the correct tag (part of speech) in the context of the sentence, are presented. A simple triclass Markov model is used, and the best way to estimate the parameters of this model, depending on the kind and amount of training data that is provided, is found. Two approaches are compared: the use of text that has been tagged by hand and comparing relative frequency counts; and use text without tags and training the model as a hidden Markov process, according to a maximum likelihood principle. Experiments show that the best training is obtained by using as much tagged text as is available, a maximum likelihood training may improve the accuracy of the tagging.<<ETX>>"
            },
            "slug": "Tagging-text-with-a-probabilistic-model-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Tagging text with a probabilistic model"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show that the best training is obtained by using as much tagged text as is available, and a maximum likelihood training may improve the accuracy of the tagging."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 50
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 79
                            }
                        ],
                        "text": "In a legal dependency parse, every word except for the head of the sentence (the EOS mark) has\nPr(words, tags, links) = Pr(words, tags) \u00b7 Pr(link presences and absences | words, tags) (1)\n\u2248 \u220f\n1\u2264i\u2264n\nPr(tword(i) | tword(i + 1), tword(i + 2)) \u00b7 \u220f\n1\u2264i,j\u2264n\nPr(Lij | tword(i), tword(j)) (2)\nPr(tword(i) |\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 62680996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352dbd26580856ba4b9877d43aeba304343af66d",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-part-of-speech-tagging-using-a-hidden-Markov-Kupiec",
            "title": {
                "fragments": [],
                "text": "Robust part-of-speech tagging using a hidden Markov model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "In a legal dependency parse, every word except for the head of the sentence (the EOS mark) has\nPr(words, tags, links) = Pr(words, tags) \u00b7 Pr(link presences and absences | words, tags) (1)\n\u2248 \u220f\n1\u2264i\u2264n\nPr(tword(i) | tword(i + 1), tword(i + 2)) \u00b7 \u220f\n1\u2264i,j\u2264n\nPr(Lij | tword(i), tword(j)) (2)\nPr(tword(i) |\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60744956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d64cae6538bb3b8e595007585477a9fdef106602",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string. In speech recognition, a language model is used to direct the hypothesis search for the sentence that was spoken. In fact, its probability assignments are a factor in evaluating the likelihood that any word string was uttered, and in ordering the word string hypotheses to be examined (1). Language models are also useful in text encoding for compression or transmission, in character recognition of printed or handwritten text, etc. Ideally, the probability assigned by a language model to any word should depend on the entire past sequence of words. However, it is practically impossible to implement such a dependence: 1) The probability estimate involved would have to be extracted from a giant sample of training text containing billions of words; 2) If the probabilities were extractable, sufficient space in computer memory could not be found to store them."
            },
            "slug": "Markov-Source-Modeling-of-Text-Generation-Jelinek",
            "title": {
                "fragments": [],
                "text": "Markov Source Modeling of Text Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "(Collins, 1996) uses this (n5) algorithm directly (together with pruning)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 63
                            }
                        ],
                        "text": ", 1992) or have augmented the formalism with lexical headwords (Magerman, 1995; Collins, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 1
                            }
                        ],
                        "text": "(Collins, 1996) uses this \u2126(n5) algorithm directly (together with pruning)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 280
                            }
                        ],
                        "text": "Other researchers, not wishing to abandon context-free grammar (CFG) but disillusioned with its lexical blind spot, have tried to re-parameterize stochastic CFG in context-sensitive ways (Black et al., 1992) or have augmented the formalism with lexical headwords (Magerman, 1995; Collins, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 170
                            }
                        ],
                        "text": "3 Model C: Recursive generation The nal model we propose is a generation model, as opposed to the comprehension models A and B (and to other comprehension models such as (La erty et al., 1992; Magerman, 1995; Collins, 1996))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 180
                            }
                        ],
                        "text": "The final model we propose is a generation model, as opposed to the comprehension models A and B (and to other comprehension models such as (Lafferty et al., 1992; Magerman, 1995; Collins, 1996))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new statistical parser"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101717074"
                        ],
                        "name": "Fred Jelinek",
                        "slug": "Fred-Jelinek",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168988203"
                        ],
                        "name": "John Lafrerty",
                        "slug": "John-Lafrerty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafrerty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Lafrerty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 77
                            }
                        ],
                        "text": "It is useful to look into these basic questions before trying to fine-tune the performance of systems whose behavior is harder to understand.1\nThe main contribution of the work is to propose three distinct, lexicalist hypotheses about the probability space underlying sentence structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ar X\niv :c\nm p-\nlg /9\n70 60\n03 v2\n7 J\nun 1\n99 7"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 188
                            }
                        ],
                        "text": "Other researchers, not wishing to abandon context-free grammar (CFG) but disillusioned with its lexical blind spot, have tried to re-parameterize stochastic CFG in context-sensitive ways (Black et al., 1992) or have augmented the formalism with lexical headwords (Magerman, 1995; Collins, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "This is the philosophy behind stochastic CFG (Jelinek et al.1992), \u201chistory-based\u201d phrase-structure parsing (Black et al., 1992), and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62560700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "576ef0b63a066a058fb260312d91d6c5a7ddbe1d",
            "isKey": true,
            "numCitedBy": 30,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "slug": "Towards-History-based-Grammars:-Using-Richer-Models-Black-Jelinek",
            "title": {
                "fragments": [],
                "text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way, and significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "For example, we follow standard practice (Church, 1988) in n-gram tagging by using (3) to approximate the first term in (2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 56
                            }
                        ],
                        "text": "1 Model A: Bigram lexical a nities N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "For example, we follow standard practice (Church, 1988) in n-gram tagging by using (3) to approximate the rst term in (2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic parts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067146548"
                        ],
                        "name": "Thomas B\u00e4ck",
                        "slug": "Thomas-B\u00e4ck",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "B\u00e4ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas B\u00e4ck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "Full results on a moderate corpus of 25,000+ tagged, dependency-annotated Wall Street Journal sentences, discussed in (Eisner, 1996), were not complete at press time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 232868397,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b016ee42664fe062d54f4fb089ff07ae2bcdcbd1",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Empirical-Comparison-B\u00e4ck",
            "title": {
                "fragments": [],
                "text": "An Empirical Comparison"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405337742"
                        ],
                        "name": "I. Mel'cuk",
                        "slug": "I.-Mel'cuk",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Mel'cuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mel'cuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203672231,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1215415ac4e5abb82d7596538bc81e6247d4f020",
            "isKey": false,
            "numCitedBy": 1326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Syntax:-Theory-and-Practice-Mel'cuk",
            "title": {
                "fragments": [],
                "text": "Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ar X\niv :c\nm p-\nlg /9\n70 60\n03 v2\n7 J\nun 1\n99 7"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards historyba ,sed gramnl~u:s: using richer mod(,.ls [br probabilistic i,~trsing. [u Fifth I)AI~,FA Worksh.op ou \u00a3'pecch and Natural Language"
            },
            "venue": {
                "fragments": [],
                "text": "Towards historyba ,sed gramnl~u:s: using richer mod(,.ls [br probabilistic i,~trsing. [u Fifth I)AI~,FA Worksh.op ou \u00a3'pecch and Natural Language"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tagging text with ;L probabilistic model, lu l~rocccdinw of the IBM Natural Language. 17'L, Paris, Fra.nce"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic parts progi:ntll a, nd noun l)hra,se parser for unrestri(:tcd text"
            },
            "venue": {
                "fragments": [],
                "text": "/'roe, of the 2rid (;onf. on Applied Natural Lang'uage lJroccssing"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new statistical parser based on bigr~un lexi(:~fl del)cndeucies, h, l~rocc.cd iTtfJS of tit("
            },
            "venue": {
                "fragments": [],
                "text": "A new statistical parser based on bigr~un lexi(:~fl del)cndeucies, h, l~rocc.cd iTtfJS of tit("
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 85
                            }
                        ],
                        "text": "Link grammars (Sleator and Temperley, 1991) and lexicalized tree-adjoining grammars (Schabes, 1992) have now received stochastic treatments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic lexicalized tree"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical (:omp~H'ison of prob~dfility nlodcls for dependeucy gl:a, lnnlaJ"
            },
            "venue": {
                "fragments": [],
                "text": "An empirical (:omp~H'ison of prob~dfility nlodcls for dependeucy gl:a, lnnlaJ"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 85
                            }
                        ],
                        "text": "Link grammars (Sleator and Temperley, 1991) and lexicalized tree-adjoining grammars (Schabes, 1992) have now received stochastic treatments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic lexicalized treeadjoining grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of COLING- 92"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic lexi(:alized tree-~tdjoining gra.mmars, lit l'rocccdings of C()lHNG'-92, Na.nl.es, I)')'auce"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ar X\niv :c\nm p-\nlg /9\n70 60\n03 v2\n7 J\nun 1\n99 7"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pro:sing I\",nglish with ~t I,iuk (h:,~mm~m Te"
            },
            "venue": {
                "fragments": [],
                "text": "hnicifl report CM U.-('S-91-196. (iS Dept., C~m,egic Melk)n tl uiv"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic lexi(:alized tree~tdjoining gra.mmars, lit l'rocccdings of C()lHNG'- 92, Na.nl.es, I)')'auce, .lnly"
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic lexi(:alized tree~tdjoining gra.mmars, lit l'rocccdings of C()lHNG'- 92, Na.nl.es, I)')'auce, .lnly"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1!392. I{obust l)arDof-speech ta.gging using a. hidden Ma, rkov model. (7omputcr \u00a3'pccch .rid Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "l);wid M~tgerul~n. 1l!)95. St~ttisti(:~d decision-tree models for p~u'sing"
            },
            "venue": {
                "fragments": [],
                "text": "~uid I)~vy '['cmperley. 1992. (~ramm~LticaJ trigr~mm: A prob~bilistic model of link gr~mnnar In 15\"oc. of the AAAI Conf. on t)robabilistic Approaches to Natural Language"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tagging text with a probabilistic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical (:omp~H'ison of prob-~dfility nlodcls for dependeucy gl:a, lnnlaJ:. Teehnic;d ILeport IRCS 96 11"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pro:sing I"
            },
            "venue": {
                "fragments": [],
                "text": "nglish with ~t I,iuk (h:,~mm~m Te(:hnicifl report CM"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "N -gram taggers like (Church, 1988; Jelinek 1985; Kupiec 1992; Merialdo 1990) take the following view of how a tagged sentence enters the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov source modeling of text"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knpie.c. 1!392. I{obust l)arDof-speech ta.gging using a. hidden Ma, rkov model"
            },
            "venue": {
                "fragments": [],
                "text": "Knpie.c. 1!392. I{obust l)arDof-speech ta.gging using a. hidden Ma, rkov model"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new statistical parser based on bigr~un lexi(:~fl del)cndeucies, h, l~rocc.cd -iTtfJS of tit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(:uk. 1988. l)cpcndcncy Syntax: 7?worg and l'racticc"
            },
            "venue": {
                "fragments": [],
                "text": "(:uk. 1988. l)cpcndcncy Syntax: 7?worg and l'racticc"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards historyba,sed gramnl~u:s: using richer mod(,.ls [br probabilistic i,~trsing"
            },
            "venue": {
                "fragments": [],
                "text": "Fifth I)AI~,FA Worksh.op ou \u00a3'pecch and Natural Language"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "M~rkov sour(:e modeliug of text gener~Ltiou"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "It cannot be emphasized too strongly that a grammatical representation (dependency parses, tag sequences, phrase-structure trees) does not entail any particular probability model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tagging text with ;L probabilistic model, lu l~rocccdinw of the IBM Natural Language"
            },
            "venue": {
                "fragments": [],
                "text": "Tagging text with ;L probabilistic model, lu l~rocccdinw of the IBM Natural Language"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Three-New-Probabilistic-Models-for-Dependency-An-Eisner/adfef97814b292a09520d8c78a141e7a4baf8726?sort=total-citations"
}