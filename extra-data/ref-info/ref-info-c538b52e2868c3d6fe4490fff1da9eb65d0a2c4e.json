{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095398"
                        ],
                        "name": "Alexander Budanitsky",
                        "slug": "Alexander-Budanitsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Budanitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Budanitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 90
                            }
                        ],
                        "text": ", 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001). Although many researchers compare performance against systems without similarity components, unfortunately only Lee (1999) and Budanitsky (1999) have actually performed evaluation of multiple approaches within an application framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 81
                            }
                        ],
                        "text": "Resnik used these results to evaluate his W ORDNET semantic distance measure and Budanitsky (1999) and Budanitsky and Hirst (2001) extend this evaluation to several measures described in the literature. McDonald (2000) demonstrates the psychological plausibility of his similarity measure using the Miller and Charles judgements and reaction times from a lexical priming task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 89
                            }
                        ],
                        "text": ", 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "Resnik used these results to evaluate his W ORDNET semantic distance measure and Budanitsky (1999) and Budanitsky and Hirst (2001) extend this evaluation to several measures described in the literature."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 385288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d95e890ee904f70701fa27326d31980424d5dd",
            "isKey": true,
            "numCitedBy": 134,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical Semantic Relatedness and Its Application in Natural Language Processing Alexander Budanitsky Department of Computer Science University of Toronto August 1999 A great variety of Natural Language Processing tasks, from word sense disambiguation to text summarization to speech recognition, rely heavily on the ability to measure semantic relatedness or distance between words of a natural language. This report is a comprehensive study of recent computational methods of measuring lexical semantic relatedness. A survey of methods, as well as their applications, is presented, and the question of evaluation is addressed both theoretically and experimentally. Application to the speci c task of intelligent spelling checking is discussed in detail: the design of a prototype system for the detection and correction of malapropisms (words that are similar in spelling or sound to, but quite di erent in meaning from, intended words) is described, and results of experiments on using various measures as plug-ins are considered. Suggestions for research directions in the areas of measuring semantic relatedness and intelligent spelling checking are o ered."
            },
            "slug": "Lexical-Semantic-Relatedness-and-Its-Application-in-Budanitsky",
            "title": {
                "fragments": [],
                "text": "Lexical Semantic Relatedness and Its Application in Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This report is a comprehensive study of recent computational methods of measuring lexical semantic relatedness, and a survey of methods, as well as their applications, is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2437192,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9375af95efb07ab206897755fc86ab643460e75b",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Current natural language generation or machine translation systems cannot distinguish among near-synonyms\u2014words that share the same core meaning but vary in their lexical nuances. This is due to a lack of knowledge about differences between near-synonyms in existing computational lexical resources. \nThe goal of this thesis is to automatically acquire a lexical knowledge-base of near-synonym differences (LKB of NS) from multiple sources, and to show how it can be used in a practical natural language processing system. \nI designed a method to automatically acquire knowledge from dictionaries of near-synonym discrimination written for human readers. An unsupervised decision-list algorithm learns patterns and words for classes of distinctions. The patterns are learned automatically, followed by a manual validation step. The extraction of distinctions between near-synonyms is entirely automatic. The main types of distinctions are: stylistic (for example, inebriated is more formal than drunk), attitudinal (for example, skinny is more pejorative than slim), and denotational (for example, blunder implies accident and ignorance, while error does not). \nI enriched the initial LKB of NS with information extracted from other sources. First, information about the senses of the near-synonym was added (WordNet senses). Second, knowledge about the collocational behaviour of the near-synonyms was acquired from free text. Collocations between a word and the near-synonyms in a dictionary entry were classified into: preferred collocations, less-preferred collocations, and anti-collocations. Third, knowledge about distinctions between near-synonyms was acquired from machine-readable dictionaries (the General Inquirer and the Macquarie Dictionary). These distinctions were merged with the initial LKB of NS, and inconsistencies were resolved. \nThe generic LKB of NS needs to be customized in order to be used in a natural language processing system. The parts that need customization are the core denotations and the strings that describe peripheral concepts in the denotational distinctions. To show how the LKB of NS can be used in practice, I present Xenon, a natural language generation system that chooses the near-synonym that best matches a set of input preferences. I implemented Xenon by adding a near-synonym choice module and a near-synonym collocation module to an existing general-purpose surface realizer."
            },
            "slug": "Building-a-lexical-knowledge-base-of-near-synonym-Hirst-Inkpen",
            "title": {
                "fragments": [],
                "text": "Building a lexical knowledge-base of near-synonym differences"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The goal of this thesis is to automatically acquire a lexical knowledge-base of near-synonym differences (LKB of NS) from multiple sources, and to show how it can be used in a practical natural language processing system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15475212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30ef09a2a371ac80044617000de47e71906b93d0",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the diiculties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suuce for a particular task; for this reason we are interested in nding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to reene the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to nd instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we tried to make a term-similarity determination technique choose where, in an existing thesaurus, to install a lexical relation. The union of these two corpus-based methods is promising , although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future."
            },
            "slug": "A-Method-for-Re-ning-Automatically-Discovered-Weak-Grefenstette-Hearst",
            "title": {
                "fragments": [],
                "text": "A Method for Re ning Automatically-Discovered Lexical Relations: Combining Weak Techniques for Stronger Results"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A term-similarity determination technique choose where, in an existing thesaurus, to install a lexical relation, and the union of these two corpus-based methods is promising, although only partially successful in the experiments run so far."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17086579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4b66a159672c613f9d4c7cdb8e6c6f85d871bf5",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we applied a termsimilarity determination technique to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation. The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future."
            },
            "slug": "Refining-Automatically-Discovered-Lexical-Combining-Hearst-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Refining Automatically-Discovered Lexical Relations: Combining Weak Techniques for Stronger Results"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A termsimilarity determination technique is applied to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation, and the union of these two corpus-based methods is promising, although only partially successful in the experiments run so far."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144654543"
                        ],
                        "name": "Roberto Basili",
                        "slug": "Roberto-Basili",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Basili",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Basili"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756982"
                        ],
                        "name": "A. Cucchiarelli",
                        "slug": "A.-Cucchiarelli",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Cucchiarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cucchiarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47164641"
                        ],
                        "name": "Carlo Consoli",
                        "slug": "Carlo-Consoli",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Consoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Consoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802339"
                        ],
                        "name": "M. Pazienza",
                        "slug": "M.-Pazienza",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Pazienza",
                            "middleNames": [
                                "Teresa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazienza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782522"
                        ],
                        "name": "P. Velardi",
                        "slug": "P.-Velardi",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Velardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Velardi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1995) and also to prune senses and synonym relation links based on evidence from domain-specific corpora (Basili et al., 1998; Turcato et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14589746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7a126abad08d8a7c198886a784ef57ea9ed73dd",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantically tagging a corpus is useful for many intermediate NLP tasks such as: acquisition of word argument structures in sublanguages, acquisition of syntactic disambiguation cues, terminology learning, etc. Semantic categories allow the generalization of observed word patterns, and facilitate the discovery of irecurrent sublanguage phenomena and selectional rules of various types. Yet, as opposed to POS tags in morphology, there is no consensus in literature about the type and granularity of the category inventory. In addition, most available on-line taxonomies, as WordNet, are over ambiguous and, at the same time, may not include many domain-dependent senses of words. In this paper we describe a method to adapt a general purpose taxonomy to an application sub[anguage: flint, we prune branches of the Wordnet hierarchy that are too \" fine grained\" for the domain: then. a statistical model of classes is built from corpus contexts to sort the different classifications or assign a classification to known and unknown words, respectively. 1 I n t r o d u c t i o n Lexical learning methods based on the use of semantic categories are faced with the problem of overambiguity and entangled structures of Thesaura and dictionaries. WordNet and Roget's Thesaura were not initially conceived, despite their success among researchers in lexical statistics, as tools for automatic language processing. The purpose was rather to provide the linguists with a very refined, general purpose, linguistically motivated source of taxonomic knowledge. As a consequence, in most on-fine Thesaura words are extremely ambiguous. with very subtle distinctions among senses. High ambiguity, entangled nodes, and asymmetry have already been emphasized in (Hearst and Shutze, 1993) as being an obstacle to the effective use of on-line Thesaura in corpus linguistics. In most cases, the noise introduced by overambiguity almost overrides the positive effect of semantic clustering. For example, in (BriIl and Resnik, 1994) clustering PP heads according to WordNet synsets produced only a [% improvement in a PP disambiguation task. with respect to the non-clustered method. A subsequent paper (Resnik. 1997) reports of a 40% precision in a sense disambiguation task. always based on generalization through WordNet synsets. Context-based sense clisambiguation becomes a prohibitive task on a wide-scale basis, because when words in the context of unambiguous word are replaced by their s.vnsets, there is a multiplication of possible contexts, rather than a generalization. [n (Agirre and Rigau. 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still does not reach 50%. On the other hand, (Dolan. 1994) and (Krovetz and Croft. 1992) claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications. Our experience supports this claim: often, what matters is to be able to distinguish among contrastive (Pustejowsky. 1995) ambiguities of the bank_river bank_organisation flavor. The problem however is that the notion of\"coutrast ive\" is domain-dependent. Depending upon the sublanguage (e.g. medicine, finance, computers. etc.) and upon the specific NLP application (e.g. Information Extraction, Dialogue etc.) a given semantic label may be too general or too specific for the task at hand. For example, the word line has 27 senses in WordNet. many of which draw subtle distinctions e.g. line of ~cork (sense 26) and line of products (sense [9). In aa"
            },
            "slug": "Automatic-Adaptation-of-WordNet-to-Sublanguages-and-Basili-Cucchiarelli",
            "title": {
                "fragments": [],
                "text": "Automatic Adaptation of WordNet to Sublanguages and to Computational Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper describes a method to adapt a general purpose taxonomy to an application sub[anguage: flint, and prune branches of the Wordnet hierarchy that are too \" fine grained\" for the domain, as well as a statistical model of classes built from corpus contexts to sort the different classifications or assign a classification to known and unknown words."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d37dff2d8e65764e7293750051d519359d8835d",
            "isKey": false,
            "numCitedBy": 405,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \u201ceat a peach\u201d and \u201deat a beach\u201d is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words.We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task."
            },
            "slug": "Similarity-Based-Models-of-Word-Cooccurrence-Dagan-Lee",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Models of Word Cooccurrence Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words, and describes probabilistic word association models based on distributional word similarity, and applies them to two tasks, language modeling and pseudo-word disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022557"
                        ],
                        "name": "X. Farreres",
                        "slug": "X.-Farreres",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Farreres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Farreres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144444054"
                        ],
                        "name": "H. Rodr\u00edguez",
                        "slug": "H.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rodr\u00edguez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 794942,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "cfd6fadb25a0db9a05d776a6fb24264f322b1b98",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summar ises a set of methodologies and techniques for the fast construction of multilingual WordNets. The English WordNet is used in this approach as a backbone for Catalan and Spanish WordNets and as a lexical knowledge resource for several subtasks. 1 Motivation and Introduction One of the main issues in last years as regards NLP activit ies is the i nc r ea s ing ly fast development of generic language resources. A lot of such resources, including both software and l ingware items (lexicons, lexical databases, grammars, corpora marked in several ways) have been made available for research a n d industrial applications. Special interest presents, for knowledge-based NLP tasks, the availability of wide coverage ontologies. Most known ontologies (as GUM, CYC, ONTOS, MICROKOSMOS, EDR or WORDNET, see [Gomez 98] for an extensive survey) defe r in great extent on several characteristics (e.g. broad coverage vs. domain specific, lexicaUy oriented vs. conceptually oriented, granularity, kind of information placed in nodes, kind of relations, way of building, etc.). It is clear, however, that for a wide range of applications, WordNet (WN) [Miller 90] as become a de-facto standard. The success of WordNet has determined the emergence of several projects that aim the construction of WordNets for other languages than English (e.g., [Hamp & Feldweg 97], [Artale et al. 97]) or to develop multilingual WordNets (the mos t important project in this line is EuroWordNet (EWN)I). lhttp://www.let.uva.rd/~ewn/The aim of EWN vroject is to braid a multi.lingual database with WordN'ets for several european languages (in the first phase, Dutch, Italian and Spanish in addltion to English). The construction of a WN for a language Lg (LgWN) can be tackled in d i f fe ren t ways according to the lexical sources available. Of course the manual construction can be undertaken quite straightforwardly and leads to the best results in terms of accuracy, but has the important drawback of its cost. So, other approaches have been carried out taking profi t of available resources in fully automatic or semi-automatic ways. Which are these lexical resources? Basically four kinds of resources have been used: 1) English WN (EnWN0, as an initial skeleton for trying to attach the words of Lg to it, 2) a l ready existing taxonomies of Lg (both at word and at sense level), 3) bilingual (English and Lg) and 4) monolingual (Lg) dictionaries. All the approaches using EnWN as skeleton are based on the assumption of a close conceptual similarity between English and Lg, in such a way that most of the structure (relations) in EnWN could be maintained for LgWN. In the case of bilingual dictionaries the usual approach is to try to link the English counterpart of entries to synsets in EnWN and to assume that the entry can be ]inked to the same synset. Monolingual dictionaries have been used basically as a source for extracting taxonomic (hypemym) links between words (or senses [Bruce & Guthrie 92], [Rigau et al. 97]) and in lower extent for extracting other kinds of semantic relations [Richardson 97] (e.g. meronymic links). Once a taxonomy of Lg (already existing or built from a monolingual MILD) is available, the task can consist of 1) enriching the taxonomic structure with other semantic links (manually or automatically), as is the case of bu i ld ing individual WNs, or 2) merging this structure with other already existing ontologies (as EnWN or EWN). This paper presents our approach to the construction of WNs for two languages, Spanish and Catalan, and linking the first one to EWN. We have developed a methodology that uses as core source EnWN 2. The methodology implies 1) 2We have used WordNet 1.5. 65"
            },
            "slug": "Using-WordNet-for-Building-WordNets-Farreres-Rigau",
            "title": {
                "fragments": [],
                "text": "Using WordNet for Building WordNets"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A set of methodologies and techniques for the fast construction of multilingual WordNets using EnWN as skeleton are presented, based on the assumption of a close conceptual similarity between English and Lg, in such a way that most of the structure (relations) in EnWN could be maintained for LgWN."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2032603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe1bf905eacefd9758696d3805f23325d5aa5d2",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods for automatically extracting information about associations between words or documents from large collections of text have the potential to have considerable impact in a number of areas, such as information retrieval and natural-language-based user interfaces. However, even huge bodies of text yield highly unreliable estimates of the probability of relatively common events, and, in fact, perfectly reasonable events may not occur in the training data at all. This is known as the sparse data problem. Traditional approaches to the sparse data problem use crude approximations. We propose a different solution: if we are able to organize the data into classes of similar events, then, if information about an event is lacking, we can estimate its behavior from information about similar events. This thesis presents two such similarity-based approaches, where, in general, we measure similarity by the Kullback-Leibler divergence, an information-theoretic quantity. \nOur first approach is to build soft, hierarchical clusters: soft, because each event belongs to each cluster with some probability; hierarchical, because cluster centroids are iteratively split to model finer distinctions. Our clustering method, which uses the technique of deterministic annealing, represents (to our knowledge) the first application of soft clustering to problems in natural language processing. We use this method to cluster words drawn from 44 million words of Associated Press Newswire and 10 million words from Grolier's encyclopedia, and find that language models built from the clusters have substantial predictive power. Our algorithm also extends with no modification to other domains, such as document clustering. \nOur second approach is a nearest-neighbor approach: instead of calculating a centroid for each class, we in essence build a cluster around each word. We compare several such nearest-neighbor approaches on a word sense disambiguation task and find that as a whole, their performance is far superior to that of standard methods. In another set of experiments, we show that using estimation techniques based on the nearest-neighbor model enables us to achieve perplexity reductions of more than 20 percent over standard techniques in the prediction of low-frequency events, and statistically significant speech recognition error-rate reduction."
            },
            "slug": "Similarity-Based-Approaches-to-Natural-Language-Lee",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Approaches to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The clustering method, which uses the technique of deterministic annealing, represents (to the authors' knowledge) the first application of soft clustering to problems in natural language processing, and is compared to several such nearest-neighbor approaches on a word sense disambiguation task and finds that as a whole, their performance is far superior to that of standard methods."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660941"
                        ],
                        "name": "P. Edmonds",
                        "slug": "P.-Edmonds",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Edmonds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Edmonds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2001), topic categories from Roget\u2019s thesaurus (Mandala et al., 1999; Nastase and Szpakowicz, 2001) and information from LDOCE (Kwong, 1998). Stevenson (2002) uses three different similarity metrics to align WORDNET with CIDE+ to augment WORDNET with related term links."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Edmonds and Hirst (2002) show how a coarse-grained ontology can be combined with sub-clusters containing differentiated plesionyms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12998616,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69f89d88dd025806b741b0fa56c9ff73adbd6c2a",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 146,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation. We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together. We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior. An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus."
            },
            "slug": "Near-Synonymy-and-Lexical-Choice-Edmonds-Hirst",
            "title": {
                "fragments": [],
                "text": "Near-Synonymy and Lexical Choice"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A preliminary theory to account for near-synonymy is proposed, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a Context-independent core meaning and a set of explicit differences to its near- synonyms."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094441889"
                        ],
                        "name": "Laura Benitez",
                        "slug": "Laura-Benitez",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Benitez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Benitez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103023299"
                        ],
                        "name": "Sergi Cervell",
                        "slug": "Sergi-Cervell",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Cervell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Cervell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145786318"
                        ],
                        "name": "G. Escudero",
                        "slug": "G.-Escudero",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Escudero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Escudero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150219190"
                        ],
                        "name": "Monica Lopez",
                        "slug": "Monica-Lopez",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Lopez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monica Lopez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145553000"
                        ],
                        "name": "M. Taul\u00e9",
                        "slug": "M.-Taul\u00e9",
                        "structuredName": {
                            "firstName": "Mariona",
                            "lastName": "Taul\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Taul\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2102,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2e14ef22280480b3291bf05f792722e4eb77ebc0",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper we introduce the methodology used andthe basic phases we followed to develop the Catalan WordNet,and which lexical resources have been employed in its building.This methodology, as well as the tools we made use of, havebeen thought in a general way so that they could be applied toany other language. 1. Introduction In recent years the research in the NaturalLanguage Processing (NLP) field has proved the need forextensive and complete Lexical Knowledge Bases(LKBs). Acquiring such lexical/semantic structures is ahard problem and has been usually approached byreusing, merging and tuning existing lexical material.While in English the \"lexical bottleneck\" problem seemsto be softened (e.g. WordNet (Miller, 1990), AlveyLexicon (Grover et al. 1993), COMLEX (Grishman et al.1994) and so on) there are no available wide rangelexicons for NLP for other languages. Manualconstruction of lexicons is the most reliable technique forobtaining structured lexicons but it is costly and highlytime-consuming. This is the reason for many researchersto have focussed on the massive acquisition of lexicalknowledge and semantic information from pre-existingstructured lexical resources as automatically as possible.The work presented here shows a fast productionmethodology to build large scale multilingual LKBs fromconventional dictionaries.In this way, the English WordNet developed atPrinceton University is consolidating as a standard defacto for the lexical-semantic representation of the naturallanguage for English. Considering this English WordNetas a reference, new WordNets in some other languages arebeing built, such as the ones for Galician, Basque, Spanishand Catalan, for instance. Besides, multilingual links areestablished automatically through the connections withthe English WordNet.The research we are now presenting is embeddedin the Catalan WordNet project, which is being carried outby the Grup de Tractament de Llenguatge Natural(GTLN-UPC-UB) of the Centre de Referencia enEnginyeria Linguistica (CREL)"
            },
            "slug": "Methods-and-Tools-for-Building-the-Catalan-WordNet-Benitez-Cervell",
            "title": {
                "fragments": [],
                "text": "Methods and Tools for Building the Catalan WordNet"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The methodology used and the basic phases the authors followed to develop the Catalan WordNet are introduced, and which lexical resources have been employed in its building, which means the English WordNet developed at Princeton University is consolidating as a standard defacto for the lexical-semantic representation of the naturallanguage for English."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574304"
                        ],
                        "name": "Manfred Stede",
                        "slug": "Manfred-Stede",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Stede",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Stede"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hirst and collaborators have explored near-synonymy, which is important for lexical choice in Machine Translation and Natural Language Generation (Stede, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18191905,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "33bedb4a05709e527fd88ec0e348b439dfd1bfd0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis develops a new approach to automatic language generation that focuses on the need to produce a range of different paraphrases from the same input representation. One novelty of the system is its solidly grounding representations of word meaning in a background knowledge base, which enables the production of paraphrases stemming from certain inferences, rather than from purely lexical relationships alone. \nThe system is designed in such a way that the paraphrasing mechanism extends naturally to a multilingual generator; specifically, we will be concerned with producing English and German sentences. The focus of the system is on lexical paraphrases, and one of the contributions of the thesis is in identifying, analyzing and extending relevant linguistic research so that it can be used to handle the problems of lexical semantics in a language generation system. The lexical entries are more complex than in previous generators, and they separate the various aspects of word meaning, so that different ways of paraphrasing can be systematically related to the different motivations for saying a sentence in a particular way. One result of accounting for lexical semantics in this fashion is a formalization of a number of verb alternations, for which a generative treatment is given. \nWhile the actual choice of one paraphrase as the best-suited utterance in a given situation is not a focal point of the thesis, two dimensions of preferring a variant of a sentence are discussed: that of assigning salience to the different elements of the sentence, and that of connotational or sylistic features of the utterance. These dimensions are integrated into the system, and it can thus determine a preferred paraphrase from a set of alternatives. \nTo demonstrate the feasibility of the approach, the proposed generation architecture has been implemented as a prototype, along with a domain model that serves as the background knowledge base for specifying the input to the generator. A range of generated examples is presented to show the functionality of the system."
            },
            "slug": "Lexical-semantics-and-knowledge-representation-in-Hirst-Stede",
            "title": {
                "fragments": [],
                "text": "Lexical semantics and knowledge representation in multilingual sentence generation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This thesis develops a new approach to automatic language generation that focuses on the need to produce a range of different paraphrases from the same input representation, and is designed in such a way that the paraphrasing mechanism extends naturally to a multilingual generator; specifically, it is concerned with producing English and German sentences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704249"
                        ],
                        "name": "O. Kwong",
                        "slug": "O.-Kwong",
                        "structuredName": {
                            "firstName": "Oi",
                            "lastName": "Kwong",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kwong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 133
                            }
                        ],
                        "text": "Lapata also uses human judgements to evaluate probabilistic models for logical metonymy (Lapata and Lascarides, 2003) and smoothing (Lapata et al., 2001). Bannard et al. (2003) elicit judgements for determining whether verbparticle expressions are non-compositional."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 66
                            }
                        ],
                        "text": ", 1999; Nastase and Szpakowicz, 2001) and information from LDOCE (Kwong, 1998). Stevenson (2002) uses three different similarity metrics to align W ORDNET with CIDE+ to augment WORDNET with related term links."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3242521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13ebfa2ff766c0f8f3becbc01cc1dea8d022f8dc",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the relationship between WordNet and other conventional linguistically-based lexical resources. We introduce an algorithm for aligning word senses from different resources, and use it in our exper~nent to sketch the role played by WordNet, as far as sense discrimination is concerned, when put in the context of other lexical databases. The results show how and where the resources systematically differ from one another with respect to the degree of polysemy, and suggest how we can (i) overcome the inadequacy of individual resources to achieve an overall balanced degree of sense discrimination, and (ii) use a combination of semantic classification schemes to enrich lexical information for NLP."
            },
            "slug": "Aligning-WordNet-with-Additional-Lexical-Resources-Kwong",
            "title": {
                "fragments": [],
                "text": "Aligning WordNet with Additional Lexical Resources"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm for aligning word senses from different resources is introduced and used in this paper to sketch the role played by WordNet, as far as sense discrimination is concerned, when put in the context of other lexical databases."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Hearst (1992) and Caraballo and Charniak (1999) compare their hyponym extraction and specificity ordering techniques against the W ORDNET hierarchy. Lin (1999) uses an idiom dictionary to evaluate the identification of non-compositional expressions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 350,
                                "start": 43
                            }
                        ],
                        "text": "Widdows (2003) uses a similar technique to Hearst and Sch \u00fctze to insert words into the WORDNET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures with Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. The same technique as Hearst and Sch \u00fctze (1993) and Widdows (2003) is used in my approach to supersense tagging."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Hearst (1992) and Caraballo and Charniak (1999) compare their hyponym extraction and specificity ordering techniques against the W ORDNET hierarchy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Hearst and Grefenstette (1992) find a significant improvement in recall, which is a major problem for hyponym extraction systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Hearst (1992), has also extracted contextual information from reference texts, such as dictionaries or encyclopaedias."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "This work was pioneered by Hearst (1992), who showed that it was possible to extract hyponym related terms using templates like: \u2022 X, ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 29
                            }
                        ],
                        "text": "This is the approach used by Hearst and Sch \u00fctze (1993) and Widdows (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Nakov and Hearst (2003) suggest this is because polysemous words often have related senses rather than randomly selected pseudoword pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15763200,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "dbfd191afbbc8317577cbc44afe7156df546e143",
            "isKey": true,
            "numCitedBy": 3647,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
            },
            "slug": "Automatic-Acquisition-of-Hyponyms-from-Large-Text-Hearst",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Hyponyms from Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest are identified."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148782"
                        ],
                        "name": "Michael Sussna",
                        "slug": "Michael-Sussna",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sussna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Sussna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first set of methods involves weighting the edges of the graph by the number of outgoing and incoming links (Sussna, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17299699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6921e3c7219c5246a3f6105156cdcbd708603c7",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantics-free, word-based information retrieval is thwarted by two complementary problems. First, search for relevant documents returns irrelevant items when all meanings of a search term are used, rather than just the meaning intended. This causes low precision. Second, relevant items are missed when they are indexed not under the actual search terms, but rather under related terms. This causes low recall. With semantics-free approaches there is generally no way to improve both precision and recall at the same time. Word sense disambiguation during document indexing should improve precision. We have investigated using the massive Word Net semantic network for disambigu at ion during indexing. With the unconstrained text of the SMART ret rieval environment, we have had to derive our own content description from the input text, given only part-ofspeech tagging of the input. We employ the notion of semantic distance between network nodes. Input text terms with multiple senses are disambiguated by finding the combination of senses from a set of contiguous terms which minimizes total pairwise dist ante between senses. Results so far have been encouraging. Improvement in disamblguation compared with chance is clear"
            },
            "slug": "Word-sense-disambiguation-for-free-text-indexing-a-Sussna",
            "title": {
                "fragments": [],
                "text": "Word sense disambiguation for free-text indexing using a massive semantic network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work investigates using the massive Word Net semantic network for disambiguation during document indexing to improve precision and improvement in disamblguation compared with chance."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068932662"
                        ],
                        "name": "J. Shepherd",
                        "slug": "J.-Shepherd",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 109
                            }
                        ],
                        "text": "In these ways, it is also similar to the bootstrapping techniques for learning members of a particular class (Riloff and Shepherd, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 77
                            }
                        ],
                        "text": "Another approach, often used for common and proper nouns, uses bootstrapping (Riloff and Shepherd, 1997) and multi-level bootstrapping (Riloff and Jones, 1999) to find a set of terms related to an initial seed set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1437,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b3e9130ecab419f8267fccadf80c1ee2489be793",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon."
            },
            "slug": "A-Corpus-Based-Approach-for-Building-Semantic-Riloff-Shepherd",
            "title": {
                "fragments": [],
                "text": "A Corpus-Based Approach for Building Semantic Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a corpus-based method that can be used to build semantic lexicons for specific categories using a small set of seed words for a category and a representative text corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055976119"
                        ],
                        "name": "S. Marcus",
                        "slug": "S.-Marcus",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Marcus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309269"
                        ],
                        "name": "Shaul Markovitch",
                        "slug": "Shaul-Markovitch",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Markovitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaul Markovitch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1154960,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In recent years there is much interest in word co-occurrence relations, such as n-grams, verb\u2013object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co-occurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models. A background survey is included, covering issues of lexical co-occurrence, data sparseness and smoothing, word similarity and clustering, and mutual information."
            },
            "slug": "Contextual-Word-Similarity-and-Estimation-From-Data-Dagan-Marcus",
            "title": {
                "fragments": [],
                "text": "Contextual Word Similarity and Estimation From Sparse Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several computational attempts to reduce the number of sense distinctions and increase the size of each synset in WORDNET (Buitelaar, 1998; Ciaramita et al., 2003; Hearst and Sch\u00fctze, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1372302,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "52af59382abca0fd549074353020f846a8731165",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a method for augmenting and rearranging a structured lexicon in order to make it more suitable for a topic labefing task, by making use of lexical association information from a large text corpus. We first describe an algorithm for converting the hierarchical structure of WordNet [13] into a set of flat categories. We then use lexical cooccurrence statistics in combination with these categories to classify proper names, assign more specific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 I n t r o d u c t i o n Much effort is being appl ied to the creation of lexicons and the acquisi t ion of semant ic and syntact ic a t t r ibu tes of the lexical i tems tha t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given may not suit the requirements of a par t icu la r computa t iona l task. Because lexicons are expensive to build, ra ther than create new ones from scratch, i t is preferable to ad jus t existing ones to meet an appl ica t ion ' s needs. In this paper we describe such an effort: we add associat ional informat ion to a hierarchical ly s t ructured lexicon in order to bet ter serve a text labeling task. An a lgor i thm for par t i t ion ing a full-length exposi tory text into a sequence of subtopiea l discussions is described in [9]. Once the par t i t ioning is done, we need to assign labels 1 indicat ing what the subtopical discussions are about , for the purposes of informat ion retr ieval and hyper tex t navigat ion. One way to label texts, when working within a l imited domain of discourse, is to s t a r t with a pre-defined set of topics and specify the word contexts tha t indicate the topics of interest (e.g., [10]). Another way, assuming tha t a large collection of prelabeled texts exists, is to use s tat is t ics to au tomat i ca l ly infer which lexical i tems indicate which labels (e.g., [12]). In contrast , we are interested in assigning labels to general, domainindependent text , wi thout benefit of pre-classified texts. In all three cases, a lexicon tha t specifies which lexical i tems correspond to which topics is required. The topic label ing method we use is s ta t is t ica l and thus requires a large number of representat ive lexical i tems for each category. The s ta r t ing point for our lexicon is WordNet [13], which is readi ly available online and provides a large reposi tory of English lexical i tems. WordNet 2 is composed of synse t s , 1 The terms \"label\" and \"topic\" are used interchangeably in this paper. 2 All work described here pertains to Version 1.3 of WordNet."
            },
            "slug": "Customizing-a-Lexicon-to-Better-Suit-a-Task-Hearst-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Customizing a Lexicon to Better Suit a Computational Task"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An algorithm for converting the hierarchical structure of WordNet into a set of flat categories is described, which uses lexical cooccurrence statistics in combination with these categories to classify proper names, assign more specific senses to broadly defined terms, and classify new words into existing categories."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6922975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb09327e68400bf05e6f373e046a3a08e82510e",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \"most similar\" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."
            },
            "slug": "Similarity-Based-Estimation-of-Word-Cooccurrence-Dagan-Pereira",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Estimation of Word Cooccurrence Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probabilistic word association model based on distributional word similarity is described, and it is applied to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362587"
                        ],
                        "name": "Jinxi Xu",
                        "slug": "Jinxi-Xu",
                        "structuredName": {
                            "firstName": "Jinxi",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinxi Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14612056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dc2b0794a871ebbc9f52bb2d6826b11ced31e1e",
            "isKey": false,
            "numCitedBy": 367,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Stemming is used in many information retrieval (IR) systems to reduce variant word forms to common roots. It is one of the simplest applications of natural-language processing to IR and is one of the most effective in terms of user acceptance and consistency, though small retrieval improvements. Current stemming techniques do not, however, reflect the language use in specific corpora, and this can lead to occasional serious retrieval failures. We propose a technique for using corpus-based word variant cooccurrence statistics to modify or create a stemmer. The experimental results generated using English newspaper and legal text and Spanish text demonstrate the viability of this technique and its advantages relative to conventional approaches that only employ morphological rules."
            },
            "slug": "Corpus-based-stemming-using-cooccurrence-of-word-Xu-Croft",
            "title": {
                "fragments": [],
                "text": "Corpus-based stemming using cooccurrence of word variants"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a technique for using corpus-based word variant cooccurrence statistics to modify or create a stemmer, and demonstrates the viability of this technique and its advantages relative to conventional approaches that only employ morphological rules."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published as Curran and Moens (2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published in Curran and Moens (2002a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published as Curran and Moens (2002b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10782902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8066c5522ebfa8f0f08589dcbe5f315bfec90c1",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Context is used in many NLP systems as an indicator of a term's syntactic and semantic function. The accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term. However, the quantity variable is no longer fixed by limited corpus resources. Given fixed training time and computational resources, it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus. However, with an effectively limitless quantity of text available, extraction rate and representation size need to be considered. We use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
            },
            "slug": "Scaling-Context-Space-Curran-Moens",
            "title": {
                "fragments": [],
                "text": "Scaling Context Space"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work uses thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "On the other hand, early experiments in word sense disambiguation used very large windows of up to 500 words (Yarowsky, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1693468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d922631a6bf8361d7602e12cafb9e15d421c827",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature."
            },
            "slug": "Word-Sense-Disambiguation-Using-Statistical-Models-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Statistical Models of Roget\u2019s Categories Trained on Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories, enabling training on unrestricted monolingual text without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6972699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e15b6a849a41d18ee7773a95d61dc698f30ca52",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an \"enhancer\" of existing broad-coverage resources."
            },
            "slug": "Noun-Phrase-Co-Occurence-Statistics-for-Semantic-Roark-Charniak",
            "title": {
                "fragments": [],
                "text": "Noun-Phrase Co-Occurence Statistics for Semi-Automatic Semantic Lexicon Construction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars, that could be viewed as an \"enhancer\" of existing broad-coverage resources."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1529624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3317f2788b2b07d9ba4cb4335e29316fcf8a971a",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses."
            },
            "slug": "Discovering-word-senses-from-text-Pantel-Lin",
            "title": {
                "fragments": [],
                "text": "Discovering word senses from text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text that initially discovers a set of tight clusters called committees that are well scattered in the similarity space."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399394776"
                        ],
                        "name": "D. St-Onge",
                        "slug": "D.-St-Onge",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "St-Onge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. St-Onge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 167
                            }
                        ],
                        "text": "Lexical chains in turn have been used for automatically inserting hypertext links into newspaper articles (Green, 1996) and for detecting and correcting malapropisms (Hirst and St-Onge, 1998). Jarmasz (2003) gives an overview of the applications of Roget\u2019s thesaurus in NLP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14394781,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language utterances are, in general, highlyambiguous, and a unique interpretationcan usuallybe determined only by taking into account the constraining in\ufb02uence of the context in which theutterance occurred. Much of the research in natural language understanding in the last twenty yearscan be thought of as attempts to characterize and represent context and then derive interpretationsthat\ufb01t best with that context. Typically, this research was heavy with AI, taking context to be nothing lessthan a complete conceptual understanding of the preceding utterances. This was reasonable, as suchan understanding of a text was often the main task anyway. However, there are many text-processingtasksthatrequireonlya partialunderstandingofthetext, andhencea \u2018lighter\u2019representationofcontextis suf\ufb01cient. In this paper, we examine the idea oflexical chains as such a representation. We showhow they can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms.A malapropism is the confounding of an intended word with another word of similar sound orsimilar spelling that has a quite different and malapropos meaning, e.g., an ingenuous [for ingenious]machine forpeelingoranges. In thisexample, there isaone-letterdifference betweenthe malapropismand the correct word. Ignorance, or a simple typing mistake, might cause such errors. However, sinceingenuous is a correctly spelled word, traditional spelling checkers cannot detect this kind of mistake.In section 4, we will propose an algorithm for detecting and correcting malapropisms that is based onthe construction of lexical chains."
            },
            "slug": "Lexical-chains-as-representations-of-context-for-of-Hirst-St-Onge",
            "title": {
                "fragments": [],
                "text": "Lexical chains as representations of context for the detection and correction of malapropisms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "How lexical chains can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, I would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the results from Widdows (2003) using latent semantic indexing."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6987562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c2195bdf698caba4c3b9753cb0ce5acddff5b73",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words. The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information. We then place the unknown word in the part of the taxonomy where these neighbors are most concentrated, using a class-labelling algorithm developed especially for this task. This method is used to reconstruct parts of the existing Word-Net database, obtaining results for common nouns, proper nouns and verbs. We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "slug": "Unsupervised-methods-for-developing-taxonomies-by-Widdows",
            "title": {
                "fragments": [],
                "text": "Unsupervised methods for developing taxonomies by combining syntactic and statistical information"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An unsupervised algorithm for placing unknown words into a taxonomy and its accuracy on a large and varied sample of words is evaluated and it is shown that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32780943"
                        ],
                        "name": "M. Jarmasz",
                        "slug": "M.-Jarmasz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Jarmasz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jarmasz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66114341"
                        ],
                        "name": "Stanialaw Szpakowicz",
                        "slug": "Stanialaw-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stanialaw",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanialaw Szpakowicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The hierarchy structure in Roget\u2019s has also been used in edge counting measures of semantic similarity (Jarmasz and Szpakowicz, 2003; McHale, 1998), and for computing lexical cohesion using lexical chains (Morris and Hirst, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1189582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a67f083830790586ed41823d45a7b330d0a2fd95",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We have implemented a system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests. We compare the results of these tests with those produced by WordNet-based similarity measures. One of the benchmarks is Miller and Charles' list of 30 noun pairs to which human judges had assigned similarity measures. We correlate these measures with those computed by several NLP systems. The 30 pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have also studied. Our Roget's-based system gets correlations of .878 for the smaller and .818 for the larger list of noun pairs; this is quite close to the .885 that Resnik obtained when he employed humans to replicate the Miller and Charles experiment. We further evaluate our measure by using Roget's and WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the correct synonym must be selected amongst a group of four words. Our system gets 78.75%, 82.00% and 74.33% of the questions respectively."
            },
            "slug": "Roget's-thesaurus-and-semantic-similarity-Jarmasz-Szpakowicz",
            "title": {
                "fragments": [],
                "text": "Roget's thesaurus and semantic similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests, comparing the results with those produced by WordNet-based similarity measures."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34802966"
                        ],
                        "name": "Jordi Atserias Batalla",
                        "slug": "Jordi-Atserias-Batalla",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Batalla",
                            "middleNames": [
                                "Atserias"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordi Atserias Batalla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145475960"
                        ],
                        "name": "S. Climent",
                        "slug": "S.-Climent",
                        "structuredName": {
                            "firstName": "Salvador",
                            "lastName": "Climent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Climent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022557"
                        ],
                        "name": "X. Farreres",
                        "slug": "X.-Farreres",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Farreres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Farreres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144444054"
                        ],
                        "name": "H. Rodr\u00edguez",
                        "slug": "H.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rodr\u00edguez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3945367,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "867df8335af2370a4f5556b7e745e895eb619aad",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the automatic construction of a multilingual Lexical Knowledge Base from preexisting lexical resources. First, a set of automatic and complementary techniques for linking Spanish words collected from monolingual and bilingual MRDs to English WordNet synsets are described. Second, we show how resulting data provided by each method is then combined to produce a preliminary version of a Spanish WordNet with an accuracy over 85%. The application of these combinations results on an increment of the extracted connexions of a 40% without losing accuracy. Both coarse-grained (class level) and fine-grained (synset assignment level) confidence ratios are used and evaluated. Finally, the results for the whole process are presented."
            },
            "slug": "Combining-Multiple-Methods-for-the-Automatic-of-Batalla-Climent",
            "title": {
                "fragments": [],
                "text": "Combining Multiple Methods for the Automatic Construction of Multilingual WordNets"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A set of automatic and complementary techniques for linking Spanish words collected from monolingual and bilingual MRDs to English WordNet synsets are described and it is shown how resulting data provided by each method is combined to produce a preliminary version of a Spanish WordNet with an accuracy over 85%."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152890376"
                        ],
                        "name": "R. Green",
                        "slug": "R.-Green",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145351400"
                        ],
                        "name": "Lisa Pearl",
                        "slug": "Lisa-Pearl",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lisa Pearl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Grefenstette (1994) evaluates against the Deese Antonyms, a collection of 33 pairs of very common adjectives and the most frequent response in free word-association experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hearst and Grefenstette (1992) combine this approach with a vector-space similarity measure (Grefenstette, 1994), to overcome some of these problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Apart from augmenting WORDNET with different kinds of knowledge, there has been considerable work trying to align WORDNET with other lexical resources, including Levin\u2019s verb classes (Green et al., 2001), topic categories from Roget\u2019s thesaurus (Mandala et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7502112,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "9f95b9cf82bbc2178a995f86cc346343a8aecd47",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to WordNet senses. The verbs were initially grouped into 491 classes based on syntactic features. Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval. Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment."
            },
            "slug": "Mapping-Lexical-Entries-in-a-Verbs-Database-to-Green-Pearl",
            "title": {
                "fragments": [],
                "text": "Mapping Lexical Entries in a Verbs Database to WordNet Senses"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708425"
                        ],
                        "name": "Jay J. Jiang",
                        "slug": "Jay-J.-Jiang",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Jiang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay J. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075147"
                        ],
                        "name": "D. Conrath",
                        "slug": "D.-Conrath",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Conrath",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1359050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b64e068a8face2540fc436af40dbcd2b0912bbf",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
            },
            "slug": "Semantic-Similarity-Based-on-Corpus-Statistics-and-Jiang-Conrath",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts that combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data."
            },
            "venue": {
                "fragments": [],
                "text": "ROCLING/IJCLCLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801071"
                        ],
                        "name": "Frank Smadja",
                        "slug": "Frank-Smadja",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Smadja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Smadja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Dice has been used in many NLP and IR applications including compiling multi-word translation lexicons (Smadja et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6720757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0285f18f1642c3684e6abb7d5162348278c41abf",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same. For example, Champollion translates make...decision, employment equity, and stock market into prendre...decision, equite en matiere d'emploi, and bourse respectively. Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation."
            },
            "slug": "Translating-Collocations-for-Bilingual-Lexicons:-A-Smadja-McKeown",
            "title": {
                "fragments": [],
                "text": "Translating Collocations for Bilingual Lexicons: A Statistical Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A program named Champollion is described which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations, to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1921454"
                        ],
                        "name": "Rila Mandala",
                        "slug": "Rila-Mandala",
                        "structuredName": {
                            "firstName": "Rila",
                            "lastName": "Mandala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rila Mandala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37090109"
                        ],
                        "name": "T. Tokunaga",
                        "slug": "T.-Tokunaga",
                        "structuredName": {
                            "firstName": "Takenobu",
                            "lastName": "Tokunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tokunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911717"
                        ],
                        "name": "Hozumi Tanaka",
                        "slug": "Hozumi-Tanaka",
                        "structuredName": {
                            "firstName": "Hozumi",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hozumi Tanaka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1106719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b88bf1efcbb775eeb499fe035590c3bb91abba4",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "WordNet has been used in information retrieval research by many researchers, but failed to improve the performance of their retrieval system. Thereby in this paper we investigate why the use of WordNet has not been successful. Based on this analysis we propose a method of making WordNet more useful in information retrieval applications. Experiments using several standard information retrieval test collections show that our method results in a significant improvement of information retrieval performance. 1 I n t r o d u c t i o n Development of WordNet began in 1985 at Princeton University (Miller, 1990). A team lead by Prof. George Miller aimed to create a source of lexical knowledge whose organization would reflect some of the recent findings of psycholinguistic research into the human lexicon. WordNet has been used in numerous natural language processing, such as part of speech tagging (Segond et al., 97), word sense disambiguation (Resnik, 1995), text categorization (Gomez-Hidalgo and Rodriguez, 1997), information extraction (Chai and Biermann, 1997), and so on with considerable success. However the usefulness of WordNet in information retrieval applications has been debatable. Information retrieval is concerned with locating documents relevant to a user's information needs from a collection of documents. The user describes his/her information needs with a query which consists of a number of words. The information retrieval system compares the query with documents in the collection and returns the documents that are likely to satisfy the user's information requirements. A fundamental weakness of current information retrieval methods is that the vocabulary that searchers use is often not the same as the one by which the information has been indexed. Query expansion is one method to solve this problem. The query is expanded using terms which have similar meaning or bear some relation to those in the query, increasing the chances of matching words in relevant documents. Expanded terms are generally taken from a thesaurus. Obviously, given a query, the information retrieval system must present all useful articles to the user. This objective is measured by recall, i.e. the proportion of relevant articles retrieved by the system. Conversely, the information retrieval system must not present any useless article to the user. This criteria is measured by precision, i.e. the proportion of retrieved articles that are relevant. Voorhees used WordNet as a tool for query expansion (Voorhees, 1994). She conducted experiments using the TREC collection (Voorhees and Harman, 1997) in which all terms in the queries were expanded using a combination of synonyms, hypernyms, and hyponyms. She set the weights of the words contained in the original query to 1, and used a combination of 0.1, 0.3, 0.5, 1, and 2 for the expansion terms. She then used the SMART Information Retrieval System Engine (Salton, 1971) to retrieve the documents. Through this method, Voorhees only succeeded in improving the performance on short queries and a tittle with no significant improvement for long queries. She further tried to use WordNet as a tool for word sense disambiguation (Voorhees, 1993) and applied it to text retrieval, but the performance of retrieval was degraded. Stairmand (Stairmand, 1997) used WordNet to compute lexical cohesion according to the method suggested by Morris (Morris and Hirst, 199 I), and applied this to information retrieval."
            },
            "slug": "The-Use-of-WordNet-in-Information-Retrieval-Mandala-Tokunaga",
            "title": {
                "fragments": [],
                "text": "The Use of WordNet in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates why the use of WordNet has not been successful and proposes a method of making WordNet more useful in information retrieval applications, which results in a significant improvement of information retrieval performance."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Banko and Brill (2001) present learning curves for confusion set disambiguation on several different machine learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "Experiments by Banko and Brill (2001) have used a 1 billion word corpus for ensemble experiments (as described in Section 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 56
                            }
                        ],
                        "text": "The increased complexity leads to results contradicting Banko and Brill (2001), which are then explored further using ensembles of different contextual complexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 115
                            }
                        ],
                        "text": "If this trend were to continue, then we would eventually expect no benefit from using an ensemble, as suggested by Banko and Brill (2001). However, the trend shown does not give a clear indication either way as to whether the individual extractors will eventually asymptote to the ensemble methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Banko and Brill (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 251
                            }
                        ],
                        "text": "However, corpus size is no longer a limiting factor: whereas up to now researchers have typically worked with corpora of between one million and one hundred million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 0
                            }
                        ],
                        "text": "Banko and Brill (2001) present learning curves for confusion set disambiguation on several different machine learning techniques. Early similarity systems used small (Hindle, 1990) or specialist (Grefenstette, 1994) corpora (Section 2.2.1) but with growing computing power more recent work by Lin (1998d) has used 300 million words of newspaper text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Banko and Brill (2001) found that for confusion set disambiguation with corpora larger than 100 million words, the best individual classifiers outperformed ensemble methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 31
                            }
                        ],
                        "text": "This technique is also used by Banko and Brill (2001) to create extremely large \u2018annotated\u2019 datasets for disambiguating confusion sets e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 129
                            }
                        ],
                        "text": "At 300 million words, all of the ensemble methods outperform the individual extractors which contradicts the results obtained by Banko and Brill (2001) for confusion set disambiguation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6645623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7628b62d64d2e5c33a13a5a473bc41b2391c1ebc",
            "isKey": true,
            "numCitedBy": 693,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
            },
            "slug": "Scaling-to-Very-Very-Large-Corpora-for-Natural-Banko-Brill",
            "title": {
                "fragments": [],
                "text": "Scaling to Very Very Large Corpora for Natural Language Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper examines methods for effectively exploiting very large corpora when labeled data comes at a cost, and evaluates the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambigsuation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The original 65 judgements have been further replicated, with a significantly increased number of word pairs, by Finkelstein et al. (2002) in the WordSimilarity-353 dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": false,
            "numCitedBy": 13575,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5471277"
                        ],
                        "name": "Chrysanne Dimarco",
                        "slug": "Chrysanne-Dimarco",
                        "structuredName": {
                            "firstName": "Chrysanne",
                            "lastName": "Dimarco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chrysanne Dimarco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574304"
                        ],
                        "name": "Manfred Stede",
                        "slug": "Manfred-Stede",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Stede",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Stede"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 3
                            }
                        ],
                        "text": "In DiMarco et al. (1993), they analyse usage notes in theOxford Advanced Learners Dictionary(1989) andLongman\u2019s Dictionary of Contemporary English(1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "DiMarco et al. (1993) add near-synonym distinctions to a Natural Language Generation (NLG) knowledge base and DiMarco (1994) shows how near-synonym differentia can form lexical relations between words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026.119\nB.1 Roget\u2019s Thesaurus Davidson (2002) entry forc mpany . . . . . . . . . . . . .137\nxvi\nList of Tables\n1.1 Example near-synonym differentia from DiMarco et al. (1993) . . . . . . . . .4\n3.1 Experimental Corpustatistics . . . . . . . . . . . . . . . . . . . . . . . . . .44\n3.2 Large-Scale\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10931060,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f3def0109e7537ab2528b6d9791bec02b29036a3",
            "isKey": true,
            "numCitedBy": 76,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "If we want to describe the action of someone who is looking out a window for an extended time, how do we choose between the words gazing, staring, and peering? What exactly is the difference between an rgument, a dispute, and a row? In this paper, we describe our research in progress on the problem of lexical choice and the representations of world knowledge and of lexical structure and meaning that the task requires. In particular, we wish to deal with nuances and subtleties of denotation and connotation--shades of meaning and of style--such as those illustrated by the examples above. We are studying the task in two related contexts: machine translation, and the generation of multilingual text from a single representation of content. This work brings together several elements of our earlier research: unilingual lexical choice (Miezitis 1988); multilingual generation (R6sner and Stede 1992a,b); representing and preserving stylistic nuances in translation (DiMarco 1990; DiMarco and Hirst 1990; Mah 1991); and, more generally, analyzing and generating stylistic nuances in text (DiMarco and Hirst 1993; DiMarco et al 1992; MakutaGiluk 1991; Maknta-Giluk and DiMarco 1993; BenHassine 1992; Green 1992a,b, 1993; Hoyt forthcoming). In the present paper, we concentrate on issues in lexical representation. We describe a methodology, based on dictionary usage notes, that we are using to discover the dimensions along which similar words can be differentiated, and we discuss a two-part representation for lexical differentiation. (Our related work on lexical choice itself and its integration with other components of text generation is discussed by Stede (1993a,b, forthcoming).) aspects of their usage. 1 Such differences can include the collocational constraints of the words (e.g., groundhog and woodchuck denote the same set of animals; yet Groundhog Day, * Woodchuck Day) and the stylistic and interpersonal connotations of the words (e.g., die, pass away, snuff it; slim, skinny; police oI~icer, cop, pig). In addition, many groups of words are plesionyms (Cruse 1986)--that is, nearly synonymous; forest and woods, for example, or stared and gazed, or the German words einschrauben, festschrauben, and festziehen. ~ The notions of synonymy and plesionymy can be made more precise by means of a notion of semantic distance (such as that invoked by Hirst (1987), for example, lexical disambiguation); but this is troublesome to formalize satisfactorily. In this paper it will suffice to rely on an intuitive understanding. We consider two dimensions along which words can vary: semantic and stylistic, or, equivalently, denotative and connotative. If two words differ semantically (e.g., mist, fog), then substituting one for the other in a sentence or discourse will not necessarily preserve truth conditions; the denotations are not identical. If two words differ (solely) in stylistic features (e.g., frugal, stingy), then intersubstitution does preserve truth conditions, but the connotation--the stylistic and interpersonal effect of the sentence--is changed, s Many of the semantic distinctions between plesionyms do not lend themselves to neat, taxonomic differentiation; rather, they are fuzzy, with plesionyms often having an area of overlap. For example, the boundary between forest and wood \u2019tract of trees\u2019 is vague, and there are some situations in which either word might be equally appropriate. 4"
            },
            "slug": "The-semantic-and-stylistic-differentiation-of-and-Dimarco-Hirst",
            "title": {
                "fragments": [],
                "text": "The semantic and stylistic differentiation of synonyms and near-synonyms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A methodology, based on dictionary usage notes, is described that is used to discover the dimensions along which similar words can be differentiated, and a two-part representation for lexical differentiation is discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746503"
                        ],
                        "name": "A. Kilgarriff",
                        "slug": "A.-Kilgarriff",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Kilgarriff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kilgarriff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49368431"
                        ],
                        "name": "C. Yallop",
                        "slug": "C.-Yallop",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Yallop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yallop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Landauer and Dumais test set was reused by Turney (2001), along with 50 synonym selection questions from English as a Second Language (ESL) tests."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17733757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93efaf47a56e3848996edf89f34eba876e3c18ff",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We first describe four varieties of thesaurus: (1) Roget-style, produced to help people find synonyms when they are writing; (2) WordNet and EuroWordNet; (3) thesauruses produced (manually) to support information retrieval systems; and (4) thesauruses produced automatically from corpora. We then contrast thesauruses and dictionaries, and present a small experiment in which we look at polysemy in relation to thesaurus structure. It has sometimes been assumed that different dictionary senses for a word that are close in meaning will be near neighbours in the thesaurus. This hypothesis is explored, using as inputs the hierarchical structure of WordNet 1.5 and a mapping between WordNet senses and the senses of another dictionary. The experiment shows that pairs of \u2018lexicographically close\u2019 meanings are frequently found in different parts of the hierarchy. In the first part of the paper, we present different varieties of thesaurus. In the second part, we contrast thesaurus word senses with dictionary word senses and present a small experiment in which we explore whether \u2018lexicographically close\u2019 meanings are often close in the WordNet network."
            },
            "slug": "What's-in-a-Thesaurus-Kilgarriff-Yallop",
            "title": {
                "fragments": [],
                "text": "What's in a Thesaurus?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The experiment shows that pairs of \u2018lexicographically close\u2019 meanings are frequently found in different parts of the hierarchy of WordNet 1.5 and a mapping between WordNet senses and the senses of another dictionary."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published as Curran and Moens (2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published in Curran and Moens (2002a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published as Curran and Moens (2002b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8927694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea419a6e8583d12f6b91be48df41edb7a8b8d6b6",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "slug": "Improvements-in-Automatic-Thesaurus-Extraction-Curran-Moens",
            "title": {
                "fragments": [],
                "text": "Improvements in Automatic Thesaurus Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An approximation algorithm is proposed, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9567965"
                        ],
                        "name": "Shankar Kumar",
                        "slug": "Shankar-Kumar",
                        "structuredName": {
                            "firstName": "Shankar",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shankar Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054396849"
                        ],
                        "name": "Christopher D. Richards",
                        "slug": "Christopher-D.-Richards",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Richards",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Richards"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16861729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6a371a80b3afaa2c09b62a0d48da3049bd68119",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to ordinary words and names, real text contains non-standard \u201cwords\" (NSWs), including numbers, abbreviations, dates, currency amounts and acronyms. Typically, one cannot find NSWs in a dictionary, nor can one find their pronunciation by an application of ordinary \u201cletter-to-sound\" rules. Non-standard words also have a greater propensity than ordinary words to be ambiguous with respect to their interpretation or pronunciation. In many applications, it is desirable to \u201cnormalize\" text by replacing the NSWs with the contextually appropriate ordinary word or sequence of words. Typical technology for text normalization involves sets of ad hoc rules tuned to handle one or two genres of text (often newspaper-style text) with the expected result that the techniques do not usually generalize well to new domains. The purpose of the work reported here is to take some initial steps towards addressing deficiencies in previous approaches to text normalization. We developed a taxonomy of NSWs on the basis of four rather distinct text types?news text, a recipes newsgroup, a hardware-product-specific newsgroup, and real-estate classified ads. We then investigated the application of several general techniques including n-gram language models, decision trees and weighted finite-state transducers to the range of NSW types, and demonstrated that a systematic treatment can lead to better results than have been obtained by the ad hoc treatments that have typically been used in the past. For abbreviation expansion in particular, we investigated both supervised and unsupervised approaches. We report results in terms of word-error rate, which is standard in speech recognition evaluations, but which has only occasionally been used as an overall measure in evaluating text normalization systems."
            },
            "slug": "Normalization-of-non-standard-words-Sproat-Black",
            "title": {
                "fragments": [],
                "text": "Normalization of non-standard words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A taxonomy of NSWs was developed on the basis of four rather distinct text types, and several general techniques including n-gram language models, decision trees and weighted finite-state transducers were investigated, demonstrating that a systematic treatment can lead to better results than have been obtained by the ad hoc treatments that have typically been used in the past."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Harabagiu et al. (1999) and Mihalcea and Moldovan (2001) created eXtended WORDNET by parsing the WORDNET glosses to create extra links."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Hearst (1992) and Caraballo and Charniak (1999) compare their hyponym extraction and specificity ordering techniques against the WORDNET hierarchy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The application of semantic similarity to supersense tagging follows similar work by Hearst and Sch\u00fctze (1993) and Widdows (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17314470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be2a4895a1e90ff1206103a860edbabd4174eacd",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Th~s paper presents an on-going project mtended to enhance WordNet molpholog~cally and semanttcally The mottvatmn for th~s work steams from the current hm~tat~ons of WordNet when used as a hngmst~c knowledge base We enwmon a software tool that automatically parses the conceptual defining glosses, attributing part-ofspeech tags and phrasal brackets The nouns, verbs, adjectives and adverbs from every defimtmn are then d~samb~guated and hnked to the corresponding synsets Th~s increases the connectlv~ty between synsets allowing the ~etneval of topically ~elated concepts Furthermore, the tool t~ansforms the glosses, first into logical forms and then into semantm fo~ms Usmg der~vatmnal morphology new hnks are added between the synsets 1 M o t i v a t i o n WordNet has already been ~ecogmzed as a valuable ~esource m the human language technolog> and know, ledge processing commumtms Its apphcabfl~ty has been c~ted m mo~e than 200 papers and s~stems have been m~plemented usmg WordNet A Wo~dNet bkbhog~aph~ ~s mamtamed at the Umve~mt) of Penns:~l~ama ( http //www c~s upenn edu/~oseph~ /wnbtblw html) In Europe, WordNet ~s being u~ed to develop a multflmgual database w~th basic semantic relatmns between words for several European languages (the EuroWordNet project) C a p a b i h t i e s WordNet was conceived as a machine-readable dmtlonary, followmg psychohngmstm principles Unhke standard alphabetmal dmt~onaHes ~hmh o~gamze vocabula~ms using mo~phologmal mmllm ltms, WordNet structures lex~cal reformation m terms of word meanings WordNet maps word forms m ~ord senses usmg the s)ntact~c category as a parametel Although it covers onl~ fouI paits of speech nouns verbs, adjectives and adverbs, it encompasses a large majont) of Enghsh words ( http / /www cogscz pmnceton edu/~..wn) Wolds of the same syntactm catego~) that can be used to express the same meamng are grouped into a smgle synonym set, called synset Words wlth multiple meanings (polysemous) belong to multiple synsets An ~mportant part of the 99 643 synsets encoded m WordNet 1 6 contain word collocatmns, thus representing complex nominals (e g the synset {manufacturer , maker, manufac tur ing business} , (omplex velbals (eg the synset {leave o f f i c e , qu i t , s tep down}, complex adjectlvals (e g the ~ynset { t rue , dead on t a r g e t } or complex adverbmls (e g the synset {out of hand, beyond cont ro l} The iep~esentatmn of collocatmns as synset entries p~ov~des for their semantm mterp~etatmn Wolds and concepts are furthei connected through a small set of lexmo-semantm relatmns The dominant semantm relatmn is the hypernymy, xvh~ch structures the noun concepts m 11 hmraichms and the verb concepts into 512 )he, atchins Thlee melonym Ielatlons are encoded between noun concepts the ha~_member, the ha~_~tu]f and the has_part ~elatlons Loglcal opelatlon~ betx~een events or entltms ale modeled through entazlment and cause_to ~elatmns between verb concepts or antonymy relatmns among nouns, veibs ad)ect~ves or adverb words The~e are only a few mo~phologmally motivated connectmns between x~ords known as perta~mym relatmns L l m l t a t m n s The mare ~eaknesses of \\Vo~dNet c~ted m the hte~ature ale 1 The lack oi connections between noun and verb hmrarctnes 2 Limited number of connectmns between topically related words 3 The lack of morpholog!cat relations 4 The absence of thema61c relatmns/ selectmnal restnctmns 5 Some concepts (word senses) and relatmns are mmsmg \" 6 Since glosses were written manually, sometimes the2e m a lack of umform~ty and consmtency 2n the defimtmns The key idea m our project is to put to wo, k the rich sourse of mformauon contained m glosses that now can be used only by humans to Iead the deflmtmn of synsets For example, Wo, dNet 1 6 hsts the concept {cat , t rue ca t ) with the gloss (fellne mammal usually havlng thlck soft fur and belng unable to roar, domestxc cats, wxldcats) Currently, from a concept like thin, only a few other concepts could be reached In Extended Wo2dNet, the concept {cat, true cat} will be ,elated to 215othel concepts (I0 from its own gloss, 38 flom the glosses of its hypern>ms, 25 concepts that use ~t m the*r glosses as a defining concept plus other 142 concepts with which the concept mteracts in these 25 glosses) Thin level of mformatmn ,s rich enough to presume that the Extended WordNet will work well as a knowledge base for common-sense reasoning 2 R e l a t e d w o r k Machine Readable DmUonanes (MRDs) have long been ,ecogmzed as ~aluable resources m computauonal hngmstlcs In their paper, Ide and Vetoms (Ide and Veloms, 1993) plojected a rather pes~lmmuc outlook for the uuhty of MRDs as knowledge sources, a view that has impeded the enthus2asm of some researchers (Wfiks et al 1996) make a strong argument m favor of using MRDs and shine thel, posluve experience w~th using some dlcuonarms The MmdNet project at Mmrosoft alms at fully automatmg the development of a vel} large lexlcal knowledge base using t~o MRDs the Longman DicUonary of ContemporaD Enghsh (LDOCE) and the American Heritage Third EdlUon (AHD3) Man:y techmcal aspects of thin project are rooted m the works of Vanderwende (Vanderwende 1996) and Richardson (R2chardson 1997) 3 W o r d s e n s e d i s a m b i g u a t i o n o f g l o s s c o n c e p t s There are se~e, al dlffe~ences bet~een gloss dlsamblguauon and text dlsamb~guatmn A n-la]oi difference is that m our project we know the meaning of each gloss, namely the synset to whmh a gloss apphes Second, the glosses contain a defimUon, comments, and one or more examples We address the word sense dmamblguaUon problem by using three complementary methods (a) heunstms, (b) conceptual dens,ty, and (c) staustins on large corpora The first two methods rely enurely on the mfolmaUon contained m WordNet, while the th,rd one uses other corpora Specffically, the sources of knowledge available to us me (1) lexlcal mformauon that includes part of speech, posluon of ~ords (1 e head word), and lexmal lelauons (2) collocauons and s)ntacuc patterns, (3) s}nset to which a gloss belongs, (4) hypernyms of s)nset and their glosses (5) synsets of pobsemouns x~o2ds and their glosses, (6) hypernyms of synsets of polysemous words, and their glosses, and so on M e t h o d 1 Classes of heur ,s t ,cs for word sense d m a r n b l g u a t m n A statable techmque for dmamblguatmg dmuonarms is to rely on heu!mucs able to cope with d~ffe2ent sources of mformauon Work m tins alea w~ doue by Ravin (Rax m 1990) in a similar project at IBM, (Klavans et al 1990), and others We present no~ some of the heunsUcs used by us 1. ClassH y p e r n y m s A way of explaining a concept m to speclahze a more general concept (, e a hypernym) It m hkely that an explanatmn begins with a phrase whose head is one of ~ts hypernyms, and the features are expressed either as attributes m the same phrase o2 as phrases attached to the first phrase Example The gloss of synset {xntrusxon} is (ent rance by f o r c e or without permxsslon or welcome) \u2022 e n t r a n c e # 3 , the head of the fiist phrase, is a hype~n)m of znt ruszon, thus we pick sense 3 of noun en t rance (The senses in Wo2dNet a2e ,anked acco, dmg to then frequency ot occmrence m the Brown corpus, e n t r a n c e # 3 means sense 3 of wo, d en t rance ) 2 Class L m g m s t l c Pa ra l l ehsm It 2s hkel? that the s} ntacuc pal allehsm of t~ o xx ord~ uanslates into semantic parallelmm and the ~xo2ds may have a common hypernym, or one m a hypernym of the other Fo~ adjectives, the hypein) m) is replaced by the similarity relation Other heuristics in this class check ~hether or not two pol)semous words belong to the same synset, or one is a hypern} m of the other, or if they belong to the same the2 arch:y Example The gloss of { i n t e r a c t i o n } is (a mutual or. reczprocal actlon) \u2022 Adject ive r e c i p r o c a l has only one sense ,n WordNet 1 6, whereas m u t u a l has two senses But we find tha t between sense 2 of m u t u a l and r e c i p r o c a l there is a szmdar l ink m WordNe t 1 6, thus pick"
            },
            "slug": "WordNet-2-A-Morphologically-and-Semantically-Harabagiu-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet 2 - A Morphologically and Semantically Enhanced Resource"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An on-going project mtended to enhance WordNet molpholog~cally and semanttcally and a software tool that automatically parses the conceptual defining glosses, attributing part-ofspeech tags and phrasal brackets is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713574"
                        ],
                        "name": "B. Boguraev",
                        "slug": "B.-Boguraev",
                        "structuredName": {
                            "firstName": "Branimir",
                            "lastName": "Boguraev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boguraev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144528232,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "92c053c7a865477a1f9cc52b067bf2fbbd695d55",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction / B. Boguraev and E. Briscoe -- Placing the dictionary on-line / Alshawi, B. Boguraev, and D. Carter -- An independent analysis of the LDOCE grammar coding system / E. Akkerman -- Utilising the LDOCE grammar codes / B Boguraev and E. Briscoe -- The derivation of a large computational lexicon o English from LDOCE / J. Carroll and G. Grover -- LDOCE and speech recognitio D. Carter -- Analysing the dictionary definitions / H. Alshawi -- Meaning an structure in dictionary definitions / P. Vossen, W. Meijs, and M. den Broede -- A tractable machine dictionary as a resource for computational semantics Y. Wilks ... [et al.] -- Conclusion / B. Boguraev and E. Briscoe -- Appendic : Lexical database-user guide -- Semantic types of LDOCE verbs -- \"Dative\" alternations -- Lexicon development environment-user guide -- The Longman semantic codes -- The Longman grammar coding system."
            },
            "slug": "Computational-lexicography-for-natural-language-Boguraev-Briscoe",
            "title": {
                "fragments": [],
                "text": "Computational lexicography for natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": false,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118077301"
                        ],
                        "name": "Jane Morris",
                        "slug": "Jane-Morris",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Morris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jane Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The hierarchy structure in Roget\u2019s has also been used in edge counting measures of semantic similarity (Jarmasz and Szpakowicz, 2003; McHale, 1998), and for computing lexical cohesion using lexical chains (Morris and Hirst, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10970495,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "ca40dc1300ab085406455894dd42fd02f9cc36f8",
            "isKey": false,
            "numCitedBy": 1091,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being \"about the same thing,\" and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences."
            },
            "slug": "Lexical-Cohesion-Computed-by-Thesaural-Relations-as-Morris-Hirst",
            "title": {
                "fragments": [],
                "text": "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure, and provide a semantic context for interpreting words, concepts, and sentences."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745366"
                        ],
                        "name": "Doug Beeferman",
                        "slug": "Doug-Beeferman",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Beeferman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Beeferman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Beeferman (1998) also used a 500 word window for trigger (collocation) extraction in a broadcast news corpus because it approximates average document length."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In terms of adding information to the existing English WORDNET, Beeferman (1998) adds several different types of information including phonetic and rhyming similarity from the CMU pronunciation dictionary using edit distance and anagram."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1819093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b1d13145a9f7a30fd9315a7a25f3bb5b6197d3d",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The study of lexical semantics has produced a systematic analysis of binary relationships between content words that has greatly benefited lexical search tools and natural language processing algorithms. We first introduce a database system called FreeNet that facilitates the description and exploration of finite binary relations. We then describe the design and implementation of Lexical FreeNet, a semantic network that mixes WordNet-derived semantic relations with data-derived and phonetically-derived relations. We discuss how Lexical FreeNet has aided in lexical discovery, the pursuit of linguistic and factual knowledge by the computer-aided exploration of lexical relations. 1 M o t i v a t i o n This paper discusses Lexical FreeNet, a database system designed to enhance lexical discovery. By this we mean the pursuit of linguistic and factual knowledge with the computer-aided exploration of lexical relations. Lexical FreeNet is a semantic network that leverages WordNet and other knowledge and data sources to facilitate the discovery of nontrivial lexical connections between words and concepts. A semantic network allied with the proper user interface can be a useful tool in its own right. By organizing words semantically rather than alphabetically, WordNet provides a means by which users can navigate its vocabulary logically, establishing connections between concepts and not simply character sequences. Exploring the WordNet hyponym tree starting at the word mammal, for instance, reveals to us that aardvarks are mammals; exploring WordNet's meronym relation at the word tv, mr*al reveals to us that mammals have ha i r . From these two explorations we can accurately conclude that aardvarks have hair. Lexical exploration need not be limited to one step at a time, however. Viewing a semantic network as a computational structure awaiting graph-theoretic queries gives us the freedom to demand services beyond mete lookup. \"Does the aardvark have hair?\", or \"What is the closest connection between aardvarks and hair?\" or \"How interchangably can the words aardvark and a n t e a t e r be used?\" are all reasonable questions with answers staring us in the 135 face. Of course, the idea of finding shortest paths in semantic networks (through so-called activationspreading or intersection search) is not new. But these questions have typically been asked of very limited graphs, networks for domains far narrower than the lexical space of English, say. We feel that formalizing how WordNet can be employed for this broader sort of lexical discovery is a good start. We also feel that it is necessary first to enrich the network with information that, as we shall see, cannot be easily gleaned from WordNet's current battery of relations. The very large electronic corpora and wide variety of linguistic resources that today's computing technology has enabled will in turn enable this. The remainder of this paper is organized as follows. We shall first describe in Section 2 the FreeNet database system for the expression and analysis of relational data. In Section 3 we'll describe the design and construction of an instance of this database called Lexical FreeNet. We'll conclude by providing examples of applications of Lexical FreeNet to lexical discovery."
            },
            "slug": "Lexical-Discovery-with-an-Enriched-Semantic-Network-Beeferman",
            "title": {
                "fragments": [],
                "text": "Lexical Discovery with an Enriched Semantic Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces a database system called FreeNet that facilitates the description and exploration of finite binary relations and describes the design and implementation of Lexical FreeNet, a semantic network that mixes WordNet-derived semantic relations with data-derived and phonetically-derived relations."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3132873"
                        ],
                        "name": "Gabriela Cavaglia",
                        "slug": "Gabriela-Cavaglia",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Cavaglia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriela Cavaglia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Even the Dewey decimal system has been combined with WORDNET (Cavagli\u00e0, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138302,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "1808647e12429fef0c839b71adc4756a340dcc6c",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexicon definition is one of the main bottlenecks in the development of new applications in the field of Information Extraction from text. Generic resources (e.g., lexical databases) are promising for reducing the cost of specific lexica definition, but they introduce lexical ambiguity. This paper proposes a methodology for building application-specific lexica by using WordNet. Lexical ambiguity is kept under control by marking synsets in WordNet with field labels taken from the Dewey Decimal Classification."
            },
            "slug": "The-Development-of-Lexical-Resources-for-Extraction-Cavaglia",
            "title": {
                "fragments": [],
                "text": "The Development of Lexical Resources for Information Extraction from Text Combining WordNet and Dewey Decimal Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A methodology for building application-specific lexica by using WordNet, where lexical ambiguity is kept under control by marking synsets in WordNet with field labels taken from the Dewey Decimal Classification."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work has been repeated using WORDNET glosses by Banerjee and Pederson (2002, 2003). Fox et al. (1988) extract a semantic network from two MRDs and Copestake (1990) extracts a taxonomy from the Longman\u2019s Dictionary of Contemporary English."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21336774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fbfb2d55c6609235ae4f102dfbbc2a44c793c13",
            "isKey": false,
            "numCitedBy": 1003,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an adaptation of Lesk's dictionary-based word sense disambiguation algorithm. Rather than using a standard dictionary as the source of glosses for our approach, the lexical database WordNet is employed. This provides a rich hierarchy of semantic relations that our algorithm can exploit. This method is evaluated using the English lexical sample data from the SENSEVAL-2 word sense disambiguation exercise, and attains an overall accuracy of 32%. This represents a significant improvement over the 16% and 23% accuracy attained by variations of the Lesk algorithm used as benchmarks during the Senseval-2 comparative exercise among word sense disambiguation systems."
            },
            "slug": "An-Adapted-Lesk-Algorithm-for-Word-Sense-Using-Banerjee-Pedersen",
            "title": {
                "fragments": [],
                "text": "An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents an adaptation of Lesk's dictionary-based word sense disambiguation algorithm that uses the lexical database WordNet as the source of glosses for this approach, and attains an overall accuracy of 32%."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40544823"
                        ],
                        "name": "Hongyan Jing",
                        "slug": "Hongyan-Jing",
                        "structuredName": {
                            "firstName": "Hongyan",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyan Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601166"
                        ],
                        "name": "E. Tzoukermann",
                        "slug": "E.-Tzoukermann",
                        "structuredName": {
                            "firstName": "Evelyne",
                            "lastName": "Tzoukermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tzoukermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 70
                            }
                        ],
                        "text": "This is true for practically every work in the literature, except for Jing and Tzoukermann (1999), which compares all pairs of context elements using mutual information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 563753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d9d029af6f27b25789bbad3ee484a5b0f0e8339",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to information retrieval based on context distance and morphology. Context distance is a measure we use to assess the closeness of word meanings. This context distance model measures semantic distances between words using the local contexts of words within a single document as well as the lexical co-occurrence information in the set of documents to be retrieved. We also propose to integrate the context distance model with morphological analysis in determining word similarity so that the two can enhance each other. Using the standard vector-space model, we evaluated the proposed method on a subset of TREC-4 corpus (AP88 and AP90 collection, 158,240 documents, 49 queries). Results show that this method improves the 11-point average precision by 8.6%."
            },
            "slug": "Information-retrieval-based-on-context-distance-and-Jing-Tzoukermann",
            "title": {
                "fragments": [],
                "text": "Information retrieval based on context distance and morphology"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed context distance model measures semantic distances between words using the local contexts of words within a single document as well as the lexical co-occurrence information in the set of documents to be retrieved to improve the 11-point average precision."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(2003) combine several different similarity systems including Latent Semantic Analysis (Landauer and Dumais, 1997), pointwise mutual information (Turney, 2001), thesaurus-based similarity and similarity calculated using cooccurrence scores from Google."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5509836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e517e1645708e7b050787bb4734002ea194a1958",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."
            },
            "slug": "Mining-the-Web-for-Synonyms:-PMI-IR-versus-LSA-on-Turney",
            "title": {
                "fragments": [],
                "text": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In English, synonym dictionaries were slower to appear because the vocabulary was smaller and rapidly absorbing new words and evolving meanings (Landau, 1989, pp. 104\u2013105)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 39
                            }
                        ],
                        "text": "32\n2.3 New Oxford Thesaurus of English(Hanks, 2000) entry forcompany . . . . . ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 232
                            }
                        ],
                        "text": "John Carroll very graciously provided the RASP BNCdependencies used in Chapter 3, Massimiliano Ciaramita providing his supersense data used in Chapter 6 and Robert Curran kindly typed in 300 entries from the New Oxford Thesaurus of English for the evaluation described in Chapter 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 138
                            }
                        ],
                        "text": "In DiMarco et al. (1993), they analyse usage notes in theOxford Advanced Learners Dictionary(1989) andLongman\u2019s Dictionary of Contemporary English(1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 89
                            }
                        ],
                        "text": "Apart from discriminating entries in popular works such as Fowler\u2019sA Dictionary of Modern English Usage(1926), their popularity has been limited except in advanced learner dictionaries."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "The popularity of Roget\u2019s 1852 workThesaurus of English Words and Phraseswas instrumental in the assimilation of the wordthesaurus, from the Greek meaningstorehouseor treasure, into English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "A typical example was William Taylor\u2019sEnglish Synonyms Discriminated, published in 1813."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 119
                            }
                        ],
                        "text": "7\n1.2 An entry from theMedical Subject Headings. . . . . . . . . . . . . . . . . . 8\n2.1 company in Roget\u2019sThesaurus of English words and phrases(Roget, 1911) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 127
                            }
                        ],
                        "text": "These were often targeted at \u201ccoming up members of society and to eligible foreigners, whose inadequate grasp of the nuances of English synonymies might lead them to embarrassing situations\u201d (Emblen, 1970, page 263)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The English thesaurus has been a popular arbiter of similarity for 150 years (Davidson, 2002), and is strongly associated with the work of Peter Mark Roget (Emblen, 1970)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60064218,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c14a738d954c068339b89d0398ff1f1989f94c33",
            "isKey": true,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The New Oxford Thesaurus of English is the result of intensive research and exploitation of Oxford's unique bank of linguistic databases. It provides comprehensive coverage of English as it is used today, including World English, and rare, unusual, and colloquial senses. The A-Z format, elegant open design with new elements starting on a new line, and the most useful alternative words given first, make this thesaurus particularly accessible. Help on how to use senses correctly is given through practical example phrases and the cross-referencing system takes the user to other useful entries. Opposite and related words and combining forms are clearly marked to distinguish them from the rest of the entry. In-text notes give invaluable advice on the use of awkward and confusable synonyms, and boxed noun lists provide a wealth of information on a wide variety of subjects, from actors and actresses to types of whisky."
            },
            "slug": "The-New-Oxford-Thesaurus-of-English-Hanks",
            "title": {
                "fragments": [],
                "text": "The New Oxford Thesaurus of English"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The New Oxford Thesaurus of English is the result of intensive research and exploitation of Oxford's unique bank of linguistic databases, and provides comprehensive coverage of English as it is used today, including World English, and rare, unusual, and colloquial senses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146254443"
                        ],
                        "name": "Zhibiao Wu",
                        "slug": "Zhibiao-Wu",
                        "structuredName": {
                            "firstName": "Zhibiao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibiao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12009057,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb",
            "isKey": false,
            "numCitedBy": 3703,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentences as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection."
            },
            "slug": "Verb-Semantics-and-Lexical-Selection-Wu-Palmer",
            "title": {
                "fragments": [],
                "text": "Verb Semantics and Lexical Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT), and sees the approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7579604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cee045e890270abae65455667b292db355d53728",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "slug": "Snowball:-extracting-relations-from-large-Agichtein-Gravano",
            "title": {
                "fragments": [],
                "text": "Snowball: extracting relations from large plain-text collections"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper develops a scalable evaluation methodology and metrics for the task, and presents a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "venue": {
                "fragments": [],
                "text": "DL '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6763915,
            "fieldsOfStudy": [
                "Linguistics",
                "Biology"
            ],
            "id": "b0d7348e354a290fb51ac12315b10b2383c35533",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between near-synonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry."
            },
            "slug": "Acquiring-Collocations-for-Lexical-Choice-between-Inkpen-Hirst",
            "title": {
                "fragments": [],
                "text": "Acquiring Collocations for Lexical Choice between Near-Synonyms"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour is extended, and associations are looked at as a possible source of learning more about nuances that the near- synonyms may carry."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574304"
                        ],
                        "name": "Manfred Stede",
                        "slug": "Manfred-Stede",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Stede",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Stede"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 477974,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "82623280b7d5289719665d4b7ce16e1b5f0cee95",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "When a lexical item is selected in the language production process, it needs to be explained why none of its superordinates gets selected instead, since their applicability conditions are fulfilled all the same. This question has received much attention in cognitive modelling and not as much in other branches of NLG. This paper describes the various approaches taken, discusses the reasons why they are so different, and argues that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics."
            },
            "slug": "The-hyperonym-problem-revisited:-Conceptual-and-in-Stede",
            "title": {
                "fragments": [],
                "text": "The hyperonym problem revisited: Conceptual and lexical hierarchies in language generation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 41
                            }
                        ],
                        "text": ", 1995, 1994), word sense disambiguation (Dagan et al., 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lee and Pereira (1999) only used the object relations and Lapata (2001) only used the object and subject relations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1997) considers both approaches, using a back-off smoothing, with weight\u03b1(x), to the marginalp(y) for theKL -divergence:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1999) gives an algebraic manipulation of 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 117
                            }
                        ],
                        "text": "Weeds and Weir (2003) have used R ASP GRs for vector-space semantic similarity comparing the work of Lin (1998d) and Lee (1999) in terms of precision and recall."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "Some measure functions are designed to compare frequency distributions, for instance, the information theoretic measures proposed by Lee (1999). In these cases the weight function is either the relative frequency or is a normalisation (to a total probability of one) of some other previously applied weight function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1997) gives several different motivations for the use of the KL -divergence as a measure of distributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1999) gives an algebraic manipulation of 4.25 which only requires calculation over the shared attributes, giving some performance improvement over the n\u00e4\u0131ve approach. She also demonstrates that A(p,q) has a maximum value of 2log2. Lee (1997) compares divergence with other measures graphically suggesting that they are less susceptible to sampling error because their values deviate less for small changes in the parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1999) quotes a bounding relationship between the L1 norm and theKL -divergence (see Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 459591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51d1ecb07017402702feb38f2f881d6723297b2b",
            "isKey": true,
            "numCitedBy": 177,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating word co-occurrence probabilities is a problem underlying many applications in statistical natural language processing. Distance-weighted (or similarityweighted) averaging has been shown to be a promising approach to the analysis of novel co-occurrences. Many measures of distributional similarity have been proposed for use in the distance-weighted averaging framework; here, we empirically study their stability properties, finding that similarity-based estimation appears to make more efficient use of more reliable portions of the training data. We also investigate properties of the skew divergence, a weighted version of the KullbackLeibler (KL) divergence; our results indicate that the skew divergence yields better results than the KL divergence even when the KL divergence is applied to more sophisticated probability estimates."
            },
            "slug": "On-the-effectiveness-of-the-skew-divergence-for-Lee",
            "title": {
                "fragments": [],
                "text": "On the effectiveness of the skew divergence for statistical language analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that similarity-based estimation appears to make more efficient use of more reliable portions of the training data, and the skew divergence yields better results than the KL divergence even when the Kuala Lumpur divergence is applied to more sophisticated probability estimates."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876168"
                        ],
                        "name": "A. Lascarides",
                        "slug": "A.-Lascarides",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Lascarides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lascarides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lapata also uses human judgements to evaluate probabilistic models for logical metonymy (Lapata and Lascarides, 2003) and smoothing (Lapata et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5653822,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f932caac89709a716a7d3e6632caf9f34d709518",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don't rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model's ranking of meanings correlates reliably with human intuitions."
            },
            "slug": "A-Probabilistic-Account-of-Logical-Metonymy-Lapata-Lascarides",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Account of Logical Metonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article acquires the meanings of metonymic verbs and adjectives from a large corpus and proposes a probabilistic model that provides a ranking on the set of possible interpretations and identifies the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 54
                            }
                        ],
                        "text": "This work has been repeated using W ORDNET glosses by Banerjee and Pederson (2002, 2003). Fox et al. (1988) extract a semantic network from two MRDs and Copestake (1990) extracts a taxonomy from theLongman\u2019s Dictionary of Contemporary English ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56684730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bddf98047d69af505a0e33643565ecec280fd1c9",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses). This measure is unique in that it extends the glosses of the concepts under consideration to include the glosses of other concepts to which they are related according to a given concept hierarchy. We show that this new measure reasonably correlates to human judgments. We introduce a new method of word sense disambiguation based on extended gloss overlaps, and demonstrate that it fares well on the SENSEVAL-2 lexical sample data."
            },
            "slug": "Extended-Gloss-Overlaps-as-a-Measure-of-Semantic-Banerjee-Pedersen",
            "title": {
                "fragments": [],
                "text": "Extended Gloss Overlaps as a Measure of Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses) and reasonably correlates to human judgments is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119515866"
                        ],
                        "name": "G. Ruge",
                        "slug": "G.-Ruge",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Ruge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ruge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13966972,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "f6e1db64a6e7e724bfc088ed0f3c2fcf3ede06d5",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Is it possible to discover semantic term relations useful for thesauri without any semantic information? Yes, it is. A recent approach for automatic thesaurus construction is based on explicit linguistic knowledge, i.e. a domain independent parser without any semantic component, and implicit linguistic knowledge contained in large amounts of real world texts. Such texts include implicitly the linguistic, especially semantic, knowledge that the authors needed for formulating their texts. This article explains how implicit semantic knowledge can be transformed to an explicit one. Evaluations of quality and performance of the approach are very encouraging."
            },
            "slug": "Automatic-Detection-of-Thesaurus-relations-for-Ruge",
            "title": {
                "fragments": [],
                "text": "Automatic Detection of Thesaurus relations for Information Retrieval Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article explains how implicit semantic knowledge can be transformed to an explicit one and evaluates of quality and performance of the approach are very encouraging."
            },
            "venue": {
                "fragments": [],
                "text": "Foundations of Computer Science: Potential - Theory - Cognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5139774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1a3b5a20e77d8ac94967dc48173c48af3012eaf",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations."
            },
            "slug": "Constructing-Semantic-Space-Models-from-Parsed-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Constructing Semantic Space Models from Parsed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach for constructing semantic spaces that takes syntactic relations into account is presented, which is a formalisation for this class of models and their adequacy on two modelling tasks is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This work has been published as Curran and Moens (2002b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It contains over 2 billion words of text, part of which was collected and processed for Curran and Osborne (2002). It is used in the large-scale experiments and detailed evaluation in Chapter 6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the purposes of comparing systems it turns out that the different evaluation metrics are fairly strongly correlated (Curran, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 32
                            }
                        ],
                        "text": "This work has been published as Curran (2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first section describes the use of ensembles for improving the quality of similarity results, and corresponds to part of Curran (2002). The second section improves the algorithmic complexity of the na\u0131\u0308ve nearest-neighbour algorithm, and corresponds to part of Curran and Moens (2002a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5867456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f201774b00708aef88cc14c5cd4347a376b52142",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplicity of their evaluation task and individual classifiers.Our work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words. We examine our conflicting results in terms of the constraints on, and complexity of, different contextual representations, which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
            },
            "slug": "Ensemble-Methods-for-Automatic-Thesaurus-Extraction-Curran",
            "title": {
                "fragments": [],
                "text": "Ensemble Methods for Automatic Thesaurus Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words, and examines the constraints on, and complexity of, different contextual representations which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744846"
                        ],
                        "name": "Jeffrey P. Bigham",
                        "slug": "Jeffrey-P.-Bigham",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Bigham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey P. Bigham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2541093"
                        ],
                        "name": "V. Shnayder",
                        "slug": "V.-Shnayder",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Shnayder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Shnayder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "This problem is quite similar to the vocabulary tests used by Turney et al. (2003) for evaluation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 0
                            }
                        ],
                        "text": "Turney et al. (2003) use these tests to evaluate ensembles of similarity systems and added analogy questions from the SAT test for analysing the performance of their system on analogical reasoning problems. Finally, Jarmasz (2003) and Jarmasz and Szpakowicz (2003) extend the vocabulary evaluation by including questions extracted from the Word Powersection ofReader\u2019s Digest ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Turney et al. (2003) use these tests to evaluate ensembles of similarity systems and added analogy questions from the SAT test for analysing the performance of their system on analogical reasoning problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fced0c675eff3e8a9b0c18c169aa87c0d30695e3",
            "isKey": true,
            "numCitedBy": 158,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics|synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The dierences among the three rules are not statistically signicant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems."
            },
            "slug": "Combining-Independent-Modules-to-Solve-Synonym-and-Turney-Littman",
            "title": {
                "fragments": [],
                "text": "Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Three merging rules for combining probability distributions are examined: the well known mixture rule, the logarithmic rule, and a novel product rule that were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics|synonym questions and analogy questions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40639594"
                        ],
                        "name": "M. Bryant",
                        "slug": "M.-Bryant",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bryant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bryant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 48
                            }
                        ],
                        "text": "ThePOStagging uses the CLAWS4 tagset and tagger (Leech et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16137021,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3cc8d65946052ef661e1b6b4c7b5d8322ab37f35",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100-million-word British National Corpus, of which c.70 million words have been tagged at the time of writing (April 1994)) We will emphasise the goals of (a) gener~d-purpose adaptability, (b) incorporation of linguistic knowledge to improve quality ,and consistency, and (c) accuracy, measured consistently and in a linguistically informed way. The British National Corpus (BNC) consists of c.100 million words of English written texts and spoken transcriptions, sampled from a comprehensive range of text types. The BNC includes 10 million words of spoken h'mguage, c.45% of which is impromptu conversation (see Crowdy, forthcoming). It also includes ,an immense variety of written texts, including unpublished materials. The gr,'unmatical tagging of the corpus has therefore required the 'super-robustness' of a tagger which can adapt well to virtually all kinds of text. The tagger also has had to be versatile in dealing with different tagsets (sets of grammatical category labels-see 3 below) and accepting text in varied input formats. For the purposes of the BNC, l, he tagger has been requircd both to accept and to output text in a corpus-oriented TEl-confonnant mark-up definition known as CDIF (Corpus Document Interchange Format), but within this format many variant fornaats (affecting, for example, segmentation into words and sentences) can be readily accepted. In addition, CLAWS al-"
            },
            "slug": "CLAWS4:-The-Tagging-of-the-British-National-Corpus-Leech-Garside",
            "title": {
                "fragments": [],
                "text": "CLAWS4: The Tagging of the British National Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100-million-word British National Corpus, and emphasise the goals of adaptability, incorporation of linguistic knowledge to improve quality, consistency, and accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 498266,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "feb5f9a821a90b30020872275a0c1d0b0ad72106",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We automatically classify verbs into lexical semantic classes, based on distributions of indicators of verb alternations, extracted from a very large annotated corpus. We address a problem which is particularly difficult because the verb classes, although semantically different, show similar surface syntactic behavior. Five grammatical features are sufficient to reduce error rate by more than 50% over chance: we achieve almost 70% accuracy in a task whose baseline performance is 34%, and whose expert-based upper bound we calculated at 86.5%. We conclude that corpus-driven extraction of grammatical features is a promising methodology for find-grained verb classification."
            },
            "slug": "Automatic-Lexical-Acquisition-Based-on-Statistical-Stevenson-Merlo",
            "title": {
                "fragments": [],
                "text": "Automatic Lexical Acquisition Based on Statistical Distributions"
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Weeds and Weir (2003) have used RASP GRs for vector-space semantic similarity comparing the work of Lin (1998d) and Lee (1999) in terms of precision and recall."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11892605,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "76e4e034c20bea86edcc6e71bbaddb47fafeecbc",
            "isKey": false,
            "numCitedBy": 2124,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The meaning of an English word can vary widely depending on which sense is intended. Does a fireman feed fires or put them out? It depends on whether or not he is on a steam locomotive. I am trying to decide automatically which sense of a word is intended (in written English) by using machine readable dictionaries, and looking for words in the sense definitions that overlap words in the definition of nearby words. The problem of deciding which sense of a word was intended by the writer is an important problem in information retrieval systems. At present most retrieval systems rely on manual indexing; if this is to be replaced with automatic text processing, it would be very desirable to recognize the correct sense of each word as often as possible. Previous work has generally either suggested (a) detailed frames describing the particular word senses,t*\u2019 or (b) global statistics about the word occurrences.3 The first has not yet been made available in any real application, and the second may give the wrong answer in specific local instances. This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context. To consider the example in the title, look at the definition of pine in the Oxford Advanced Learner\u2019s Dictionary of Current English: there are, of course, two major senses. \u201ckind of evergreen tree with needle-shaped leaves.. .\u201d and \u201cwaste away through sorrow or illness...\u201d And cone has three separate definitions: \u201csolid body which narrows to a\u2019 point . . . . *\u2019 \u201csomething of this shape w-hether solid or hollow...,\u201d and \u201cfruit of certain evergreen trees...\u201d Note that both evergreen and tree are common to two of the sense definitions: thus a program could guess that if the two words pine cone appear together, the likely senses are those of the tree and its fruit"
            },
            "slug": "Automatic-sense-disambiguation-using-machine-how-to-Lesk",
            "title": {
                "fragments": [],
                "text": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context to decide which sense of a word is intended (in written English)."
            },
            "venue": {
                "fragments": [],
                "text": "SIGDOC '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3339702"
                        ],
                        "name": "E. Joanis",
                        "slug": "E.-Joanis",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Joanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Joanis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1543977,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "078cb05b6faae5b488630a77319ce8739adfa95e",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic Verb Classification Using a General Feature Space Eric Joanis Master of Science Graduate Department of Computer Science University of Toronto 2002 We develop a general feature space that can be used for the semantic classification of English verbs. We design a technique to extract these features from a large corpus of English, while trying to maintain portability to other languages\u2014the only language-specific tools we use to extract our core features are a part-of-speech tagger and a partial parser. We show that our general feature space reduces the chance error rate by 40% or more in ten experiments involving from two to thirteen verb classes. We also show that it usually performs as well as features that are selected using specific linguistic expertise, and that it is therefore unnecessary to manually do linguistic analysis for each class distinction of interest. Finally, we consider the use of an automatic feature selection technique, stepwise feature selection, and show that it does not work well with our feature space."
            },
            "slug": "Automatic-Verb-Classification-Using-a-General-Space-Joanis",
            "title": {
                "fragments": [],
                "text": "Automatic Verb Classification Using a General Feature Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A general feature space that can be used for the semantic classification of English verbs is developed and it is shown that it usually performs as well as features that are selected using specific linguistic expertise, and that it is therefore unnecessary to manually do linguistic analysis for each class distinction of interest."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30529950,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6d8018bd8b288baca0c55522877efd1b49258747",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 172,
            "paperAbstract": {
                "fragments": [],
                "text": "A central concern of psycholinguistic research is explaining the relative ease or difficulty involved in processing words. In this thesis, we explore the connection between lexical processing effort and measurable properties of the linguistic environment. Distributional information (information about a word\u2019s contexts of use) is easily extracted from large language corpora in the form of co-occurrence statistics. We claim that such simple distributional statistics can form the basis of a parsimonious model of lexical processing effort. Adopting the purposive style of explanation advocated by the recent rational analysis approach to understanding cognition, we propose that the primary function of the human language processor is to recover meaning from an utterance. We assume that for this task to be efficient, a useful processing strategy is to use prior knowledge in order to build expectations about the meaning of upcoming words. Processing effort can then be seen as reflecting the difference between \u2018expected\u2019 meaning and \u2018actual\u2019 meaning. Applying the tools of information theory to lexical representations constructed from simple distributional statistics, we show how this quantity can be estimated as the amount of information conveyed by a word about its contexts of use. The hypothesis that properties of the linguistic environment are relevant to lexical processing effort is evaluated against a wide range of empirical data, including both new experimental studies and computational reanalyses of published behavioural data. Phenomena accounted for using the current approach include: both singleword and multiple-word lexical priming, isolated word recognition, the effect of contextual constraint on eye movements during reading, sentence and \u2018feature\u2019 priming, and picture naming performance by Alzheimer\u2019s patients. Besides explaining a broad range of empirical findings, our model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature, such as the word frequency effect and the need for distinct mechanisms to explain semantic and associative priming. We conclude by emphasising the important role of distributional information in explanations of lexical processing effort, and suggest that environmental factors in general should given a more prominent place in theories of human language processing."
            },
            "slug": "Environmental-Determinants-of-Lexical-Processing-McDonald",
            "title": {
                "fragments": [],
                "text": "Environmental Determinants of Lexical Processing Effort"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709847"
                        ],
                        "name": "E. Eskin",
                        "slug": "E.-Eskin",
                        "structuredName": {
                            "firstName": "Eleazar",
                            "lastName": "Eskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 33
                            }
                        ],
                        "text": "Agichtein and Gravano (2000) and Agichtein et al. (2000) use a similar approach to extract information about entities, such as the location of company headquarters, and Sundaresan and Yi (2000) identify acronyms and their expansions in web pages."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10810844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98a39e8a2d31d0a6adeccb86effb990067d6ab85",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best exploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. Our Snowball system extracts these relations from document collections starting with only a handful of user-provided example tuples. Based on these tuples, Snowball generates patterns that are used, in turn, to find more tuples. In this paper we introduce a new pattern and tuple generation scheme for Snowball, with different strengths and weaknesses than those of our original system. We also show preliminary results on how we can combine the two versions of Snowball to extract tuples more accurately."
            },
            "slug": "Combining-Strategies-for-Extracting-Relations-from-Agichtein-Eskin",
            "title": {
                "fragments": [],
                "text": "Combining Strategies for Extracting Relations from Text Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a new pattern and tuple generation scheme for Snowball, with different strengths and weaknesses than those of the original system, and shows preliminary results on how the two versions of Snowball can be combined more accurately."
            },
            "venue": {
                "fragments": [],
                "text": "ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329844"
                        ],
                        "name": "A. Luk",
                        "slug": "A.-Luk",
                        "structuredName": {
                            "firstName": "Alpha",
                            "lastName": "Luk",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Luk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This in itself is not a new concept, mutual information having been successfully used as a weighting function by a number of systems in the past (Hindle, 1990; Lin, 1998c; Luk, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9177224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3650a346da2656a6d441470c50c2f85ff076532",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definition-based conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information."
            },
            "slug": "Statistical-Sense-Disambiguation-with-Relatively-Luk",
            "title": {
                "fragments": [],
                "text": "Statistical Sense Disambiguation with Relatively Small Corpora Using Dictionary Definitions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the definition-based conceptual co-occurrence data collected from the relatively small Brown corpus, the sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60354878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a145854ede2f62098bf4e92de1584ab270b676c9",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech (\"tagging\"). Section 2 is an alphabetical list of the parts of speech encoded in the annotation systems of the Penn Treebank Project, along with their corresponding abbreviations (\"tags\") and some information concerning their definition. This section allows you to find an unfamiliar tag by looking up a familiar part of speech. Section 3 recapitulates the information in Section 2, but this time the information is alphabetically ordered by tags. This is the section to consult in order to find out what an unfamiliar tag means. Since the parts of speech are probably familiar to you from high school English, you should have little difficulty in assimilating the tags themselves. However, it is often quite difficult to decide which tag is appropriate in a particular context. The two sections 4 and 5 therefore include examples and guidelines on how to tag problematic cases. If you are uncertain about whether a given tag is correct or not, refer to these sections in order to ensure a consistently annotated text. Section 4 discusses parts of speech that are easily confused and gives guidelines on how to tag such cases, while Section 5 contains an alphabetical list of specific problematic words and collocations. Finally, Section 6 discusses some general tagging conventions. One general rule, however, is so important that we state it here. Many texts are not models of good prose, and some contain outright errors and slips of the pen. Do not be tempted to correct a tag to what it would be if the text were correct; rather, it is the incorrect word that should be tagged correctly. Disciplines Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-47. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/570 Part-of-S peech Tagging Guidelines For The Penn Treebank Project (3rd Revision) MS-CIS-90-47 LINC LAB 178"
            },
            "slug": "Part-of-Speech-Tagging-Guidelines-for-the-Penn-(3rd-Santorini",
            "title": {
                "fragments": [],
                "text": "Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision)"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech (\"tagging\") and discusses parts of speech that are easily confused and gives guidelines on how to tag such cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35258592"
                        ],
                        "name": "David J. Weir",
                        "slug": "David-J.-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 192
                            }
                        ],
                        "text": "Clark and Weir (2002) have shown measures calculated over the W ORDNET hierarchy can be used for pseudo disambiguation, parse selection (Clark, 2001) and prepositional phrase ( PP) attachment (Clark and Weir, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2454033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "266f957b08b96fd65fc93c01f3dc3ad017b5e6e2",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate. In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses. There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to determine a suitable level of generalization. In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods. Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik's measure of selectional preference. In addition, the performance of our method is investigated using both the standard Pearson chi-square statistic and the log-likelihood chi-square statistic."
            },
            "slug": "Class-Based-Probability-Estimation-Using-a-Semantic-Clark-Weir",
            "title": {
                "fragments": [],
                "text": "Class-Based Probability Estimation Using a Semantic Hierarchy"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate, and a procedure is developed that uses a chi-square test to determine a suitable level of generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329862"
                        ],
                        "name": "Sharon A. Caraballo",
                        "slug": "Sharon-A.-Caraballo",
                        "structuredName": {
                            "firstName": "Sharon",
                            "lastName": "Caraballo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharon A. Caraballo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14145448,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "e824ab1e54192a9628a9986c05aeb89f0c1910d7",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we use a large text corpus to order nouns by their level of specificity. This semantic information can for most nouns be determined with over 80% accuracy using simple statistics from a text corpus without using any additional sources of semantic knowledge. This kind of semantic information can be used to help in automatically constructing or augmenting a lexical database such as WordNet."
            },
            "slug": "Determining-the-specificity-of-nouns-from-text-Caraballo-Charniak",
            "title": {
                "fragments": [],
                "text": "Determining the specificity of nouns from text"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A large text corpus is used to order nouns by their level of specificity, which can for most nouns be determined with over 80% accuracy using simple statistics from a text corpus without using any additional sources of semantic knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118384241"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8497895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d037236de0b1bc0cb5d95601b313cc90ed0b38a5",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods."
            },
            "slug": "Generalizing-Case-Frames-Using-a-Thesaurus-and-the-Li-Abe",
            "title": {
                "fragments": [],
                "text": "Generalizing Case Frames Using a Thesaurus and the MDL Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new method for automatically acquiring case frame patterns from large corpora based on the Minimum Description Length (MDL) principle is proposed, which provably obtains the optimal tree cut model for the given frequency data of a case slot."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 0
                            }
                        ],
                        "text": "Miller and Charles (1991) repeated these experiments 25 years later on a 30 pair subset with 38 subjects, who were asked specifically forsimilarity of meaningand told to ignore any other semantic relations. Later still Resnik (1995) repeated the subset experiment with 10 subjects via email."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Miller and Charles (1991) repeated these experiments 25 years later on a 30 pair subset with 38 subjects, who were asked specifically forsimilarity of meaningand told to ignore any other semantic relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145580646,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "402627e4eb8c95e4aae3026fd921aa08cd792006",
            "isKey": false,
            "numCitedBy": 1678,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be."
            },
            "slug": "Contextual-correlates-of-semantic-similarity-Miller-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of semantic similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 170735999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f18fa9f885a45853d544f73ec5e53f0e3cc67808",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The usefulness and feasibility of automatically training a syntactic wordclass tagger instead of hand-crafting it motivated a large body of work on statistical and rule-learning approaches to the problem. Syntactic wordclass taggers trained on corpora are claimed to be equally accurate as, and more robust and more portable than, hand-crafted systems1. Moreover, development time is considerably faster. Recently, inductive machine learning approaches such as connectionist learning algorithms, decision tree induction and case-based learning have also been applied to the syntactic wordclass disambiguation problem. In some cases these approaches have interesting properties not present in existing statistical and rule-based approaches."
            },
            "slug": "Machine-Learning-Approaches-Daelemans",
            "title": {
                "fragments": [],
                "text": "Machine Learning Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work has shown that inductive machine learning approaches such as connectionist learning algorithms, decision tree induction and case-based learning have interesting properties not present in existing statistical and rule-based approaches to the syntactic wordclass disambiguation problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726601"
                        ],
                        "name": "R. Hwa",
                        "slug": "R.-Hwa",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Hwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hwa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Jarmasz (2003) gives a list of stopwords he uses in similarity experiments and Grefenstette uses a stopword list (1994, page 151) in the Webster\u2019s dictionary evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There is little research into the effect of corpus type, genre and size on performance of NLP systems; exceptions include studies in cross-domain parsing (Gildea, 2001; Hwa, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7117045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3eb3a37a72087c96937a0e21f736c6e661a1d6de",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is prohibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses. They account for only 20% of all constituents. For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases."
            },
            "slug": "Supervised-Grammar-Induction-using-Training-Data-Hwa",
            "title": {
                "fragments": [],
                "text": "Supervised Grammar Induction using Training Data with Limited Constituent Information"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses, and an adaptation strategy is proposed, which produces grammars that parse almost as well as Grammars induced from fully labeled corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 431,
                                "start": 63
                            }
                        ],
                        "text": "The Robust Accurate Statistical Parsing project (RASP) parser (Briscoe and Carroll, 2002) uses a statistical model over the possible state transitions of an underlying LR parser with a manually constructed phrase structure grammar. R ASP achieves an F-score of 76.5% on a manually annotated 500 sentence subset of the SUSANNE corpus (Sampson, 1995) using the grammatical relation-based evaluation proposed by Carroll et al. (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5823614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8fe99b6dc76dc342fe9fb47740fee40381fa13d",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
            },
            "slug": "Robust-Accurate-Statistical-Annotation-of-General-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Robust Accurate Statistical Annotation of General Text"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12363172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4317b8a4490c84301907a61f5b8ebb26ab8828d",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts. While humans appear to use inference rules such as \u2018X writes Y\u2019 implies \u2018X is the author of Y\u2019 in answering questions, such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them. In this paper, we present an unsupervised algorithm for discovering inference rules from text. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus. Essentially, if two paths tend to link the same set of words, we hypothesize that their meanings are similar. We use examples to show that our system discovers many inference rules easily missed by humans."
            },
            "slug": "Discovery-of-inference-rules-for-question-answering-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "Discovery of inference rules for question-answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an unsupervised algorithm for discovering inference rules from text based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Testing this intuition, I implemented the collocation extraction statistics described in the Collocations chapter of Manning and Sch\u00fctze (1999). The evaluation shows that the TTEST weight function, based on the t-test significantly outperforms every weight functions, from IR and existing similarity systems, including MI."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is now recognised as the standard approach to this task (Manning and Sch\u00fctze, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7802,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895292"
                        ],
                        "name": "Chris Brew",
                        "slug": "Chris-Brew",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15712005,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Levin's (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these ambiguous cases the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context. In others it is not, and an application which wants to recover this information will be forced to rely on some more or less elaborate process of inference. We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference."
            },
            "slug": "Using-Subcategorization-to-Resolve-Verb-Class-Lapata-Brew",
            "title": {
                "fragments": [],
                "text": "Using Subcategorization to Resolve Verb Class Ambiguity"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple statistical model of verb class ambiguity is presented and it is shown how it can be used to carry out inference and how to recover information about the appropriate class for a particular token of a verb."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144721996"
                        ],
                        "name": "M. Sanderson",
                        "slug": "M.-Sanderson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sanderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sanderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, other work in IR and text classification often considers the whole document to be the context, that is, if a word appears in a document, then that document is part of the context vector (Crouch, 1988; Sanderson and Croft, 1999; Srinivasan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7635032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53d61dad3b913c7c8ac602ed82fb5445ca506648",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents a means of automatically deriving a hierarchical organization of concepts from a set of documents without use of training data or standard clustering techniques. Instead, salient words and phrases extracted from the documents are organized hierarchically using a type of co-occurrence known as subsumption. The resulting structure is displayed as a series of hierarchical menus. When generated from a set of retrieved documents, a user browsing the menus is provided with a detailed overview of their content in a manner distinct from existing overview and summarization techniques. The methods used to build the structure are simple, but appear to be effective: a smallscale user study reveals that the generated hierarchy possesses properties expected of such a structure in that general terms are placed at the top levels leading to related and more specific terms below. The formation and presentation of the hierarchy is described along with the user study and some other informal evaluations. The organization of a set of documents into a concept hierarchy derived automatically from the set itself is undoubtedly one goal of information retrieval. Were this goal to be achieved, the documents would be organized into a form somewhat like existing manually constructed subject hierarchies, such as the Library of Congress categories, or the Dewey Decimal system. The only difference being that the categories would be customized to the set of documents itself. For example, from a collection of media related articles, the category \"Entertainment\" might appear near the top level; below it, (amongst others) one might find the category \"Movies\", a type of entertainment; and below that, there could be the category \"Actors & Actresses\", an aspect of movies. As can be seen, the arrangement of the categories provides an overview of the topic structure of those articles."
            },
            "slug": "Deriving-concept-hierarchies-from-text-Sanderson-Croft",
            "title": {
                "fragments": [],
                "text": "Deriving concept hierarchies from text"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper presents a means of automatically deriving a hierarchical organization of concepts from a set of documents without use of training data or standard clustering techniques, using a type of co-occurrence known as subsumption."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15379653"
                        ],
                        "name": "Ann A. Copestake",
                        "slug": "Ann-A.-Copestake",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Copestake",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann A. Copestake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15007513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f365cd5d3d677b0f739481444cf66b0ea271523",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This abstract describes an approach to extracting taxonomies from machine readable dictionaries and using them to structure a lexical knowledge base which incorporates default inheritance. Taxonomy construction is based on an intuitive notion of the organisation of the substantial quantities of data in machine readable dictionaries which were developed for quite independent purposes. Our intention is to investigate how this aaects, and is aaected by, the formal semantics of the knowledge representation for the lexical knowledge base which we are attempting to create, especially with respect to inheritance."
            },
            "slug": "An-Approach-to-Building-the-Hierarchical-Element-of-Copestake",
            "title": {
                "fragments": [],
                "text": "An Approach to Building the Hierarchical Element of a Lexical Knowledge Base From a Machine Readable"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This abstract describes an approach to extracting taxonomies from machine readable dictionaries and using them to structure a lexical knowledge base which incorporates default inheritance, especially with respect to inheritance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166279"
                        ],
                        "name": "T. Gaustad",
                        "slug": "T.-Gaustad",
                        "structuredName": {
                            "firstName": "Tanja",
                            "lastName": "Gaustad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gaustad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8635092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddd1b1e56e3355a22affe11d4da908b2bdf857be",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate whether the task of disambiguating pseudowords (artificial ambiguous words) is comparable to the disambiguation of real ambiguous words. Since the two methods are inherently different, a direct comparison is not possible. An indirect approach is taken where the setup for both systems is as similar as possible, i.e. using the same corpus and settings. The results obtained clearly indicate that the tasks are quite different. We conclude that the current practice of using pseudowords cannot be taken as a substitute for testing with real ambiguous words."
            },
            "slug": "Statistical-Corpus-Based-Word-Sense-Disambiguation:-Gaustad",
            "title": {
                "fragments": [],
                "text": "Statistical Corpus-Based Word Sense Disambiguation: Pseudowords vs. Real Ambiguous Words"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is concluded that the current practice of using pseudowords cannot be taken as a substitute for testing with real ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50064811"
                        ],
                        "name": "L. Finkelstein",
                        "slug": "L.-Finkelstein",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745572"
                        ],
                        "name": "Y. Matias",
                        "slug": "Y.-Matias",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Matias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316511"
                        ],
                        "name": "Zach Solan",
                        "slug": "Zach-Solan",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Solan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach Solan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073936"
                        ],
                        "name": "G. Wolfman",
                        "slug": "G.-Wolfman",
                        "structuredName": {
                            "firstName": "Gadi",
                            "lastName": "Wolfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12956853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c01df98a6b633b25c96c1a99b713ac96f1c5be",
            "isKey": false,
            "numCitedBy": 1725,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."
            },
            "slug": "Placing-search-in-context:-the-concept-revisited-Finkelstein-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Placing search in context: the concept revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new conceptual paradigm for performing search in context is presented, that largely automates the search process, providing even non-professional users with highly relevant results."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144360624"
                        ],
                        "name": "J. Sterling",
                        "slug": "J.-Sterling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sterling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sterling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10774324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d50134461d7e48714ef822f2b70e215f0487ef3",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. This information, however, is necessarily incomplete. We report on measurements of the degree of selectional coverage obtained with different sizes of corpora. We then describe a technique for using the corpus to identify selectionally similar terms, and for using this similarity to broaden the selectional coverage for a fixed corpus size."
            },
            "slug": "Generalizing-Automatically-Generated-Selectional-Grishman-Sterling",
            "title": {
                "fragments": [],
                "text": "Generalizing Automatically Generated Selectional Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Measurements of the degree of selectional coverage obtained with different sizes of corpora are reported on and a technique for using the corpus to identify selectionally similar terms and for using this similarity to broaden the selectional Coverage for a fixed corpus size is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22455656"
                        ],
                        "name": "R. Wilensky",
                        "slug": "R.-Wilensky",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wilensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wilensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 102
                            }
                        ],
                        "text": "There have been several attempts to encode (Kilgarriff, 1995) and acquire (Buitelaar, 1998) or infer (Wilensky, 1990) systematic distinctions. A related problem is the semantic alternations that occur when words appear in context. Lapata (2001) implements simple Bayesian models of sense alternations between noun-noun compounds, adjective-noun combinations, and verbs and their complements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52827437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdd603b4377ec75cc83a952277dbbf9f6c53e974",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the acquisition of the lexicon. In particular, we propose a method that uses analogical reasoning to hypothesize new polysemous word senses. This method is one of a number of knowledge acquisition devices to be included in DIRC (Domain Independent Retargetable Consultant). DIRC is a kind of intelligent, natural language-capable consultant kit that can be retargeted at different domains. DIRC is essentially \"empty-UC\" (UNIX Consultant, Wilensky et al., 1988). DIRC is to include the language and reasoning mechanisms of UC, plus a large grammar and a general lexicon. The user must then add domain knowledge, user knowledge and lexical knowledge for the area of interest."
            },
            "slug": "Extending-the-Lexicon-by-Exploiting-Subregularities-Wilensky",
            "title": {
                "fragments": [],
                "text": "Extending the Lexicon by Exploiting Subregularities"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A method that uses analogical reasoning to hypothesize new polysemous word senses is proposed, one of a number of knowledge acquisition devices to be included in DIRC (Domain Independent Retargetable Consultant), a kind of intelligent, natural language-capable consultant kit that can be retargeted at different domains."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "Another approach, often used for common and proper nouns, uses bootstrapping (Riloff and Shepherd, 1997) and multi-level bootstrapping (Riloff and Jones, 1999) to find a set of terms related to an initial seed set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1053009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e936981f5a2d55bfec0143e9a15e23ad96436b",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories."
            },
            "slug": "Learning-Dictionaries-for-Information-Extraction-by-Riloff-Jones",
            "title": {
                "fragments": [],
                "text": "Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A multilevel bootstrapping algorithm is presented that generates both the semantic lexicon and extraction patterns simultaneously simultaneously and produces high-quality dictionaries for several semantic categories."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6313909"
                        ],
                        "name": "A. Burgun",
                        "slug": "A.-Burgun",
                        "structuredName": {
                            "firstName": "Anita",
                            "lastName": "Burgun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Burgun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950081"
                        ],
                        "name": "O. Bodenreider",
                        "slug": "O.-Bodenreider",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bodenreider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bodenreider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Burgun and Bodenreider (2001) compared an alignment of the W ORDNET hierarchy with the medical lexical resourceUMLS and found a very small degree of overlap between the two."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1186524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e936cb1d0135c66cc18d017cabbb523ac7c2c28",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Objectives : The objective of this study is to compare how a general terminological system (WordNet) and a domain-specific one (UMLS) represent linguistic and knowledge phenomena at three different levels: terms, concepts, and semantic classes. Methods : For one general class (ANIMAL ) and one domain-specific class (HEALTH DISORDER ), the set of concepts corresponding to the class was established. Then, for each semantic class, the corresponding terms were mapped from one system to the other, both ways. Results : Only 2% of the domain-specific concepts from UMLS were found in WordNet, but 83% of the domain-specific concepts from WordNet were found in the UMLS. Concept overlap between the two systems varies from 48% to 97%. Discussion : Missing terms in both systems are discussed, as well as granularity and knowledge organization issues."
            },
            "slug": "Comparing-terms,-concepts-and-semantic-classes-in-Burgun-Bodenreider",
            "title": {
                "fragments": [],
                "text": "Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This study compares how a general terminological system (WordNet) and a domain-specific one (UMLS) represent linguistic and knowledge phenomena at three different levels: terms, concepts, and semantic classes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39500614"
                        ],
                        "name": "M. McHale",
                        "slug": "M.-McHale",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McHale",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McHale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The hierarchy structure in Roget\u2019s has also been used in edge counting measures of semantic similarity (Jarmasz and Szpakowicz, 2003; McHale, 1998), and for computing lexical cohesion using lexical chains (Morris and Hirst, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 285027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44829f7fb0663d19e8f06b436f2cdeb595a1c7bd",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of using Roget\u2019s International Thesaurus as the taxonomy in a semantic similarity measurement task. Four similarity metrics were taken from the literature and applied to Roget\u2019s. The experimental evaluation suggests that the traditional edge counting approach does surprisingly well (a correlation of r=0.88 with a benchmark set of human similarity judgements, with an upper bound of r=0.90 for human subjects performing the same task.)"
            },
            "slug": "A-Comparison-of-WordNet-and-Roget\u2019s-Taxonomy-for-McHale",
            "title": {
                "fragments": [],
                "text": "A Comparison of WordNet and Roget\u2019s Taxonomy for Measuring Semantic Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents the results of using Roget\u2019s International Thesaurus as the taxonomy in a semantic similarity measurement task, and suggests that the traditional edge counting approach does surprisingly well."
            },
            "venue": {
                "fragments": [],
                "text": "WordNet@ACL/COLING"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "I have demonstrated empirically (Curran and Osborne, 2002) that reliable, stable counts are not achievable for infrequent events even when counting over massive corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1039363,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "46f0c02c9ee4f8e5336bfcae94c4cd3e40d92f3e",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities. We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus. When using one billion words, as expected, we do find that many of our estimates do converge to their eventual value. However, we also find that for some words, no such convergence occurs. This leads us to conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to the statistical modelling as well."
            },
            "slug": "A-Very-Very-Large-Corpus-Doesn\u2019t-Always-Yield-Curran-Osborne",
            "title": {
                "fragments": [],
                "text": "A Very Very Large Corpus Doesn\u2019t Always Yield Reliable Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work empirically investigates the convergence behaviour of unigram probability estimates on a one billion word corpus and concludes that simply relying upon large corpora is not in itself sufficient: one must pay attention to the statistical modelling as well."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39877126"
                        ],
                        "name": "B. Keller",
                        "slug": "B.-Keller",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13907505,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b4b8e1077c7aa34456cfd811a881d1a8e677c321",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus."
            },
            "slug": "Detecting-a-Continuum-of-Compositionality-in-Verbs-McCarthy-Keller",
            "title": {
                "fragments": [],
                "text": "Detecting a Continuum of Compositionality in Phrasal Verbs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Hatzivassiloglou and McKeown (1993) ask subjects to partition adjectives into nonoverlapping clusters, which they then compare pairwise with extracted semantic clusters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8738143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bf9fb7b8aaf072497ce0cfbb046cd7451473e62",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives. We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained."
            },
            "slug": "Towards-the-Automatic-Identification-of-Adjectival-Hatzivassiloglou-McKeown",
            "title": {
                "fragments": [],
                "text": "Towards the Automatic Identification of Adjectival Scales: Clustering Adjectives According to Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales, and shows how a clustering algorithm can use similarities to produce the groups of adjectives."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11433707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2f4b04d9cb8aa05982b2727d61e4b4c3ea33e20",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words."
            },
            "slug": "An-Unsupervised-Approach-to-Prepositional-Phrase-Pantel-Lin",
            "title": {
                "fragments": [],
                "text": "An Unsupervised Approach to Prepositional Phrase Attachment using Contextually Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work presents an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods and uses an iterative process to extract training data from an automatically parsed corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2002468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "073f01659ac69682a5376197d72705ababe5ba29",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A pseudoword is a composite comprised of two or more words chosen at random; the individual occurrences of the original words within a text are replaced by their conflation. Pseudowords are a useful mechanism for evaluating the impact of word sense ambiguity in many NLP applications. However, the standard method for constructing pseudowords has some drawbacks. Because the constituent words are chosen at random, the word contexts that surround pseudowords do not necessarily reflect the contexts that real ambiguous words occur in. This in turn leads to an optimistic upper bound on algorithm performance. To address these drawbacks, we propose the use of lexical categories to create more realistic pseudowords, and evaluate the results of different variations of this idea against the standard approach."
            },
            "slug": "Category-based-Pseudowords-Nakov-Hearst",
            "title": {
                "fragments": [],
                "text": "Category-based Pseudowords"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The use of lexical categories are proposed to create more realistic pseudowords, and the results of different variations of this idea against the standard approach are evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165139"
                        ],
                        "name": "H. V. Halteren",
                        "slug": "H.-V.-Halteren",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Halteren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Halteren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 315,
                                "start": 115
                            }
                        ],
                        "text": "Ensemble learning has been successfully applied to numerousNLP tasks, includingPOStagging (Brill and Wu, 1998; van Halteren et al., 1998), chunking (Tjong Kim Sang, 2000; Tjong Kim Sang et al., 2000), word sense disambiguation (Pederson, 2000) and statistical parsing (Henderson and Brill, 1999). Dietterich (2000) presents a broad introduction to ensemble methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 852013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b863f180d6588222663cd6e2c434f50a7f06fff3",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generator (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indvidual tagger."
            },
            "slug": "Improving-Data-Driven-Wordclass-Tagging-by-System-Halteren-Zavrel",
            "title": {
                "fragments": [],
                "text": "Improving Data Driven Wordclass Tagging by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "How the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system is examined."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2754495"
                        ],
                        "name": "Massimiliano Ciaramita",
                        "slug": "Massimiliano-Ciaramita",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Ciaramita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimiliano Ciaramita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Ciaramita and Johnson (2003) suggest that supersense tagging is a similar task to named entity recognition, which also has a very small set of options with similar granularity for labelling previously unseen terms (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 53
                            }
                        ],
                        "text": "My evaluation will use exactly the same test sets as Ciaramita and Johnson (2003). The W ORDNET 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 67
                            }
                        ],
                        "text": "The task involves repeating the experiments and evaluation used by Ciaramita and Johnson (2003) with a similaritybased approach rather than their classification-based approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 37
                            }
                        ],
                        "text": "The supersense tagger implemented by Ciaramita and Johnson (2003) is a multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 36
                            }
                        ],
                        "text": "WORDNET supersenses , as defined by Ciaramita and Johnson (2003), are the broad semantic classes created by lexicographers as the initial step of inserting words into the W ORDNET hierarchy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 122
                            }
                        ],
                        "text": "This application involves classifying previously unseen words with coarse-grained supersense tags replicating the work of Ciaramita and Johnson (2003) using semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Ciaramita and Johnson (2003) use the words that have been added to W ORDNET 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 53
                            }
                        ],
                        "text": "My evaluation will use exactly the same test sets as Ciaramita and Johnson (2003). The W ORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense. The W ORDNET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003). Massimiliano Ciaramita has kindly supplied me with the W ORDNET 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 50
                            }
                        ],
                        "text": "This involves repeating the recent experiments by Ciaramita and Johnson (2003) in categorising previously unseen words using supersenses defined in terms of the WORDNET lexicographer files."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 37
                            }
                        ],
                        "text": "6 to match the training data used by Ciaramita and Johnson (2003). For this there are a"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10275081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c2b3f4bc53f6c0aef2a2296d737f532b985322c",
            "isKey": true,
            "numCitedBy": 133,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new framework for classifying common nouns that extends named-entity classification. We used a fixed set of 26 semantic labels, which we called supersenses. These are the labels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation."
            },
            "slug": "Supersense-Tagging-of-Unknown-Nouns-in-WordNet-Ciaramita-Johnson",
            "title": {
                "fragments": [],
                "text": "Supersense Tagging of Unknown Nouns in WordNet"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns and defines a more realistic evaluation procedure than cross-validation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146665979"
                        ],
                        "name": "Jun Wu",
                        "slug": "Jun-Wu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ensemble learning has been successfully applied to numerous NLP tasks, including POS tagging (Brill and Wu, 1998; van Halteren et al., 1998), chunking (Tjong Kim Sang, 2000; Tjong Kim Sang et al., 2000), word sense disambiguation (Pederson, 2000) and statistical parsing (Henderson and Brill, 1999). Dietterich (2000) presents a broad introduction to ensemble methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ensemble learning has been successfully applied to numerous NLP tasks, including POS tagging (Brill and Wu, 1998; van Halteren et al., 1998), chunking (Tjong Kim Sang, 2000; Tjong Kim Sang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1591692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e824aaf67f4f4f068455c6dbb7a6ed877794bd6",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementatry behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers."
            },
            "slug": "Classifier-Combination-for-Improved-Lexical-Brill-Wu",
            "title": {
                "fragments": [],
                "text": "Classifier Combination for Improved Lexical Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By using contextual cues to guide tagger combination, this paper is able to derive a new tagger that achieves performance significantly greater than any of the individual taggers."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707726"
                        ],
                        "name": "J. Pustejovsky",
                        "slug": "J.-Pustejovsky",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Pustejovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pustejovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69957759"
                        ],
                        "name": "Peter Paul Buitelaar",
                        "slug": "Peter-Paul-Buitelaar",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Buitelaar",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Paul Buitelaar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several attempts to encode (Kilgarriff, 1995) and acquire (Buitelaar, 1998) or infer (Wilensky, 1990) systematic distinctions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several computational attempts to reduce the number of sense distinctions and increase the size of each synset in WORDNET (Buitelaar, 1998; Ciaramita et al., 2003; Hearst and Sch\u00fctze, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several attempts to encode (Kilgarriff, 1995) and acquire (Buitelaar, 1998) or infer (Wilensky, 1990) systematic distinctions. A related problem is the semantic alternations that occur when words appear in context. Lapata (2001) implements simple Bayesian models of sense alternations between noun-noun compounds, adjective-noun combinations, and verbs and their complements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61094615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fe0b769be2a9af8545185ce44c70a258d490536",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "CoreLex: Systematic Polysemy and Underspeci cation A dissertation presented to the Faculty of the Graduate School of Arts and Sciences of Brandeis University, Waltham, Massachusetts by Paul Buitelaar This thesis is concerned with a uni ed approach to the systematic polysemy and underspeci cation of nouns. Systematic polysemy { senses that are systematically related and therefore predictable over classes of lexical items { is fundamentally di erent from homonymy { senses that are unrelated, non-systematic and therefore not predictable. At the same time, studies in discourse analysis show that lexical items are often left underspeci ed for a number of related senses. Clearly, there is a correspondence between these phenomena, the investigation of which is the topic of this thesis. Acknowledging the systematic nature of polysemy and its relation to underspeci ed representations, allows one to structure ontologies for lexical semantic processing more e ciently, generating more appropriate interpretations within context. In order to achieve this, one needs a thorough analysis of systematic polysemy and underspeci cation on a large and useful scale. The thesis establishes an ontology and semantic database (CoreLex) of 126 semantic types, covering around 40,000 nouns and de ning a large number of systematic polysemous classes that are derived by a careful analysis of sense distributions inWordNet. The semantic types are underspeci ed representations based on generative lexicon theory. The representations are used in underspeci ed semantic tagging, addressing two problems in traditional semantic tagging: sense enumeration (the di culty on deciding the number of discrete senses), due to systematic polysemy; and multiple reference (NP's denoting more than one model-theoretic referent), due to underspeci cation. Also, traditional semantic tags that are based on discrete senses tend to be too ne-grained for practical use. For instance, WordNet has, in principle, around vi 60,000 di erent tags (synsets) for nouns alone. The CoreLex approach, on the other hand, o ers a concise set of 126 tags that are inherently more coarse-grained, by taking into account systematic polysemy and underspeci cation. Underspeci ed semantic tagging is implemented, using probabilistic classi cation in order to cover unknown nouns (not in CoreLex) and to identify context-speci c and new interpretations. The classi cation algorithm is centered around the computation of a Jaccard (similarity) score that compares lexical items in terms of the attributes (linguistic patterns acquired from domain speci c corpora) they share. vii"
            },
            "slug": "Corelex:-systematic-polysemy-and-underspecification-Pustejovsky-Buitelaar",
            "title": {
                "fragments": [],
                "text": "Corelex: systematic polysemy and underspecification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The thesis establishes an ontology and semantic database (CoreLex) of 126 semantic types, covering around 40,000 nouns and a large number of systematic polysemous classes that are derived by a careful analysis of sense distributions in WordNet."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35096557"
                        ],
                        "name": "Darren Pearce",
                        "slug": "Darren-Pearce",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Pearce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18965811,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "ea8158bbb075c46936ba6505e47f6ba74f102409",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the use of WordNet in a new technique for collocation extraction. The approach is based on restrictions on the possible substitutions for synonyms within candidate phrases. Following a general discussion of collocations and their applications, current extraction methods are briefly described. This is followed by a detailed description of the new approach and results and evaluation of experiments that utilise WordNet as a source of synonymic information."
            },
            "slug": "Synonymy-in-collocation-extraction-Pearce",
            "title": {
                "fragments": [],
                "text": "Synonymy in collocation extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The use of WordNet is described in a new technique for collocation extraction based on restrictions on the possible substitutions for synonyms within candidate phrases, which is followed by a detailed description of the new approach and results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15829786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. >"
            },
            "slug": "Dimensions-of-meaning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Dimensions of meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction and finds that dimensionality reduction by means of a singular value decomposition is employed."
            },
            "venue": {
                "fragments": [],
                "text": "Supercomputing '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9548219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c384d9ee4fd8657b26a8165244eb4ad73df4f492",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \u201cX is author of Y \u2248 X wrote Y\u201d, \u201cX solved Y \u2248 X found a solution to Y\u201d, and \u201cX caused Y \u2248 Y is triggered by X\u201d. Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-\u2013-Discovery-of-Inference-Rules-from-Text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT \u2013 Discovery of Inference Rules from Text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507437"
                        ],
                        "name": "Neel Sundaresan",
                        "slug": "Neel-Sundaresan",
                        "structuredName": {
                            "firstName": "Neel",
                            "lastName": "Sundaresan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neel Sundaresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40456079"
                        ],
                        "name": "J. Yi",
                        "slug": "J.-Yi",
                        "structuredName": {
                            "firstName": "Jeonghee",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17200828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15710d6063550cee29ed80d9ed77834464858cda",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mining-the-Web-for-relations-Sundaresan-Yi",
            "title": {
                "fragments": [],
                "text": "Mining the Web for relations"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086250482"
                        ],
                        "name": "J. Deese",
                        "slug": "J.-Deese",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Deese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145539036,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8c7aee4d436ff52a85a8c9982d28c6ebda3f2b8c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-associative-structure-of-some-common-english-Deese",
            "title": {
                "fragments": [],
                "text": "The associative structure of some common english adjectives"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2000), word sense disambiguation (Pederson, 2000) and statistical parsing (Henderson and Brill, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f49231a5bbf89e2566dc04784ac6e99f726b2e72",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results."
            },
            "slug": "A-Simple-Approach-to-Building-Ensembles-of-Naive-Pedersen",
            "title": {
                "fragments": [],
                "text": "A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context achieves accuracy rivaling the best previously published results."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2971806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f9a848dca0a80ef64987a9dd511ee6b7e19cd1",
            "isKey": false,
            "numCitedBy": 565,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \"X is author of Y \u2248 X wrote Y\", \"X solved Y \u2248 X found a solution to Y\", and \"X caused Y \u2248 Y is triggered by X\". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-@SBT@discovery-of-inference-rules-from-text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT @SBT@discovery of inference rules from text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46918277"
                        ],
                        "name": "Anna Herwig",
                        "slug": "Anna-Herwig",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Herwig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Herwig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195972758,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "fef38da62259b9b6474b5d2cabd1c7122a30aab7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past decades it has become generally acknowledged that lexicon and grammar are inseparably linked, constituting \u201ca continuum of symbolic structures\u201c (Langacker 1990:2). Yet, a comprehensive integration of the two realms of knowledge appears to be a difficult task. The present article offers a unified psycholinguistic perspective, which is centred on the mental lexicon, considering grammatical knowledge as part of the information structure of lexical items. It aims to model the complexity of lexical knowledge such that its perceived psychological reality, including various levels of linguistic description, becomes discernible. 1. Lexical Information Structure In the light of recent research undertaken in different linguistics fields (cf., e.g., Bybee 1988, Ellis 1997, Langacker 1990, Singleton 1999), the mental lexicon can be described as that domain of language where the various dimensions of linguistic information meet. A comprehensive description of lexical knowledge must therefore take account not only of formal and semantic but also of grammatical knowledge. It follows that the elements of the lexicon need to be modelled as highly complex entities, including information on representational substance (conceptual, perceptual, and articulatory patterns) and combinatorial potential. Combinatorial knowledge relates to an item\u2019s collocation and colligation, i.e., its valency structure (cf., e.g., Langacker 1987, Lutjeharms 1994, Singleton 1999). It has various facets and is relevant for phrasal construction. Combinatorial knowledge is associated with specific types of semantic and formal relations, which reflect the distributional properties of lexical elements (cf., e.g., Bybee 1988, Ellis 1997). The different knowledge components are mutually dependent and interact in our use of language. I will aim to integrate representational substance and combinatorial potential of lexical items in an all-embracing psycholinguistic component structure model, which coordinates the different levels of description. The model provides a framework for discussing grammatical processing with reference to lexical knowledge. It also draws a unified picture of lexical items, which provides the grounds for illustrating the perceived psychological reality of lexical networks. 1 For a more differentiated discussion of lexical information structure and applications of the proposed model cf. Herwig 1994."
            },
            "slug": "Lexicon-and-Grammar-Herwig",
            "title": {
                "fragments": [],
                "text": "Lexicon and Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified psycholinguistic perspective is offered, centred on the mental lexicon, considering grammatical knowledge as part of the information structure of lexical items, to model the complexity of lexicals knowledge such that its perceived psychological reality, including various levels of linguistic description, becomes discernible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Dagan et al. (1993) use a form of Jaccard which separates the left and right contexts which is equivalent to using a window extractor with the relation type equal to left or right."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1995, 1994), word sense disambiguation (Dagan et al., 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Lin (1998d) and Dagan et al. (1993)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2480472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17126346c77db6436c0f0ec59c01b1e432ee4e7b",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates."
            },
            "slug": "Similarity-Based-Methods-for-Word-Sense-Dagan-Lee",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Methods for Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This work compares four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which both unigram and bigram frequency are controlled."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 112
                            }
                        ],
                        "text": "I tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 136
                            }
                        ],
                        "text": "Another approach, often used for common and proper nouns, uses bootstrapping (Riloff and Shepherd, 1997) and multi-level bootstrapping (Riloff and Jones, 1999) to find a set of terms related to an initial seed set. Roark and Charniak (1998) use a similar approach to Riloff and Shepherd (1997) but gain significantly in performance by changing some parameters of the algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 407,
                                "start": 136
                            }
                        ],
                        "text": "Another approach, often used for common and proper nouns, uses bootstrapping (Riloff and Shepherd, 1997) and multi-level bootstrapping (Riloff and Jones, 1999) to find a set of terms related to an initial seed set. Roark and Charniak (1998) use a similar approach to Riloff and Shepherd (1997) but gain significantly in performance by changing some parameters of the algorithm. Agichtein and Gravano (2000) and Agichtein et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6204420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6edceaf0fada3588ee5f036e944c1a00661df77a",
            "isKey": true,
            "numCitedBy": 477,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Identifying-Sentence-Reynar-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Identifying Sentence Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A trainable model for identifying sentence boundaries in raw text that can be trained easily on any genre of English, and should be trainable on any other Romanalphabet language."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "Cruse\u2019s definition is summarised by (Hirst, 1995) aswords that are close in meaning . . . not fully inter-substitutable but varying in their shades of denotation, connotation, implicature, emphasis or register."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15150924,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6a29bc018f6f4a0b6a493582441c9756265d5347",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Plesionyms, or near-synonyms, are words that are almost synonyms, but not quite. The need to deal adequately with plesionymy in tasks such as lexical choice is the basis for two alternatives to conventional models of lexical knowledge: a Saussurean approach and a prototype-theory approach. In this paper, I will discuss these approaches, showing that the latter is troublesome but the former is likely to succeed."
            },
            "slug": "Near-synonymy-and-the-structure-of-lexical-Hirst",
            "title": {
                "fragments": [],
                "text": "Near-synonymy and the structure of lexical knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper will discuss two alternatives to conventional models of lexical knowledge: a Saussurean approach and a prototype-theory approach, showing that the latter is troublesome but the former is likely to succeed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": false,
            "numCitedBy": 5788,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380069640"
                        ],
                        "name": "L. Baker",
                        "slug": "L.-Baker",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Baker",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarity-based techniques have been used for text classification (Baker and McCallum, 1998) and identifying semantic orientation, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Baker and McCallum (1998) apply the distributional clustering technique to document classification because it allows for a very high degree of dimensionality reduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6146974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e733226b881f11f25c87e8bac8d602ba3d9c220e",
            "isKey": false,
            "numCitedBy": 843,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing [6], class-based clustering [l], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering."
            },
            "slug": "Distributional-clustering-of-words-for-text-Baker-McCallum",
            "title": {
                "fragments": [],
                "text": "Distributional clustering of words for text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper describes the application of Distributional Clustering to document classification and shows that it can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing, class-based clustering, feature selection by mutual information, or Markov-blanket-based feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35246188"
                        ],
                        "name": "Stephen Joseph Green",
                        "slug": "Stephen-Joseph-Green",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Green",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Joseph Green"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14516755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "491cee727c8668da06e082cceb8b171b6f6812e9",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss an automatic method for the construction of hypertext links within and between newspaper articles. The method comprises three steps: determining the lexical chains in a text, building links between the paragraphs of articles, and building links between articles. Lexical chains capture the semantic relations between words that occur throughout a text. Each chain is a set of related words that captures a portion of the cohesive structure of a text. By considering the distribution of chains within an article, we can build links between the paragraphs. By comparing the chains contained in two different articles, we can decide whether or not to place a link between them. We also present the results of an experiment designed to measure inter-linker consistency in the manual construction of hypertext links between the paragraphs of newspaper articles. The results show that inter-linker consistency is low, but better than that obtained in a previous experiment."
            },
            "slug": "Using-lexical-chains-to-build-hypertext-links-in-Green",
            "title": {
                "fragments": [],
                "text": "Using lexical chains to build hypertext links in newspaper articles"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An automatic method for the construction of hypertext links within and between newspaper articles and the results show that inter-linker consistency is low, but better than that obtained in a previous experiment are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144495560"
                        ],
                        "name": "R. Attar",
                        "slug": "R.-Attar",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Attar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Attar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793371"
                        ],
                        "name": "A. Fraenkel",
                        "slug": "A.-Fraenkel",
                        "structuredName": {
                            "firstName": "Aviezri",
                            "lastName": "Fraenkel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fraenkel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 553561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9384f7812fec87a41938ae28fe19c8f65f555c97",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "AaSTRACT. In a full-text natural-language retrieval system, local feedbacl~ is the process of formulating a new ~mproved search based on clustering terms from the documents returned m a previous search of any given query Experiments were run on a database of US patents It ~s concluded that m contrast toglobalclustermg, w h e r e the size of matrices hmmts apphcatmns to small databases and improvements are doubtful, local clustering is practical also for large databases and appears to improve overall performance, especially tf metrical constraints and weighting by proximity are embedded m the local feedback The local methods adapt themselves to each mdwtdual search and produce useful searchonyms terms which are \"synonymous\" m the context of one query Searchonyms lead to new ~mproved search formulahons both via manual and vm automahc feedback"
            },
            "slug": "Local-Feedback-in-Full-Text-Retrieval-Systems-Attar-Fraenkel",
            "title": {
                "fragments": [],
                "text": "Local Feedback in Full-Text Retrieval Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Local clustering is practical also for large databases and appears to improve overall performance, especially if metrical constraints and weighting by proximity are embedded m the local feedback."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145977875"
                        ],
                        "name": "Antal van den Bosch",
                        "slug": "Antal-van-den-Bosch",
                        "structuredName": {
                            "firstName": "Antal",
                            "lastName": "van den Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antal van den Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144075529"
                        ],
                        "name": "A. Weijters",
                        "slug": "A.-Weijters",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Weijters",
                            "middleNames": [
                                "J.",
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weijters"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Algorithms used by Memory-Based Learners (MBL), such as the IGTrees (Daelemans et al., 1997), impose an ordering over the features to efficiently search for a reasonable match."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Dagan et al. (1993, 1995), Dagan et al. (1999) and Lee (1999) have shown that using the distributionally nearest-neighbours improves language modelling and WSD."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16693670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "810c3e1e0e6e58e4172527e8b5ef538fb24c2954",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the IGTree learning algorithm, which compresses an instance base into a tree structure. The concept of information gain is used as a heuristic function for performing this compression. IGTree produces trees that, compared to other lazy learning approaches, reduce storage requirements and the time required to compute classifications. Furthermore, we obtained similar or better generalization accuracy with IGTree when trained on two complex linguistic tasks, viz. letter\u2013phoneme transliteration and part-of-speech-tagging, when compared to alternative lazy learning and decision tree approaches (viz., IB1, information-gain-weighted IB1, and C4.5). A third experiment, with the task of word hyphenation, demonstrates that when the mutual differences in information gain of features is too small, IGTree as well as information-gain-weighted IB1 perform worse than IB1. These results indicate that IGTree is a useful algorithm for problems characterized by the availability of a large number of training instances described by symbolic features with sufficiently differing information gain values."
            },
            "slug": "IGTree:-Using-Trees-for-Compression-and-in-Lazy-Daelemans-Bosch",
            "title": {
                "fragments": [],
                "text": "IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "IGTree is a useful algorithm for problems characterized by the availability of a large number of training instances described by symbolic features with sufficiently differing information gain values, and is obtained similar or better generalization accuracy with IGTree when trained on two complex linguistic tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence Review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992275"
                        ],
                        "name": "Maria Lapata",
                        "slug": "Maria-Lapata",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4349038"
                        ],
                        "name": "Olga Ourioupina",
                        "slug": "Olga-Ourioupina",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Ourioupina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Ourioupina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 151
                            }
                        ],
                        "text": "Although there are experiments that have used the web as a corpus, they only estimate cooccurrence relative frequencies using search engine hit counts (Keller et al., 2002; Modjeska et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 61
                            }
                        ],
                        "text": "The Roget\u2019s topic hierarchy has been aligned with WORDNET by Kwong (1998) and Mandala et al. (1999) to overcome the tennis problem, and Roget\u2019s terms have been disambiguated with respect to W ORDNET senses (Nastase and Szpakowicz, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "The Roget\u2019s topic hierarchy has been aligned with WORDNET by Kwong (1998) and Mandala et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1625546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79883c30922037c93392ddbbecc6fd35674a6a1c",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments."
            },
            "slug": "Using-the-Web-to-Overcome-Data-Sparseness-Keller-Lapata",
            "title": {
                "fragments": [],
                "text": "Using the Web to Overcome Data Sparseness"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "It is shown that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023469"
                        ],
                        "name": "D. Bikel",
                        "slug": "D.-Bikel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bikel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bikel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6727337,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "13b1259c3ffbb77150ba0f57ce5bc2161c5d6b5f",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the automatic construction of a Korean WordNet from pre-existing lexical resources. A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets. We will show how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns."
            },
            "slug": "Automatic-WordNet-Mapping-Using-Word-Sense-Bikel",
            "title": {
                "fragments": [],
                "text": "Automatic WordNet Mapping Using Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets and it is shown how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Brown et al. (1992) showed that class-based"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This approach has been implemented by many different researchers in NLP including Hindle (1990); Brown et al. (1992); Pereira et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Clustering (Brown et al., 1992; Pereira et al., 1993) using methods such as k-means also reduces the number of similarity comparisons that need to be performed, because each comparison is to a small number of attribute vectors that summarise each cluster."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095398"
                        ],
                        "name": "Alexander Budanitsky",
                        "slug": "Alexander-Budanitsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Budanitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Budanitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770371"
                        ],
                        "name": "K. Alcock",
                        "slug": "K.-Alcock",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Alcock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Alcock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072593976"
                        ],
                        "name": "Jiang\u2014 Conrath",
                        "slug": "Jiang\u2014-Conrath",
                        "structuredName": {
                            "firstName": "Jiang\u2014",
                            "lastName": "Conrath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang\u2014 Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14764558,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6db4e86e6377cd703aaaf3a3b471b62e033757ae",
            "isKey": false,
            "numCitedBy": 916,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system. It was found that Jiang and Conrath\u2019s measure gave the best results overall. That of Hirst and St-Onge seriously over-related, that of Resnik seriously under-related, and those of Lin and of Leacock and Chodorow fell in between."
            },
            "slug": "Semantic-distance-in-WordNet:-An-experimental,-of-Budanitsky-Hirst",
            "title": {
                "fragments": [],
                "text": "Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system and found that Jiang and Conrath\u2019s measure gave the best results overall."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145702640"
                        ],
                        "name": "Guido Minnen",
                        "slug": "Guido-Minnen",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Minnen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guido Minnen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35096557"
                        ],
                        "name": "Darren Pearce",
                        "slug": "Darren-Pearce",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Pearce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "morpha has wide coverage \u2013 nearly 100% against the CELEX lexical database (Minnen et al., 2001) \u2013 and is very efficient, analysing over 80 000 words per second (Minnen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Pasca and Harabagiu (2001) state that lexical-semantic knowledge is required in all modules of a state-of-the-art QA system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34553826,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "e979925b15861153a0e9ce8ace39a28d319e613d",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two newly developed computational tools for morphological processing: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser. The tools are fast, being based on finite-state techniques, have wide coverage, incorporating data from various corpora and machine readable dictionaries, and are robust, in that they are able to deal effectively with unknown words. The tools are freely available. We evaluate the accuracy and speed of both tools and discuss a number of practical applications in which they have been put to use."
            },
            "slug": "Applied-morphological-processing-of-English-Minnen-Carroll",
            "title": {
                "fragments": [],
                "text": "Applied morphological processing of English"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Two newly developed computational tools for morphological processing are described: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser, which are fast, being based on finite-state techniques, and robust, in that they are able to deal effectively with unknown words."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144267118"
                        ],
                        "name": "Marc Light",
                        "slug": "Marc-Light",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Light",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Light"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CASS has been used in various NLP tasks including vector-space similarity for word sense disambiguation (Lee and Pereira, 1999), induction of selectional preferences (Abney and Light, 1999) and modelling lexical-semantic relations (Lapata, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5686249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "976fff9e039776bbebed5c295adf25e6e3152e25",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new model of selectional preference induction. Unlike previous approaches, we provide a stochastic generation model for the words that appear as arguments of a predicate. More specifically, we define a hidden Markov model with the general shape of a given semantic class hierarchy. This model has a number of at tract ive features, among them that selectional preference can be seen as distributions over words. Initial results are promising. However, unsupervised parameter estimation has proven problematic. A central problem is word sense ambiguity in the training corpora. We describe a t tempts to modify the forward-backward algorithm, an EM algorithm, to handle such disambiguation. Although these a t tempts were unsuccessful at improving performance, we believe they give insight into the nature of the bottlenecks and into the behavior of the EM algorithm."
            },
            "slug": "Hiding-a-Semantic-Hierarchy-in-a-Markov-Model-Abney-Light",
            "title": {
                "fragments": [],
                "text": "Hiding a Semantic Hierarchy in a Markov Model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "T tempts to modify the forward-backward algorithm, an EM algorithm, to handle word sense ambiguity in the training corpora are described and believe they give insight into the nature of the bottlenecks and into the behavior of the EM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 1999"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12055574,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "564c777ac811717c522fabf5824476b811f78768",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes, based on distributional approximations of diatheses, extracted from a very large annotated corpus. Distributions of four grammatical features are sufficient to reduce error rate by 50% over chance. We conclude that corpus data is a usable repository of verb class information, and that corpus-driven extraction of grammatical features is a promising methodology for automatic lexical acquisition."
            },
            "slug": "Automatic-Verb-Classification-Using-Distributions-Stevenson-Merlo",
            "title": {
                "fragments": [],
                "text": "Automatic Verb Classification Using Distributions of Grammatical Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is concluded that corpus data is a usable repository of verb class information, and that corpus-driven extraction of grammatical features is a promising methodology for automatic lexical acquisition."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35258592"
                        ],
                        "name": "David J. Weir",
                        "slug": "David-J.-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Clark and Weir (2002) have shown measures calculated over the WORDNET hierarchy can be used for pseudo disambiguation, parse selection (Clark, 2001) and prepositional phrase (PP) attachment (Clark and Weir, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1077169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24a2f50147a3722b99384bd983d902969a8b6bad",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge of which words are able to fill particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment."
            },
            "slug": "A-Class-based-Probabilistic-approach-to-Structural-Clark-Weir",
            "title": {
                "fragments": [],
                "text": "A Class-based Probabilistic approach to Structural Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel way of using a semantic hierarchy to estimate the probabilities, and a probabilistic approach is taken to acquire knowledge of which words are able to fill particular argument slots of a predicate."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144795097"
                        ],
                        "name": "Mark Stevenson",
                        "slug": "Mark-Stevenson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10621900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d37e342155b8b74189537227c01b664c0649b0e7",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics. The combined approach is evaluated by comparing their results against the annotated SEMCOR corpus. An implementation is described in which WordNet is augmented with thesaural information from the CIDE+ machine readable dictionary."
            },
            "slug": "Augmenting-Noun-Taxonomies-by-Combining-Lexical-Stevenson",
            "title": {
                "fragments": [],
                "text": "Augmenting Noun Taxonomies by Combining Lexical Similarity Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics and an implementation is described in which WordNet is augmented with thesaural information from the CIDE+ machine readable dictionary."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091160"
                        ],
                        "name": "B. Levin",
                        "slug": "B.-Levin",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Levin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lin (1999) has used similarity measures to determine if relationships between words are idiomatic or non-compositional and Baldwin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Practically all recent work in semantic clustering of verbs evaluates against the Levin (1993) classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62585813,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6cbc1eb25f4ab29a613418b3b0740e74141a0f17",
            "isKey": false,
            "numCitedBy": 3246,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."
            },
            "slug": "English-Verb-Classes-and-Alternations:-A-Levin",
            "title": {
                "fragments": [],
                "text": "English Verb Classes and Alternations: A Preliminary Investigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Beth Levin shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329862"
                        ],
                        "name": "Sharon A. Caraballo",
                        "slug": "Sharon-A.-Caraballo",
                        "structuredName": {
                            "firstName": "Sharon",
                            "lastName": "Caraballo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharon A. Caraballo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is a necessary step for determining hyponymy relationships between words and building a noun hierarchy (Caraballo, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Hearst (1992) and Caraballo and Charniak (1999) compare their hyponym extraction and specificity ordering techniques against the WORDNET hierarchy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1767510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab329ef59d21060c31afce413f6e447b1c0b8b7",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that automatic methods can be used in building semantic lexicons. This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet."
            },
            "slug": "Automatic-construction-of-a-hypernym-labeled-noun-Caraballo",
            "title": {
                "fragments": [],
                "text": "Automatic construction of a hypernym-labeled noun hierarchy from text"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705950"
                        ],
                        "name": "E. Fox",
                        "slug": "E.-Fox",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Fox",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2923563"
                        ],
                        "name": "J. T. Nutter",
                        "slug": "J.-T.-Nutter",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Nutter",
                            "middleNames": [
                                "Terry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. T. Nutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923342"
                        ],
                        "name": "T. Ahlswede",
                        "slug": "T.-Ahlswede",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ahlswede",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ahlswede"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2186744"
                        ],
                        "name": "M. Evens",
                        "slug": "M.-Evens",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Evens",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Evens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12176518"
                        ],
                        "name": "Judith A. Markowitz",
                        "slug": "Judith-A.-Markowitz",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "Markowitz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judith A. Markowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16290098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4bb51e9600c06d0da49e0e1aeddb8d856fd1558",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Information retrieval systems that support searching of large textual databases are typically accessed by trained search intermediaries who provide assistance to end users in bridging the gap between the languages of authors and inquirers. We are building a thesaurus in the form of a large semantic network to support interactive query expansion and search by end users. Our lexicon is being built by analyzing and merging data from several large English dictionaries; testing of its value for retrieval is with the SMART and CODER systems."
            },
            "slug": "Building-a-Large-Thesaurus-for-Information-Fox-Nutter",
            "title": {
                "fragments": [],
                "text": "Building a Large Thesaurus for Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Thesaurus in the form of a large semantic network to support interactive query expansion and search by end users is built, being built by analyzing and merging data from several large English dictionaries."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067634566"
                        ],
                        "name": "James Brooks",
                        "slug": "James-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The morphological analysis makes a significant impact on the data-sparseness problem, unlike the minimal improvement for PP-attachment (Collins and Brooks, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9e5bf851dc95369e26f1869c2637b1d8919e6c",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events \u2014 ignoring events which occur less than 5 times in training data reduces performance to 81.6%."
            },
            "slug": "Prepositional-Phrase-Attachment-through-a-Model-Collins-Brooks",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment through a Backed-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper shows that the problem of prepositional phrase attachment ambiguity is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2754495"
                        ],
                        "name": "Massimiliano Ciaramita",
                        "slug": "Massimiliano-Ciaramita",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Ciaramita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimiliano Ciaramita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The task involves repeating the experiments and evaluation used by Ciaramita and Johnson (2003) with a similaritybased approach rather than their classification-based approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The supersense tagger implemented by Ciaramita and Johnson (2003) is a multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "My results on this task significantly outperform the existing work of Ciaramita et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1 test set as a final comparison of best results with Ciaramita and Johnson (2003). The synonyms were extracted from the 2 billion word corpus using SEXTANT(NB) with the JACCARD measure and TTEST weight function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This application involves classifying previously unseen words with coarse-grained supersense tags replicating the work of Ciaramita and Johnson (2003) using semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ciaramita et al. (2003) have found that common nouns missing from WORDNET 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "My evaluation will use exactly the same test sets as Ciaramita and Johnson (2003). The WORDNET 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This involves repeating the recent experiments by Ciaramita and Johnson (2003) in categorising previously unseen words using supersenses defined in terms of the WORDNET lexicographer files."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Further, Ciaramita et al. (2003) state that most of the key distinctions between senses of a word are still maintained with supersenses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "WORDNET supersenses, as defined by Ciaramita and Johnson (2003), are the broad semantic classes created by lexicographers as the initial step of inserting words into the WORDNET hierarchy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11734261,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "4d95e7c77a0c6a9da73399c1e2717870dc083a89",
            "isKey": true,
            "numCitedBy": 16,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate the impact of morphological features on the task of automatically extending a dictionary. We approach the problem as a pattern classification task and compare the performance of several models in classifying nouns that are unknown to a broad coverage dictionary. We used a boosting classifier to compare the performance of models that use different sets of features. We show how adding simple morphological features to a model greatly improves the classification performance."
            },
            "slug": "Boosting-automatic-lexical-acquisition-with-Ciaramita",
            "title": {
                "fragments": [],
                "text": "Boosting automatic lexical acquisition with morphological information"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is shown how adding simple morphological features to a model greatly improves the classification performance, and a boosting classifier is used to compare the performance of models that use different sets of features."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Na\u0131\u0308ve Bayes and maximum entropy chunkers are trained on the entire Penn Treebank (Marcus et al., 1994) chunks extracted using the CoNLL-2000 script (Buchholz, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All three taggers have been trained on the Penn Treebank (Marcus et al., 1994), so the remaining components are designed to handle the Penn POS tag set (Santorini, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5471277"
                        ],
                        "name": "Chrysanne Dimarco",
                        "slug": "Chrysanne-Dimarco",
                        "structuredName": {
                            "firstName": "Chrysanne",
                            "lastName": "Dimarco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chrysanne Dimarco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Example near-synonym differentia from DiMarco et al. (1993) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 110
                            }
                        ],
                        "text": "DiMarco et al. (1993) add near-synonym distinctions to a Natural Language Generation (NLG) knowledge base and DiMarco (1994) shows how near-synonym differentia can form lexical relations between words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1: Example near-synonym differentia from DiMarco et al. (1993)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16344099,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "721675445ba202dece73ace1d00867e60ae59fd7",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction The words gawp, gaze, ~md stare all <lenote a, ldnd of prolonge(I look: they a.re nc\u00a2u'-syno'nyms, or plcsionyms [Cruse 1986]. [h)wever, a.s we learn from their indivi(lu;tl enl.ries in the Oxford advanced learner's dictionary (OALD; fourth edi-rio:n, 198f))~ to ga.ze is to look long a.nd steadily; to st;axe is to (Io this with the eyes wide open; a.nd gawping has the additiom~l requirement tha.t the ~ct I)e impolite or stupid. In recent work [l)i-Marco, llirst, m~d Stede 1()93; I)iM~u'co and IIirst 1993], we ddress 1;1l(; problen~ of representing the lexica.l ['ea, tures theft distinguish groups of ne++r-synonynts. Our lexical tim.tures for (lil/erei~Lia.tion are not intended to be any kind of l)rimit;ives for (le(:om-l)ositiomfl semantics: they iir~'(~ ]lOt being used to rel>resent whole meanings, t)ut r+~ther l;o rep]'esent d{[.'fi'~vnec.~ between ]ne,~nings. These dift'erences between plesionyms (a ,, I)e sh,~des of (tenota, Lion or (:onnota,tion, or emt)ha.ses on dift'e]'ent (:Oral)O-nen t s o [\" t he meani n g. Our eveutuM goM is ~ represent;a.tio], for a lexicon in which sem~mti(\" and stylistic distiuctions ca,Jl I)e l/lade bel;ween syn(tttylns a,n(l plesionyms, I)oth within a.nd ~(;ross la.ngua.ges, ['or the purpose of lexica.1 choit:e in natural l,~ngua,ge gener,~tion a, nd machine tra.nslation. The na,ture o[ these distinctions suggests tha.t timy can be viewed a.s rc-lations I)etween nea,r-synot~yms. In this l)a.per, we undertake ~ study of the ch~u'~cteristics of ne~> synonymic rel;~tions as a. stel) towa:r<ls a. knowl edge rel)resent,~tiou for lexi(:M discrimination. As a first st;eli , whi(;h we des(:ribed in [l)iMar(:o, llirst, ~u,d Ste<le 1993], we carried out a. stu<ly o1' dictiona,ry usage notes in order to compile a. list. of the kinds of dime, nsions flint axe used frequently ~s (lenotat;iw; or connot~tive dil[erentiae. We i)r(,-duced zt l>relimina, ry list; of 26 (lenota.tional dimensions and 12 eonnota,tive dimensions (including a few l;h~t we aAded from the discussion on lexi(:a,l a.sl)e(:ts t)y Viua.y m~d l)a.rl)eltmt [1958]). (This set is not, yet; complete 1)1' definitive, of course, bul; we h;we mamtge(t to in(:lu(le ~ fairly (:ompre Imnsive selection of the most common diffexences between ne~r-synonyms.) Some of the dimensions ~re sin@e l)in~u'y choices; others are continuous. We show ;~ representative sa, ml)le in Table 1. l)3a(:h line of the ta\u00a3le shows ~ dimension of differentia-lion followed by ex~unple sentences in which two i)lesionyms vary a.long th,~t dimension. 3 Chafl]u and Herrmalan 3.1. Basic theory (]ha,[fin ++nd llerrma.n n [1988] lt~+ve provMed ++ \u2026"
            },
            "slug": "The-Nature-of-Near-Synonymic-Relations-Dimarco",
            "title": {
                "fragments": [],
                "text": "The Nature of Near-Synonymic Relations"
            },
            "venue": {
                "fragments": [],
                "text": "COLING 1994"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806133"
                        ],
                        "name": "H. Kimoto",
                        "slug": "H.-Kimoto",
                        "structuredName": {
                            "firstName": "Haruo",
                            "lastName": "Kimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794558"
                        ],
                        "name": "T. Iwadera",
                        "slug": "T.-Iwadera",
                        "structuredName": {
                            "firstName": "Toshiaki",
                            "lastName": "Iwadera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Iwadera"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12575915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8b9717cfc1224a15adf8566f9f67577a753e1cd",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "An information retrieval system based on a dynamic thesaurus was developed utilizing the connectionist approach. The dynamic thesaurus consists of nodes, which represent each term of a thesaurus, and links, which represent the connections between nodes. Term information that is automatically extracted from user's relevant documents is used to change node weights and generate links. Node weights and links reflect a user's particular interest. A document retrieval experiment was conducted in which both a high recall rate and a high precision rate were achieved.\nThe topics discussed in this paper:\nConnectionist Model, Automatic Indexing, Information Retrieval, and Thesaurus."
            },
            "slug": "Construction-of-a-dynamic-Thesaurus-and-its-use-for-Kimoto-Iwadera",
            "title": {
                "fragments": [],
                "text": "Construction of a dynamic Thesaurus and its use for associated information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An information retrieval system based on a dynamic thesaurus was developed utilizing the connectionist approach, in which both a high recall rate and a high precision rate were achieved."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '90"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40435330"
                        ],
                        "name": "Shaojun Zhao",
                        "slug": "Shaojun-Zhao",
                        "structuredName": {
                            "firstName": "Shaojun",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaojun Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118756175"
                        ],
                        "name": "Lijuan Qin",
                        "slug": "Lijuan-Qin",
                        "structuredName": {
                            "firstName": "Lijuan",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lijuan Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143849609"
                        ],
                        "name": "M. Zhou",
                        "slug": "M.-Zhou",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2220173,
            "fieldsOfStudy": [
                "Biology",
                "Linguistics"
            ],
            "id": "54a38a1341dc81fd8781c393d18179ff42d79354",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "There have been many proposals to compute similarities between words based on their distributions in contexts. However, these approaches do not distinguish between synonyms and antonyms. We present two methods for identifying synonyms among distributionally similar words."
            },
            "slug": "Identifying-Synonyms-among-Distributionally-Similar-Lin-Zhao",
            "title": {
                "fragments": [],
                "text": "Identifying Synonyms among Distributionally Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work presents two methods for identifying synonyms among distributionally similar words and presents two approaches to compute similarities between words based on their distributions in contexts."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There is little research into the effect of corpus type, genre and size on performance of NLP systems; exceptions include studies in cross-domain parsing (Gildea, 2001; Hwa, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model."
            },
            "slug": "Corpus-Variation-and-Parser-Performance-Gildea",
            "title": {
                "fragments": [],
                "text": "Corpus Variation and Parser Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines how other types of text might a ect parser performance, and how portable parsing models are across corpora by comparing results for the Brown and WSJ corpora, and considers which parts of the parser's probability model are particularly tuned to the corpus on which it was trained."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51981856"
                        ],
                        "name": "M. Masterman",
                        "slug": "M.-Masterman",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Masterman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Masterman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2971978"
                        ],
                        "name": "Y. Wilks",
                        "slug": "Y.-Wilks",
                        "structuredName": {
                            "firstName": "Yorick",
                            "lastName": "Wilks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wilks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63859389,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0579d197d82f42d9569d6d5bfedfa32bfe61b4e6",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "1. In M.T. literature, it is usually assumed that compiling an M.T. dictionary is, for the linguist, a matter of routine; that the main problem lies in providing sufficient computer-storage to accommodate it. Such judgments fail to take account either of the unpredictability of language, (Reifler, 1956), or of the profound change in the conception of a dictionary produced by the substitution of the M.T. chunk for the free word (Firth, B.B.C. Talk, 1955)."
            },
            "slug": "Language,-Cohesion-and-Form:-The-potentialities-of-Masterman-Wilks",
            "title": {
                "fragments": [],
                "text": "Language, Cohesion and Form: The potentialities of a mechanical thesaurus"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This chapter discusses the construction and maintenance of an M.T.T.'s chunk for the free word, and some of the techniques used to construct and maintain this chunk."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 82
                            }
                        ],
                        "text": "This approach has been implemented by many different researchers in NLP including Hindle (1990); Brown et al. (1992); Pereira et al. (1993); Ruge (1997) and Lin (1998d),"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "This is particularly common with mutual information weighted scores, for example, Hindle (1990) and Luk (1995) (see Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 418,
                                "start": 145
                            }
                        ],
                        "text": "This in itself is not a new concept, mutual informationhaving been successfully used as a weighting function by a number of systems in the past (Hindle, 1990; Lin, 1998c; Luk, 1995). However, this is the first research to connect weighting with collocational strength and test various weight functions systematically. I have implemented most of the approaches in the Collocationschapter of Manning and Sch \u00fctze (1999), including the t-test, \u03c72-test and likelihood ratio."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "This approach has been implemented by many different researchers in NLP including Hindle (1990); Brown et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 82
                            }
                        ],
                        "text": "This approach has been implemented by many different researchers in NLP including Hindle (1990); Brown et al. (1992); Pereira et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15862538,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f3f3dcfcaa960ec201e0381f4d026e57e64bea76",
            "isKey": true,
            "numCitedBy": 689,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "slug": "Noun-Classification-from-Predicate-Argument-Hindle",
            "title": {
                "fragments": [],
                "text": "Noun Classification from Predicate-Argument Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59702881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99328d4b34d1ac02252258a9437b8b2c1acdb92c",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we first present a dependency-based method for parser evaluation. We then use the method to evaluate a broad-coverage parser, called MINIPAR, with the SUSANNE corpus. The method allows us to evaluate not only the overall performance of the parser, but also its performance with respect to different grammatical relationships and phenomena. The evaluation results show that MINIPAR is able to cover about 79% of the dependency relationships in the SUSANNE corpus with about 89% precision."
            },
            "slug": "Dependency-Based-Evaluation-of-Minipar-Lin",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Evaluation of Minipar"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A dependency-based method for parser evaluation is presented and a broad-coverage parser, called MINIPAR, is evaluated with the SUSANNE corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5906107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "691414980e2575b6788578ed300fb6f1d39d5a72",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch."
            },
            "slug": "Language-Independent-NER-using-a-Maximum-Entropy-Curran-Clark",
            "title": {
                "fragments": [],
                "text": "Language Independent NER using a Maximum Entropy Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "determining if a review is positive or negative (Turney, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 484335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e7c7853a16a378cc24a082153b282257a9675b7",
            "isKey": false,
            "numCitedBy": 5418,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews."
            },
            "slug": "Thumbs-Up-or-Thumbs-Down-Semantic-Orientation-to-of-Turney",
            "title": {
                "fragments": [],
                "text": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (Thumbs down) if the average semantic orientation of its phrases is positive."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2754495"
                        ],
                        "name": "Massimiliano Ciaramita",
                        "slug": "Massimiliano-Ciaramita",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Ciaramita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimiliano Ciaramita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "My results on this task significantly outperform the existing work of Ciaramita et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In Chapter 6 I describe the use of similarity measurements for the task of predicting the supersense tags of previously unseen words (Ciaramita et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several computational attempts to reduce the number of sense distinctions and increase the size of each synset in WORDNET (Buitelaar, 1998; Ciaramita et al., 2003; Hearst and Sch\u00fctze, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 156
                            }
                        ],
                        "text": "There have been several computational attempts to reduce the number of sense distinctions and increase the size of each synset in WORDNET (Buitelaar, 1998; Ciaramita et al., 2003; Hearst and Sch\u0308utze, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16001872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52a3adde8439ddca704f820801346e9d9492852a",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \u201cworld knowledge\u201d. The model compiles knowledge contained in a dictionaryontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \u201chierarchical classifier\u201d outperforms a state-of-the-art standard \u201cflat\u201d one."
            },
            "slug": "Hierarchical-Semantic-Classification:-Word-Sense-Ciaramita-Hofmann",
            "title": {
                "fragments": [],
                "text": "Hierarchical Semantic Classification: Word Sense Disambiguation with World Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments on a word sense disambiguation task provide empirical evidence that this \u201chierarchical classifier\u201d outperforms a state-of-the-art standard \u201cflat\u201d one."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4336033"
                        ],
                        "name": "Natalia N. Modjeska",
                        "slug": "Natalia-N.-Modjeska",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Modjeska",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natalia N. Modjeska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686341"
                        ],
                        "name": "K. Markert",
                        "slug": "K.-Markert",
                        "structuredName": {
                            "firstName": "Katja",
                            "lastName": "Markert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Markert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742475"
                        ],
                        "name": "M. Nissim",
                        "slug": "M.-Nissim",
                        "structuredName": {
                            "firstName": "Malvina",
                            "lastName": "Nissim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nissim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although there are experiments that have used the web as a corpus, they only estimate cooccurrence relative frequencies using search engine hit counts (Keller et al., 2002; Modjeska et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 693910,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "2716a03ede1e61da55a4a476ed85aef42e6bcbfe",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a machine learning framework for resolving other-anaphora. Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web. We search the Web via lexico-syntactic patterns that are specific to other-anaphors. Incorporating this innovative feature leads to an 11.4 percentage point improvement in the classifier's F-measure (25% improvement relative to results without this feature)."
            },
            "slug": "Using-the-Web-in-Machine-Learning-for-Resolution-Modjeska-Markert",
            "title": {
                "fragments": [],
                "text": "Using the Web in Machine Learning for Other-Anaphora Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, the algorithm obtains additional semantic knowledge from the Web via lexico-Syntactic patterns that are specific to other-anaphors."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088917230"
                        ],
                        "name": "Matthew Berland",
                        "slug": "Matthew-Berland",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Berland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Berland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Berland and Charniak (1999) use a similar approach for identifying whole-part relations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 0
                            }
                        ],
                        "text": "Berland and Charniak (1999) use a similar approach for identifying whole-part relations. Caraballo (1999) constructs a hierarchical structure using the hyponym relations extracted by Hearst (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17767129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9823d8c686227316e55ad42b44bcc3f17148b722",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for extracting parts of objects from wholes (e.g. \"speedometer\" from \"car\"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon."
            },
            "slug": "Finding-Parts-in-Very-Large-Corpora-Berland-Charniak",
            "title": {
                "fragments": [],
                "text": "Finding Parts in Very Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for extracting parts of objects from wholes using a very large corpus with 55% accuracy for the top 50 words as ranked by the system is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2854524"
                        ],
                        "name": "D. Turcato",
                        "slug": "D.-Turcato",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Turcato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Turcato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2822470"
                        ],
                        "name": "F. Popowich",
                        "slug": "F.-Popowich",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Popowich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Popowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34565751"
                        ],
                        "name": "J. Toole",
                        "slug": "J.-Toole",
                        "structuredName": {
                            "firstName": "Janine",
                            "lastName": "Toole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14064211"
                        ],
                        "name": "D. Fass",
                        "slug": "D.-Fass",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Fass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49369272"
                        ],
                        "name": "D. Nicholson",
                        "slug": "D.-Nicholson",
                        "structuredName": {
                            "firstName": "Devlan",
                            "lastName": "Nicholson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nicholson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2551866"
                        ],
                        "name": "Gordon Tisher",
                        "slug": "Gordon-Tisher",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Tisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gordon Tisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 107
                            }
                        ],
                        "text": ", 1995) and also to prune senses and synonym relation links based on evidence from domain-specific corpora (Basili et al., 1998; Turcato et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 750455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d726144c1a93e639428f955bc103ced9945a233",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for adapting a general purpose synonym database, like WordNet, to a specific domain, where only a subset of the synonymy relations defined in the general database hold. The method adopts an eliminative approach, based on incrementally pruning the original database. The method is based on a preliminary manual pruning phase and an algorithm for automatically pruning the database. This method has been implemented and used for an Information Retrieval system in the aviation domain."
            },
            "slug": "Adapting-a-synonym-database-to-specific-domains-Turcato-Popowich",
            "title": {
                "fragments": [],
                "text": "Adapting a synonym database to specific domains"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper describes a method for adapting a general purpose synonym database, like WordNet, to a specific domain, where only a subset of the synonymy relations defined in the general database hold."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63043568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb180e7615fedf3479707aea9db9ee156f9406e",
            "isKey": false,
            "numCitedBy": 1659,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, Experiment 3: Combining Local Context and Wordnet Similarity Measures, Conclusions, References"
            },
            "slug": "Combining-Local-Context-and-Wordnet-Similarity-for-Fellbaum-Miller",
            "title": {
                "fragments": [],
                "text": "Combining Local Context and Wordnet Similarity for Word Sense Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, and Combining Local Context and Wordnet Similarity Measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118384241"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Abe and Li (1996) use a tree-cut model over the WORDNET hierarchy, selected with the minimum description length (MDL) principle, to estimate the association norm between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12171833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ec7d4b728df932965c25451697ebcd6da783c4c",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning co-occurrence information between two word categories, or more in general between two discrete random variables taking values in a hierarchically classified domain. In particular, we consider the problem of learning the `association norm' defined by A(x,y)=p(x, y)/(p(x)*p(y)), where p(x, y) is the joint distribution for x and y and p(x) and p(y) are marginal distributions induced by p(x, y). We formulate this problem as a sub-task of learning the conditional distribution p(x|y), by exploiting the identity p(x|y) = A(x,y)*p(x). We propose a two-step estimation method based on the MDL principle, which works as follows: It first estimates p(x) as p1 using MDL, and then estimates p(x|y) for a fixed y by applying MDL on the hypothesis class of {A * p1 | A \\in B} for some given class B of representations for association norm. The estimation of A is therefore obtained as a side-effect of a near optimal estimation of p(x|y). We then apply this general framework to the problem of acquiring case-frame patterns. We assume that both p(x) and A(x, y) for given y are representable by a model based on a classification that exists within an existing thesaurus tree as a `cut,' and hence p(x|y) is represented as the product of a pair of `tree cut models.' We then devise an efficient algorithm that implements our general strategy. We tested our method by using it to actually acquire case-frame patterns and conducted disambiguation experiments using the acquired knowledge. The experimental results show that our method improves upon existing methods."
            },
            "slug": "Learning-Word-Association-Norms-Using-Tree-Cut-Pair-Abe-Li",
            "title": {
                "fragments": [],
                "text": "Learning Word Association Norms Using Tree Cut Pair Models"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The problem of learning co-occurrence information between two word categories, or more in general between two discrete random variables taking values in a hierarchically classified domain is considered, and a two-step estimation method based on the MDL principle is proposed, which improves upon existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992275"
                        ],
                        "name": "Maria Lapata",
                        "slug": "Maria-Lapata",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6619588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe1fcf5a5f4c930e685202629f10c95e8ff5e057",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the interpretation of nominalizations in domain independent wide-coverage text. We present a statistical model which interprets nominalizations based on the cooccurrence of verb-argument tuples in a large balanced corpus. We propose an algorithm which treats the interpretation task as a disambiguation problem and achieves a performance of approximately 80% by combining partial parsing, smoothing techniques and domain independent taxonomic information (e.g., WordNet)."
            },
            "slug": "The-Automatic-Interpretation-of-Nominalizations-Lapata",
            "title": {
                "fragments": [],
                "text": "The Automatic Interpretation of Nominalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A statistical model which interprets nominalizations based on the cooccurrence of verb-argument tuples in a large balanced corpus is presented and an algorithm which treats the interpretation task as a disambiguation problem and achieves a performance of approximately 80% is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1752785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265be00bf112c6cb2fa3e8176bff8394a114dbde",
            "isKey": false,
            "numCitedBy": 3889,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66)."
            },
            "slug": "Using-Information-Content-to-Evaluate-Semantic-in-a-Resnik",
            "title": {
                "fragments": [],
                "text": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content, which performs encouragingly well and is significantly better than the traditional edge counting approach."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 17
                            }
                        ],
                        "text": "The CASS parser (Abney, 1991, 1996), part of Abney\u2019s SCOL system (1997), uses cascaded finite state transducers ( FSTs) to produce a limited-depth parse of POS tagged text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32440409"
                        ],
                        "name": "T. Rose",
                        "slug": "T.-Rose",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Rose",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144795097"
                        ],
                        "name": "Mark Stevenson",
                        "slug": "Mark-Stevenson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Stevenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068521476"
                        ],
                        "name": "Miles Whitehead",
                        "slug": "Miles-Whitehead",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Whitehead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miles Whitehead"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9239414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "590cfa38094122f42412d747350229f79b7e6412",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Reuters, the global information, news and technology group, has for the first time made available free of charge, large quantities of archived Reuters news stories for use by research communities around the world. The Reuters Corpus Volume 1 (RCV1) includes over 800,000 news stories typical of the annual English language news output of Reuters. This paper describes the origins of RCV1, the motivations behind its creation, and how it differs from previous corpora. In addition we discuss the system of category coding, whereby each story is annotated for topic, region and industry sector. We also discuss the process by which these codes were applied, and examine the issues involved in maintaining quality and consistency of coding in an operational, commercial environment."
            },
            "slug": "The-Reuters-Corpus-Volume-1-from-Yesterday\u2019s-News-Rose-Stevenson",
            "title": {
                "fragments": [],
                "text": "The Reuters Corpus Volume 1 -from Yesterday\u2019s News to Tomorrow\u2019s Language Resources"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The origins of RCV1, the motivations behind its creation, and how it differs from previous corpora are described, and the system of category coding, whereby each story is annotated for topic, region and industry sector is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2131960"
                        ],
                        "name": "Herv\u00e9 D\u00e9jean",
                        "slug": "Herv\u00e9-D\u00e9jean",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "D\u00e9jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Herv\u00e9 D\u00e9jean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299876"
                        ],
                        "name": "R. Koeling",
                        "slug": "R.-Koeling",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Koeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349412"
                        ],
                        "name": "Yuval Krymolowski",
                        "slug": "Yuval-Krymolowski",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Krymolowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Krymolowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3263632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13687a29e5b51c3f1b51593f95aa3dc2c67990e6",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We use seven machine learning algorithms for one task: identifying base noun phrases. The results have been processed by different system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top five systems, to a standard data set and managed to improve the best published result for this data set."
            },
            "slug": "Applying-System-Combination-to-Base-Noun-Phrase-Sang-Daelemans",
            "title": {
                "fragments": [],
                "text": "Applying System Combination to Base Noun Phrase Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work uses seven machine learning algorithms for one task: identifying base noun phrases and applies the seven learners with the best combinator, a majority vote of the top five systems, to a standard data set and improves the best published result."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1400617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1358bc877f106ffd066796f66ed6f8242b99d2d1",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar."
            },
            "slug": "Investigating-GIS-and-Smoothing-for-Maximum-Entropy-Curran-Clark",
            "title": {
                "fragments": [],
                "text": "Investigating GIS and Smoothing for Maximum Entropy Taggers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary, and the use of a Gaussian prior and a simple cutoff for smoothing is explored."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11694248,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "85eed2cc5af25d6987fdaad06defb0e5ecec07cd",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Verbs were clustered semantically on the basis of their alternation behaviour, as characterised by their syntactic subcategorisation frames extracted from maximum probability parses of a robust statistical parser, and completed by assigning WordNet classes as selectional preferences to the frame arguments. The clustering was achieved (a) iteratively by measuring the relative entropy between the verbs' probability distributions over the frame types, and (b) by utilising a latent class analysis based on the joint frequencies of verbs and frame types."
            },
            "slug": "Clustering-Verbs-Semantically-According-to-their-Walde",
            "title": {
                "fragments": [],
                "text": "Clustering Verbs Semantically According to their Alternation Behaviour"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Verbs were clustered semantically on the basis of their alternation behaviour, as characterised by their syntactic subcategorisation frames extracted from maximum probability parses of a robust statistical parser, and completed by assigning WordNet classes as selectional preferences to the frame arguments."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145702640"
                        ],
                        "name": "Guido Minnen",
                        "slug": "Guido-Minnen",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Minnen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guido Minnen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35096557"
                        ],
                        "name": "Darren Pearce",
                        "slug": "Darren-Pearce",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Pearce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since MINIPAR and RASP perform morphological analysis on the context relations we have added an existing morphological analyser (Minnen et al., 2000) to the other extractors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2001) \u2013 and is very efficient, analysing over 80 000 words per second (Minnen et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9671238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbfef439903cd8abc6b17e1efb83fd247088e6c6",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application."
            },
            "slug": "Robust,-applied-morphological-generation-Minnen-Carroll",
            "title": {
                "fragments": [],
                "text": "Robust, applied morphological generation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required is presented."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35258592"
                        ],
                        "name": "David J. Weir",
                        "slug": "David-J.-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15205002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0f6af15c46a8f834f2d30ce21330cbf0b4a0397",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns."
            },
            "slug": "A-General-Framework-for-Distributional-Similarity-Weeds-Weir",
            "title": {
                "fragments": [],
                "text": "A General Framework for Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49125917"
                        ],
                        "name": "Takaaki Tanaka",
                        "slug": "Takaaki-Tanaka",
                        "structuredName": {
                            "firstName": "Takaaki",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takaaki Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lin (1999) has used similarity measures to determine if relationships between words are idiomatic or non-compositional and Baldwin et al. (2003) and Bannard et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1695436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0756f5cb5ae444d153734edf68d1ce9b95a96d1a",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet."
            },
            "slug": "An-Empirical-Model-of-Multiword-Expression-Baldwin-Bannard",
            "title": {
                "fragments": [],
                "text": "An Empirical Model of Multiword Expression Decomposability"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A construction-inspecific model of multiword expression decomposability based on latent semantic analysis is presented, and evidence is furnished for the calculated similarities being correlated with the semantic relational content of WordNet."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82323309"
                        ],
                        "name": "Karen Sparck Jones",
                        "slug": "Karen-Sparck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sparck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sparck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5207282,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "be5406dcea7c9c734cdb1f6797712b71fcf71b79",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "356,302. Golf bags. DENT, L. M. E., 2, Porchester Gardens, Bayswater, and MACKENZIE, A., 38A, Northside, Clapham Common, both in London. June 10, 1930, No. 17844. [Class 132 (ii).] The reinforcing ring at the mouth of a golfclub bag is made in two or more parts so interconnected that they may be moved relatively to expand or contract the compass of the mouth. As shown in the Figure, the ring is composed of two bars 12, 13 h n ed together at their right-hand ends and pivoted at their other ends to links 17, 18 hinged together at 19. By pressing the links 17, 18 downwards the ends 12, 13 of the bars may be brought together and the links may be retained in this position by a catch. A rigid reinforcing ring may be provided at the base of the bag or a collapsable one similar to that at the mouth may be used. The bag is provided with flexible stiffeners 21, 22, carrying straps 26, 27, hood 25, dividing strap 24, and umbrella strap 31 and support 32."
            },
            "slug": "Synonymy-and-semantic-classification-Jones",
            "title": {
                "fragments": [],
                "text": "Synonymy and semantic classification"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "The reinforcing ring at the mouth of a golfclub bag is made in two or more parts so interconnected that they may be moved relatively to expand or contract the compass of the mouth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992275"
                        ],
                        "name": "Maria Lapata",
                        "slug": "Maria-Lapata",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lapata also uses human judgements to evaluate probabilistic models for logical metonymy (Lapata and Lascarides, 2003) and smoothing (Lapata et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8487875,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "495bb113512576b76ce84891d40e243cc75708f4",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous research has shown that the plausibility of an adjective-noun combination is correlated with its corpus co-occurrence frequency. In this paper, we estimate the co-occurrence frequencies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings. Both class-based smoothing and distance-weighted averaging yield frequency estimates that are significant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques."
            },
            "slug": "Evaluating-Smoothing-Algorithms-against-Judgements-Lapata-Keller",
            "title": {
                "fragments": [],
                "text": "Evaluating Smoothing Algorithms against Plausibility Judgements"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper estimates the co-occurrence frequencies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compares them to human plausibility ratings to provide independent evidence for the validity of these smoothing methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "spired by pseudo-words which were first introduced for word sense disambiguation (WSD) evaluation, where two distinct words were concatenated to produce an artificial ambiguity (Gale et al., 1992; Sch\u00fctze, 1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18817171,
            "fieldsOfStudy": [
                "Law"
            ],
            "id": "54859dd7fcfd9112e01eded68a29cd26611d66e8",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Table 1: Sample Concordances of duty (split into two senses) Sense Examples (from Canadian Hansards) tax fewer cases of companies paying duty and then claiming a refund and impose a countervailing duty of 29,1 per cent on candian exports of the united states imposed a duty on canadian salttish last year obligation it is my honour and duty to present a petition duly approved working well beyond the call of duty ? SENT i know what time they start in addition, it is my duty to present the government\u2019s comments"
            },
            "slug": "Work-on-Statistical-Methods-for-Word-Sense-Gale-Church",
            "title": {
                "fragments": [],
                "text": "Work on Statistical Methods for Word Sense Disambiguation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8184544"
                        ],
                        "name": "C. D. Marcken",
                        "slug": "C.-D.-Marcken",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Marcken",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Marcken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The CMU POS tagger, a trigram tagger based on de Marcken (1990) and trained on the Brown Corpus (Francis and Kucera, 1982), is used to disambiguate the set of POS tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 851638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18dde31c2716b0bf10a35f334ed24af4e4e37fa5",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a rapid and robust parsing system currently used to learn from large bodies of unedited text. The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze. The results of applying the disambiguator and parser to large sections of the Lancaster/Oslo-Bergen corpus are presented."
            },
            "slug": "Parsing-the-LOB-Corpus-Marcken",
            "title": {
                "fragments": [],
                "text": "Parsing the LOB Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A rapid and robust parsing system currently used to learn from large bodies of unedited text, which contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144210097"
                        ],
                        "name": "C. Matheson",
                        "slug": "C.-Matheson",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Matheson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Matheson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847246"
                        ],
                        "name": "Andrei Mikheev",
                        "slug": "Andrei-Mikheev",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Mikheev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Mikheev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 223
                            }
                        ],
                        "text": "From Distributional to Semantic Similarity\nJames Richard Curran\nT H\nE\nU N I V E R\nS\nI T\nY\nO F\nE D I N B U\nR G\nH\nDoctor of Philosophy\nInstitute for Communicating and Collaborative Systems\nSchool of Informatics\nUniversity of Edinburgh\n2003\nAbstract\nLexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 65
                            }
                        ],
                        "text": "Hindering our easy collaboration is the greatest cost of leaving Edinburgh and I will miss our regular conversations dearly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 85
                            }
                        ],
                        "text": "The application of semantic similarity to supersense tagging follows similar work by Hearst and Sc\u1e27utze (1993) and Widdows (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 101
                            }
                        ],
                        "text": "S EXTANT(LT) uses theLT-POS tagger from the Language Technology Group at the University of Edinburgh (Grover et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 41
                            }
                        ],
                        "text": "I remember fondly the times when a large Edinburgh posse went to conferences, in particular to my room mates in fancy (and not so fancy) hotels David Schlangen and Stephen Clark, and my conference \u2018buddy\u2019 Malvina Nissim."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "Edinburgh has been an exceptionally intellectually fertile environment to undertake a PhD and I appreciate the many courses, reading groups, discussions and collaborations I have been involved in over the last three years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "Edinburgh has the kind of buzz I would like to emulate for my students in the future."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 102
                            }
                        ],
                        "text": "Discussions with Marc have been particularly enjoyable affairs, and the greatest regret of my time in Edinburgh is neither of us saw fit to schedule more of them."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 85
                            }
                        ],
                        "text": "The application of semantic similarity to supersense tagging follows similar work by Hearst and Sc\u1e27utze (1993) and Widdows (2003). To classify a previously unseen word my approach extracts synonyms and uses their supersenses as an indication of the supersense of the unseen word."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "S EXTANT(NB) uses simple N\u00e4\u0131ve Bayes tagging/chunking models, S EXTANT(LT) uses theText Tokenisation Toolkit (Grover et al., 2000), and S EXTANT(MX ) uses theC&C maximum entropy tools (Curran and Clark, 2003a,b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 118
                            }
                        ],
                        "text": "That Kathryn, Robert and Elizabeth have forgone the option of excommunicating their extremely geeky brother whilst in Edinburgh is also appreciated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 71
                            }
                        ],
                        "text": "Returning home to be with you all is the only possible competition for Edinburgh."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 45
                            }
                        ],
                        "text": "Without you, I would not have undertaken our Edinburgh adventure and without you I could not have enjoyed it in the way we did: chatting and debating, cycling, cooking, art-house cinema, travelling, Edinburgh festivals, more chatting, snooker, backgammon and our ever growing book collection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 252573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "082dd9f93c88993b866aa55ed06eda4ace7f36c8",
            "isKey": true,
            "numCitedBy": 122,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe LT TTT, a recently developed software system which provides tools to perform text tokenisation and mark-up. The system includes ready-made components to segment text into paragraphs, sentences, words and other kinds of token but, crucially, it also allows users to tailor rule-sets to produce mark-up appropriate for particular applications. We present three case studies of our use of LT TTT: named-entity recognition (MUC-7), citation recognition and mark-up and the preparation of a corpus in the medical domain. We conclude with a discussion of the use of browsers to visualise marked-up text."
            },
            "slug": "LT-TTT-A-Flexible-Tokenisation-Tool-Grover-Matheson",
            "title": {
                "fragments": [],
                "text": "LT TTT - A Flexible Tokenisation Tool"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "LT TTT, a recently developed software system which provides tools to perform text tokenisation and mark-up, is described and three case studies of the use are presented: named-entity recognition (MUC-7), citation recognition and Mark-up and the preparation of a corpus in the medical domain."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145035103"
                        ],
                        "name": "John C. Henderson",
                        "slug": "John-C.-Henderson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Henderson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2000), word sense disambiguation (Pederson, 2000) and statistical parsing (Henderson and Brill, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5666926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab19c24b56f6dc482db69e15be20ee27670bebc0",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank."
            },
            "slug": "Exploiting-Diversity-in-Natural-Language-Combining-Henderson-Brill",
            "title": {
                "fragments": [],
                "text": "Exploiting Diversity in Natural Language Processing: Combining Parsers"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy, and both parametric and non-parametric models are explored."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12739674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e69d3057fa8088c3af1b0e36b872d082997347f",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Document clustering is useful in many information retrieval tasks: document browsing, organization and viewing of retrieval results, generation of Yahoo-like hierarchies of documents, etc. The general goal of clustering is to group data elements such that the intra-group similarities are high and the inter-group similarities are low. We present a clustering algorithm called CBC (Clustering By Committee) that is shown to produce higher quality clusters in document clustering tasks as compared to several well known clustering algorithms. It initially discovers a set of tight clusters (high intra-group similarity), called committees, that are well scattered in the similarity space (low inter-group similarity). The union of the committees is but a subset of all elements. The algorithm proceeds by assigning elements to their most similar committee. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and manually constructed classes (the answer key). This evaluation measure is more intuitive and easier to interpret than previous evaluation measures."
            },
            "slug": "Document-clustering-with-committees-Pantel-Lin",
            "title": {
                "fragments": [],
                "text": "Document clustering with committees"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A new evaluation methodology that is based on the editing distance between output clusters and manually constructed classes (the answer key) is presented, which is more intuitive and easier to interpret than previous evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One difficulty that Grefenstette (1994) describes is the interpretation of long noun compounds in SEXTANT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Early similarity systems used small (Hindle, 1990) or specialist (Grefenstette, 1994) corpora (Section 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Semantic EXtraction from Text via Analysed Networks of Terms (SEXTANT) system has been designed specifically for automatic thesaurus extraction (Grefenstette, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hearst and Grefenstette (1992) have proposed a combination of the results of their respective similarity systems to produce a hyponym hierarchy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, Hearst and Grefenstette (1992) find a significant improvement in recall, which is a major problem for hyponym extraction systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Grefenstette (1994) does this by only comparing headwords which have a frequency greater than 10."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hearst and Grefenstette (1992) combine this approach with a vector-space similarity measure (Grefenstette, 1994), to overcome some of these problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122880978"
                        ],
                        "name": "James M. Hodgson",
                        "slug": "James-M.-Hodgson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hodgson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Hodgson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143378752,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f26ab52b2b191a6ada08645c95a1282378c02455",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A popular explanation of automatic semantic priming attributes those effects to the spreading of activation across a meaning-based network in lexical memory. A number of conflicting claims have been advanced regarding the kinds of information captured in that pre-lexical network. Lexical decision and naming experiments are reported which indicate (1) that automatic priming is supported equally well by a very broad range of relations, (2) that the quality of relation for individual prime-target pairs, as reflected in such measures as traditional associative strength, has no impact on the amount of priming produced, (3) that asymmetries between lexical decision and naming in the amount of priming they generate under stringent conditions of automaticity require the postulation of an extra-lexical source of automatic priming, and (4) the residual priming left for the putative lexical network to explain is at most a few milliseconds. Taken together, these findings suggest that explanations of automati..."
            },
            "slug": "Informational-constraints-on-pre-lexical-priming-Hodgson",
            "title": {
                "fragments": [],
                "text": "Informational constraints on pre-lexical priming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 11
                            }
                        ],
                        "text": "Schulte im Walde (2000) reports the precision and recall of verbs clustered into Levin classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 11
                            }
                        ],
                        "text": "Schulte im Walde (2000) reports the precision and recall of verbs clustered into Levin classes. However, in later work for German verbs, Schulte im Walde (2003) introduces an alternative evaluation using theadjusted Rand index (Hubert and Arabie, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2608007,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "099017f651f063b293f7281e22e976575be696d6",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of verb features is crucial for the learning of verb classes. This paper presents clustering experiments on 168 German verbs, which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification."
            },
            "slug": "Experiments-on-the-Choice-of-Features-for-Learning-Walde",
            "title": {
                "fragments": [],
                "text": "Experiments on the Choice of Features for Learning Verb Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Cl clustering experiments on 168 German verbs are presented, which present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724629"
                        ],
                        "name": "Marius Pasca",
                        "slug": "Marius-Pasca",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Pasca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Pasca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10619607"
                        ],
                        "name": "R. Goodrum",
                        "slug": "R.-Goodrum",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Goodrum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodrum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145154896"
                        ],
                        "name": "V. Rus",
                        "slug": "V.-Rus",
                        "structuredName": {
                            "firstName": "Vasile",
                            "lastName": "Rus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 130
                            }
                        ],
                        "text": "Other work has focused on using lexical resources to calculate the similarity between the candidate answers and the question type (Moldovan et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27571384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a172a5c7856f27588ac45c1df6d1463a0c4eb3b5",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the architecture, operation and results obtained with the LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU. To find answers, the system relies on a combination of syntactic and semantic techniques. The search for the answer is based on a novel form of indexing called paragraph indexing. A score of 55.5% for short answers and 64.5% for long answers was achieved at the TREC-8 competition."
            },
            "slug": "The-Structure-and-Performance-of-an-Open-Domain-Moldovan-Harabagiu",
            "title": {
                "fragments": [],
                "text": "The Structure and Performance of an Open-Domain Question Answering System"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU relies on a combination of syntactic and semantic techniques and a novel form of indexing called paragraph indexing to find answers."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084325087"
                        ],
                        "name": "E. Schmidt",
                        "slug": "E.-Schmidt",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "SEXTANT uses a lexical analyser and sentence splitter generated from a lex grammar (Lesk and Schmidt, 1975), reproduced in Grefenstette (1994, pp 149\u2013150)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": "The text in both corpora has been retokenized using a lex (Lesk and Schmidt, 1975) grammar extending on the tokenizer described in (Grefenstette, 1994, pp."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 101
                            }
                        ],
                        "text": "Weeds and Weir (2003) have used R ASP GRs for vector-space semantic similarity comparing the work of Lin (1998d) and Lee (1999) in terms of precision and recall."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7900881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9c4b8a9a40fdf274b1314a94f9a2ea1f6b7645",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Lex helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. Lex source is a table of regular expressions and corresponding program fragments. The table is translated to a program which reads an input stream, copying it to an output stream and partitioning the input into strings which match the given expressions. As each such string is recognized the corresponding program fragment is executed. The recognition of the expressions is performed by a deterministic finite automaton generated by Lex. The program fragments written by the user are executed in the order in which the corresponding regular expressions occur in the input stream. The lexical analysis programs written with Lex accept ambiguous specifications and choose the longest match possible at each input point. If necessary, substantial lookahead is performed on the input, but the input stream will be backed up to the end of the current partition, so that the user has general freedom to manipulate it. Lex can generate analyzers in either C or Ratfor, a language which can be translated automatically to portable Fortran. It is available on the PDP-11 UNIX, Honeywell GCOS, and IBM OS systems. This manual, however, will only discuss generating analyzers in C on the UNIX system, which is the only supported form of Lex under UNIX Version 7. Lex is designed to simplify interfacing with Yacc, for those with access to this compiler-compiler system."
            },
            "slug": "Lex\u2014a-lexical-analyzer-generator-Lesk-Schmidt",
            "title": {
                "fragments": [],
                "text": "Lex\u2014a lexical analyzer generator"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This manual will only discuss generating analyzers in C on the UNIX system, which is the only supported form of Lex under UNIX Version 7.0, and is designed to simplify interfacing with Yacc, for those with access to this compiler-compiler system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34730654"
                        ],
                        "name": "J. Caroll",
                        "slug": "J.-Caroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Caroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Caroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143661868"
                        ],
                        "name": "A. Sanfilippo",
                        "slug": "A.-Sanfilippo",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Sanfilippo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanfilippo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7042755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bfc525e48e3dab1087b7308da3ba462d6fc382c",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a critical overview of the state-of-the-art in parser evaluation methodologies and metrics. A discussion of their relative strengths and weaknesses motivates a new\u2014and we claim more informative and generally applicable\u2014technique of measuring parser accuracy, based on the use of grammatical relations. We conclude with some preliminary results of experiments in which we use this new scheme to evaluate a robust parser of English."
            },
            "slug": "Parser-evaluation:-a-survey-and-a-new-proposal-Caroll-Briscoe",
            "title": {
                "fragments": [],
                "text": "Parser evaluation: a survey and a new proposal"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new\u2014and reportedly more informative and generally applicable\u2014technique of measuring parser accuracy, based on the use of grammatical relations, is claimed to be applicable to parser accuracy evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3264129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8682725f2968836a5cd933eed4fdce80b0833bbc",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of machine learning algorithms can be improved by combining the output of different systems. In this paper we apply this idea to the recognition of noun phrases. We generate different classifiers by using different representations of the data. By combining the results with voting techniques described in (Van Halteren et al., 1998) we manage to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases."
            },
            "slug": "Noun-Phrase-Recognition-by-System-Combination-Sang",
            "title": {
                "fragments": [],
                "text": "Noun Phrase Recognition by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper generates different classifiers by using different representations of the data for noun phrases recognition to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15987897,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b848263f688c838790c0f3304ae52c7110b332ba",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Xenon, a natural language generation system capable of distinguishing between nearsynonyms. It integrates a near-synonym choice module with an existing sentence realization module. We evaluate Xenon using English and French nearsynonyms."
            },
            "slug": "Near-synonym-choice-in-natural-language-generation-Inkpen-Hirst",
            "title": {
                "fragments": [],
                "text": "Near-synonym choice in natural language generation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Xenon, a natural language generation system capable of distinguishing between nearsynonyms, is presented, which integrates a near-synonym choice module with an existing sentence realization module."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145148360"
                        ],
                        "name": "L. Hirschman",
                        "slug": "L.-Hirschman",
                        "structuredName": {
                            "firstName": "Lynette",
                            "lastName": "Hirschman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hirschman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409747275"
                        ],
                        "name": "Jong C. Park",
                        "slug": "Jong-C.-Park",
                        "structuredName": {
                            "firstName": "Jong C.",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jong C. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143670300"
                        ],
                        "name": "L. Wong",
                        "slug": "L.-Wong",
                        "structuredName": {
                            "firstName": "Limsoon",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744726"
                        ],
                        "name": "Cathy H. Wu",
                        "slug": "Cathy-H.-Wu",
                        "structuredName": {
                            "firstName": "Cathy",
                            "lastName": "Wu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cathy H. Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 125
                            }
                        ],
                        "text": "Already NLP is considered crucial for exploiting textual information in expanding scientific domains such as bioinformatics (Hirschman et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8648890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e875652115fcd5dc9ec57bfcdffb99baf42773fb",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We review recent results in literature data mining for biology and discuss the need and the steps for a challenge evaluation for this field. Literature data mining has progressed from simple recognition of terms to extraction of interaction relationships from complex sentences, and has broadened from recognition of protein interactions to a range of problems such as improving homology search, identifying cellular location, and so on. To encourage participation and accelerate progress in this expanding field, we propose creating challenge evaluations, and we describe two specific applications in this context."
            },
            "slug": "Accomplishments-and-challenges-in-literature-data-Hirschman-Park",
            "title": {
                "fragments": [],
                "text": "Accomplishments and challenges in literature data mining for biology"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "To encourage participation and accelerate progress in this expanding field of literature data mining, it is proposed creating challenge evaluations, and two specific applications are described in this context."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612932"
                        ],
                        "name": "Greg A. Keim",
                        "slug": "Greg-A.-Keim",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Keim",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg A. Keim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7427980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0c356faa0059166ed07b7c6d1c9c9a028008558",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-probabilistic-approach-to-solving-crossword-Littman-Keim",
            "title": {
                "fragments": [],
                "text": "A probabilistic approach to solving crossword puzzles"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6106375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b8e5d7da8441f0d44dac7ac3b87050d653bfa1a",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an efficient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries."
            },
            "slug": "PRINCIPAR-An-Efficient,-Broad-coverage,-Parser-Lin",
            "title": {
                "fragments": [],
                "text": "PRINCIPAR - An Efficient, Broad-coverage, Principle-based Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An efficient, broad-coverage, principle-based parser for English that contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876168"
                        ],
                        "name": "A. Lascarides",
                        "slug": "A.-Lascarides",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Lascarides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lascarides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bannard et al. (2003) elicit judgements for determining whether verbparticle expressions are non-compositional."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2356182,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7feb6ba5666a5b106c5c141c4356587164d15614",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off). We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions."
            },
            "slug": "A-Statistical-Approach-to-the-Semantics-of-Bannard-Baldwin",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to the Semantics of Verb-Particles"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A distributional approach to the semantics of verb-particle constructions (e.g. put up, make off) is described and some techniques for using statistical models acquired from corpus data to infer the meaning of verbs are reported on."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1995, 1994), word sense disambiguation (Dagan et al., 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative to the JS-divergence is to only add a weighted amount of the second distribution to the first, which leads to the \u03b1-skew divergence (Lee, 1999):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Other work, such as (Lee, 1999), only considers the 1000 most frequent common nouns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6305097,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6f3250ba47fdb413a0c113cc16d274517864f8ab",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "slug": "Measures-of-Distributional-Similarity-Lee",
            "title": {
                "fragments": [],
                "text": "Measures of Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784037"
                        ],
                        "name": "T. Brants",
                        "slug": "T.-Brants",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brants"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Brill and Wu (1998) call this complementary disagreement complementarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The only similar performing tool is the Trigrams \u2018n\u2019 Tags tagger (Brants, 2000) which uses a much simpler statistical model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1452591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d560a8d279075a529e9cadb0d664b27957aac5a2",
            "isKey": false,
            "numCitedBy": 1905,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora."
            },
            "slug": "TnT-A-Statistical-Part-of-Speech-Tagger-Brants",
            "title": {
                "fragments": [],
                "text": "TnT - A Statistical Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Contrary to claims found elsewhere in the literature, it is argued that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739260"
                        ],
                        "name": "A. Strehl",
                        "slug": "A.-Strehl",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Strehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Strehl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724702"
                        ],
                        "name": "Joydeep Ghosh",
                        "slug": "Joydeep-Ghosh",
                        "structuredName": {
                            "firstName": "Joydeep",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joydeep Ghosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122918146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1c7f0ae558070ae7b2affcb4906206b13eb84d",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 163,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation takes a relationship-based approach to cluster analysis of high (1000 and more) dimensional data that side-steps the \u2018curse of dimensionality\u2019 issue by working in a suitable similarity space instead of the original feature space. We propose two frameworks that leverage graph algorithms to achieve relationship-based clustering and visualization, respectively. In the visualization framework, the output from the clustering algorithm is used to reorder the data points so that the resulting permuted similarity matrix can be readily visualized in 2 dimensions, with clusters showing up as bands. Results on retail transaction, document (bag-of-words), and web-log data show that our approach can yield superior results while also taking additional balance constraints into account. \nThe choice of similarity is a critical step in relationship-based clustering and this motivates our systematic comparative study of the impact of similarity measures on the quality of document clusters . The key findings of our experimental study are: (i)\u00a0Cosine, correlation, and extended Jaccard similarities perform comparably; (ii)\u00a0Euclidean distances do not work well; (iii)\u00a0graph partitioning tends to be superior to k-means and SOMs especially when balanced clusters are desired; and (iv)\u00a0performance curves generally do not cross. We also propose a cluster quality evaluation measure based on normalized mutual information and find an analytical relation between similarity measures. \nIt is widely recognized that combining multiple classification or regression models typically provides superior results compared to using a single, well-tuned model. However, there are no well known approaches to combining multiple clusterings. The idea of combining cluster labelings without accessing the original features leads to a general knowledge reuse framework that we call cluster ensembles. We propose a formal definition of the cluster ensemble as an optimization problem. Taking a relationship-based approach we propose three effective and efficient combining algorithms for solving it heuristically based on a hypergraph model. Results on synthetic as well as real data-sets show that cluster ensembles can (i)\u00a0improve quality and robustness, and (ii)\u00a0enable distributed clustering, and (iii)\u00a0speed up processing significantly with little loss in quality."
            },
            "slug": "Relationship-based-clustering-and-cluster-ensembles-Strehl-Ghosh",
            "title": {
                "fragments": [],
                "text": "Relationship-based clustering and cluster ensembles for high-dimensional data mining"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This dissertation takes a relationship-based approach to cluster analysis of high (1000 and more) dimensional data that side-steps the \u2018curse of dimensionality\u2019 issue by working in a suitable similarity space instead of the original feature space, and proposes two frameworks that leverage graph algorithms to achieve relationship- based clustering and visualization, respectively."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5659557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0c3033ea7d4e19e1f5ac71934759507e126162",
            "isKey": false,
            "numCitedBy": 4466,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains."
            },
            "slug": "An-Information-Theoretic-Definition-of-Similarity-Lin",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Definition of Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work presents an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model and demonstrates how this definition can be used to measure the similarity in a number of different domains."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CASS has been used in various NLP tasks including vector-space similarity for word sense disambiguation (Lee and Pereira, 1999), induction of selectional preferences (Abney and Light, 1999) and modelling lexical-semantic relations (Lapata, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15127206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7237ce8b28dfc7e7112aa70bdf73f8e15fc297f",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributional similarity is a useful notion in estimating the probabilities of rare joint events. It has been employed both to cluster events according to their distributions, and to directly compute averages of estimates for distributional neighbors of a target event. Here, we examine the tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events."
            },
            "slug": "Distributional-Similarity-Models:-Clustering-vs.-Lee",
            "title": {
                "fragments": [],
                "text": "Distributional Similarity Models: Clustering vs. Nearest Neighbors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events are examined."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lin (1999) has used similarity measures to determine if relationships between words are idiomatic or non-compositional and Baldwin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 94
                            }
                        ],
                        "text": "I describe the functions evaluated in this chapter using an extension of the notation used by Lin (1998a), where an asterisk indicates a set of values ranging over all existing values of that component of the relation tuple."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001a) use a similarity measure to identify synonymous paths in dependency trees, by extension of the word similarity calculations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "It extends notation introduced by Lin (1998a) to conveniently describe the measure and weight functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 93
                            }
                        ],
                        "text": "The large-scale corpusconsists of theBNC, RCV1, and much of the English news holdings of the Linguistic Data Consortium ( LDC). It contains over 2 billion words of text, part of which was collected and processed for Curran and Osborne (2002). It is used in the large-scale experiments and detailed evaluation in Chapter 6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 0
                            }
                        ],
                        "text": "Lin (1999) has used similarity measures to determine if relationships between words are idiomatic or non-compositional and Baldwin et al. (2003) and Bannard et al. (2003) have used similar techniques to determine whether particle-verb constructions are non-compositional."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 11
                            }
                        ],
                        "text": "Pantel and Lin (2002a) incorporate a relation frequency-based correction function that can also be considered as a local weight."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 11
                            }
                        ],
                        "text": "Pantel and Lin (2002a) have developed a method of identifying new word senses using an efficient similarity-based clustering algorithm designed for document clustering (Pantel and Lin, 2002b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9541345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d431d03433275a68bd4ccd6b97af665bb979294d",
            "isKey": true,
            "numCitedBy": 202,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sentences from (van Riemsdijk and Williams, 1986)."
            },
            "slug": "Principle-Based-Parsing-without-Overgeneration-Lin",
            "title": {
                "fragments": [],
                "text": "Principle-Based Parsing without Overgeneration"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A message passing algorithm for principle-based parsing that avoids the overgeneration problem is presented and has been implemented in C++ and successfully tested with example sentences from (van Riemsdijk and Williams, 1986)."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992275"
                        ],
                        "name": "Maria Lapata",
                        "slug": "Maria-Lapata",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8858671,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "cfcad6c2cea38c275a3eb9f7551a65351d65d966",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 158,
            "paperAbstract": {
                "fragments": [],
                "text": "A vehicle propulsion system including as elements thereof-A pressurized work fluid generator- B a primary engine unit receiving driving fluid from the fluid generator and producing propulsion for a vehicle by division of the work energy, thereby producing kinetic energy driving road-wheel means as one propulsion effect and simultaneously producing propulsion effect by a supplementary propulsion system."
            },
            "slug": "Acquisition-and-modeling-of-lexical-knowledge:-a-of-Lapata",
            "title": {
                "fragments": [],
                "text": "Acquisition and modeling of lexical knowledge: a corpus-based investigation of systematic polysemy"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A vehicle propulsion system producing kinetic energy driving road-wheel means as one propulsion effect and simultaneously producing propulsion effect by a supplementary propulsion system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In IR indexing and querying collections with controlled vocabularies, and query expansion using structured thesauri or extracted similar terms have proved successful (Salton and McGill, 1983; van Rijsbergen, 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57116029,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f7a8d621e2531c20252a01a9428a1157466ebf64",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Text Retrreval Background, Concept Matching, Query Expansion, Conclusion, References"
            },
            "slug": "Using-Wordnet-for-Text-Retrieval-Fellbaum-Miller",
            "title": {
                "fragments": [],
                "text": "Using Wordnet for Text Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, Text Retrreval Background, Concept Matching, Query Expansion, Conclusion, References."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844204"
                        ],
                        "name": "William H. Mischo",
                        "slug": "William-H.-Mischo",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Mischo",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William H. Mischo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60599988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4eec8838d0e45b1279dee3c9418fb5d56bf1b29a",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The shortcomings and limitations of Library of Congress Subject Headings (LCSH) are examined, focusing on problems of subject term specificity, inconsistent identification and selection of concepts as subject headings, retention of outmoded headings, inadequate cross-reference structure, and low level of indexing exhaustivity. These problems are attributable to both Cutter's Rules for a Dictionary Catalog (the theoretical model upon which LCSH are based), and the unsystematic manner in which LC has applied Cutter's Rules in constructing entries for its subject catalog. Methods of improving subject access in libraries, for both on-line and printed catalog environments, are discussed."
            },
            "slug": "Library-of-congress-subject-headings:-A-review-of-Mischo",
            "title": {
                "fragments": [],
                "text": "Library of congress subject headings: A review of the problems, and prospects for improved subject access"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The shortcomings and limitations of Library of Congress Subject Headings (LCSH) are examined, focusing on problems of subject term specificity, inconsistent identification and selection of concepts as subject headings, retention of outmodedHeadings, inadequate cross-reference structure, and low level of indexing exhaustivity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69550117"
                        ],
                        "name": "H. Morton",
                        "slug": "H.-Morton",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Morton",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Morton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This often results in the inclusion of other materials and difficulties with censorship and trademarks (Landau, 1989; Morton, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 153528628,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9fcda1f58b178665780bc052c06d38baabbe052b",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The best of times and the worst: a prologue Part I. Philip Gove and the Genesis of Webster's Third: 2. Gove's formative years: the road to Springfield 3. The Webster and Merriam tradition 4. The new editor takes hold Part II. The Making of the Dictionary: Gove's Intentions: 5. The meaning of words: definers at work 6. The origins of words: the etymologist's task 7. The sound of words and other matters 8. Usage and final tasks Part III. The War of Words: 9. Early Returns: the fuse is lit 10. The controversy heats up 11. 1962: calamity or calumny? 12. Commercial intrusions: trademarks, takeover threats, competition 13. Ideology and politics in the running debate 14. The judgment of peers Part IV. Sorting it All Out: 15. Gove and Webster's Third: the legacy 16. Concluding words."
            },
            "slug": "The-Story-of-Webster's-Third:-Philip-Gove's-and-its-Morton",
            "title": {
                "fragments": [],
                "text": "The Story of Webster's Third: Philip Gove's Controversial Dictionary and its Critics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1% (Abney, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1127776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed7dfac6641dc4bbd1ff4d6abedceae57d6ddf4",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Finite state cascades represent an attractive architecture for parsing unrestricted text. Deterministic parsers specified by finite state cascades are fast and reliable. They can be extended at modest cost to construct parse trees with finite feature structures. Finally, such deterministic parsers do not necessarily involve trading off accuracy against speed \u2014 they may in fact be more accurate than exhaustive search stochastic context free parsers."
            },
            "slug": "Partial-parsing-via-finite-state-cascades-Abney",
            "title": {
                "fragments": [],
                "text": "Partial parsing via finite-state cascades"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Deterministic parsers specified by finite state cascades may be more accurate than exhaustive search stochastic context free parsers and extended at modest cost to construct parse trees with finite feature structures."
            },
            "venue": {
                "fragments": [],
                "text": "Nat. Lang. Eng."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The supersense tagger implemented by Ciaramita and Johnson (2003) is a multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8729730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28b9bacde6499f8cc6f7e70feee4232107211e39",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study online classification algorithms for multiclass problems in the mistake bound model. The hypotheses we use maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and then sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultracon-servativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems."
            },
            "slug": "Ultraconservative-Online-Algorithms-for-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "Ultraconservative Online Algorithms for Multiclass Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper studies online classification algorithms for multiclass problems in the mistake bound model and introduces the notion of ultracon-servativeness, a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": ", 1994), so the remaining components are designed to handle the Penn POStag set (Santorini, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18146635,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "An enclosed-type magnetic disc recording and/or reproducing apparatus according to this invention, having a pressure chamber to enclose a bearing unit of a flange rotating along with a magnetic disc, introduces an air pressure at a high-pressure portion, created by an air flow produced by rotation of the magnetic disc and the flange, into the pressure chamber so as to raise the pressure within the pressure chamber to a level higher than 1 atm., thereby preventing the lubricant evaporated from the bearing member from sticking to the magnetic disc or head to lower the faculty thereof."
            },
            "slug": "Part-of-speech-tagging-guidelines-for-the-penn-Santorini",
            "title": {
                "fragments": [],
                "text": "Part-of-speech tagging guidelines for the penn treebank project"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49915195"
                        ],
                        "name": "B. Verghese",
                        "slug": "B.-Verghese",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Verghese",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Verghese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 56612335,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f72592a8a963ed8c168961fafd4b361f5db5a87a",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Thesaurus of English words and phrases , Thesaurus of English words and phrases , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Thesaurus-of-English-Words-and-Phrases-Verghese",
            "title": {
                "fragments": [],
                "text": "Thesaurus of English Words and Phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Thesaurus of English words and phrases is a collection ofverbs, idioms and phrases used in everyday usage to describe English phrases and words."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42794,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35765580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d94832e245906775b428e949ac1f635bfb28ad3",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "PREFACE 1. OVERVIEW 2. TEXT COMPRESSION 3. INDEXING 4. QUERYING 5. INDEX CONSTRUCTION 6. IMAGE COMPRESSION 7. TEXTUAL IMAGES 8. MIXED TEXT AND IMAGES 9. IMPLEMENTATION 10. THE INFORMATION EXPLOSION A. GUIDE TO THE MG SYSTEM B. GUIDE TO THE NZDL REFERENCES INDEX"
            },
            "slug": "Managing-Gigabytes:-Compressing-and-Indexing-and-Witten-Moffat",
            "title": {
                "fragments": [],
                "text": "Managing Gigabytes: Compressing and Indexing Documents and Images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A guide to the MG system and its applications, as well as a comparison to the NZDL reference index, are provided."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703559"
                        ],
                        "name": "W. Gropp",
                        "slug": "W.-Gropp",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gropp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gropp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695428"
                        ],
                        "name": "E. Lusk",
                        "slug": "E.-Lusk",
                        "structuredName": {
                            "firstName": "Ewing",
                            "lastName": "Lusk",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lusk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32834137"
                        ],
                        "name": "N. Doss",
                        "slug": "N.-Doss",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Doss",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Doss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734586"
                        ],
                        "name": "A. Skjellum",
                        "slug": "A.-Skjellum",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Skjellum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skjellum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Message Passing Interface MPI library (Gropp et al., 1996) is the emerging standard for implementing message-passing algorithms portably in high performance computing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 304801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61608bde8d502dbac561b9de1f36e676491b0b30",
            "isKey": false,
            "numCitedBy": 2401,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-High-Performance,-Portable-Implementation-of-the-Gropp-Lusk",
            "title": {
                "fragments": [],
                "text": "A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608024"
                        ],
                        "name": "W. Vetterling",
                        "slug": "W.-Vetterling",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vetterling",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vetterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To measure the complementary disagreement between individual ensemble members, a and b, I have calculated both Brill and Wu\u2019s complementarity C and the Spearman rank-order correlation Rs (Press et al., 1992) to compare their output:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61769312,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ca2832d2c30287a9ee5b8584cc498d2b1cb14753",
            "isKey": false,
            "numCitedBy": 16689,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Numerical-recipes-in-C-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215185"
                        ],
                        "name": "G. Sampson",
                        "slug": "G.-Sampson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sampson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "5% on a manually annotated 500 sentence subset of the SUSANNE corpus (Sampson, 1995) using the grammatical relation-based evaluation proposed by Carroll et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In an evaluation on the SUSANNE corpus (Sampson, 1995) MINIPAR achieves about 88% precision and 80% recall on dependency relationships (Lin, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60905589,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "d2de222d0e627eeb13419eff3077da43b9932763",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Zirconium soaps are prepared by reacting precipitated zirconium basic sulphate and one or more organic monocarboxylic acids in the presence of at least one alkali metal hydroxide or carbonate or alkaline earth metal oxides and/or hydroxide in an amount stoichiometrically equivalent to the sulphate content of the zirconium basic sulphate. Acids mentioned are 2-ethyl hexoic, caprylic, nonylic, lauric, linoleic, stearic, palmitic, abietic, oleic, ricinoleic, naphthenic, phenyl-acetic, butoxy- and phenoxyacetic, and butyl- and dodecyl-thiopropionic acids and examples describe the preparation of zirconium naphthenate and 2-ethyl hexoic acid esters."
            },
            "slug": "English-for-the-computer-Sampson",
            "title": {
                "fragments": [],
                "text": "English for the computer"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Zirconium soaps are prepared by reacting precipitated zirconia basic sulphate and one or more organic monocarboxylic acids in the presence of at least one alkali metal hydroxides or carbonate or alkaline earth metal oxides and/or hydroxide."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50718271"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 60820748,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "965ea5946708f0b669403a377c48e78e9326be61",
            "isKey": false,
            "numCitedBy": 8264,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count, and a spike pulse of greater selected amplitude is transmitted, occurring immediately after the axle count pulse to which it relates, whenever an overheated axle box is detected. To enable the signal receiving equipment to determine on which side of a train the overheated box is located, the spike pulses are of two different amplitudes corresponding, respectively, to opposite sides of the train."
            },
            "slug": "Finding-Groups-in-Data:-An-Introduction-to-Cluster-Kaufman-Rousseeuw",
            "title": {
                "fragments": [],
                "text": "Finding Groups in Data: An Introduction to Cluster Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114433446"
                        ],
                        "name": "D. Emblen",
                        "slug": "D.-Emblen",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Emblen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Emblen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "The English thesaurus has been a popular arbiter of similarity for 150 years (Davidson, 2002), and is strongly associated with the work of Peter Mark Roget (Emblen, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 167
                            }
                        ],
                        "text": "The biggest sales boost for the thesaurus was the overwhelming popularity of crossword puzzles which began with their regular publication in theN w York Worldin 1913 (Emblen, 1970, page 278)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "These were often targeted at \u201ccoming up members of society and to eligible foreigners, whose inadequate grasp of the nuances of English synonymies might lead them to embarrassing situations\u201d (Emblen, 1970, page 263)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 191301931,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "1d36fd2e3e66abcb7046119f2c06427c34ca9031",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A biography of Roget, including a description of Roget's wheel (pp.184-6), and of his work on the kaleidoscope (p.121. Illustrated with portraits and reproductions of documents and paintings."
            },
            "slug": "Peter-Mark-Roget:-The-Word-and-the-Man-Emblen",
            "title": {
                "fragments": [],
                "text": "Peter Mark Roget: The Word and the Man"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 211
                            }
                        ],
                        "text": "The detailed evaluation uses a larger set of 300 nouns, covering several frequency bands, based on counts from thePTB, BNC, the Brown Corpus, and 100 million words of New York Times text from theACQUAINT Corpus (Graff, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 600,
                                "start": 594
                            }
                        ],
                        "text": ".53\n3.4.1 Lexical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\n3.4.2 Part of Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . .53\n3.4.3 Phrase Chunking . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\n3.4.4 Morphological Analysis . . . . . . . . . . . . . . . . . . . . . . . . .54\n3.4.5 Grammatical Relation Extraction . . . . . . . . . . . . . . . . . . . .55\n3.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\n3.5.1 Context Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\n3.5.2 Corpus Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": ".59\n3.5.3 Corpus Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63\n3.5.4 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 296
                            }
                        ],
                        "text": ".102\n6.1 Example nouns and their supersenses . . . . . . . . . . . . . . . . . . . . . . .119\nB.1 Roget\u2019s Thesaurus Davidson (2002) entry forc mpany . . . . . . . . . . . . .137\nxvi\nList of Tables\n1.1 Example near-synonym differentia from DiMarco et al. (1993) . . . . . . . . .4\n3.1 Experimental Corpustatistics . . . . . . . . . . . . . . . . . . . . . . . . . .44\n3.2 Large-Scale Corpustatistics . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 135
                            }
                        ],
                        "text": ", 1995); North American News Text Corpus ( NANTC, Graff, 1995); theNANTC supplement ( NANTS, MacIntyre, 1998); and the ACQUAINT Corpus (Graff, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 22
                            }
                        ],
                        "text": ".44\n3.2.2 Large-Scale Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . .45\n3.3 Existing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\n3.3.1 Window Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 113
                            }
                        ],
                        "text": ".43\n3.2 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43\n3.2.1 Experimental Corpus . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TheAQUAINT corpus of English news text"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report LDC2002T31, Linguistic Data Consortium, Philadelphia, PA USA,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 298
                            }
                        ],
                        "text": "From Distributional to Semantic Similarity\nJames Richard Curran\nT H\nE\nU N I V E R\nS\nI T\nY\nO F\nE D I N B U\nR G\nH\nDoctor of Philosophy\nInstitute for Communicating and Collaborative Systems\nSchool of Informatics\nUniversity of Edinburgh\n2003\nAbstract\nLexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 731,
                                "start": 724
                            }
                        ],
                        "text": ".94\n5.5 Simple ensembles perform worse than best individuals . . . . . . . . . . . . .95\n5.6 Relation statistics over the large-scale corpus . . . . . . . . . . . . . . . . . .103\n5.7 Results from the 2 billion word corpus on the 70 experimental word set . . . .105\n6.1 Performance on the 300 word evaluation set . . . . . . . . . . . . . . . . . . .110\n6.2 Performance compared with relative frequency of the headword . . . . . . . .111\n6.3 Performance compared with the number of extracted attributes . . . . . . . . .111\n6.4 Performance compared with the number of extracted contexts . . . . . . . . . .112\n6.5 Performance compared with polysemy of the headword . . . . . . . . . . . . .112\n6.6 Performance compared with WORDNET root(s) of the headword . . . . . . . .113\n6.7 Lexical-semantic relations from WORDNET for the synonyms ofcompany ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 72
                            }
                        ],
                        "text": "Finally, I apply my similarity metric to the task of assigning words to WORDNET semantic categories."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "WORDNET has been consistently criticised for making sense distinctions that are too fine-grained, many of which are very difficult for non-experts to distinguish between."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 601,
                                "start": 594
                            }
                        ],
                        "text": "1.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\n1.2 Lexical Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3\n1.2.1 Synonymy and Hyponymy . . . . . . . . . . . . . . . . . . . . . . . .3\n1.2.2 Polysemy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n1.3 Lexical Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n1.3.1 Roget\u2019s Thesaurus . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n1.3.2 Controlled vocabularies . . . . . . . . . . . . . . . . . . . . . . . . .7\n1.3.3 WORDNET . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 42
                            }
                        ],
                        "text": "Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "There have been several computational attempts to reduce the number of sense distinctions and increase the size of each synset in WORDNET (Buitelaar, 1998; Ciaramita et al., 2003; Hearst and Sch\u0308utze, 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 33
                            }
                        ],
                        "text": "Sense distinctions in Roget\u2019s and WORDNET are made by placing words into different"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 62
                            }
                        ],
                        "text": "I also perform a detailed analysis of the final results using WORDNET."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 45
                            }
                        ],
                        "text": ".114\n6.9 25 lexicographer files for nouns in WORDNET 1.7.1 . . . . . . . . . . . . . . .115\n6.10 Hand-coded rules for supersense guessing . . . . . . . . . . . . . . . . . . . .120\nA.1 300 headword evaluation set . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 24
                            }
                        ],
                        "text": "Another example is that WORDNET does not distinguish types from instances in the noun hierarchy: bothepistemologist andSocrates appear as hyponyms ofphilosopher, so in practice we cannot make this distinction using WORDNET."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complementing WordNet with Roget and corpus-based automatically constructed thesauri for information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "e\u0301al, Qu\u0301ebec, Canada,"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746503"
                        ],
                        "name": "A. Kilgarriff",
                        "slug": "A.-Kilgarriff",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Kilgarriff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kilgarriff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993) use mutual information for estimating cooccurrence probabilities. Luk (1995) uses mutual information to score cooccurrences in definition concepts as part of a word sense disambiguation system. Turney (2001) uses mutual information with cooccurrence probabilities for selecting the correct word in vocabulary tests."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993, 1995), Dagan et al. (1999) and Lee (1999) have shown that using the distributionally nearest-neighbours improves language modelling and WSD."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 41
                            }
                        ],
                        "text": ", 1995, 1994), word sense disambiguation (Dagan et al., 1997; Lee, 1999), information retrieval (Grefenstette, 1994) and malapropism detection (Budanitsky, 1999; Budanitsky and Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993) use mutual information for estimating cooccurrence probabilities. Luk (1995) uses mutual information to score cooccurrences in definition concepts as part of a word sense disambiguation system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993) use mutual information for estimating cooccurrence probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993, 1995), Dagan et al. (1999) and Lee (1999) have shown that using the distributionally nearest-neighbours improves language modelling and WSD. Lee and Pereira (1999) compare the performance of clustering and nearest-neighbour approaches. Baker and McCallum (1998) apply the distributional clustering technique to document classification because it allows for a very high degree of dimensionality reduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993) use a form of Jaccard which separates the left and right contexts which is equivalent to using a window extractor with the relation type equal to left or right."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 439,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993, 1995), Dagan et al. (1999) and Lee (1999) have shown that using the distributionally nearest-neighbours improves language modelling and WSD. Lee and Pereira (1999) compare the performance of clustering and nearest-neighbour approaches. Baker and McCallum (1998) apply the distributional clustering technique to document classification because it allows for a very high degree of dimensionality reduction. Lapata (2000) has used distributional similarity smoothing in the interpretation of nominalizations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 0
                            }
                        ],
                        "text": "Dagan et al. (1993, 1995), Dagan et al. (1999) and Lee (1999) have shown that using the distributionally nearest-neighbours improves language modelling and WSD. Lee and Pereira (1999) compare the performance of clustering and nearest-neighbour approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63433341,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "319af0958e268b3243975b8628262ebc1980ce40",
            "isKey": true,
            "numCitedBy": 264,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "of-the-European-Chapter-of-the-Association-for-Kilgarriff-Baroni",
            "title": {
                "fragments": [],
                "text": "of the European Chapter of the Association for Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 465,
                                "start": 23
                            }
                        ],
                        "text": "Global query expansion may involve adding synonyms, cooccurring terms from the text, or variants formed by stemming and morphological analysis (Baeza-Yates and Ribeiro-Neto, 1999). Previously this has involved the use of controlled vocabularies, regular thesauri such as Roget\u2019s, and also more recent work with W ORDNET. Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 59
                            }
                        ],
                        "text": "A practical problem with larger window models is that they may become too large to manipulate. For instance, in work on dimensionality reduction for vector-space models Sch \u00fctze (1992a,b) uses a window of 100 words either side, but only considers the 1000 most frequent terms within the window. This fixes the context matrix to have rows of length 1000. Landauer and Dumais (1997) use a similar technique with Latent Semantic Indexing (Deerwester et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 589,
                                "start": 23
                            }
                        ],
                        "text": "Global query expansion may involve adding synonyms, cooccurring terms from the text, or variants formed by stemming and morphological analysis (Baeza-Yates and Ribeiro-Neto, 1999). Previously this has involved the use of controlled vocabularies, regular thesauri such as Roget\u2019s, and also more recent work with W ORDNET. Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses. Grefenstette (1994) found query expansion with automatically extracted synonyms beneficial, as did Jing and Tzoukermann (1999) when they combined extracted synonyms with morphological information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 641,
                                "start": 37
                            }
                        ],
                        "text": "There are also other properties that may be important depending on the application; for instance, whether the similarity function is symmetric, sim(a,b) \u2261 sim(b,a), and whether it satisfies thetriangle inequality, sim(a,b)+sim(b,c) \u2265 sim(a,c). These properties are important for clustering and search applications which sometimes rely on these assumptions for their correctness. For my evaluation methodology, which only relies on ranking, such properties are not important; whether the function calculates similarity or dissimilarity simply just changes whether ranking must be in ascending or descending order. Other work, e.g. Lee (2001), has used a negative exponential to convert distance measures into similarity measures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 786,
                                "start": 23
                            }
                        ],
                        "text": "Global query expansion may involve adding synonyms, cooccurring terms from the text, or variants formed by stemming and morphological analysis (Baeza-Yates and Ribeiro-Neto, 1999). Previously this has involved the use of controlled vocabularies, regular thesauri such as Roget\u2019s, and also more recent work with W ORDNET. Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses. Grefenstette (1994) found query expansion with automatically extracted synonyms beneficial, as did Jing and Tzoukermann (1999) when they combined extracted synonyms with morphological information. Xu and Croft (1998) attempt another similarity/morphology combination by filtering stemmer variations using mutual information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 910,
                                "start": 23
                            }
                        ],
                        "text": "Global query expansion may involve adding synonyms, cooccurring terms from the text, or variants formed by stemming and morphological analysis (Baeza-Yates and Ribeiro-Neto, 1999). Previously this has involved the use of controlled vocabularies, regular thesauri such as Roget\u2019s, and also more recent work with W ORDNET. Query expansion using Roget\u2019s and WORDNET (Mandala et al., 1998; Voorhees, 1998) has not been particularly successful, although Voorhees (1998) did see an improvement when the query terms were manually disambiguated with respect to WORDNET senses. Grefenstette (1994) found query expansion with automatically extracted synonyms beneficial, as did Jing and Tzoukermann (1999) when they combined extracted synonyms with morphological information. Xu and Croft (1998) attempt another similarity/morphology combination by filtering stemmer variations using mutual information. Voorhees (1998) also attempts word sense disambiguation using W ORDNET, while Sc\u1e27utze and Pedersen (1995) use an approach based on extracted synonyms and see a significant improvement in performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eric Brill and Jun Wu"
            },
            "venue": {
                "fragments": [],
                "text": "Classifier combination for improved lexical disambiguation. In"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116658576"
                        ],
                        "name": "David A. Evans",
                        "slug": "David-A.-Evans",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Evans",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Evans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 80
                            }
                        ],
                        "text": "Grefenstette (1994) assigns a set of possible POStags from theCLARIT dictionary (Evans et al., 1991) which occurs as part of morphological normalisation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": "Grefenstette (1994) uses the CLARIT morphological normaliser (Evans et al., 1991) before POS tagging."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 81
                            }
                        ],
                        "text": "Grefenstette (1994) assigns a set of possible POStags from theCLARIT dictionary (Evans et al., 1991) which occurs as part of morphological normalisation. The CMU POS tagger, a trigram tagger based on de Marcken (1990) and trained on the Brown Corpus (Francis and Kucera, 1982), is used to disambiguate the set of POStags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5848177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f982ac20179602817712c3f4c6ea05575e523d9c",
            "isKey": true,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Summary-of-the-CLARIT-project-Evans",
            "title": {
                "fragments": [],
                "text": "A Summary of the CLARIT project"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73170469"
                        ],
                        "name": "L. Zgusta",
                        "slug": "L.-Zgusta",
                        "structuredName": {
                            "firstName": "Ladislav",
                            "lastName": "Zgusta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zgusta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Zgusta (1971) defines absolute synonymy as agreement indes gnatum, the essential properties that define a concept;connotation, the associated features of a concept; andrange of application, the contexts in which the word may be used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143759212,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "16d74bca54ed0941198de5a2650cd3fbfd3f6fe5",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Manual-of-Lexicography-Zgusta",
            "title": {
                "fragments": [],
                "text": "Manual of Lexicography"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69446506"
                        ],
                        "name": "Anne H. Soukhanov",
                        "slug": "Anne-H.-Soukhanov",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Soukhanov",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anne H. Soukhanov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141816282,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ead8543c17f745f599c569c801bc27fd873790c4",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Roget's-II-:-the-new-thesaurus-Soukhanov",
            "title": {
                "fragments": [],
                "text": "Roget's II : the new thesaurus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48897539"
                        ],
                        "name": "P. J. Huber",
                        "slug": "P.-J.-Huber",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Huber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Huber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122267035,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "854733ed15e1b373532a198947acd2616b6861d8",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wiley-Series-in-Probability-and-Mathematics-Huber",
            "title": {
                "fragments": [],
                "text": "Wiley Series in Probability and Mathematics Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 69740961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97ac3841601e82e900a32f3a1796faa2899b96b5",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Managing-gigabytes-Witten",
            "title": {
                "fragments": [],
                "text": "Managing gigabytes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50381258"
                        ],
                        "name": "G. Barnard",
                        "slug": "G.-Barnard",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Barnard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30021514"
                        ],
                        "name": "R. Fano",
                        "slug": "R.-Fano",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fano",
                            "middleNames": [
                                "Mario"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 67110416,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "6b6dc1167d8f6f44f230309e9c27e4268578f1f7",
            "isKey": false,
            "numCitedBy": 941,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Transmission-of-Information:-A-Statistical-Theory-Barnard-Fano",
            "title": {
                "fragments": [],
                "text": "Transmission of Information: A Statistical Theory of Communications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144684950"
                        ],
                        "name": "P. Srinivasan",
                        "slug": "P.-Srinivasan",
                        "structuredName": {
                            "firstName": "Padmini",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Srinivasan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64780713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b129152171a78a6c900d766628b100700bedc9b4",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Thesaurus-construction-Srinivasan",
            "title": {
                "fragments": [],
                "text": "Thesaurus construction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82323309"
                        ],
                        "name": "Karen Sparck Jones",
                        "slug": "Karen-Sparck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sparck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sparck Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62724133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ad7c895c1d6cf03992f2495ed72f530f47a8721",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-keyword-classification-for-information-Jones",
            "title": {
                "fragments": [],
                "text": "Automatic keyword classification for information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Apart from discriminating entries in popular works such as Fowler\u2019s A Dictionary of Modern English Usage (1926), their popularity has been limited except in advanced learner dictionaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 165920968,
            "fieldsOfStudy": [],
            "id": "f71dfc65b2175230080829d66c9bdd8e6cd4d9c7",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Dictionary of Modern English Usage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1926
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is because Zipf\u2019s law (Zipf, 1949) applies to headwords and their relations, and so small increments of the cutoff eliminate many headwords from the tail of the distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141120597,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac",
            "isKey": false,
            "numCitedBy": 7038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-behavior-and-the-principle-of-least-effort-Zipf",
            "title": {
                "fragments": [],
                "text": "Human behavior and the principle of least effort"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097577"
                        ],
                        "name": "W. Francis",
                        "slug": "W.-Francis",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Francis",
                            "middleNames": [
                                "Nelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Francis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16318570"
                        ],
                        "name": "H. Kucera",
                        "slug": "H.-Kucera",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kucera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kucera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053677731"
                        ],
                        "name": "Andrew Mackie",
                        "slug": "Andrew-Mackie",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mackie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Mackie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "6) presents results over a very wide range of corpora including: the standard Brown corpus (Francis and Kucera, 1982); HARVARD and SPORT corpora which consist of entries from extracted from Grolier\u2019s encyclopedia containing a hyponym of institution and sport from WORDNET; MED corpus of medical abstracts; and the MERGERS corpus of Wall Street Journal articles indexed with the merger keyword."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The CMU POS tagger, a trigram tagger based on de Marcken (1990) and trained on the Brown Corpus (Francis and Kucera, 1982), is used to disambiguate the set of POS tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60972899,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "729316fbded86763104f3412cadf98f00a9a3993",
            "isKey": false,
            "numCitedBy": 2159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "FREQUENCY-ANALYSIS-OF-ENGLISH-USAGE:-LEXICON-AND-Francis-Kucera",
            "title": {
                "fragments": [],
                "text": "FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59794025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc8dc8edd327d64a9c8b8857da6da5dfcc29bf01",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Compressing-and-indexing-documents-and-images-Witten-Moffat",
            "title": {
                "fragments": [],
                "text": "Compressing and indexing documents and images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143682244"
                        ],
                        "name": "I. Cruz",
                        "slug": "I.-Cruz",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Cruz",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cruz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 85438592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73223875dd4f0b49dd90ec77892823ed0c376317",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Emerging-Semantic-Web-Cruz",
            "title": {
                "fragments": [],
                "text": "The Emerging Semantic Web"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736799"
                        ],
                        "name": "M. Chodorow",
                        "slug": "M.-Chodorow",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Chodorow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chodorow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 107
                            }
                        ],
                        "text": "The simplest approaches involve computing the shortest number of links from one node in WORDNET to another (Leacock and Chodorow, 1998; Rada et al., 1989) using breadth-first search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59721988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54cd5ea7f987a23d663605640113859207490400",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-local-context-and-wordnet-similarity-for-Leacock-Chodorow",
            "title": {
                "fragments": [],
                "text": "Combining local context and wordnet similarity for word sense identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66307430"
                        ],
                        "name": "M. Anderberg",
                        "slug": "M.-Anderberg",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Anderberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anderberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58959986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1900e25f1783a501a4f35666b4717013a303dd2f",
            "isKey": false,
            "numCitedBy": 5281,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cluster-Analysis-for-Applications-Anderberg",
            "title": {
                "fragments": [],
                "text": "Cluster Analysis for Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50759487"
                        ],
                        "name": "A. Gilchrist",
                        "slug": "A.-Gilchrist",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Gilchrist",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gilchrist"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144631971"
                        ],
                        "name": "J. Aitchison",
                        "slug": "J.-Aitchison",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Aitchison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The manual development of a controlled vocabulary thesaurus, described in detail by Aitchison et al. (2002), tends to be undertaken by government bodies in the few domains where they are still maintained."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58219568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ef1eacd6ee1b3087e61fe2ade535a27a302da2a",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Thesaurus-construction:-a-practical-manual-Gilchrist-Aitchison",
            "title": {
                "fragments": [],
                "text": "Thesaurus construction: a practical manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "I(X) for an event X, defined as the negative log probability (Cover and Thomas, 1991):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53827957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67b6fd4dd2c5a5bdd87797a4b6caab253267f92b",
            "isKey": false,
            "numCitedBy": 1801,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Elements-of-Information-Theory-(Wiley-Series-in-and-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49508747"
                        ],
                        "name": "L. R. Dice",
                        "slug": "L.-R.-Dice",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Dice",
                            "middleNames": [
                                "Raymond"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. R. Dice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 76
                            }
                        ],
                        "text": "Taking the product of the two weight functions will lead to the generalised Dice measure. An alternative that I consider here, as does Lin (1998a), is to consider the sum of the two weights, and remove the constant so the assumptions are still satisfied: \u2211 wgt(w1,\u2217r ,\u2217w\u2032)+wgt(w2,\u2217r ,\u2217w\u2032) \u2211 wgt(w1,\u2217,\u2217)+ \u2211 wgt(w2,\u2217,\u2217) (4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53335638,
            "fieldsOfStudy": [
                "Geography"
            ],
            "id": "23045299013e8738bc8eff73827ef8de256aef66",
            "isKey": false,
            "numCitedBy": 9574,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measures-of-the-Amount-of-Ecologic-Association-Dice",
            "title": {
                "fragments": [],
                "text": "Measures of the Amount of Ecologic Association Between Species"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1945
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144263847"
                        ],
                        "name": "J. Deese",
                        "slug": "J.-Deese",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Deese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Grefenstette (1994) evaluates against the Deese Antonyms, a collection of 33 pairs of very common adjectives and the most frequent response in free word-association experiments. Deese (1962) found that the responses were predominantly a contrastive adjective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35757129,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "id": "9ccafde7c80cfa67ea4f91917fa007f1b414090e",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-structure-of-associative-meaning.-Deese",
            "title": {
                "fragments": [],
                "text": "On the structure of associative meaning."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38655972"
                        ],
                        "name": "F. Rogers",
                        "slug": "F.-Rogers",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Rogers",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rogers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4245918,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "c8932be429d642d53d6eea62b7b13ff0514c5fce",
            "isKey": false,
            "numCitedBy": 620,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Medical-Subject-Headings-Rogers",
            "title": {
                "fragments": [],
                "text": "Medical Subject Headings"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51073678"
                        ],
                        "name": "Marius Paca",
                        "slug": "Marius-Paca",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Paca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Paca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39148984"
                        ],
                        "name": "S. Eggers",
                        "slug": "S.-Eggers",
                        "structuredName": {
                            "firstName": "Shauna",
                            "lastName": "Eggers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eggers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In my reimplementations, SEXTANT(NB) uses a very simple Na\u0131\u0308ve Bayes POS tagger with the same feature set as Ratnaparkhi (1996). This tagger makes local classification decisions rather than maximising the probability over the sequence using Viterbi or beam search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59852170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dde07bd5ef716230da0e48ed13780b9f9dddbf68",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Informative-Role-of-WordNet-in-Open-Domain-Paca-Harabagiu",
            "title": {
                "fragments": [],
                "text": "The Informative Role of WordNet in Open-Domain Question Answering"
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403215454"
                        ],
                        "name": "C. M. Sperberg-McQueen",
                        "slug": "C.-M.-Sperberg-McQueen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Sperberg-McQueen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Sperberg-McQueen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776627"
                        ],
                        "name": "L. Burnard",
                        "slug": "L.-Burnard",
                        "structuredName": {
                            "firstName": "Lou",
                            "lastName": "Burnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69348601"
                        ],
                        "name": "Linguistic Computing",
                        "slug": "Linguistic-Computing",
                        "structuredName": {
                            "firstName": "Linguistic",
                            "lastName": "Computing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linguistic Computing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 152874370,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "59b19bfe925d53261133c7e69e043f17b73261f6",
            "isKey": false,
            "numCitedBy": 954,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Guidelines-for-electronic-text-encoding-and-Sperberg-McQueen-Burnard",
            "title": {
                "fragments": [],
                "text": "Guidelines for electronic text encoding and interchange"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For an ensemble to be more effective than its constituents, the individual classifiers must have better than 50% accuracy and must produce diverse erroneous classifications (Dietterich, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56776745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b900b13a8ae7814c1fb00960ef1da66c4580859",
            "isKey": false,
            "numCitedBy": 6135,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly."
            },
            "slug": "Ensemble-Methods-in-Machine-Learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Ensemble Methods in Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly."
            },
            "venue": {
                "fragments": [],
                "text": "Multiple Classifier Systems"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60841479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c311b01efa0c402c950d7b7bb8219a1d1640ccd2",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "eXtended-WordNet:-progress-report-Mihalcea-Moldovan",
            "title": {
                "fragments": [],
                "text": "eXtended WordNet: progress report"
            },
            "venue": {
                "fragments": [],
                "text": "HTL 2001"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Morton.The Story of Webster\u2019s Third: Philip Gove\u2019s Controversial Dictionary and its Critics"
            },
            "venue": {
                "fragments": [],
                "text": "Tom Morton. Grok tokenizer,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 148
                            }
                        ],
                        "text": "Other methods constrain the breadth-first search by only allowing certain types of lexical relations to be followed at certain stages of the search (Hirst and St-Onge, 1998; St-Onge, 1995; Wu and Palmer, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detecting and correcting malapropism with lexical chains"
            },
            "venue": {
                "fragments": [],
                "text": "Master\u2019s thesis, Department of Computer Science,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CSR-III Text"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report LDC95T6,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "TheLDC has recently released the English Gigawordcorpus (Graff, 2003) including most of the corpora listed above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English Gigaword"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report LDC2003T05, Linguistic Data Consortium, Philadelphia, PA USA,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Global query expansion may involve adding synonyms, cooccurring terms from the text, or variants formed by stemming and morphological analysis (Baeza-Yates and Ribeiro-Neto, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 51
                            }
                        ],
                        "text": "TheJaccardmeasure, also called the Tanimotomeasure (Tanimoto, 1958), compares the number of common attributes with the number of unique attributes for a pair of headwords: |(w1,\u2217,\u2217)\u2229 (w2,\u2217,\u2217)| |(w1,\u2217,\u2217)\u222a (w2,\u2217,\u2217)| (4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An element mathematical theory of classification"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, I.B.M. Research,"
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "North American News Text Corpus"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report LDC95T21, Linguistic Data Consortium, Philadelphia, PA USA,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mario Jarmasz and Stan Szpakowicz"
            },
            "venue": {
                "fragments": [],
                "text": "Roget\u2019s thesaurus and semantic similarity. In"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Herbert Rubenstein and John B"
            },
            "venue": {
                "fragments": [],
                "text": "Goodenough. Contextual correlates of synonymy."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "These approaches all use the W EBEXP system (Keller et al., 1998) to collect similarity judgements from participants on the web."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WebExp: A Java toolbox for web-based psychological experiments"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report HCRC/TR99, Human Communication Research Centre,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Augmenting the Princeton WordNet with a domain specific ontology"
            },
            "venue": {
                "fragments": [],
                "text": "PA USA,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00fctze. Foundations of Statistical Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "North American News Text Supplement"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report LDC98T30, Linguistic Data Consortium, Philadelphia, PA USA,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Leonard Kaufman and Peter J"
            },
            "venue": {
                "fragments": [],
                "text": "Rousseeuw."
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 74,
            "methodology": 79,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 236,
        "totalPages": 24
    },
    "page_url": "https://www.semanticscholar.org/paper/From-distributional-to-semantic-similarity-Curran/c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e?sort=total-citations"
}