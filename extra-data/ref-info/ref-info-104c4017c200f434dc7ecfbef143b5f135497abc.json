{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784450"
                        ],
                        "name": "M. Hagenbuchner",
                        "slug": "M.-Hagenbuchner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Hagenbuchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagenbuchner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "Then,xn andon are defined as follows\nxn = fw(ln, lco[n], xne[n], lne[n])\non = gw(xn, ln) , (1)\nwhereln, lco[n], xne[n], lne[n] are respectively the label ofn, the labels of its edges, the states and the labels of the nodes in the neighborhood ofn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "In this paper, we will consider the universal approximationproperties of a recently introduced neural network model called graph neural network (GNN) which\ncan be used to process structured data inputs, e.g. acyclic graph, cyclic graph, directed or un-directed graphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206756462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3efd851140aa28e95221b55fcc5659eea97b172d",
            "isKey": false,
            "numCitedBy": 3206,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities."
            },
            "slug": "The-Graph-Neural-Network-Model-Scarselli-Gori",
            "title": {
                "fragments": [],
                "text": "The Graph Neural Network Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains, and implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34750387"
                        ],
                        "name": "V. Massa",
                        "slug": "V.-Massa",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Massa",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Massa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15143034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a725e8cf01b687a1d61ba8d9b760f4e0c0a3c50d",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks (RNNs) and graph neural networks (GNNs) are two connectionist models that can directly process graphs. RNNs and GNNs exploit a similar processing framework, but they can be applied to different input domains. RNNs require the input graphs to be directed and acyclic, whereas GNNs can process any kind of graphs. The aim of this paper consists in understanding whether such a difference affects the behaviour of the models on a real application. An experimental comparison on an image classification problem is presented, showing that GNNs outperforms RNNs. Moreover the main differences between the models are also discussed w.r.t. their input domains, their approximation capabilities and their learning algorithms."
            },
            "slug": "A-Comparison-between-Recursive-Neural-Networks-and-Massa-Monfardini",
            "title": {
                "fragments": [],
                "text": "A Comparison between Recursive Neural Networks and Graph Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An experimental comparison on an image classification problem is presented, showing that GNNs outperforms RNNs, and the main differences between the models are discussed w.r.t. their input domains, their approximation capabilities and their learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "The 2006 IEEE International Joint Conference on Neural Network Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34750387"
                        ],
                        "name": "V. Massa",
                        "slug": "V.-Massa",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Massa",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Massa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32740478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2f40c13c39dbad40db88cbe17bb57ca9896e302",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph Neural Networks (GNNs) are a recently proposed connectionist model that extends previous neural methods to structured domains. GNNs can be applied on datasets that contain very general types of graphs and, under mild hypotheses, they have been proven to be universal approximators on graphical domains. Whereas most of the common approaches to graphs processing are based on a preliminary phase that maps each graph onto a simpler data type, like a vector or a sequence of reals, GNNs have the ability to directly process input graphs, thus embedding their connectivity into the processing scheme. In this paper, the main theoretical properties of GNNs are briefly reviewed and they are proposed as a tool for object localization. An experimentation has been carried out on the task of locating the face of a popular Walt Disney character in comic covers. In the dataset the character is shown in a number of different poses, often in cluttered backgrounds, and in high variety of colors. The proposed learning framework provides a way to deal with complex data arising from image segmentation process, without exploiting any prior knowledge on the dataset. The results are very encouraging, prove the viability of the method and the effectiveness of the structural representation of images."
            },
            "slug": "Graph-Neural-Networks-for-Object-Localization-Monfardini-Massa",
            "title": {
                "fragments": [],
                "text": "Graph Neural Networks for Object Localization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed learning framework provides a way to deal with complex data arising from image segmentation process, without exploiting any prior knowledge on the dataset, and proves the viability of the method and the effectiveness of the structural representation of images."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20480879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ca9f28676ad788d04ba24a51141a9a0a0df4d67",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model."
            },
            "slug": "A-new-model-for-learning-in-graph-domains-Gori-Monfardini",
            "title": {
                "fragments": [],
                "text": "A new model for learning in graph domains"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new neural model, called graph neural network (GNN), capable of directly processing graphs, which extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6197973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5eb96540ef53b49eac2246d6b13635fe6e54451",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden statespace representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i.e., cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements. This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model."
            },
            "slug": "A-general-framework-for-adaptive-processing-of-data-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "A general framework for adaptive processing of data structures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information, where relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5942593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e33eca03933caaec671e20692e79d1acc9527e1",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach. In fact, feature-based approaches usually fail to give satisfactory solutions because of the sensitivity of the approach to the a priori selection of the features, and the incapacity to represent any specific information on the relationships among the components of the structures. However, we show that neural networks can, in fact, represent and classify structured patterns. The key idea underpinning our approach is the use of the so called \"generalized recursive neuron\", which is essentially a generalization to structures of a recurrent neuron. By using generalized recursive neurons, all the supervised networks developed for the classification of sequences, such as backpropagation through time networks, real-time recurrent networks, simple recurrent networks, recurrent cascade correlation networks, and neural trees can, on the whole, be generalized to structures. The results obtained by some of the above networks (with generalized recursive neurons) on the classification of logic terms are presented."
            },
            "slug": "Supervised-neural-networks-for-the-classification-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Supervised neural networks for the classification of structures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that neural networks can, in fact, represent and classify structured patterns and all the supervised networks developed for the classification of sequences can, on the whole, be generalized to structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020416"
                        ],
                        "name": "M. Bianchini",
                        "slug": "M.-Bianchini",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Bianchini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The graphical domain considered in this paper is the setD of pairs of graph\u2013node, i.e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "C OMPUTATIONAL CAPABILITIES OF GNNS\nFeedforward neural networks have been proved to be universal approximators [7], [8], [9] for functions having Euclidean domain\nand codomain, i.e. they can approximate any map\u03c4 : IRa \u2192 IRb."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9967076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51b46f63038fb9571f6e922002e2f731824a09c3",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-neural-networks-for-processing-graphs-and-Bianchini-Maggini",
            "title": {
                "fragments": [],
                "text": "Recursive neural networks for processing graphs with labelled edges: theory and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784450"
                        ],
                        "name": "M. Hagenbuchner",
                        "slug": "M.-Hagenbuchner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Hagenbuchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagenbuchner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4675456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf35b7f3683a2d5c3fafa54dd33654cda8afd416",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new neural network model, called graph neural network model, which is a generalization of two existing approaches, viz., the graph focused approach, and the node focused approach. The graph focused approach considers the mapping from a graph structure to a real vector, in which the mapping is independent of the particular node involved; while the node focused approach considers the mapping from a graph structure to a real vector, in which the mapping depends on the properties of the node involved. It is shown that the graph neural network model maintains some of the characteristics of the graph focused models and the node focused models respectively. A supervised learning algorithm is derived to estimate the parameters of the graph neural network model. Some experimental results are shown to validate the proposed learning algorithm, and demonstrate the generalization capability of the proposed model."
            },
            "slug": "Graphical-Based-Learning-Environments-for-Pattern-Scarselli-Tsoi",
            "title": {
                "fragments": [],
                "text": "Graphical-Based Learning Environments for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A supervised learning algorithm is derived to estimate the parameters of the graph neural network model, which is a generalization of two existing approaches, viz., the graph focused approach, and the node focused approach."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR/SPR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784450"
                        ],
                        "name": "M. Hagenbuchner",
                        "slug": "M.-Hagenbuchner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Hagenbuchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagenbuchner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14473161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d24fc001bc5a8bf375b2f35f4f2e222995165c",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in the area of neural networks produced models capable of dealing with structured data. Here, we propose the first fully unsupervised model, namely an extension of traditional self-organizing maps (SOMs), for the processing of labeled directed acyclic graphs (DAGs). The extension is obtained by using the unfolding procedure adopted in recurrent and recursive neural networks, with the replicated neurons in the unfolded network comprising of a full SOM. This approach enables the discovery of similarities among objects including vectors consisting of numerical data. The capabilities of the model are analyzed in detail by utilizing a relatively large data set taken from an artificial benchmark problem involving visual patterns encoded as labeled DAGs. The experimental results demonstrate clearly that the proposed model is capable of exploiting both information conveyed in the labels attached to each node of the input DAGs and information encoded in the DAG topology."
            },
            "slug": "A-self-organizing-map-for-adaptive-processing-of-Hagenbuchner-Sperduti",
            "title": {
                "fragments": [],
                "text": "A self-organizing map for adaptive processing of structured data"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work proposes the first fully unsupervised model, namely an extension of traditional self-organizing maps (SOMs), for the processing of labeled directed acyclic graphs (DAGs) by using the unfolding procedure adopted in recurrent and recursive neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "(6)), whereL(y) is (y \u2212 \u00b5)2 if y > \u00b5 and0 otherwise, and the parameter\u00b5 \u2208 (0, 1) defines the desired contraction constant ofFw."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14084521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5126188f4b8180a648b7fda958e354d923afda35",
            "isKey": false,
            "numCitedBy": 463,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Universal-Approximation-Using-Feedforward-Neural-A-Scarselli-Tsoi",
            "title": {
                "fragments": [],
                "text": "Universal Approximation Using Feedforward Neural Networks: A Survey of Some Existing Methods, and Some New Results"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7443740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "517a81081636a27920934be8a231e978b5807357",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The information theoretical learnability of folding networks, a very successful approach capable of dealing with tree structured inputs, is examined. We find bounds on the VC, pseudo-, and fat shattering dimension of folding networks with various activation functions. As a consequence, valid generalization of folding networks can be guaranteed. However, distribution independent bounds on the generalization error cannot exist in principle. We propose two approaches which take the specific distribution into account and allow us to derive explicit bounds on the deviation of the empirical error from the real error of a learning algorithm. The first approach requires the probability of large trees to be limited a priori and the second approach deals with situations where the maximum input height in a concrete learning example is restricted."
            },
            "slug": "Generalization-Ability-of-Folding-Networks-Hammer",
            "title": {
                "fragments": [],
                "text": "Generalization Ability of Folding Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Two approaches are proposed which take the specific distribution of the distribution into account and allow us to derive explicit bounds on the deviation of the empirical error from the real error of a learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020416"
                        ],
                        "name": "M. Bianchini",
                        "slug": "M.-Bianchini",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Bianchini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15633336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e943409eac3bd89f0155f28bae34fab011e877db",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks are a powerful tool for processing structured data. According to the recursive learning paradigm, the input information consists of directed positional acyclic graphs (DPAGs). In fact, recursive networks are fed following the partial order defined by the links of the graph. Unfortunately, the hypothesis of processing DPAGs is sometimes too restrictive, being the nature of some real-world problems intrinsically cyclic. In this paper, a methodology is proposed, which allows us to process any cyclic directed graph. Therefore, the computational power of recursive networks is definitely established, also clarifying the underlying limitations of the model."
            },
            "slug": "Recursive-processing-of-cyclic-graphs-Bianchini-Gori",
            "title": {
                "fragments": [],
                "text": "Recursive processing of cyclic graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A methodology is proposed, which allows us to process any cyclic directed graph, and the computational power of recursive networks is definitely established, also clarifying the underlying limitations of the model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020416"
                        ],
                        "name": "M. Bianchini",
                        "slug": "M.-Bianchini",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Bianchini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15774428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e2ad0ffd39f7d5e117b49272a8f598130b07d7",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Many researchers have recently focused their efforts on devising efficient algorithms, mainly based on optimization schemes, for learning the weights of recurrent neural networks. As in the case of feedforward networks, however, these learning algorithms may get stuck in local minima during gradient descent, thus discovering sub-optimal solutions. This paper analyses the problem of optimal learning in recurrent networks by proposing conditions that guarantee local minima free error surfaces. An example is given that also shows the constructive role of the proposed theory in designing networks suitable for solving a given task. Moreover, a formal relationship between recurrent and static feedforward networks is established such that the examples of local minima for feedforward networks already known in the literature can be associated with analogous ones in recurrent networks."
            },
            "slug": "On-the-problem-of-local-minima-in-recurrent-neural-Bianchini-Gori",
            "title": {
                "fragments": [],
                "text": "On the problem of local minima in recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper analyses the problem of optimal learning in recurrent networks by proposing conditions that guarantee local minima free error surfaces and a formal relationship between recurrent and static feedforward networks is established."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2305072"
                        ],
                        "name": "Yoshifusa Ito",
                        "slug": "Yoshifusa-Ito",
                        "structuredName": {
                            "firstName": "Yoshifusa",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshifusa Ito"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121943964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "822c439a17abff518a327e8f51941466d1c9ef02",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Differentiable-approximation-by-means-of-the-Radon-Ito",
            "title": {
                "fragments": [],
                "text": "Differentiable approximation by means of the Radon transformation and its applications to neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "C OMPUTATIONAL CAPABILITIES OF GNNS\nFeedforward neural networks have been proved to be universal approximators [7], [8], [9] for functions having Euclidean domain\nand codomain, i.e. they can approximate any map\u03c4 : IRa \u2192 IRb."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17914413,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "fe37c583893e457d10147c89f8b8451e7980d437",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show several approximation results for folding networks { a generalization of partial recurrent neural networks such that not only time sequences but arbitrary trees can serve as input: Any measurable function can be approximated in probability. Any continuous function can be approximated in the maximum norm on inputs with restricted height, but the resources necessarily increase at least exponentially in the input height. In general, approximation on arbitrary inputs is not possible in the maximum norm."
            },
            "slug": "Approximation-capabilities-of-folding-networks-Hammer",
            "title": {
                "fragments": [],
                "text": "Approximation capabilities of folding networks"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several approximation results for folding networks { a generalization of partial recurrent neural networks such that not only time sequences but arbitrary trees can serve as input: Any measurable function can be approximated in probability."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2115167,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f0441c6fbdf67b5237ad30e81fb34b429251ebfb",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a multilayer perceptron (MLP) with a fixed architecture, there are functions that can be approximated up to any degree of accuracy, without having to increase the number of the hidden nodes. Those functions belong to the closure F of the set F of the maps realizable by the MLP. In this paper, we give a list of maps with this property. In particular, it is proven that 1) rational functions belongs to F for networks with inverse tangent activation function and 2) products of polynomials and exponentials belongs to F for networks with sigmoid activation function. Moreover, for a restricted class of MLP's, we prove that the list is complete and give an analytic definition of F."
            },
            "slug": "On-the-closure-of-the-set-of-functions-that-can-be-Gori-Scarselli",
            "title": {
                "fragments": [],
                "text": "On the closure of the set of functions that can be realized by a given multilayer perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A list of maps with the property that 1) rational functions belongs to F for networks with inverse tangent activation function and 2) products of polynomials and exponentials belongs tof networks with sigmoid activation function is given."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849160"
                        ],
                        "name": "Werner Uwents",
                        "slug": "Werner-Uwents",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Uwents",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Werner Uwents"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755851"
                        ],
                        "name": "H. Blockeel",
                        "slug": "H.-Blockeel",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Blockeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Blockeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15305743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ec3fa56511ee336075c5b2368366c255524204",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, two recently developed connectionist models for learning from relational or graph-structured data, i.e. Relational Neural Networks (RelNNs) and Graph Neural Networks (GNNs), are compared. We first introduce a general paradigm for connectionist learning from graphs that covers both approaches, and situate the approaches in this general paradigm. This gives a first view on how they relate to each other. As RelNNs have been developed with learning aggregate functions in mind, we compare them to GNNs for this specific task. Next, we compare both with other relational learners on a number of benchmark problems (mutagenesis, biodegradability). The results are promising and suggest that RelNNs and GNNs can be a viable approach for learning on relational data. They also point out a number of differences in behavior between both approaches that deserve further study."
            },
            "slug": "Two-connectionist-models-for-graph-processing:-An-Uwents-Monfardini",
            "title": {
                "fragments": [],
                "text": "Two connectionist models for graph processing: An experimental comparison on relational data"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Two recently developed connectionist models for learning from relational or graph-structured data, i.e. Relational Neural Networks and Graph Neural Networks, are compared and it is suggested that RelNNs and GNNs can be a viable approach for learning on relational data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884106"
                        ],
                        "name": "Sweah Liang Yong",
                        "slug": "Sweah-Liang-Yong",
                        "structuredName": {
                            "firstName": "Sweah",
                            "lastName": "Yong",
                            "middleNames": [
                                "Liang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sweah Liang Yong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784450"
                        ],
                        "name": "M. Hagenbuchner",
                        "slug": "M.-Hagenbuchner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Hagenbuchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagenbuchner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8167952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "769bfd4a4b45979cf83bb56c054ebcaaaf8b35d7",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "An artificial neural network model, capable of processing general types of graph structured data, has recently been proposed. This paper applies the new model to the computation of customised page ranks problem in the World Wide Web. The class of customised page ranks that can be implemented in this way is very general and easy because the neural network model is learned by examples. Some preliminary experimental findings show that the model generalizes well over unseen Web pages, and hence, may be suitable for the task of page rank computation on a large Web graph."
            },
            "slug": "Graph-neural-networks-for-ranking-Web-pages-Scarselli-Yong",
            "title": {
                "fragments": [],
                "text": "Graph neural networks for ranking Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Some preliminary experimental findings show that the new artificial neural network model generalizes well over unseen Web pages, and hence, may be suitable for the task of page rank computation on a large Web graph."
            },
            "venue": {
                "fragments": [],
                "text": "The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14333248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9552ac39a57daacf3d75865a268935b5a0df9bbb",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-principal-component-analysis:-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal component analysis: Learning from examples without local minima"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 174
                            }
                        ],
                        "text": "(6)\nIn GNNs, the minimization is achieved by a new learning algorithm [5] that combines Backpropagation Through Structure algorithm [4], which is used in recursive neural networks, with the Almeida\u2013Pineda algorithm [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6386,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7343126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d35f1e533b72370683d8fa2dabff5f0fc16490cc",
            "isKey": false,
            "numCitedBy": 4664,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-capabilities-of-multilayer-networks-Hornik",
            "title": {
                "fragments": [],
                "text": "Approximation capabilities of multilayer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884106"
                        ],
                        "name": "Sweah Liang Yong",
                        "slug": "Sweah-Liang-Yong",
                        "structuredName": {
                            "firstName": "Sweah",
                            "lastName": "Yong",
                            "middleNames": [
                                "Liang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sweah Liang Yong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784450"
                        ],
                        "name": "M. Hagenbuchner",
                        "slug": "M.-Hagenbuchner",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Hagenbuchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagenbuchner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27688664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b87edd8d4583d1135428245548c68ba03c3fc6eb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Graph Neural Network is a relatively new machine learning method capable of encoding data as well as relationships between data elements. This paper applies the Graph Neural Network for the first time to a given learning task at an international competition on the classification of semi-structured documents. Within this setting, the Graph Neural Network is trained to encode and process a relatively large set of XML formatted documents. It will be shown that the performance using the Graph Neural Network approach significantly outperforms the results submitted by the best competitor."
            },
            "slug": "Document-Mining-Using-Graph-Neural-Network-Yong-Hagenbuchner",
            "title": {
                "fragments": [],
                "text": "Document Mining Using Graph Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper applies the Graph Neural Network for the first time to a given learning task at an international competition on the classification of semi-structured documents and it will be shown that the performance significantly outperforms the results submitted by the best competitor."
            },
            "venue": {
                "fragments": [],
                "text": "INEX"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61166081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f1d04f57e420f0f1b2cd059deab309bc7073ca1",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested.<<ETX>>"
            },
            "slug": "The-problem-of-learning-long-term-dependencies-in-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "The problem of learning long-term dependencies in recurrent networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results are presented showing that learning long-term dependencies in recurrent neural networks using gradient descent is a very difficult task, and how this difficulty arises when robustly latching bits of information with certain attractors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706663"
                        ],
                        "name": "A. Tesi",
                        "slug": "A.-Tesi",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Tesi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tesi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8098333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "322261ad6b5071c64ad93edd364eeb77fb90b1a3",
            "isKey": false,
            "numCitedBy": 643,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors propose a theoretical framework for backpropagation (BP) in order to identify some of its limitations as a general learning procedure and the reasons for its success in several experiments on pattern recognition. The first important conclusion is that examples can be found in which BP gets stuck in local minima. A simple example in which BP can get stuck during gradient descent without having learned the entire training set is presented. This example guarantees the existence of a solution with null cost. Some conditions on the network architecture and the learning environment that ensure the convergence of the BP algorithm are proposed. It is proven in particular that the convergence holds if the classes are linearly separable. In this case, the experience gained in several experiments shows that multilayered neural networks (MLNs) exceed perceptrons in generalization to new examples. >"
            },
            "slug": "On-the-Problem-of-Local-Minima-in-Backpropagation-Gori-Tesi",
            "title": {
                "fragments": [],
                "text": "On the Problem of Local Minima in Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A theoretical framework for backpropagation (BP) is proposed and it is proven in particular that the convergence holds if the classes are linearly separable and that multilayered neural networks (MLNs) exceed perceptrons in generalization to new examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48429353"
                        ],
                        "name": "Pineda",
                        "slug": "Pineda",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Pineda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40994937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6602985bd326d9996c68627b56ed389e2c90fd08",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "slug": "Generalization-of-back-propagation-to-recurrent-Pineda",
            "title": {
                "fragments": [],
                "text": "Generalization of back-propagation to recurrent neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An adaptive neural network with asymmetric connections is introduced that bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "(6)\nIn GNNs, the minimization is achieved by a new learning algorithm [5] that combines Backpropagation Through Structure algorithm [4], which is used in recursive neural networks, with the Almeida\u2013Pineda algorithm [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17351,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9898,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695738"
                        ],
                        "name": "F. Preparata",
                        "slug": "F.-Preparata",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Preparata",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Preparata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18686062,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "696dad038f6aa3309a7b44b1b658abbd722bcad7",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "All published approaches to DNA sequencing by hybridization (SBH) consist of the biochemical acquisition of the spectrum of a target sequence (the set of its subsequences conforming to a given probing pattern) followed by the algorithmic reconstruction of the sequence from its spectrum. In the \"standard\" or \"uniform\" approach, the probing pattern is a string of length L and the length of reliably reconstructible sequences is known to be m/sub len/ = O(2/sup L/). For a fixed microarray area, higher sequencing performance can be achieved by inserting nonprobing gaps (\"wild-cards\") in the probing pattern. The reconstruction, however, must cope with the emergence of fooling probes due to the gaps and algorithmic failure occurs when the spectrum becomes too densely populated, although we can achieve m/sub comp/ = O(4/sup L/). Despite the combinatorial success of gapped probing, all current approaches are based on a biochemically unrealistic spectrum-acquisition model (digital-spectrum). The reality of hybridization is much more complex. Departing from the conventional model, in this paper, we propose an alternative, called the analog-spectrum model, which more closely reflects the biochemical process. This novel modeling reestablishes probe length as the performance-governing factor, adopting \"semidegenerate bases\" as suitable emulators of currently inadequate universal bases. One important conclusion is that accurate biochemical measurements are pivotal to the success of SBH. The theoretical proposal presented in this paper should be a convincing stimulus for the needed biotechnological work."
            },
            "slug": "Sequencing-by-hybridization-revisited:-the-proposal-Preparata",
            "title": {
                "fragments": [],
                "text": "Sequencing-by-hybridization revisited: the analog-spectrum proposal"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an alternative, called the analog-spectrum model, which more closely reflects the biochemical process, and reestablishes probe length as the performance-governing factor, adopting \"semidegenerate bases\" as suitable emulators of currently inadequate universal bases."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695738"
                        ],
                        "name": "F. Preparata",
                        "slug": "F.-Preparata",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Preparata",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Preparata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1890127"
                        ],
                        "name": "M. Shamos",
                        "slug": "M.-Shamos",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Shamos",
                            "middleNames": [
                                "Ian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shamos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120573598,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "fd9c5c52a26e99e6bc3155db67cb94efe6054e04",
            "isKey": false,
            "numCitedBy": 6837,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the reviews: \"This book offers a coherent treatment, at the graduate textbook level, of the field that has come to be known in the last decade or so as computational geometry...The book is well organized and lucidly written; a timely contribution by two founders of the field. It clearly demonstrates that computational geometry in the plane is now a fairly well-understood branch of computer science and mathematics. It also points the way to the solution of the more challenging problems in dimensions higher than two.\""
            },
            "slug": "Computational-geometry:-an-introduction-Preparata-Shamos",
            "title": {
                "fragments": [],
                "text": "Computational geometry: an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This book offers a coherent treatment, at the graduate textbook level, of the field that has come to be known in the last decade or so as computational geometry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144128701"
                        ],
                        "name": "M. Khamsi",
                        "slug": "M.-Khamsi",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Khamsi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Khamsi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145734132"
                        ],
                        "name": "W. A. Kirk",
                        "slug": "W.-A.-Kirk",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Kirk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Kirk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "(1) can be written in a vectorial form as follows:\nx = Fw(x, l) o = Gw(x, lN ) (4)\nwhereFw andGw are the composition of|N | instances of w andgw, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118080951,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "46cb7584ba5838c67368fbb98748a662bd3b427e",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. METRIC SPACES. Introduction. Metric Spaces. Metric Contraction Principles. Hyperconvex Spaces. \"Normal\" Structures in Metric Spaces. BANACH SPACES. Banach Spaces: Introduction. Continuous Mappings in Banach Spaces. Metric Fixed Point Theory. Banach Space Ultrapowers. Appendix: Set Theory. Bibliography. Index."
            },
            "slug": "An-Introduction-to-Metric-Spaces-and-Fixed-Point-Khamsi-Kirk",
            "title": {
                "fragments": [],
                "text": "An Introduction to Metric Spaces and Fixed Point Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13533363,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "37807e97c624fb846df7e559553b32539ba2ea5d",
            "isKey": false,
            "numCitedBy": 1805,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Universal-approximation-of-an-unknown-mapping-and-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "O ther experiments, whose goal is to assess the performance an d the properties of the GNN model on wider and real\u2013life applicati ons, can be found in [5], [23], [24], [6], [25], [26], [27], The following facts hold for each experiment, unless otherw ise specified."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "the father is not included (see [5] for a more detailed co mparison)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "In GNNs, the transition and the output functions are implem ented by multilayer feedforward neural network [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Readers are referred to [13], [5] for further di scussions about the GNN model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "(6) In GNNs, the minimization is achieved by a new learning algor ithm [5] that combines Backpropagation Through Structure algorithm [4], which is used in recursive neural networks, w ith the Almeida\u2013Pineda algorithm [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "GRAPH NEURAL NETWORKS GNN model was proposed in [13], [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and G"
            },
            "venue": {
                "fragments": [],
                "text": " Monfardini, \u201cThe graph neural network model,\u201d Department o f Information Engineering, University of Siena, Tech. Rep. DII-1/08"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "It also extends the universal approximation property of recursive neural networks [11], [12], a neural m odel which is a predecessor of GNNs and can process only acyclic directed g raphs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Re cently, also recursive neural networks have been shown to approximate in probability any function on trees up to any degree of precisi on [11], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "In fact, it has been proved that RNNs can approximate in probability any function on trees [11], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "It can be easily observed that under this definiti on of neighborhood, any function on trees, which satisfies the unf olding equivalence and Theorems 2, 3, reproduces those pres nted in [11], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and F"
            },
            "venue": {
                "fragments": [],
                "text": "Scarselli, \u201cR  ecursive neural networks for processing graphs with labell  ed edges: Theory and applications,\u201d  Neural Networks - Special Issue on Neural Networks and Kernel Metho  ds f r Structured Domains  "
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [10], it is proved that three layered networks with non\u2013polynomia l analytic activation functions can implement any polynomial on compa ct sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "The de pth of the trees, measured after the completion of the constr uction process, usually belonged to the interval [10, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Several versions of the result have been proposed, which ad opt different classes of functions, different measures of the a pproximation and different network architectures [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Commonly used feedforward neural networks (FNNs) are unive rsal approximators [7], [8], [9], [10] and, obviously, they can also approximate the functions f andg of Theorem 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "With such a simplification, the universal approximation literature p ovide several other results about the approximation of a fu nction along with its derivatives [20], [21], [10]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Such a result can be considered an extension of the universal appro ximation property that was proved for feedforward neural ne tworks [7], [8], [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Universal approximation u  sing feedforward neural networks: a survey of some existing  methods"
            },
            "venue": {
                "fragments": [],
                "text": "and some new results,\u201d  Neural networks, pp. 15\u201337"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145887784"
                        ],
                        "name": "J. Nazuno",
                        "slug": "J.-Nazuno",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Nazuno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nazuno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 147456412,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "ca91cd1e67cb558dd2b7d94d37c3dba208b7cd40",
            "isKey": false,
            "numCitedBy": 3263,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Haykin,-Simon.-Neural-networks:-A-comprehensive-Nazuno",
            "title": {
                "fragments": [],
                "text": "Haykin, Simon. Neural networks: A comprehensive foundation, Prentice Hall, Inc. Segunda Edici\u00f3n, 1999"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Then, any function\u03d5 : D \u2192 IR m generated by\u03d5(G, n) = on is referred to as aharmolodic function1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62756844,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "027e3321f34c9b2af721f9d102b9d9eb85147dcf",
            "isKey": false,
            "numCitedBy": 3944,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-efficient-method-for-finding-the-minimum-of-a-of-Powell",
            "title": {
                "fragments": [],
                "text": "An efficient method for finding the minimum of a function of several variables without calculating derivatives"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969335"
                        ],
                        "name": "H. Priestley",
                        "slug": "H.-Priestley",
                        "structuredName": {
                            "firstName": "Hilary",
                            "lastName": "Priestley",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Priestley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118601276,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "364a5c9e38627d1bb5ac2fa85989dda260a8fae8",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Integration-Priestley",
            "title": {
                "fragments": [],
                "text": "Introduction to Integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14764415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e1ba967e32956f30846764898b26031f4d668c3",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vapnik-Chervonenkis-dimension-of-neural-nets-Bartlett-Maass",
            "title": {
                "fragments": [],
                "text": "Vapnik-Chervonenkis dimension of neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the clousure ofset of functions that can be realized by a multilayer percepton"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "Gi l(G, n) dlG is computed using Lebesgue measure theory [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "According to well known results on convolutions [18],l 4 is an infinitely differentiable function and lim\u03b4\u21920 l \u03b4 4 = l3 uniformly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priestley,Introduction to Integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation capabilities of feedforward neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Netw"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "With such a simplification, the universal approximation literature p ovide several other results about the approximation of a fu nction along with its derivatives [20], [21], [10]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation capabilities of feedforwar  d neural networks,\u201dNeural"
            },
            "venue": {
                "fragments": [],
                "text": "Networks  ,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal c  omponent analysis: Learning from examples without local mi  ni a,"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks  ,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differentiable approximation by means of Radon transformation and its application to neural net"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Appl. Math"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Pattern Recognition: Joint IAPR International Workshops, SSPR 2004 and SPR 2004"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the clousure of set of functions that can be realized by a multilayer perceptron"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "The Banach fixed point theorem [14] provides a sufficient condition for the existence and uniqueness of the solution of such a system of equations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "According to Banach\u2019s theorem [14], (4) has a unique solution provided that is a contraction map with respect to the state, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Khamsi, An Introduction to Metric Spaces and Fixed Point Theory"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Wiley,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "Such a result can be considered an extension of the universal appro ximation property that was proved for feedforward neural ne tworks [7], [8], [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "C OMPUTATIONAL CAPABILITIES OF GNNS Feedforward neural networks have been proved to be universa l approximators [7], [8], [9] for functions having Euclidea n domain and codomain, i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "Commonly used feedforward neural networks (FNNs) are unive rsal approximators [7], [8], [9], [10] and, obviously, they can also approximate the functions f andg of Theorem 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmo  idal function,\u201dMathematics of Control, Signals, and Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the problem of local minima in bac  kpropagation,\u201dIEEE Transactions on Pattern Analysis and Machine Intellig  nce"
            },
            "venue": {
                "fragments": [],
                "text": "vol. PAMI\u201314,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "O ther experiments, whose goal is to assess the performance an d the properties of the GNN model on wider and real\u2013life applicati ons, can be found in [5], [23], [24], [6], [25], [26], [27], The following facts hold for each experiment, unless otherw ise specified."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and M"
            },
            "venue": {
                "fragments": [],
                "text": "Gori, \u201cDocument mining using graph neural networks,\u201d in  Proceedings of the 5th International Workshop of the Initiative for the Evaluation of XML Retriev  al,, N. Fuhr, M. Lalmas, and A. Trotman, Eds."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [19], it is proved that the class of three layered neural ne tworks with activation function\u03c3 in the hidden neurons and a linear activation function in the output neurons can approx imate any functionl and its derivatives onR, provided that there exists a linear combination k of scaled shifted rotations of \u03c3 such that \u2202x is a square integrable function of uniformly locally bounded variation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differentiable approximation by means of Rado  n trasformation and its application to neural net,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of computational and applied mathematics  ,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the clousure ofset of functions that can be realized by a multilayer percepton"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new model forlearning in graph domains"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Neural Networks"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "In this paper, we will consider the universal approximationproperties of a recently introduced neural network model called graph neural network (GNN) which\ncan be used to process structured data inputs, e.g. acyclic graph, cyclic graph, directed or un-directed graphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphicalbased learning environment for pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the clousure ofset of functions that can be realized by a multilayer percepton"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the closure of set of functions that can be realized by a multilayer perceptron"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Neural Netw ."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Such a result can be considered an extension of the universal appro ximation property that was proved for feedforward neural ne tworks [7], [8], [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "C OMPUTATIONAL CAPABILITIES OF GNNS Feedforward neural networks have been proved to be universa l approximators [7], [8], [9] for functions having Euclidea n domain and codomain, i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Commonly used feedforward neural networks (FNNs) are unive rsal approximators [7], [8], [9], [10] and, obviously, they can also approximate the functions f andg of Theorem 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and H"
            },
            "venue": {
                "fragments": [],
                "text": "White, \u201cMultilayer fee  dforward networks are universal approximators,\u201d  Neural Networks  , vol. 2, pp. 359\u2013366"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differentiable approximation by means of Radon trasformation and its application to neural net"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of computational and applied mathematics"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the closure of set of functions that can be realized by a multilayer perceptron"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Netw"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the problem of local minima in bac kpropagation,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Pattern Analysis and Machine Intellig nce, vol. PAMI\u201314,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "O ther experiments, whose goal is to assess the performance an d the properties of the GNN model on wider and real\u2013life applicati ons, can be found in [5], [23], [24], [6], [25], [26], [27], The following facts hold for each experiment, unless otherw ise specified."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "It was shown empirically that GNNs can be used to model graph structured data, and that trained GNNs can generalize to unforeseen data [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and F"
            },
            "venue": {
                "fragments": [],
                "text": "Scarselli, \u201cA new model for  learning in graph domains,\u201d inProceedings of the International Joint Conference on Neura  l Networks"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 8,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Computational-Capabilities-of-Graph-Neural-Networks-Scarselli-Gori/104c4017c200f434dc7ecfbef143b5f135497abc?sort=total-citations"
}