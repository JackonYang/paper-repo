{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13609029,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "641483b9dcaa2bac13f95a3f2f6738140c170184",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed. The approach has been tested extensively on a large variety of video frame sizes such 352/spl times/240 up to 1920/spl times/1280 and a large representative set of video sequences such as home videos, newscasts, title sequences and commercials. 95% of the text bounding boxes in videos were localized correctly. 80% of all characters were segmented correctly, while 7.8% characters were damaged. 90% of the correctly segmented characters were recognized correctly by standard OCR software."
            },
            "slug": "On-the-segmentation-of-text-in-videos-Wernicke-Lienhart",
            "title": {
                "fragments": [],
                "text": "On the segmentation of text in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed that has been tested extensively on a large variety of video frame sizes and a large representative set of video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Even though some address text detection in video frames [1], [5], [11]\u2013[13], [16], [20], [34], they usually treat each video frame as an independent image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "After color quantization, the connected components of monotonous color that obey certain size, shape, and spatial alignment constraints are extracted as text [11], [13], [17], [20], [30], [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Multi-frame integration based caption enhancement techniques, such as the minimum pixel search [27] and frame averaging method [18], can be employed to reduce the influence of the complex background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 393,
                                "start": 389
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "In comparison to OCR for document images, caption extraction and recognition in videos presents several new challenges [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Since characters are composed of line segments, text regions contain rich edge information [1], [2], [8], [16], [27], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9520237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14ce174bddee5b6b2750cf14574773b537ac4d42",
            "isKey": true,
            "numCitedBy": 206,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The automatic extraction and recognition of news captions and annotations can be of great help locating topics of interest in digital news video libraries. To achieve this goal, we present a technique, called Video OCR (Optical Character Reader), which detects, extracts, and reads text areas in digital video data. In this paper, we address problems, describe the method by which Video OCR operates, and suggest applications for its use in digital news archives. To solve two problems of character recognition for videos, low-resolution characters and extremely complex backgrounds, we apply an interpolation filter, multi-frame integration and character extraction filters. Character segmentation is performed by a recognition-based segmentation method, and intermediate character recognition results are used to improve the segmentation. We also include a method for locating text areas using text-like properties and the use of a language-based postprocessing technique to increase word recognition rates. The overall recognition results are satisfactory for use in news indexing. Performing Video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR:-indexing-digital-news-libraries-by-of-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR: indexing digital news libraries by recognition of superimposed captions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "To solve two problems of character recognition for videos, low-resolution characters and extremely complex backgrounds, an interpolation filter, multi-frame integration and character extraction filters are applied and the overall recognition results are satisfactory for use in news indexing."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242803"
                        ],
                        "name": "L. Agnihotri",
                        "slug": "L.-Agnihotri",
                        "structuredName": {
                            "firstName": "Lalitha",
                            "lastName": "Agnihotri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Agnihotri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1667502629"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Natasa",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Even though some address text detection in video frames [1], [5], [11]\u2013[13], [16], [20], [34], they usually treat each video frame as an independent image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Since characters are composed of line segments, text regions contain rich edge information [1], [2], [8], [16], [27], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 150
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60735577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3b2c6b7bfaf9ab0d44c7103585fa0c81f60f3b9",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information brings important semantic clues in video content analysis. We describe a method for detection and representation of text in video segments. The method consists of seven steps: channel separation, image enhancement, edge detection, edge filtering, character detection, text box detection, and text line detection. Our results show that this method can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "slug": "Text-detection-for-video-analysis-Agnihotri-Dimitrova",
            "title": {
                "fragments": [],
                "text": "Text detection for video analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes a method for detection and representation of text in video segments that can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Content-Based Access of Image and Video Libraries (CBAIVL'99)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055230387"
                        ],
                        "name": "K. Jeong",
                        "slug": "K.-Jeong",
                        "structuredName": {
                            "firstName": "Ki-Young",
                            "lastName": "Jeong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jeong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715155"
                        ],
                        "name": "Eun Yi Kim",
                        "slug": "Eun-Yi-Kim",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kim",
                            "middleNames": [
                                "Yi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eun Yi Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "We also observe that many of the existing works require supervised training [12], [19], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Texture features extracted through multichannel processing [19], [36], spatial variance computing [39], and neural networks [12] have been used to detect text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Finally, the majority of the existing researches on caption extraction have been focusing on English captions, with a few exceptions addressing captions in Korean and Japanese [12], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9424151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de034f320270091bcf7e80400ad5bf5340b3a46a",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The retrieval of video clips from multimedia databases has been increasingly spotlighted. Texts in videos include useful information for automatic annotation or indexing. Text location is the first step for recognizing the textual information. This paper proposes a neural network-based text location method for news video indexing. Text can be characterized by texture, location, alignment, and font size. The proposed method classifies text pixels and non-text pixels using a network that operates as a set of texture discrimination filters. We find and locate text regions using histogram analysis after removing errors in the classification results. Experimental results show that the proposed method is effective at locating texts."
            },
            "slug": "Neural-network-based-text-location-for-news-video-Jeong-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location for news video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A neural network-based text location method for news video indexing using a network that operates as a set of texture discrimination filters and results show that the proposed method is effective at locating texts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "We also observe that many of the existing works require supervised training [12], [19], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "However, since these types of captions are used far less frequently than regular captions, we can afford to employ more complex caption detection and tracing algorithms, such as the ones developed in [19], to detect them."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 371
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Texture features extracted through multichannel processing [19], [36], spatial variance computing [39], and neural networks [12] have been used to detect text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": true,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115338247"
                        ],
                        "name": "Ryan Keener",
                        "slug": "Ryan-Keener",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Keener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Keener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 165
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57425516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d784187382befa5cb50b62f10ddb87feb487102",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Video indexing is an important problem that has occupied recent research efforts. The text appearing in video can provide semantic information about the scene content. Detecting and recognizing text events can provide indices into the video for content based querying. We describe a system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video. Preliminary results are presented."
            },
            "slug": "A-system-for-automatic-text-detection-in-video-Gargi-Crandall",
            "title": {
                "fragments": [],
                "text": "A system for automatic text detection in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video to provide indices into the video for content based querying is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744904"
                        ],
                        "name": "Yannis Avrithis",
                        "slug": "Yannis-Avrithis",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Avrithis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yannis Avrithis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709633"
                        ],
                        "name": "N. Tsapatsoulis",
                        "slug": "N.-Tsapatsoulis",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Tsapatsoulis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Tsapatsoulis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707243"
                        ],
                        "name": "S. Kollias",
                        "slug": "S.-Kollias",
                        "structuredName": {
                            "firstName": "Stefanos",
                            "lastName": "Kollias",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kollias"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 351,
                                "start": 348
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13588565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1fb22dbbf5fcb50945b8037924537b6535bc76a",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic content-based analysis and indexing of broadcast news recordings or digitized news archives is becoming an important tool in the framework of many multimedia interactive services such as news summarization, browsing, retrieval and news-on-demand (NoD) applications. Existing approaches have achieved high performance in such applications but heavily rely on textual cues such as closed caption tokens and teletext transcripts. We present an efficient technique for temporal segmentation and parsing of news recordings based on visual cues that can either be employed as a stand-alone application for non-closed captioned broadcasts or integrated with audio and textual cues of existing systems. The technique involves robust face detection by means of color segmentation, skin color matching and shape processing, and is able to identify typical news instances like anchor persons, reports and outdoor shots."
            },
            "slug": "Broadcast-news-parsing-using-visual-cues:-a-robust-Avrithis-Tsapatsoulis",
            "title": {
                "fragments": [],
                "text": "Broadcast news parsing using visual cues: a robust face detection approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents an efficient technique for temporal segmentation and parsing of news recordings based on visual cues that can either be employed as a stand-alone application for non-closed captioned broadcasts or integrated with audio and textual cues of existing systems."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33852772"
                        ],
                        "name": "H. Kuwano",
                        "slug": "H.-Kuwano",
                        "structuredName": {
                            "firstName": "Hidetaka",
                            "lastName": "Kuwano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuwano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113938"
                        ],
                        "name": "Y. Taniguchi",
                        "slug": "Y.-Taniguchi",
                        "structuredName": {
                            "firstName": "Yukinobu",
                            "lastName": "Taniguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Taniguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153027997"
                        ],
                        "name": "Hiroyuki Arai",
                        "slug": "Hiroyuki-Arai",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroyuki Arai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113759578"
                        ],
                        "name": "M. Mori",
                        "slug": "M.-Mori",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267020"
                        ],
                        "name": "S. Kurakake",
                        "slug": "S.-Kurakake",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Kurakake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurakake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054536982"
                        ],
                        "name": "H. Kojima",
                        "slug": "H.-Kojima",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kojima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26591876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddcc1bae71d70c52be48301c2fe45bc570d2e69b",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval. Superimposed texts are important as they provide semantic information about scene contents. Their attributes such as fonts, size, and position in a frame are important as they are carefully designed by the video editor and so reflect the intent of captioning. In news programs, for instance, the headline text is displayed in larger fonts than the subtitles. Our system takes into account not only the texts themselves but also their attributes for structuring videos. We describe: (i) novel methods for detecting and extracting texts that are robust against the presence of complex backgrounds and intensity degradation of the character patterns, and (ii) a method for structuring a video based on the text attributes."
            },
            "slug": "Telop-on-demand:-video-structuring-and-retrieval-on-Kuwano-Taniguchi",
            "title": {
                "fragments": [],
                "text": "Telop-on-demand: video structuring and retrieval based on text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval and a method for structuring a video based on the text attributes."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107824420"
                        ],
                        "name": "C. Garcia",
                        "slug": "C.-Garcia",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346037"
                        ],
                        "name": "X. Apostolidis",
                        "slug": "X.-Apostolidis",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Apostolidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Apostolidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Since characters are composed of line segments, text regions contain rich edge information [1], [2], [8], [16], [27], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46620452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57930a675de539c59bc33f56d9894c999d264f72",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints related to the texture of words. Then, classification and binarization of potential text areas are achieved in a single scheme performing color quantization and characters periodicity analysis. We report a high rate of good detection results with very few false alarms and reliable text binarization."
            },
            "slug": "Text-detection-and-segmentation-in-complex-color-Garcia-Apostolidis",
            "title": {
                "fragments": [],
                "text": "Text detection and segmentation in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background and to binarize efficiently the detected text areas so that they can be processed by standard OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701063"
                        ],
                        "name": "M. Maybury",
                        "slug": "M.-Maybury",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Maybury",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maybury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37305767"
                        ],
                        "name": "Andrew Merlino",
                        "slug": "Andrew-Merlino",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Merlino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Merlino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3253348"
                        ],
                        "name": "James K. Rayson",
                        "slug": "James-K.-Rayson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rayson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James K. Rayson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 377
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18048683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8671aa50ceee1c1de785dfb172a8cb0ade957066",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports the development of a broadcast news video corpora and novel techniques to automatically segment stories, extract proper names, and visualize associated metadata. We report story segmentation and proper name extraction results using an information retrieval inspired evaluation methodology, measuring the precision and recall performance of our techniques. We briefly describe our implementation of a Broadcast News Analysis (BNA \u2019~) system and an associated viewer, Broadcast News Navigator (BNNTM). We point to current efforts toward more robust processing using multistream analysis on imagery, audio, and closed-caption streams and future efforts in automatic video summarization and user-tailored presentation generation."
            },
            "slug": "Segmentation,-Content-Extraction-and-Visualization-Maybury-Merlino",
            "title": {
                "fragments": [],
                "text": "Segmentation, Content Extraction and Visualization of Broadcast News Video using Multistream Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The development of a broadcast news video corpora and novel techniques to automatically segment stories, extract proper names, and visualize associated metadata are reported, measuring the precision and recall performance of the techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856656"
                        ],
                        "name": "Hae-Kwang Kim",
                        "slug": "Hae-Kwang-Kim",
                        "structuredName": {
                            "firstName": "Hae-Kwang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hae-Kwang Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8081258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455a0f0e6d402648da33930fb39ef5587aab0e3",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed. Target frames are selected at fixed time intervals from shots detected by a scene-change detection method. For each selected frame, segmentation by color clustering is performed around color peaks using a color histogram. For each color plane, text-lines are detected using heuristics, and the temporal and spatial position and the text-image of each text-line are stored in a database. Experimental results for text detection in video images and the performance of the method are reported for various video documents. A user interface for text-image based browsing is designed for direct content-based access to video documents, and other applications are discussed."
            },
            "slug": "Efficient-Automatic-Text-Location-Method-and-and-of-Kim",
            "title": {
                "fragments": [],
                "text": "Efficient Automatic Text Location Method and Content-Based Indexing and Structuring of Video Database"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Multi-frame integration based caption enhancement techniques, such as the minimum pixel search [27] and frame averaging method [18], can be employed to reduce the influence of the complex background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "When temporal information are utilized, they are used only for text enhancement through multiframe averaging [18] or time-based minimum pixel search [15],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Some multiple threshold methods have been proposed [2], [18], [35], [36], including adaptive thresholding and floating three threshold method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15408079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b4f79ebb7eaeb0c374da94303ff3fe3a14be63",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of text extraction, enhancement and recognition in digital video. Compared with optical character recognition (OCR) from document images, text extraction and recognition in digital video presents several new challenges. First, the text in video is often embedded in complex backgrounds, making text extraction and separation difficult. Second, image data contained in video frames is often digitized and/or subsampled at a much lower resolution than is typical for document images. As a result, most commercial OCR software can not recognize text extracted from video. We have implemented a hybrid wavelet/neural network segmenter to extract text regions and use a two stage enhancement scheme prior to recognition. First, we use Shannon interpolation to raise the image resolution, and second we postprocess the block with normal/inverse text classification and adaptive thresholding. Experimental results show that our text extraction scheme can extract both scene text and graphical text robustly and reasonable OCR results are achieved after enhancement."
            },
            "slug": "Text-Extraction,-Enhancement-and-OCR-in-Digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Text Extraction, Enhancement and OCR in Digital Video"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results show that the text extraction scheme can extract both scene text and graphical text robustly and reasonable OCR results are achieved after enhancement."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115439468"
                        ],
                        "name": "Yuntao Cui",
                        "slug": "Yuntao-Cui",
                        "structuredName": {
                            "firstName": "Yuntao",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntao Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2459300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e75b09dff3656243fde5cacf9baf1c42a4ea972",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images. We model the extraction of characters as a Markov random field (MRF). With the MRF modeling, the extraction of characters is formulated as the problem of maximizing the a posteriori probability based on given prior and observations. A genetic algorithm with local greedy mutation operator is employed to optimize the objective function. Experiments and comparison study were conducted. It is shown that our approach provides better performance than other single frame methods."
            },
            "slug": "Character-extraction-of-license-plates-from-video-Cui-Huang",
            "title": {
                "fragments": [],
                "text": "Character extraction of license plates from video"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown that this approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images provides better performance than other single frame methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52819119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "390d27ae3ffe41ff8781f09796c8b0c0f1dfd8fa",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital video is rapidly becoming important for education, entertainment and a host of multimedia applications. With the size of the video collections growing to thousands of hours, technology is needed to effectively browse segments in a short time without losing the content of the video. We propose a method to extract the significant audio and video information and create a skim video which represents a very short synopsis of the original. The goal of this work is to show the utility of integrating language and image understanding techniques for video skimming by extraction of significant information, such as specific objects, audio keywords and relevant video structure. The resulting skim video is much shorter; where compaction is as high as 20:1, and yet retains the essential content of the original segment. We have conducted a user-study to test the content summarization and effectiveness of the skim as a browsing tool."
            },
            "slug": "Video-skimming-and-characterization-through-the-of-Smith-Kanade",
            "title": {
                "fragments": [],
                "text": "Video skimming and characterization through the combination of image and language understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The goal of this work is to show the utility of integrating language and image understanding techniques for video skimming by extraction of significant information, such as specific objects, audio keywords and relevant video structure."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685742"
                        ],
                        "name": "Y. Aslandogan",
                        "slug": "Y.-Aslandogan",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Aslandogan",
                            "middleNames": [
                                "Alp"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aslandogan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642131"
                        ],
                        "name": "Clement T. Yu",
                        "slug": "Clement-T.-Yu",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Yu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement T. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15845719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d50337dabf3e71fba0d4283ad9837fd63cf634",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Storage and retrieval of multimedia has become a requirement for many contemporary information systems. These systems need to provide browsing, querying, navigation, and, sometimes, composition capabilities involving various forms of media. In this survey, we review techniques and systems for image and video retrieval. We first look at visual features for image retrieval such as color, texture, shape, and spatial relationships. The indexing techniques are discussed for these features. Nonvisual features include captions, annotations, relational attributes, and structural descriptions. Temporal aspects of video retrieval and video segmentation are discussed next. We review several systems for image and video retrieval including research, commercial, and World Wide Web-based systems. We conclude with an overview of current challenges and future trends for image and video retrieval."
            },
            "slug": "Techniques-and-Systems-for-Image-and-Video-Aslandogan-Yu",
            "title": {
                "fragments": [],
                "text": "Techniques and Systems for Image and Video Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This survey reviews techniques and systems for image and video retrieval including research, commercial, and World Wide Web-based systems and concludes with an overview of current challenges and future trends."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Even though some address text detection in video frames [1], [5], [11]\u2013[13], [16], [20], [34], they usually treat each video frame as an independent image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44601365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84b397a325afb12f768080705f5bb7b719397f0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the development and implementation of a new algorithm for extracting text in digitized color video. The algorithm operates by locating potential text line segments from horizontal scan lines. Detected text line segments are expanded or combined with text line segments from adjacent scan lines to form larger text blocks, which are then subject to filtering and refinement. Text pixels within text blocks are then found using bi-color clustering and connected-component analysis. Morphological contour smoothing and morphological resolution enhancement algorithms are then applied to the detected binary texts to enhance their visual quality. The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text. A variety of color images digitized from broadcast television are used to test the algorithm and excellent performance result was obtained."
            },
            "slug": "A-robust-algorithm-for-text-extraction-in-color-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A robust algorithm for text extraction in color video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2010[29], [ 32 ], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8345108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4d170b81ffc895e5b7960040a3f44a044d6bb8",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources."
            },
            "slug": "Intelligent-Access-to-Digital-Video:-Informedia-Wactlar-Kanade",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video: Informedia Project"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval, and focused the work on two corpuses."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678564"
                        ],
                        "name": "Y. Ariki",
                        "slug": "Y.-Ariki",
                        "structuredName": {
                            "firstName": "Yasuo",
                            "lastName": "Ariki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ariki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054406191"
                        ],
                        "name": "Katsumi Matsuura",
                        "slug": "Katsumi-Matsuura",
                        "structuredName": {
                            "firstName": "Katsumi",
                            "lastName": "Matsuura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katsumi Matsuura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463613"
                        ],
                        "name": "S. Takao",
                        "slug": "S.-Takao",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Takao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Takao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Some multiple threshold methods have been proposed [2], [18], [35], [36], including adaptive thresholding and floating three threshold method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Since characters are composed of line segments, text regions contain rich edge information [1], [2], [8], [16], [27], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16467520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f9aa875dbc00f5c643f965a6e7d89444c04d106",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this study is to automatically extract telop and flip characters. The process starts with the extraction of stable frame sections including the telop and flip characters. The next process is character region extraction and we propose the application of local line density to discriminate the telop characters from other elements such as lines and symbols. The final process is telop character extraction and we show the effectiveness of floating adaptive three level thresholding (FATLT) which thresholds the image intensity into three levels and finally binarizes the character regions precisely even at low contrast, taking topological relationships between characters and their background into consideration."
            },
            "slug": "Telop-and-flip-frame-detection-and-character-from-Ariki-Matsuura",
            "title": {
                "fragments": [],
                "text": "Telop and flip frame detection and character extraction from TV news articles"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The effectiveness of floating adaptive three level thresholding (FATLT) which thresholds the image intensity into three levels and finally binarizes the character regions precisely even at low contrast is shown, taking topological relationships between characters and their background into consideration."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198175"
                        ],
                        "name": "T. Tasdizen",
                        "slug": "T.-Tasdizen",
                        "structuredName": {
                            "firstName": "Tolga",
                            "lastName": "Tasdizen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tasdizen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10184381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bad076c5f89ac43026a2b5fca648d488f54f45e",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of locating and extracting text from WWW images. A previous algorithm based on color clustering and connected components analysis works well as long as the color of each character is relatively uniform and the typography is fairly simple. It breaks down quickly, however, when these assumptions are violated. In this paper, we describe more robust techniques for dealing with this challenging problem. We present an improved color clustering algorithm that measures similarity based on both RGB and spatial proximity. Layout analysis is also incorporated to handle more complex typography. THese changes significantly enhance the performance of our text detection procedure."
            },
            "slug": "Finding-text-in-color-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Finding text in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An improved color clustering algorithm that measures similarity based on both RGB and spatial proximity is presented, and layout analysis is also incorporated to handle more complex typography."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 410,
                                "start": 406
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Texture features extracted through multichannel processing [19], [36], spatial variance computing [39], and neural networks [12] have been used to detect text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Some multiple threshold methods have been proposed [2], [18], [35], [36], including adaptive thresholding and floating three threshold method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": true,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267020"
                        ],
                        "name": "S. Kurakake",
                        "slug": "S.-Kurakake",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Kurakake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurakake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33852772"
                        ],
                        "name": "H. Kuwano",
                        "slug": "H.-Kuwano",
                        "structuredName": {
                            "firstName": "Hidetaka",
                            "lastName": "Kuwano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuwano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080357"
                        ],
                        "name": "K. Odaka",
                        "slug": "K.-Odaka",
                        "structuredName": {
                            "firstName": "Kazumi",
                            "lastName": "Odaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Odaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "When temporal information are utilized, they are used only for text enhancement through multiframe averaging [18] or time-based minimum pixel search [15],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46142323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdf40964371c7543aaf5bdfac5326f69d5a76451",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An indexing method for content-based image retrieval by using textual information in video is proposed. Indices extracted from textual information make it possible to retrieve video data by a conceptual query, such as a topic or a person's name, and organize flat video data into structured video data based on its conceptual content. To this end, we developed a text extraction and recognition algorithm and a visual feature matching algorithm for indexing and organizing video data at a conceptual level. The text extraction and recognition algorithm identifies frames in the video which contain text, extracts the text regions from the frame, finds text lines, and recognizes characters in the text line. The visual feature matching algorithm measures the similarity of frames containing text of find frames with similar appearances text, which can be considered topic change frames. Experiments using real video data showed that our algorithm can index textual information reliably and that it has good potential as a tool for making content-based conceptual-level queries to video databases."
            },
            "slug": "Recognition-and-visual-feature-matching-of-text-in-Kurakake-Kuwano",
            "title": {
                "fragments": [],
                "text": "Recognition and visual feature matching of text region in video for conceptual indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An indexing method for content-based image retrieval by using textual information in video by developing a text extraction and recognition algorithm and a visual feature matching algorithm for indexing and organizing video data at a conceptual level."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111286789"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109167204"
                        ],
                        "name": "Zhu Liu",
                        "slug": "Zhu-Liu",
                        "structuredName": {
                            "firstName": "Zhu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14833627"
                        ],
                        "name": "A. Rosenberg",
                        "slug": "A.-Rosenberg",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387879"
                        ],
                        "name": "D. Gibbon",
                        "slug": "D.-Gibbon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibbon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413848"
                        ],
                        "name": "B. Shahraray",
                        "slug": "B.-Shahraray",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahraray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shahraray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15741191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b70f9900c5c46c2f68a883ea73fe5dba4c76bf9a",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of generating semantically meaningful content by integrating information from different media. The goal is to automatically construct a compact yet meaningful abstraction of the multimedia data that can serve as an effective index table, allowing users to browse through large amounts of data in a non-linear fashion with flexibility, efficiency, and confidence. We propose an integrated solution in the context of broadcast news that simultaneously utilizes cues from video, audio, and test to achieve the goal. Some experimental results are presented and discussed."
            },
            "slug": "Automated-generation-of-news-content-hierarchy-by-Huang-Liu",
            "title": {
                "fragments": [],
                "text": "Automated generation of news content hierarchy by integrating audio, video, and text information"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An integrated solution in the context of broadcast news that simultaneously utilizes cues from video, audio, and test to achieve the goal of automatically constructing a compact yet meaningful abstraction of the multimedia data."
            },
            "venue": {
                "fragments": [],
                "text": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40536331"
                        ],
                        "name": "H. Ueda",
                        "slug": "H.-Ueda",
                        "structuredName": {
                            "firstName": "Hirotada",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3291936"
                        ],
                        "name": "T. Miyatake",
                        "slug": "T.-Miyatake",
                        "structuredName": {
                            "firstName": "Takafumi",
                            "lastName": "Miyatake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Miyatake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2916406"
                        ],
                        "name": "S. Yoshizawa",
                        "slug": "S.-Yoshizawa",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Yoshizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yoshizawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "To detect the shot boundaries, we use two popular frame difference metrics, the histogram difference metric (HDM) and the spatial difference metric (SDM), to measure the dissimilarity between the adjoining frame pair [23], [31], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12676281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e39eafc656ae89a13ef9d5e1515da3e2b22daf",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach to achieving a natural-motion-picture dedicated multi-media authoring system is proposed. The main point of this approach, discussed in this paper, is that the user\u2019s environment or interface is improved to encourage user\u2019s creativity, with image processing and recognition technology. According to the discussion, a prototype motion picture authoring system dtat has several image-processing functions is developed. The newly developed functions include object extraction of the picture, semi-automatic visualization of motion pictures structure, and certain descriptions of the scene. Result of using the prototype shows the appropriateness of the proposed approach."
            },
            "slug": "IMPACT:-an-interactive-natural-motion-picture-Ueda-Miyatake",
            "title": {
                "fragments": [],
                "text": "IMPACT: an interactive natural-motion-picture dedicated multimedia authoring system"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new approach to achieving a natural-motion-picture dedicated multi-media authoring system is proposed, which has several image-processing functions that include object extraction of the picture, semi-automatic visualization of motion pictures structure, and certain descriptions of the scene."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Some multiple threshold methods have been proposed [2], [18], [35], [36], including adaptive thresholding and floating three threshold method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Texture features extracted through multichannel processing [19], [36], spatial variance computing [39], and neural networks [12] have been used to detect text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 188
                            }
                        ],
                        "text": "After color quantization, the connected components of monotonous color that obey certain size, shape, and spatial alignment constraints are extracted as text [11], [13], [17], [20], [30], [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29853292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4af75831ed098d9fea02507f36cdbc38852fe6",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a substantial interest in retrieving images from a large database using the textual information contained in the images. An algorithm which will automatically locate the textual regions in the input image will facilitate this task; the optical character recognizer can then be applied to only those regions of the image which contain text. We present a method for automatically locating text in complex color images. The algorithm first finds the approximate locations of text lines using horizontal spatial variance, and then extracts text components in these boxes using color segmentation. The proposed method has been used to locate text in compact disc (CD) and book cover images, as well as in the images of traffic scenes captured by a video camera. Initial results are encouraging and suggest that these algorithms can be used in image retrieval applications."
            },
            "slug": "Locating-text-in-complex-color-images-Zhong-Karu",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed algorithm has been used to locate text in compact disc and book cover images, as well as in the images of traffic scenes captured by a video camera, and initial results suggest that these algorithms can be used in image retrieval applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36058334"
                        ],
                        "name": "H. Kronenberg",
                        "slug": "H.-Kronenberg",
                        "structuredName": {
                            "firstName": "Heino",
                            "lastName": "Kronenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kronenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "After color quantization, the connected components of monotonous color that obey certain size, shape, and spatial alignment constraints are extracted as text [11], [13], [17], [20], [30], [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14868685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ba04958180cb158da0fe02a8599a7e301844456",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed. To reduce the amount of small variations in color, a clustering algorithm is applied in a preprocessing step. Two methods have been developed for extracting text hypotheses. One is based on a top-down analysis using successive splitting of image regions. The other is a bottom-up region growing algorithm. The results of both methods are combined to robustly distinguish between text and non-text elements. Text elements are binarized using automatically extracted information about text color. The binarized text regions can be used as input for a conventional OCR module. Results are shown for parts of book and journal covers of different complexity. The proposed method is not restricted to cover pages, but can be applied to the extraction of text from other types of color images as well."
            },
            "slug": "Identification-of-text-on-colored-book-and-journal-Sobottka-Bunke",
            "title": {
                "fragments": [],
                "text": "Identification of text on colored book and journal covers"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed and a clustering algorithm is applied in a preprocessing step to reduce the amount of small variations in color."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "After color quantization, the connected components of monotonous color that obey certain size, shape, and spatial alignment constraints are extracted as text [11], [13], [17], [20], [30], [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721679"
                        ],
                        "name": "Sang-Kyoon Kim",
                        "slug": "Sang-Kyoon-Kim",
                        "structuredName": {
                            "firstName": "Sang-Kyoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Kyoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145132323"
                        ],
                        "name": "D. Kim",
                        "slug": "D.-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Wook"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9889790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53d015192ffef21874606d60b8e3666d05acb399",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting a license plate is an important stage in automatic vehicle identification. It is very difficult because vehicle images are usually degraded and processing the images is computationally intensive. We propose a new method to extract the plate region using a distributed genetic algorithm. The algorithm offers robustness in dealing with deformation of vehicle images and inherent parallelism to improve the processing time. A test with seventy images shows an extraction rate of 92.8%, working well within real world situations. This results suggest that the proposed method is pertinent to be put into practical use."
            },
            "slug": "A-recognition-of-vehicle-license-plate-using-a-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A recognition of vehicle license plate using a genetic algorithm based segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new method to extract the plate region using a distributed genetic algorithm that offers robustness in dealing with deformation of vehicle images and inherent parallelism to improve the processing time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329435"
                        ],
                        "name": "G. Piccioli",
                        "slug": "G.-Piccioli",
                        "structuredName": {
                            "firstName": "Giulia",
                            "lastName": "Piccioli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piccioli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7409396"
                        ],
                        "name": "E. D. Micheli",
                        "slug": "E.-D.-Micheli",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Micheli",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741785"
                        ],
                        "name": "P. Parodi",
                        "slug": "P.-Parodi",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Parodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074426"
                        ],
                        "name": "M. Campani",
                        "slug": "M.-Campani",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Campani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Campani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5372532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19aca01bafe52131ec95473dac105889aa6a4d33",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-method-for-road-sign-detection-and-Piccioli-Micheli",
            "title": {
                "fragments": [],
                "text": "Robust method for road sign detection and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Among these techniques, video caption based methods have attracted particular attention due to the rich content information contained in caption text [1], [2], [6], [9], [11]\u2013[13], [15], [16], [19], [20], [27], [33], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27678281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95dffcc92bda88d9f4f5b112d100f43951745b8c",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information. Multiscale feature vectors are classified locally using a neural network to allow soft/fuzzy multi-class membership assignments. Segmentation is performed by integrating soft local decision vectors to reduce their \"ambiguities\"."
            },
            "slug": "Multiscale-Segmentation-of-Unstructured-Document-Etemad-Doermann",
            "title": {
                "fragments": [],
                "text": "Multiscale Segmentation of Unstructured Document Pages Using Soft Decision Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708545"
                        ],
                        "name": "N. Pal",
                        "slug": "N.-Pal",
                        "structuredName": {
                            "firstName": "Nikhil",
                            "lastName": "Pal",
                            "middleNames": [
                                "Ranjan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4929024"
                        ],
                        "name": "J. Bezdek",
                        "slug": "J.-Bezdek",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bezdek",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bezdek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48540610"
                        ],
                        "name": "E. Tsao",
                        "slug": "E.-Tsao",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Tsao",
                            "middleNames": [
                                "Chen-Kuo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tsao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "We choose a self-organizing neural network as the classifier because the network can adapt to different datasets fairly easily with little human intervention, which is critical toward develping a fully automated system [22], [25], [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Here we adopt Pal\u2019s scheme and take as , where is the maximum number of iterations that the learning process is allowed to execute andis the initial value of the learning parameter [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20433379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8bc69dc27612a0b9e96e802448402e2c828739d",
            "isKey": false,
            "numCitedBy": 435,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The relationship between the sequential hard c-means (SHCM) and learning vector quantization (LVQ) clustering algorithms is discussed. The impact and interaction of these two families of methods with Kohonen's self-organizing feature mapping (SOFM), which is not a clustering method but often lends ideas to clustering algorithms, are considered. A generalization of LVQ that updates all nodes for a given input vector is proposed. The network attempts to find a minimum of a well-defined objective function. The learning rules depend on the degree of distance match to the winner node; the lesser the degree of match with the winner, the greater the impact on nonwinner nodes. Numerical results indicate that the terminal prototypes generated by this modification of LVQ are generally insensitive to initialization and independent of any choice of learning coefficient. IRIS data obtained by E. Anderson's (1939) is used to illustrate the proposed method. Results are compared with the standard LVQ approach."
            },
            "slug": "Generalized-clustering-networks-and-Kohonen's-Pal-Bezdek",
            "title": {
                "fragments": [],
                "text": "Generalized clustering networks and Kohonen's self-organizing scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A generalization of LVQ that updates all nodes for a given input vector that is generally insensitive to initialization and independent of any choice of learning coefficient is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "analysis (PCA) method [7]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62359231,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1277592f221ea26fa1d2321a38b64c58b33d75b",
            "isKey": false,
            "numCitedBy": 8009,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises."
            },
            "slug": "Introduction-to-Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition, which is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40190676"
                        ],
                        "name": "S. Mitra",
                        "slug": "S.-Mitra",
                        "structuredName": {
                            "firstName": "Sushmita",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736103"
                        ],
                        "name": "S. Pal",
                        "slug": "S.-Pal",
                        "structuredName": {
                            "firstName": "Sankar",
                            "lastName": "Pal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "We choose a self-organizing neural network as the classifier because the network can adapt to different datasets fairly easily with little human intervention, which is critical toward develping a fully automated system [22], [25], [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42882079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcfdb36703bda7e3ced9d02f20f3efca93429b86",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a self-organizing artificial neural network, based on Kohonen's model of self-organization, which is capable of handling fuzzy input and of providing fuzzy classification. Unlike conventional neural net models, this algorithm incorporates fuzzy set-theoretic concepts at various stages. The input vector consists of membership values for linguistic properties along with some contextual class membership information which is used during self-organization to permit efficient modeling of fuzzy (ambiguous) patterns. A new definition of gain factor for weight updating is proposed. An index of disorder involving mean square distance between the input and weight vectors is used to determine a measure of the ordering of the output space. This controls the number of sweeps required in the process. Incorporation of the concept of fuzzy partitioning allows natural self-organization of the input data, especially when they have ill-defined boundaries. The output of unknown test patterns is generated in terms of class membership values. Incorporation of fuzziness in input and output is seen to provide better performance as compared to the original Kohonen model and the hard version. The effectiveness of this algorithm is demonstrated on the speech recognition problem for various network array sizes, training sets and gain factors. >"
            },
            "slug": "Self-organizing-neural-network-as-a-fuzzy-Mitra-Pal",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural network as a fuzzy classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "ASelf-organizing artificial neural network, based on Kohonen's model of self-organization, which is capable of handling fuzzy input and of providing fuzzy classification and the effectiveness of this algorithm is demonstrated on the speech recognition problem for various network array sizes, training sets and gain factors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127383383"
                        ],
                        "name": "David Zhang",
                        "slug": "David-Zhang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736103"
                        ],
                        "name": "S. Pal",
                        "slug": "S.-Pal",
                        "structuredName": {
                            "firstName": "Sankar",
                            "lastName": "Pal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 231
                            }
                        ],
                        "text": "We choose a self-organizing neural network as the classifier because the network can adapt to different datasets fairly easily with little human intervention, which is critical toward develping a fully automated system [22], [25], [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16377070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5541a36b9ab67d06f4faa3d3ac084a04e6551932",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A system design methodology for fuzzy clustering neural networks (FCNs) is presented. This methodology emphasizes coordination between FCN model definition, architectural description, and systolic implementation. Two mapping strategies both from FCN model to system architecture and from the given architecture to systolic arrays are described. The effectiveness of the methodology is illustrated by: 1) applying the design to an effective FCN model; 2) developing the corresponding parallel architecture with special feedforward and feedback paths; and 3) building the systolic array (SA) suitable for very large scale integration (VLSI) implementation."
            },
            "slug": "A-fuzzy-clustering-neural-networks-(FCNs)-system-Zhang-Pal",
            "title": {
                "fragments": [],
                "text": "A fuzzy clustering neural networks (FCNs) system design methodology"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A system design methodology for fuzzy clustering neural networks (FCNs) emphasizes coordination between FCN model definition, architectural description, and systolic implementation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64321642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ac243459eb0a0ab8a102ec7137ba1ad7f367e9a",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Statistical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153294902"
                        ],
                        "name": "Akio Nagasaka",
                        "slug": "Akio-Nagasaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Nagasaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akio Nagasaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865865"
                        ],
                        "name": "Yuzuru Tanaka",
                        "slug": "Yuzuru-Tanaka",
                        "structuredName": {
                            "firstName": "Yuzuru",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzuru Tanaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "To detect the shot boundaries, we use two popular frame difference metrics, the histogram difference metric (HDM) and the spatial difference metric (SDM), to measure the dissimilarity between the adjoining frame pair [23], [31], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44976007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257ff5ae00fb89e0f51b8d5c15ee25db409ccf32",
            "isKey": false,
            "numCitedBy": 838,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Video-Indexing-and-Full-Video-Search-for-Nagasaka-Tanaka",
            "title": {
                "fragments": [],
                "text": "Automatic Video Indexing and Full-Video Search for Object Appearances"
            },
            "venue": {
                "fragments": [],
                "text": "VDB"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Texture features extracted through multichannel processing [19], [36], spatial variance computing [39], and neural networks [12] have been used to detect text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "After color quantization, the connected components of monotonous color that obey certain size, shape, and spatial alignment constraints are extracted as text [11], [13], [17], [20], [30], [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "97) received the B.S. degree from Zhengzhou University, China and the Ph.D. degree from the Technical University of Denmark, both in electrical engineering"
            },
            "venue": {
                "fragments": [],
                "text": "97) received the B.S. degree from Zhengzhou University, China and the Ph.D. degree from the Technical University of Denmark, both in electrical engineering"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPATIAL-TEMPORAL APPROACH FOR VIDEO CAPTION DETECTION 971"
            },
            "venue": {
                "fragments": [],
                "text": "SPATIAL-TEMPORAL APPROACH FOR VIDEO CAPTION DETECTION 971"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "To detect the shot boundaries, we use two popular frame difference metrics, the histogram difference metric (HDM) and the spatial difference metric (SDM), to measure the dissimilarity between the adjoining frame pair [23], [31], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic partitioning of video"
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Syst.  , vol. 1, pp. 10\u201328, 1993."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Agnihotri and N . Dimitrova , \u201c Text detection for video analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "96) received the B.S. degree in 1990 from the University of Science and Technology of China, Hefei, China, and the M.S. degree in 1991 from the University of Rochester"
            },
            "venue": {
                "fragments": [],
                "text": "He received the Ph.D. degree in 1996 from the Massachusetts Institute of Technology (MIT), Cambridge. He is currently an Assistant Professor in the Department of Information Engineering of the Chinese University of Hong Kong. His research interests include video processing and pattern recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 387,
                                "start": 383
                            }
                        ],
                        "text": "To construct such systems, both low-level features such as object shape, region intensity, color, texture, motion descriptors, audio measurements, and high-level techniques such as human face detection, speaker identification, and character recognition have been studied for indexing and retrieving image and video information in recent years [3], [4], [10], [11], [13], [19], [21], [24], [27]\u2013[29], [32], [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Many of the existing works deal with text detection in static images [8], [14], [17], [24], [26], [30], [35], [36], [39], [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing characters in scene images,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. Pattern Anal. Machine Intell.  ,"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 23,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-spatial-temporal-approach-for-video-caption-and-Tang-Gao/1697bfbb2c701fba6032d63309a904d21b4f0d09?sort=total-citations"
}