{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752875"
                        ],
                        "name": "Yelong Shen",
                        "slug": "Yelong-Shen",
                        "structuredName": {
                            "firstName": "Yelong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yelong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935910"
                        ],
                        "name": "G. Mesnil",
                        "slug": "G.-Mesnil",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Mesnil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mesnil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207210855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "941f318e41147773ae69d9da4f8de9b8dbea70f4",
            "isKey": false,
            "numCitedBy": 563,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn low-dimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local fea-tures in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, us-ing stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other se-mantic models, which were state-of-the-art in retrieval performance prior to this work."
            },
            "slug": "Learning-semantic-representations-using-neural-for-Shen-He",
            "title": {
                "fragments": [],
                "text": "Learning semantic representations using convolutional neural networks for web search"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper presents a series of new latent semantic models based on a convolutional neural network to learn low-dimensional semantic vectors for search queries and Web documents that significantly outperforms other se-mantic models in retrieval performance."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421691"
                        ],
                        "name": "Po-Sen Huang",
                        "slug": "Po-Sen-Huang",
                        "structuredName": {
                            "firstName": "Po-Sen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Sen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46819684"
                        ],
                        "name": "Larry Heck",
                        "slug": "Larry-Heck",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Heck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Larry Heck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "In order to make the results comparable, we reimplement these models following the descriptions in [11][15][20][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "DSSM (Row 12 and 13) is the best variant of DSSM proposed in [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Among them, the DSSM proposed in [20] is most relevant to our work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "The clickthrough data consist of a list of queries and their clicked documents, similar to the clickthrough data have been used in earlier studies, such as [12][15][20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [20], we first convert the semantic relevance score between a query and a positive document to the posterior probability of that document given the query through softmax:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Therefore, we resort to a technique called word hashing proposed in [20], which represents a word by a letter-trigram vector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "More recently, semantic modeling methods based on neural networks have also been proposed for information retrieval (IR) [16][32][20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The data set used in [20] is encoded in a bag-of-words representation format and thus not suitable for this study (personal communication)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "A Deep Structured Semantic Model (DSSM) for Web search was proposed in [20], which is reported to give very strong IR performance on a large-scale web search task when clickthrough data are exploited as weakly-supervised information in training the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "Deep architectures have been shown to be highly effective in discovering from training data the hidden structures and features at different levels of abstraction useful for a variety of tasks [32][18][20][37][7][34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8384258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdb813d8b927bdd21ae1858cafa6c34b66a36268",
            "isKey": false,
            "numCitedBy": 1451,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper."
            },
            "slug": "Learning-deep-structured-semantic-models-for-web-Huang-He",
            "title": {
                "fragments": [],
                "text": "Learning deep structured semantic models for web search using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them are developed."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "BLTM (Row 7) is the best performer among different versions of the bilingual topic models described in [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 227
                            }
                        ],
                        "text": "Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (BLTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "LDA gives slightly better results than the PLSA, and LDA with 500 topics significantly outperforms BM25 and ULM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "In order to make the results comparable, we reimplement these models following the descriptions in [11][15][20][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "It is shown that, by learning the model on clicked query-title pairs, the BLTM gives superior performance over PLSA [15]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "PLSA (Rows 3 and 4) is our implementation of the model proposed in [19], and was trained on documents only (i.e., the title side of the query/clicked-title pairs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "The clickthrough data consist of a list of queries and their clicked documents, similar to the clickthrough data have been used in earlier studies, such as [12][15][20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Recent studies show that this approach is highly effective when large amounts of clickthrough data are available for training [12][15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "We see that using clickthrough data for model training leads to improvement over PLSA and LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "The number of topics is set to 100 as in [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Different from [19], our version of PLSA was learned using MAP estimation as in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "The second set includes a set of state-of-the-art latent semantic models which are learned either on documents only in an unsupervised manner (PLSA and LDA) or on clickthrough data in a supervised way (BLTM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "They include Probabilistic Latent Semantic Analysis (PLSA) [19] and its extensions such as Latent Dirichlet Allocation (LDA) [4][39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "PLSA assumes that each document has a multinomial distribution over topics (called the document-topic distribution), where each of the topics is in turn of a multinomial distribution over words (called the topic-word distribution)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "In the next subsection, we will present an indepth analysis on the performance of the CLSM. # Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.305 0.328 0.388\n2 ULM 0.304 0.327 0.385 3 PLSA (T=100) 0.305 0.335 \u03b1 0.402 \u03b1 4 PLSA (T=500) 0.308 0.337 \u03b1 0.402 \u03b1 5 LDA (T=100) 0.308 0.339 \u03b1 0.403 \u03b1 6 LDA (T=500) 0.310 \u03b1 0.339 \u03b1 0.405 \u03b1 7 BLTM 0.316 \u03b1 0.344 \u03b1 0.410 \u03b1 8 MRF 0.315 \u03b1 0.341 \u03b1 0.409 \u03b1 9 LCE 0.312 \u03b1 0.337 \u03b1 0.407 \u03b1\n10 WTM 0.315 \u03b1 0.342 \u03b1 0.411 \u03b1 11 PTM (maxlen = 3) 0.319 \u03b1 0.347 \u03b1 0.413 \u03b1 12 DSSM ( 4) 0.320 \u03b1 0.355 \u03b1\u03b2 0.431 \u03b1\u03b2 13 DSSM ( 50) 0.327 \u03b1\u03b2 0.363 \u03b1\u03b2 0.438 \u03b1\u03b2 14 CLSM ( 4) 0.342 \u03b1\u03b2\u03b3 0.374 \u03b1\u03b2\u03b3 0.447 \u03b1\u03b2\u03b3 15 CLSM ( ) 0.348 \u03b1\u03b2\u03b3 0.379 \u03b1\u03b2\u03b3 0.449\u03b1\u03b2\u03b3\nTable 5: Comparative results with the previous state of the art approaches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "The CLSM also outperforms the state-of-the-art topic model based approaches (i.e., PLSA, LDA, and BLTM) with a statistically significant margin from 3.2% to 4.0%."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "For example, a generative model called Bi-Lingual Topic Model (BLTM) is proposed for Web search in [15], which assumes that a query and its clicked document share the same document-topic distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16213872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83e89037edfa113cf15b01a218cfcf12c6463bcb",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art."
            },
            "slug": "Clickthrough-based-latent-semantic-models-for-web-Gao-Toutanova",
            "title": {
                "fragments": [],
                "text": "Clickthrough-based latent semantic models for web search"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR) are presented."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417334"
                        ],
                        "name": "Michael Gamon",
                        "slug": "Michael-Gamon",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gamon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Gamon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 327
                            }
                        ],
                        "text": "Extended from our previous work [20] [33], the CLSM and its variations have also been demonstrated giving superior performance on a range of natural language processing tasks beyond information retrieval, including machine translation [13], semantic parsing and question answering [40], entity search and online recommendation [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2141094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7cf9047604f39e517951d129b2b3eecf9e1cfb",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a deep semantic similarity model (DSSM), a special type of deep neural networks designed for text analysis, for recommending target documents to be of interest to a user based on a source document that she is reading. We observe, identify, and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents, which we collect from commercial Web browser logs. The DSSM is trained on millions of Web transitions, and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized. The effectiveness of the DSSM is demonstrated using two interestingness tasks: automatic highlighting and contextual entity search. The results on large-scale, real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks, outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models."
            },
            "slug": "Modeling-Interestingness-with-Deep-Neural-Networks-Gao-Pantel",
            "title": {
                "fragments": [],
                "text": "Modeling Interestingness with Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results on large-scale, real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks, outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815447"
                        ],
                        "name": "Michael Bendersky",
                        "slug": "Michael-Bendersky",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bendersky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Bendersky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1305488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33527fc66a8f60e2d406901b8b34ca66b65ed83c",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The majority of the current information retrieval models weight the query concepts (e.g., terms or phrases) in an unsupervised manner, based solely on the collection statistics. In this paper, we go beyond the unsupervised estimation of concept weights, and propose a parameterized concept weighting model. In our model, the weight of each query concept is determined using a parameterized combination of diverse importance features. Unlike the existing supervised ranking methods, our model learns importance weights not only for the explicit query concepts, but also for the latent concepts that are associated with the query through pseudo-relevance feedback. The experimental results on both newswire and web TREC corpora show that our model consistently and significantly outperforms a wide range of state-of-the-art retrieval models. In addition, our model significantly reduces the number of latent concepts used for query expansion compared to the non-parameterized pseudo-relevance feedback based models."
            },
            "slug": "Parameterized-concept-weighting-in-verbose-queries-Bendersky-Metzler",
            "title": {
                "fragments": [],
                "text": "Parameterized concept weighting in verbose queries"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper goes beyond the unsupervised estimation of concept weights, and proposes a parameterized concept weighting model, which significantly reduces the number of latent concepts used for query expansion compared to the non-parameterized pseudo-relevance feedback based models."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "Deep architectures have been shown to be highly effective in discovering from training data the hidden structures and features at different levels of abstraction useful for a variety of tasks [32][18][20][37][7][34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16021391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top-down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF-IDF we achieve higher accuracy than TF-IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document."
            },
            "slug": "Discovering-Binary-Codes-for-Documents-by-Learning-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Discovering Binary Codes for Documents by Learning Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document is described, which allows more accurate and much faster retrieval than latent semantic analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Top. Cogn. Sci."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "An alternative approach is proposed in [34], where a parsing tree for a given sentence is extracted, which is then mapped to a fixed-length representation using recursive auto-encoders."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "Deep architectures have been shown to be highly effective in discovering from training data the hidden structures and features at different levels of abstraction useful for a variety of tasks [32][18][20][37][7][34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 806709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27e38351e48fe4b7da2775bf94341738bc4da07e",
            "isKey": false,
            "numCitedBy": 1265,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them."
            },
            "slug": "Semantic-Compositionality-through-Recursive-Spaces-Socher-Huval",
            "title": {
                "fragments": [],
                "text": "Semantic Compositionality through Recursive Matrix-Vector Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A recursive neural network model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length and can learn the meaning of operators in propositional logic and natural language is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404233"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 191
                            }
                        ],
                        "text": "Recently, a neural network based DeepMatch model is also proposed to directly capture the correspondence between two short texts without explicitly relying on semantic vector representations [23]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14617645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aba54ea82bf99ed4690d45051f1b25d8b9554b5",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents, etc.). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufficient for capturing the rich structure in the matching process of more complicated objects. In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More specifically, we apply this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models."
            },
            "slug": "A-Deep-Architecture-for-Matching-Short-Texts-Lu-Li",
            "title": {
                "fragments": [],
                "text": "A Deep Architecture for Matching Short Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains and applies this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or relevant answers to a given question."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748169"
                        ],
                        "name": "Kuansan Wang",
                        "slug": "Kuansan-Wang",
                        "structuredName": {
                            "firstName": "Kuansan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuansan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108672800"
                        ],
                        "name": "Xiaolong Li",
                        "slug": "Xiaolong-Li",
                        "structuredName": {
                            "firstName": "Xiaolong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "1 In modern search engines, a Web document is described by multiple fields [12][38], including title, body, anchor text etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6379957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59756f6a8b99813b7055d1100bcac5de314d087f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Web documents are typically associated with many text streams, including the body, the title and the URL that are determined by the authors, and the anchor text or search queries used by others to refer to the documents. Through a systematic large scale analysis on their cross entropy, we show that these text streams appear to be composed in different language styles, and hence warrant respective language models to properly describe their properties. We propose a language modeling approach to Web document retrieval in which each document is characterized by a mixture model with components corresponding to the various text streams associated with the document. Immediate issues for such a mixture model arise as all the text streams are not always present for the documents, and they do not share the same lexicon, making it challenging to properly combine the statistics from the mixture components. To address these issues, we introduce an 'open-vocabulary' smoothing technique so that all the component language models have the same cardinality and their scores can simply be linearly combined. To ensure that the approach can cope with Web scale applications, the model training algorithm is designed to require no labeled data and can be fully automated with few heuristics and no empirical parameter tunings. The evaluation on Web document ranking tasks shows that the component language models indeed have varying degrees of capabilities as predicted by the cross-entropy analysis, and the combined mixture model outperforms the state-of-the-art BM25F based system."
            },
            "slug": "Multi-style-language-model-for-web-scale-retrieval-Wang-Li",
            "title": {
                "fragments": [],
                "text": "Multi-style language model for web scale information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A language modeling approach to Web document retrieval in which each document is characterized by a mixture model with components corresponding to the various text streams associated with the document, and the combined mixture model outperforms the state-of-the-art BM25F based system."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms Algorithms, Experimentation\nKeywords Convolutional Neural Network; Semantic Representation; Web Search"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "It models the whole document collection using a documentterm matrix , where n is the number of documents and d is the number of word types. is first factored into the product of three matrices using singular value decomposition (SVD) as , where the orthogonal matrices and are called term and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1501682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a",
            "isKey": true,
            "numCitedBy": 1260,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantic-hashing-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Semantic hashing"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Approx. Reason."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059835924"
                        ],
                        "name": "Fei Song",
                        "slug": "Fei-Song",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "Recently, within the framework of language models for IR, various approaches that go beyond unigrams have been proposed to capture certain term dependencies, notably the bigram and trigram models [35], the dependence model [11], and the MRF based models [25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8264008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4327f800f44e641033bcf328aa94e3e642c9088",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance."
            },
            "slug": "A-general-language-model-for-information-retrieval-Song-Croft",
            "title": {
                "fragments": [],
                "text": "A general language model for information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new language model for information retrieval is presented, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40282698"
                        ],
                        "name": "Yuanhua Lv",
                        "slug": "Yuanhua-Lv",
                        "structuredName": {
                            "firstName": "Yuanhua",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanhua Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736467"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6678854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebf58dbe6d58845f3253d00e25e8c7280797fc80",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Although many variants of language models have been proposed for information retrieval, there are two related retrieval heuristics remaining \"external\" to the language modeling approach: (1) proximity heuristic which rewards a document where the matched query terms occur close to each other; (2) passage retrieval which scores a document mainly based on the best matching passage. Existing studies have only attempted to use a standard language model as a \"black box\" to implement these heuristics, making it hard to optimize the combination parameters. In this paper, we propose a novel positional language model (PLM) which implements both heuristics in a unified language model. The key idea is to define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of \"soft\" passage retrieval. We propose and study several representative density functions and several different PLM-based document ranking strategies. Experiment results on standard TREC test collections show that the PLM is effective for passage retrieval and performs better than a state-of-the-art proximity-based retrieval model."
            },
            "slug": "Positional-language-models-for-information-Lv-Zhai",
            "title": {
                "fragments": [],
                "text": "Positional language models for information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel positional language model (PLM) is proposed which implements both heuristics in a unified language model and is effective for passage retrieval and performs better than a state-of-the-art proximity-based retrieval model."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [28], the vector representation of a word sequence is computed as a summation of embedding vectors of all words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26053,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619007"
                        ],
                        "name": "Jian-Yun Nie",
                        "slug": "Jian-Yun-Nie",
                        "structuredName": {
                            "firstName": "Jian-Yun",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Yun Nie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "PTM (Row 11) is the phrase-based translation model proposed in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "The translation model-based approach proposed in [12] tries to extract phrase-to-phrase relationships according to clickthrough data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "The clickthrough data consist of a list of queries and their clicked documents, similar to the clickthrough data have been used in earlier studies, such as [12][15][20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [12] a phrase-based translation model was proposed to learn the translation probability of a multi-term phrase in a query given a phrase in a document."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Recent studies show that this approach is highly effective when large amounts of clickthrough data are available for training [12][15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "As an alternative, retrieval methods that directly model phrases (or word n-grams) and term dependencies are proposed in [12][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "1 In modern search engines, a Web document is described by multiple fields [12][38], including title, body, anchor text etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [12], it was shown that the phrase model is more powerful than word translation models [3] because words in the relationships are considered with some context words within a phrase."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "WTM (Row 10) is our implementation of the word-based translation model described in [12], which is a special case of the phrase-based translation model, listed here for comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15274053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "981124696585b4a7db5c217fa4e066e3850a9eae",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Web search is challenging partly due to the fact that search queries and Web documents use different language styles and vocabularies. This paper provides a quantitative analysis of the language discrepancy issue, and explores the use of clickthrough data to bridge documents and queries. We assume that a query is parallel to the titles of documents clicked on for that query. Two translation models are trained and integrated into retrieval models: A word-based translation model that learns the translation probability between single words, and a phrase-based translation model that learns the translation probability between multi-term phrases. Experiments are carried out on a real world data set. The results show that the retrieval systems that use the translation models outperform significantly the systems that do not. The paper also demonstrates that standard statistical machine translation techniques such as word alignment, bilingual phrase extraction, and phrase-based decoding, can be adapted for building a better Web document retrieval system."
            },
            "slug": "Clickthrough-based-translation-models-for-web-from-Gao-He",
            "title": {
                "fragments": [],
                "text": "Clickthrough-based translation models for web search: from word models to phrase models"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper provides a quantitative analysis of the language discrepancy issue, and explores the use of clickthrough data to bridge documents and queries, and demonstrates that standard statistical machine translation techniques can be adapted for building a better Web document retrieval system."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "Extended from our previous work [20] [33], the CLSM and its variations have also been demonstrated giving superior performance on a range of natural language processing tasks beyond information retrieval, including machine translation [13], semantic parsing and question answering [40], entity search and online recommendation [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10473972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34992ceb89e251f2ed5c1a792fbd594bcf8246c2",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations, whose distributed nature enables the sharing of related phrases in their representations. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points."
            },
            "slug": "Learning-Continuous-Phrase-Representations-for-Gao-He",
            "title": {
                "fragments": [],
                "text": "Learning Continuous Phrase Representations for Translation Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619007"
                        ],
                        "name": "Jian-Yun Nie",
                        "slug": "Jian-Yun-Nie",
                        "structuredName": {
                            "firstName": "Jian-Yun",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Yun Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66006716"
                        ],
                        "name": "Guangyuan Wu",
                        "slug": "Guangyuan-Wu",
                        "structuredName": {
                            "firstName": "Guangyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320836"
                        ],
                        "name": "Guihong Cao",
                        "slug": "Guihong-Cao",
                        "structuredName": {
                            "firstName": "Guihong",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guihong Cao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Recently, within the framework of language models for IR, various approaches that go beyond unigrams have been proposed to capture certain term dependencies, notably the bigram and trigram models [35], the dependence model [11], and the MRF based models [25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "In order to make the results comparable, we reimplement these models following the descriptions in [11][15][20][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Our results are consistent with that of [11], showing that phrase models are more effective for retrieval than word models when large amounts of clickthrough data are available for training."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 461312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a9236b1687498e951dc9519a266ee8b4a14062d",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new dependence language modeling approach to information retrieval. The approach extends the basic language modeling approach based on unigram by relaxing the independence assumption. We integrate the linkage of a query as a hidden variable, which expresses the term dependencies within the query as an acyclic, planar, undirected graph. We then assume that a query is generated from a document in two stages: the linkage is generated first, and then each term is generated in turn depending on other related terms according to the linkage. We also present a smoothing method for model parameter estimation and an approach to learning the linkage of a sentence in an unsupervised manner. The new approach is compared to the classical probabilistic retrieval model and the previously proposed language models with and without taking into account term dependencies. Results show that our model achieves substantial and significant improvements on TREC collections."
            },
            "slug": "Dependence-language-model-for-information-retrieval-Gao-Nie",
            "title": {
                "fragments": [],
                "text": "Dependence language model for information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The linkage of a query is integrated as a hidden variable, which expresses the term dependencies within the query as an acyclic, planar, undirected graph, which extends the basic language modeling approach based on unigram by relaxing the independence assumption."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "In [12], it was shown that the phrase model is more powerful than word translation models [3] because words in the relationships are considered with some context words within a phrase."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10837883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15281680463698dca403697bd627af4efebc98a2",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or \"translate\" a given document into a query. To assess the relevance of a document to a user's query, we estimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justification of the \"language modeling\" strategy recently proposed by Ponte and Croft. In a series of experiments on TREC data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques. This prototype system only begins to tap the full potential of translation-based retrieval."
            },
            "slug": "Information-retrieval-as-statistical-translation-Berger-Lafferty",
            "title": {
                "fragments": [],
                "text": "Information retrieval as statistical translation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple, well motivated model of the document-to-query translation process is proposed, and an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents is described."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "In order to make the results comparable, we reimplement these models following the descriptions in [11][15][20][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": ", term n-grams and skipgrams) of the query and the document for ranking, while in [26] a latent concept expansion (LCE) model is proposed which leverages the term-dependent information by adding n-gram and (unordered n-gram) as features into the log-linear ranking model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "In our experiments, we re-implemented LCE following [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "LCE (Row 9) is a latent concept expansion model proposed in [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 258
                            }
                        ],
                        "text": "Recently, within the framework of language models for IR, various approaches that go beyond unigrams have been proposed to capture certain term dependencies, notably the bigram and trigram models [35], the dependence model [11], and the MRF based models [25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "As an alternative, retrieval methods that directly model phrases (or word n-grams) and term dependencies are proposed in [12][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14811099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58bd7dfb4c7c8dec78fe722bd67b02565eb1b0b7",
            "isKey": true,
            "numCitedBy": 258,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to improve retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based onthe Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expansion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our technique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demonstrates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation."
            },
            "slug": "Latent-concept-expansion-using-markov-random-fields-Metzler-Croft",
            "title": {
                "fragments": [],
                "text": "Latent concept expansion using markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A robust query expansion technique based on the Markov random field model for information retrieval, called latent concept expansion, provides a mechanism for modeling term dependencies during expansion and the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "learning approaches that are related to the CLSM include word-tovector mapping (also known as word embedding) using deep neural networks learned on large amounts of raw text [1][27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7478738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "isKey": false,
            "numCitedBy": 3051,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems."
            },
            "slug": "Linguistic-Regularities-in-Continuous-Space-Word-Mikolov-Yih",
            "title": {
                "fragments": [],
                "text": "Linguistic Regularities in Continuous Space Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The vector-space word representations that are implicitly learned by the input-layer weights are found to be surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "also be trained using noise contrastive estimation as in [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14992849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ca064b9f1b92951c1997e90b776e95b0880e52",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. \n \nWe propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-the-art method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones."
            },
            "slug": "Learning-word-embeddings-efficiently-with-Mnih-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Learning word embeddings efficiently with noise-contrastive estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation, and achieves results comparable to the best ones reported, using four times less data and more than an order of magnitude less computing time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876441"
                        ],
                        "name": "Xing Wei",
                        "slug": "Xing-Wei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 231
                            }
                        ],
                        "text": "Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (BLTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "LDA (Row 5 and 6) is our implementation of the model in [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "They include Probabilistic Latent Semantic Analysis (PLSA) [19] and its extensions such as Latent Dirichlet Allocation (LDA) [4][39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3343003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94a8ace25d5112e22f7235bbba26570b008a73e9",
            "isKey": false,
            "numCitedBy": 1145,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."
            },
            "slug": "LDA-based-document-models-for-ad-hoc-retrieval-Wei-Croft",
            "title": {
                "fragments": [],
                "text": "LDA-based document models for ad-hoc retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an LDA-based document model within the language modeling framework, and evaluates it on several TREC collections, and shows that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (BLTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "LDA gives slightly better results than the PLSA, and LDA with 500 topics significantly outperforms BM25 and ULM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The most well-known linear projection model for IR is LSA [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "It is shown that, by learning the model on clicked query-title pairs, the BLTM gives superior performance over PLSA [15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PLSA (Rows 3 and 4) is our implementation of the model proposed in [19], and was trained on documents only (i.e., the title side of the query/clicked-title pairs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We see that using clickthrough data for model training leads to improvement over PLSA and LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Different from [19], our version of PLSA was learned using MAP estimation as in [15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "The second set includes a set of state-of-the-art latent semantic models which are learned either on documents only in an unsupervised manner (PLSA and LDA) or on clickthrough data in a supervised way (BLTM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "They include Probabilistic Latent Semantic Analysis (PLSA) [19] and its extensions such as Latent Dirichlet Allocation (LDA) [4][39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "For example, latent semantic models such as latent semantic analysis (LSA) are able to map a query to its\nrelevant documents at the semantic level where lexical matching often fails (e.g., [9][10][31])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PLSA assumes that each document has a multinomial distribution over topics (called the document-topic distribution), where each of the topics is in turn of a multinomial distribution over words (called the topic-word distribution)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 175
                            }
                        ],
                        "text": "In the next subsection, we will present an indepth analysis on the performance of the CLSM. # Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.305 0.328 0.388\n2 ULM 0.304 0.327 0.385 3 PLSA (T=100) 0.305 0.335 \u03b1 0.402 \u03b1 4 PLSA (T=500) 0.308 0.337 \u03b1 0.402 \u03b1 5 LDA (T=100) 0.308 0.339 \u03b1 0.403 \u03b1 6 LDA (T=500) 0.310 \u03b1 0.339 \u03b1 0.405 \u03b1 7 BLTM 0.316 \u03b1 0.344 \u03b1 0.410 \u03b1 8 MRF 0.315 \u03b1 0.341 \u03b1 0.409 \u03b1 9 LCE 0.312 \u03b1 0.337 \u03b1 0.407 \u03b1\n10 WTM 0.315 \u03b1 0.342 \u03b1 0.411 \u03b1 11 PTM (maxlen = 3) 0.319 \u03b1 0.347 \u03b1 0.413 \u03b1 12 DSSM ( 4) 0.320 \u03b1 0.355 \u03b1\u03b2 0.431 \u03b1\u03b2 13 DSSM ( 50) 0.327 \u03b1\u03b2 0.363 \u03b1\u03b2 0.438 \u03b1\u03b2 14 CLSM ( 4) 0.342 \u03b1\u03b2\u03b3 0.374 \u03b1\u03b2\u03b3 0.447 \u03b1\u03b2\u03b3 15 CLSM ( ) 0.348 \u03b1\u03b2\u03b3 0.379 \u03b1\u03b2\u03b3 0.449\u03b1\u03b2\u03b3\nTable 5: Comparative results with the previous state of the art approaches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "The CLSM also outperforms the state-of-the-art topic model based approaches (i.e., PLSA, LDA, and BLTM) with a statistically significant margin from 3.2% to 4.0%."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": true,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "In order to make the results comparable, we reimplement these models following the descriptions in [11][15][20][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [25], the Markov Random Field (MRF) is used to model dependencies among terms (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 254
                            }
                        ],
                        "text": "Recently, within the framework of language models for IR, various approaches that go beyond unigrams have been proposed to capture certain term dependencies, notably the bigram and trigram models [35], the dependence model [11], and the MRF based models [25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "MRF (Row 8) models the term dependency using a MRF as described in [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "As an alternative, retrieval methods that directly model phrases (or word n-grams) and term dependencies are proposed in [12][25][26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Modeling contextual information in search queries and documents is a long-standing research topic in IR [11][25][12][26][2][22][24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1118305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6",
            "isKey": true,
            "numCitedBy": 957,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections."
            },
            "slug": "A-Markov-random-field-model-for-term-dependencies-Metzler-Croft",
            "title": {
                "fragments": [],
                "text": "A Markov random field model for term dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data, and significant improvements are possible by modeling dependencies, especially on the larger web collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The LDA model is learned via Gibbs sampling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 220
                            }
                        ],
                        "text": "Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (BLTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "LDA gives slightly better results than the PLSA, and LDA with 500 topics significantly outperforms BM25 and ULM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "We see that using clickthrough data for model training leads to improvement over PLSA and LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "LDA (Row 5 and 6) is our implementation of the model in [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "The second set includes a set of state-of-the-art latent semantic models which are learned either on documents only in an unsupervised manner (PLSA and LDA) or on clickthrough data in a supervised way (BLTM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "They include Probabilistic Latent Semantic Analysis (PLSA) [19] and its extensions such as Latent Dirichlet Allocation (LDA) [4][39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 240
                            }
                        ],
                        "text": "In the next subsection, we will present an indepth analysis on the performance of the CLSM. # Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.305 0.328 0.388\n2 ULM 0.304 0.327 0.385 3 PLSA (T=100) 0.305 0.335 \u03b1 0.402 \u03b1 4 PLSA (T=500) 0.308 0.337 \u03b1 0.402 \u03b1 5 LDA (T=100) 0.308 0.339 \u03b1 0.403 \u03b1 6 LDA (T=500) 0.310 \u03b1 0.339 \u03b1 0.405 \u03b1 7 BLTM 0.316 \u03b1 0.344 \u03b1 0.410 \u03b1 8 MRF 0.315 \u03b1 0.341 \u03b1 0.409 \u03b1 9 LCE 0.312 \u03b1 0.337 \u03b1 0.407 \u03b1\n10 WTM 0.315 \u03b1 0.342 \u03b1 0.411 \u03b1 11 PTM (maxlen = 3) 0.319 \u03b1 0.347 \u03b1 0.413 \u03b1 12 DSSM ( 4) 0.320 \u03b1 0.355 \u03b1\u03b2 0.431 \u03b1\u03b2 13 DSSM ( 50) 0.327 \u03b1\u03b2 0.363 \u03b1\u03b2 0.438 \u03b1\u03b2 14 CLSM ( 4) 0.342 \u03b1\u03b2\u03b3 0.374 \u03b1\u03b2\u03b3 0.447 \u03b1\u03b2\u03b3 15 CLSM ( ) 0.348 \u03b1\u03b2\u03b3 0.379 \u03b1\u03b2\u03b3 0.449\u03b1\u03b2\u03b3\nTable 5: Comparative results with the previous state of the art approaches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The CLSM also outperforms the state-of-the-art topic model based approaches (i.e., PLSA, LDA, and BLTM) with a statistically significant margin from 3.2% to 4.0%."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": true,
            "numCitedBy": 30944,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736467"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "ULM (Row 2) is a unigram language model with Dirichlet smoothing [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5793143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "054080d32f41ec461758fdc382935a45b033836b",
            "isKey": false,
            "numCitedBy": 1319,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections."
            },
            "slug": "A-study-of-smoothing-methods-for-language-models-to-Zhai-Lafferty",
            "title": {
                "fragments": [],
                "text": "A study of smoothing methods for language models applied to Ad Hoc information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper examines the sensitivity of retrieval performance to the smoothing parameters and compares several popular smoothing methods on different test collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 281
                            }
                        ],
                        "text": "Extended from our previous work [20] [33], the CLSM and its variations have also been demonstrated giving superior performance on a range of natural language processing tasks beyond information retrieval, including machine translation [13], semantic parsing and question answering [40], entity search and online recommendation [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6343829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a129f612a9eff903d9133244a6f0914ef3cbda72",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA). We focus on single-relation questions and decompose each question into an entity mention and a relation pattern. Using convolutional neural network models, we measure the similarity of entity mentions with entities in the knowledge base (KB) and the similarity of relation patterns and relations in the KB. We score relational triples in the KB using these measures and select the top scoring relational triple to answer the question. When evaluated on an open-domain QA task, our method achieves higher precision across different recall points compared to the previous approach, and can improve F1 by 7 points."
            },
            "slug": "Semantic-Parsing-for-Single-Relation-Question-Yih-He",
            "title": {
                "fragments": [],
                "text": "Semantic Parsing for Single-Relation Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A semantic parsing framework based on semantic similarity for open domain question answering (QA) that achieves higher precision across different recall points compared to the previous approach, and can improve F1 by 7 points."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms Algorithms, Experimentation\nKeywords Convolutional Neural Network; Semantic Representation; Web Search"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10570679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26a6534f18879926ab0a921e6e93e246262e9066",
            "isKey": true,
            "numCitedBy": 117,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). Both of these variants start with a basic model of documents (PCA and PLSA). Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best."
            },
            "slug": "Translingual-Document-Representations-from-Platt-Toutanova",
            "title": {
                "fragments": [],
                "text": "Translingual Document Representations from Discriminative Projections"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work uses discriminative training to create a projection of documents from multiple languages into a single translingual vector space and evaluates these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21432929"
                        ],
                        "name": "Michael Karlen",
                        "slug": "Michael-Karlen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Karlen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Karlen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46283650"
                        ],
                        "name": "P. Kuksa",
                        "slug": "P.-Kuksa",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Kuksa",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuksa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Deep convolutional neural networks (CNN) have been applied successfully in speech, image, and natural language processing [8][41][7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 208
                            }
                        ],
                        "text": "Deep architectures have been shown to be highly effective in discovering from training data the hidden structures and features at different levels of abstraction useful for a variety of tasks [32][18][20][37][7][34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 351666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1022b031dc6c7019696492e8116598097a8c12",
            "isKey": false,
            "numCitedBy": 6657,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            },
            "slug": "Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing (Almost) from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748051"
                        ],
                        "name": "G\u00f6khan T\u00fcr",
                        "slug": "G\u00f6khan-T\u00fcr",
                        "structuredName": {
                            "firstName": "G\u00f6khan",
                            "lastName": "T\u00fcr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G\u00f6khan T\u00fcr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395813836"
                        ],
                        "name": "Dilek Z. Hakkani-T\u00fcr",
                        "slug": "Dilek-Z.-Hakkani-T\u00fcr",
                        "structuredName": {
                            "firstName": "Dilek",
                            "lastName": "Hakkani-T\u00fcr",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dilek Z. Hakkani-T\u00fcr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Deep architectures have been shown to be highly effective in discovering from training data the hidden structures and features at different levels of abstraction useful for a variety of tasks [32][18][20][37][7][34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16408169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3275d20929462b051cc99a47383af9c7ca0ac0e",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the recent advances in deep learning techniques, in this paper, we present the application of special type of deep architecture - deep convex networks (DCNs) - for semantic utterance classification (SUC). DCNs are shown to have several advantages over deep belief networks (DBNs) including classification accuracy and training scalability. However, adoption of DCNs for SUC comes with non-trivial issues. Specifically, SUC has an extremely sparse input feature space encompassing a very large number of lexical and semantic features. This is about a few thousand times larger than the feature space for acoustic modeling, yet with a much smaller number of training samples. Experimental results we obtained on a domain classification task for spoken language understanding demonstrate the effectiveness of DCNs. The DCN-based method produces higher SUC accuracy than the Boosting-based discriminative classifier with word trigrams."
            },
            "slug": "Towards-deeper-understanding:-Deep-convex-networks-T\u00fcr-Deng",
            "title": {
                "fragments": [],
                "text": "Towards deeper understanding: Deep convex networks for semantic utterance classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The DCN-based method produces higher SUC accuracy than the Boosting-based discriminative classifier with word trigrams, and experimental results obtained on a domain classification task for spoken language understanding demonstrate the effectiveness of DCNs."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82323309"
                        ],
                        "name": "Karen Sparck Jones",
                        "slug": "Karen-Sparck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sparck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sparck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27084667,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "255ae3e0d65724290e6897bc6d4f79c1593d1be7",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the value of linguistically-motivated indexing (LMI) for document and text retrieval. After reviewing the basic concepts involved and the assumptions on which LMI is based, namely that complex index descriptions and terms are necessary, I consider past and recent research on LMI, and specifically on automated LMI via NLP. Experiments in the first phase of research, to the late eighties, did not demonstrate value in LMI, but were very limited; but the much larger tests of the Nineties, with full text, have not done so either. My conclusion is that LMI is not needed for effective retrieval, but has other important roles within information-selection systems."
            },
            "slug": "What-is-the-Role-of-NLP-in-Text-Retrieval-Jones",
            "title": {
                "fragments": [],
                "text": "What is the Role of NLP in Text Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is concluded that LMI is not needed for effective retrieval, but has other important roles within information-selection systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393667185"
                        ],
                        "name": "Ossama Abdel-Hamid",
                        "slug": "Ossama-Abdel-Hamid",
                        "structuredName": {
                            "firstName": "Ossama",
                            "lastName": "Abdel-Hamid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ossama Abdel-Hamid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is shown that, by learning the model on clicked query-title pairs, the BLTM gives superior performance over PLSA [15]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2032998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ec87a97d202d5e432ca96490e409d931bbead7f",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and present a novel deep convolutional neural network architecture, where heterogeneous pooling is used to provide constrained frequency-shift invariance in the speech spectrogram while minimizing speech-class confusion induced by such invariance. The design of the pooling layer is guided by domain knowledge about how speech classes would change when formant frequencies are modified. The convolution and heterogeneous-pooling layers are followed by a fully connected multi-layer neural network to form a deep architecture interfaced to an HMM for continuous speech recognition. During training, all layers of this entire deep net are regularized using a variant of the \u201cdropout\u201d technique. Experimental evaluation demonstrates the effectiveness of both heterogeneous pooling and dropout regularization. On the TIMIT phonetic recognition task, we have achieved an 18.7% phone error rate, lowest on this standard task reported in the literature with a single system and with no use of information about speaker identity. Preliminary experiments on large vocabulary speech recognition in a voice search task also show error rate reduction using heterogeneous pooling in the deep convolutional neural network."
            },
            "slug": "A-deep-convolutional-neural-network-using-pooling-Deng-Abdel-Hamid",
            "title": {
                "fragments": [],
                "text": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel deep convolutional neural network architecture is developed, where heterogeneous pooling is used to provide constrained frequency-shift invariance in the speech spectrogram while minimizing speech-class confusion induced by such invariance."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144639556"
                        ],
                        "name": "Graham W. Taylor",
                        "slug": "Graham-W.-Taylor",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Taylor",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham W. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Deep convolutional neural networks (CNN) have been applied successfully in speech, image, and natural language processing [8][41][7]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 975170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d743430cb2329caa5d446c17fc9ec07f5e916ab0",
            "isKey": false,
            "numCitedBy": 1028,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods."
            },
            "slug": "Adaptive-deconvolutional-networks-for-mid-and-high-Zeiler-Taylor",
            "title": {
                "fragments": [],
                "text": "Adaptive deconvolutional networks for mid and high level feature learning"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling, relying on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2768186"
                        ],
                        "name": "K. J\u00e4rvelin",
                        "slug": "K.-J\u00e4rvelin",
                        "structuredName": {
                            "firstName": "Kalervo",
                            "lastName": "J\u00e4rvelin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J\u00e4rvelin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2732839"
                        ],
                        "name": "Jaana Kek\u00e4l\u00e4inen",
                        "slug": "Jaana-Kek\u00e4l\u00e4inen",
                        "structuredName": {
                            "firstName": "Jaana",
                            "lastName": "Kek\u00e4l\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaana Kek\u00e4l\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "First, we observe that the CLSM ( 50) outperforms the stateof-the-art term matching based document ranking models, BM25 and ULM, with a substantial margin of 4.3% in NDCG@1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "NDCG @10\n1 DSSM ( 50) 0.327 0.363 0.438 2 CLSM ( 50) win =1 0.340 \u03b1 0.374 \u03b1 0.443 \u03b1 3 CLSM ( 50) win"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "Further, compared to previous term-dependency models, the CLSM with the best setting outperforms MRF, LCE and PTM by a substantial improvement of 3.3%, 3.6%, and 2.9% NDCG@1 respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "The performance of all ranking models we have evaluated has been measured by mean Normalized Discounted Cumulative Gain (NDCG) [21], and we will report NDCG scores at truncation levels 1, 3, and 10."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Finally, we obtain significant 2.2% to 2.3% NDCG@1 improvement of the CLSM over DSSM, a state-of-the-art neural network based model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "# Models NDCG\n@1 NDCG"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "In the next subsection, we will present an indepth analysis on the performance of the CLSM. # Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.305 0.328 0.388\n2 ULM 0.304 0.327 0.385 3 PLSA (T=100) 0.305 0.335 \u03b1 0.402 \u03b1 4 PLSA (T=500) 0.308 0.337 \u03b1 0.402 \u03b1 5 LDA (T=100) 0.308 0.339 \u03b1 0.403 \u03b1 6 LDA (T=500) 0.310 \u03b1 0.339 \u03b1 0.405 \u03b1 7 BLTM 0.316 \u03b1 0.344 \u03b1 0.410 \u03b1 8 MRF 0.315 \u03b1 0.341 \u03b1 0.409 \u03b1 9 LCE 0.312 \u03b1 0.337 \u03b1 0.407 \u03b1\n10 WTM 0.315 \u03b1 0.342 \u03b1 0.411 \u03b1 11 PTM (maxlen = 3) 0.319 \u03b1 0.347 \u03b1 0.413 \u03b1 12 DSSM ( 4) 0.320 \u03b1 0.355 \u03b1\u03b2 0.431 \u03b1\u03b2 13 DSSM ( 50) 0.327 \u03b1\u03b2 0.363 \u03b1\u03b2 0.438 \u03b1\u03b2 14 CLSM ( 4) 0.342 \u03b1\u03b2\u03b3 0.374 \u03b1\u03b2\u03b3 0.447 \u03b1\u03b2\u03b3 15 CLSM ( ) 0.348 \u03b1\u03b2\u03b3 0.379 \u03b1\u03b2\u03b3 0.449\u03b1\u03b2\u03b3\nTable 5: Comparative results with the previous state of the art approaches."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7644747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3464374899e799cbd516d00f75e425efd495150e",
            "isKey": true,
            "numCitedBy": 1125,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query1) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods."
            },
            "slug": "IR-evaluation-methods-for-retrieving-highly-J\u00e4rvelin-Kek\u00e4l\u00e4inen",
            "title": {
                "fragments": [],
                "text": "IR evaluation methods for retrieving highly relevant documents"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121300608"
                        ],
                        "name": "J. Allan",
                        "slug": "J.-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Allan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f20e2d9b5f8db0b6ae37c9646aeda6a79f35e7eb",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Routing-and-Retrieval-Using-Smart:-TREC-2-Buckley-Allan",
            "title": {
                "fragments": [],
                "text": "Automatic Routing and Retrieval Using Smart: TREC-2"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 174
                            }
                        ],
                        "text": "learning approaches that are related to the CLSM include word-tovector mapping (also known as word embedding) using deep neural networks learned on large amounts of raw text [1][27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7558,
            "numCiting": 345,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747181"
                        ],
                        "name": "J. Hartmanis",
                        "slug": "J.-Hartmanis",
                        "structuredName": {
                            "firstName": "Juris",
                            "lastName": "Hartmanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hartmanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143817739"
                        ],
                        "name": "J. V. Leeuwen",
                        "slug": "J.-V.-Leeuwen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Leeuwen",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Leeuwen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "The weights of the neural network are randomly initialized as suggested in [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26661612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1a5961609c623fc816aaa77565ba38b25531a8e",
            "isKey": false,
            "numCitedBy": 1303,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially images. In many cases, these algorithms involve multi-layered networks of features (eg, neural networks) that are sometimes tricky to train and tune and are difficult. Abstract A commonly encountered problem in MLP (multi-layer perceptron) classification problems is related to the prior probabilities of the individual classes-if the number of training examples that correspond to each class varies significantly between the classes, then. Abstract Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (\u00e2 \u0153early stopping\u00e2 ). The exact criterion used for validation-based early stopping, however, is usually. Abstract Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing \u00e2 \u0153flavors\u00e2 . While being practical, conceptually simple, and easy. Preface In many cases, the amount of labeled data is limited and does not allow for fully identifying the function that needs to be learned. When labeled data is scarce, the learning algorithm is exposed to simultaneous underfitting and overfitting. The learning algorithm. Abstract Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide.A commonly encountered problem in MLP (multi-layer perceptron) classification problems is related to the prior probabilities of the individual classes-if the number of training examples that correspond to each class varies significantly between the classes, then. Abstract Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (\u00e2 \u0153early stopping\u00e2 ). The exact criterion used for validation-based early stopping, however, is usually. Abstract Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing \u00e2 \u0153flavors\u00e2 . While being practical, conceptually simple, and easy. Preface In many cases, the amount of labeled data is limited and does not allow for fully identifying the function that needs to be learned. When labeled data is scarce, the learning algorithm is exposed to simultaneous underfitting and overfitting. The learning algorithm. Abstract Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide. It is our belief that researchers and practitioners acquire, through experience and word-ofmouth, techniques and heuristics that help them successfully apply neural networks to di cult real world problems. Often these\\ tricks\" are theo-tically well motivated. Sometimes they. Abstract The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This. Abstract Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a. Abstract WeChapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a. Abstract We show how nonlinear semi-supervised embedding algorithms popular for use with \u00e2 \u0153shallow\u00e2 learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer."
            },
            "slug": "Neural-Networks:-Tricks-of-the-Trade-Hartmanis-Leeuwen",
            "title": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how nonlinear semi-supervised embedding algorithms popular for use with \u00e2 \u0153shallow\u00e2 learning techniques such as kernel methods can be easily applied to deep multi-layer architectures."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144191547"
                        ],
                        "name": "A. Kab\u00e1n",
                        "slug": "A.-Kab\u00e1n",
                        "structuredName": {
                            "firstName": "Ata",
                            "lastName": "Kab\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kab\u00e1n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10188064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad5419dc0be34b00a4c87818c04125dbab47353a",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation (LDA) is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of Probabilistic Latent Semantic Indexing (PLSI). This paper shows that PLSI is a maximum a posteriori estimated LDA model under a uniform Dirichlet prior, therefore the perceived shortcomings of PLSI can be resolved and elucidated within the LDA framework."
            },
            "slug": "On-an-equivalence-between-PLSI-and-LDA-Girolami-Kab\u00e1n",
            "title": {
                "fragments": [],
                "text": "On an equivalence between PLSI and LDA"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "PLSI is a maximum a posteriori estimated LDA model under a uniform Dirichlet prior, therefore the perceived shortcomings of PLSI can be resolved and elucidated within the LDA framework."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "This loss function is also widely used in speech recognition and other applications [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17976965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d63cdc1d1f023c63f8aa3b64cd5e853670680c3e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we studied the objective functions of MMI, MCE, and MPE/MWE for discriminative learning in sequential pattern recognition. We presented an approach that unifies the objective functions of MMI, MCE, and MPE/MWE in a common rational-function form of (25). The exact structure of the rational-function form for each discriminative criterion was derived and studied. While the rational-function form of MMI has been known in the past, we provided the theoretical proof that the similar rational-function form exists for the objective functions of MCE and MPE/MWE. Moreover, we showed that the rational function forms for objective functions of MMI, MCE, and MPE/MWE differ in the constant weighting factors CDT (s1 . . . sR) and these weighting factors depend only on the labeled sequence s1 . . . sR, and are independent of the parameter set - to be optimized. The derived rational-function form for MMI, MCE, and MPE/MWE allows the GT/EBW-based parameter optimization framework to be applied directly in discriminative learning. In the past, lack of the appropriate rational-function form was a difficulty for MCE and MPE/MWE, because without this form, the GT/EBW-based parameter optimization framework cannot be directly applied. Based on the unified rational-function form, in a tutorial style, we derived the GT/EBW-based parameter optimization formulas for both discrete HMMs and CDHMMs in discriminative learning using MMI, MCE, and MPE/MWE criteria. The unifying review provided in this article has been based upon a large number of earlier contributions that have been cited and discussed throughout the article. Here we provide a brief summary of such background work. Extension to large-scale speech recognition tasks was accomplished in the work of [59] and [60]. The dissertation of [47] further improved the MMI criterion to that of MPE/MWE. In a parallel vein, the work of [20] provided an alternative approach to that of [41], with an attempt to more rigorously provide a CDHMM model re-estimation formula that gives positive growth of the MMI objective function. A crucial error of this attempt was corrected in [2] for establishing an existence proof of such positive growth. The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discriminative learning paradigm."
            },
            "slug": "Discriminative-learning-in-sequential-pattern-He-Deng",
            "title": {
                "fragments": [],
                "text": "Discriminative learning in sequential pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discrim inative learning paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296031"
                        ],
                        "name": "Tal Shaked",
                        "slug": "Tal-Shaked",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Shaked",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Shaked"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859813"
                        ],
                        "name": "Erin Renshaw",
                        "slug": "Erin-Renshaw",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Renshaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin Renshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078999999"
                        ],
                        "name": "Ari Lazier",
                        "slug": "Ari-Lazier",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Lazier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari Lazier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398663319"
                        ],
                        "name": "Matt Deeds",
                        "slug": "Matt-Deeds",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Deeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Deeds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067005422"
                        ],
                        "name": "Nicole Hamilton",
                        "slug": "Nicole-Hamilton",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Hamilton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicole Hamilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398663291"
                        ],
                        "name": "Greg Hullender",
                        "slug": "Greg-Hullender",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Hullender",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Hullender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "(5) and (6) covers the pairwise loss that has been widely used for learning-to-rank [5] as a special case if we allow only one unclicked document to be sampled."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11168734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63aaf12163fe9735dfe9a69114937c4fa34f303a",
            "isKey": false,
            "numCitedBy": 2463,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine."
            },
            "slug": "Learning-to-rank-using-gradient-descent-Burges-Shaked",
            "title": {
                "fragments": [],
                "text": "Learning to rank using gradient descent"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "RankNet is introduced, an implementation of these ideas using a neural network to model the underlying ranking function, and test results on toy data and on data from a commercial internet search engine are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388546388"
                        ],
                        "name": "Grgoire Montavon",
                        "slug": "Grgoire-Montavon",
                        "structuredName": {
                            "firstName": "Grgoire",
                            "lastName": "Montavon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grgoire Montavon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382473772"
                        ],
                        "name": "Genevive Orr",
                        "slug": "Genevive-Orr",
                        "structuredName": {
                            "firstName": "Genevive",
                            "lastName": "Orr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Genevive Orr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068918803"
                        ],
                        "name": "Klaus-Robert Mller",
                        "slug": "Klaus-Robert-Mller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "Mller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus-Robert Mller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39578794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b305c18d17fd6a17e8e52a21bcd680220d322cc3",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "The twenty last years have been marked by an increase in available data and computing power. In parallel to this trend, the focus of neural network research and the practice of training neural networks has undergone a number of important changes, for example, use of deep learning machines. The second edition of the book augments the first edition with more tricks, which have resulted from 14 years of theory and experimentation by some of the world's most prominent neural network researchers. These tricks can make a substantial difference (in terms of speed, ease of implementation, and accuracy) when it comes to putting algorithms to work on real problems."
            },
            "slug": "Neural-Networks:-Tricks-of-the-Trade-Montavon-Orr",
            "title": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The second edition of the book augments the first edition with more tricks, which have resulted from 14 years of theory and experimentation by some of the world's most prominent neural network researchers."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms Algorithms, Experimentation\nKeywords Convolutional Neural Network; Semantic Representation; Web Search"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60583802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3cfdd4cc455b98ae3e661416550a656847825e",
            "isKey": true,
            "numCitedBy": 101,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-cross-linguistic-information-retrieval-Dumais-Landauer",
            "title": {
                "fragments": [],
                "text": "Automatic cross-linguistic information retrieval using latent semantic indexing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms Algorithms, Experimentation\nKeywords Convolutional Neural Network; Semantic Representation; Web Search"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "The translation model-based approach proposed in [12] tries to extract phrase-to-phrase relationships according to clickthrough data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet allocation. In Journal of Machine Learning Research"
            },
            "venue": {
                "fragments": [],
                "text": "Latent Dirichlet allocation. In Journal of Machine Learning Research"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "Early work tries to relax the independence assumption by including phrases, in addition to single terms, as indexing units [6][36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic retrieval approaches using SMART: TREC-2"
            },
            "venue": {
                "fragments": [],
                "text": "Information Processing and Management,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (BLTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "More recently, semantic modeling methods based on neural networks have also been proposed for information retrieval (IR) [16][32][20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Salakhutdinov and Hinton proposed the Semantic Hashing method based on a deep auto-encoder in [32][16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On an equivalence between PLSA and LDA"
            },
            "venue": {
                "fragments": [],
                "text": "In SIGIR,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic retrieval approaches using SMART: TREC-2. Information Processing and Management"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "Deep convolutional neural networks (CNN) have been applied successfully in speech, image, and natural language processing [8][41][7]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion, in ICASSP"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 30,
            "methodology": 17,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Latent-Semantic-Model-with-Convolutional-Pooling-Shen-He/7e8d5a108c28cdfb92f419ce919fbf7993dfebfc?sort=total-citations"
}