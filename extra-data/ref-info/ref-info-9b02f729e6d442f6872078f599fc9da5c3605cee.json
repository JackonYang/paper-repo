{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40016884"
                        ],
                        "name": "J. M. Romeu",
                        "slug": "J.-M.-Romeu",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Romeu",
                            "middleNames": [
                                "Mas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Romeu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50313341"
                        ],
                        "name": "D. F. Mota",
                        "slug": "D.-F.-Mota",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mota",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. F. Mota"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467588"
                        ],
                        "name": "Jon Almaz\u00e1n",
                        "slug": "Jon-Almaz\u00e1n",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Almaz\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Almaz\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578506"
                        ],
                        "name": "Llu\u00eds-Pere de las Heras",
                        "slug": "Llu\u00eds-Pere-de-las-Heras",
                        "structuredName": {
                            "firstName": "Llu\u00eds-Pere",
                            "lastName": "Heras",
                            "middleNames": [
                                "de",
                                "las"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds-Pere de las Heras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Older tasks are covered in previous reports [6][5][4] while up to date information is available on the competition Web site."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "The competition dates back to 2003[1] [2] [3], and was substantially revised in 2011 and 2013 [4] [5] [6], creating a comprehensive reference framework for robust reading pipelines evaluation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 57
                            }
                        ],
                        "text": "For details please refer to previous competition reports [4, 5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "See the 2013 competition report [6] for details about these metrics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "The evaluation protocol is based on a standard edit distance metric, with equal costs for additions, deletions and substitutions [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 132
                            }
                        ],
                        "text": "The best Recall is obtained by \u201dDeep2Text-II\u201d, which is based on \u201cUSTB TexStar\u201d (see performance details in the 2013 edition report [6]) with an extra diversification step, coupled with a CNN-based recogniser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206777226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd7b547bf0a6646a282f521db880e74974aa838",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the final results of the ICDAR 2013 Robust Reading Competition. The competition is structured in three Challenges addressing text extraction in different application domains, namely born-digital images, real scene images and real-scene videos. The Challenges are organised around specific tasks covering text localisation, text segmentation and word recognition. The competition took place in the first quarter of 2013, and received a total of 42 submissions over the different tasks offered. This report describes the datasets and ground truth specification, details the performance evaluation protocols used and presents the final results along with a brief summary of the participating methods."
            },
            "slug": "ICDAR-2013-Robust-Reading-Competition-Karatzas-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 Robust Reading Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The datasets and ground truth specification are described, the performance evaluation protocols used are details, and the final results are presented along with a brief summary of the participating methods."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "Older tasks are covered in previous reports [6][5][4] while up to date information is available on the competition Web site."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 57
                            }
                        ],
                        "text": "For details please refer to previous competition reports [4, 5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "The competition dates back to 2003[1] [2] [3], and was substantially revised in 2011 and 2013 [4] [5] [6], creating a comprehensive reference framework for robust reading pipelines evaluation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40016884"
                        ],
                        "name": "J. M. Romeu",
                        "slug": "J.-M.-Romeu",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Romeu",
                            "middleNames": [
                                "Mas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Romeu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38417650"
                        ],
                        "name": "F. Nourbakhsh",
                        "slug": "F.-Nourbakhsh",
                        "structuredName": {
                            "firstName": "Farshad",
                            "lastName": "Nourbakhsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Nourbakhsh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40813600"
                        ],
                        "name": "P. Roy",
                        "slug": "P.-Roy",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Roy",
                            "middleNames": [
                                "Pratim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Older tasks are covered in previous reports [6][5][4] while up to date information is available on the competition Web site."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 57
                            }
                        ],
                        "text": "For details please refer to previous competition reports [4, 5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "The competition dates back to 2003[1] [2] [3], and was substantially revised in 2011 and 2013 [4] [5] [6], creating a comprehensive reference framework for robust reading pipelines evaluation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4377688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c507148d502245c459df2ec883dc02fabc0ecad",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of the first Challenge of ICDAR 2011 Robust Reading Competition. Challenge 1 is focused on the extraction of text from born-digital images, specifically from images found in Web pages and emails. The challenge was organized in terms of three tasks that look at different stages of the process: text localization, text segmentation and word recognition. In this paper we present the results of the challenge for all three tasks, and make an open call for continuous participation outside the context of ICDAR 2011."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-1:-Karatzas-Mestre",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition - Challenge 1: Reading Text in Born-Digital Images (Web and Email)"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper presents the results of the first Challenge of ICDAR 2011 Robust Reading Competition, focused on the extraction of text from born-digital images, specifically from images found in Web pages and emails."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 439,
                                "start": 409
                            }
                        ],
                        "text": "He, Z. Chen, F. Yin, and C. L. Liu, CASIA NLPR [16, 17] \u2022 \u201cRTST Lucas-Kanade-2\u201d Y. Zhou, and H. Lai, NJUCS Nanjing University [18] \u2022 \u201cStradvision-1\u201d H. Cho, M. Sung, and B. Jun, Stradvision \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u201cStradvision-2\u201d \u2013//\u2013 \u2022 \u2022 \u2022 \u201cTextCatcher-1\u201d J. Fabrizio, M. Robert-Seidowsky, LRDE \u2022 \u201cTextCatcher-2\u201d J. Fabrizio, M. Robert-Seidowsky, E. Carlinet, T. Geraud, LRDE \u2022 \u2022 \u201cUSTB-TexVideo\u201d X. Yin, S. Tian, Z. Y. Zuo, W. Y. Pei, and C. Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022 \u201cUSTB TexVideo-II-1\u201d \u2013//\u2013 \u2022 \u2022 \u201cUSTB texVideo-II-2\u201d \u2013//\u2013 \u2022 \u2022 \u201cVGGMaxBBNet\u201d A. Gupta, M. Jaderberg, A. Zisserman, Visual Geometry Group, University of Oxford [12] \u2022 \u201cCNN MSER\u201d W."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Zisserman, Visual Geometry Group, University of Oxford [12] \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 431,
                                "start": 374
                            }
                        ],
                        "text": "The\n1http://rrc.cvc.uab.es\n\u201dMethod\u201d, Authors, Affiliation Challenge Task 1.4 2.4 3.1 3.4 4.1 4.3 4.4 \u201cDSM\u201d S. Kim, Qualcomm \u2022 \u201cAJOU\u201d H. I. Koo, and Y. G. Kim, Ajou University [8, 9] \u2022 \u2022 \u201cBeam Search CUNI\u201d J. Libovicky\u0301, and P. Pecina, Charles University in Prague \u2022 \u2022 \u201cBeam Search CUNI +S\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-I\u201d X. C. Yin, C. Yang, J. B. Hou, W. Y. Pei, X. Yin, and K. Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022 \u201cDeep2Text-II\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-MO\u201d X. C. Yin, W. Y. Pei, C. Yang, Y. Zheng, Q. Gao, G. Ji, and X. Yin, University of Science and Technology Beijing \u2022 \u2022 \u201cHUST MCLAB\u201d B. Shi, C. Zhang, C. Yao, X. Bai, and Z. Zhang, Huazhong University of Science and Technology \u2022 \u201cMAPS\u201d D. Kumar and A. G. Ramakrishnan, Dayananda Sagar Institutions and Indian Institute of Science [14] \u2022 \u201cNESP\u201d \u2013//\u2013 [15] \u2022 \u201cMSER MRF\u201d X. Liu, NanJing University \u2022 \u201cNJU Text\u201d F. Su, H. Xu, and T. Lu, Nanjing University \u2022 \u2022 \u2022 \u2022 \u201cPAL\u201d Y. C. Wu, K. Chen, X."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207252329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5ae7436b5946bd37d17fc1ed26374389a86deff",
            "isKey": true,
            "numCitedBy": 886,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present an end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query."
            },
            "slug": "Reading-Text-in-the-Wild-with-Convolutional-Neural-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Reading Text in the Wild with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval and a real-world application to allow thousands of hours of news footage to be instantly searchable via a text query is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065499715"
                        ],
                        "name": "Kazuki Ashida",
                        "slug": "Kazuki-Ashida",
                        "structuredName": {
                            "firstName": "Kazuki",
                            "lastName": "Ashida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuki Ashida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055905787"
                        ],
                        "name": "Hiroki Nagai",
                        "slug": "Hiroki-Nagai",
                        "structuredName": {
                            "firstName": "Hiroki",
                            "lastName": "Nagai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroki Nagai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47471571"
                        ],
                        "name": "Masayuki Okamoto",
                        "slug": "Masayuki-Okamoto",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Okamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masayuki Okamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152933693"
                        ],
                        "name": "Hiroaki Yamamoto",
                        "slug": "Hiroaki-Yamamoto",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593830"
                        ],
                        "name": "H. Miyao",
                        "slug": "H.-Miyao",
                        "structuredName": {
                            "firstName": "Hidetoshi",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146280843"
                        ],
                        "name": "JunMin Zhu",
                        "slug": "JunMin-Zhu",
                        "structuredName": {
                            "firstName": "JunMin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JunMin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873679"
                        ],
                        "name": "WuWen Ou",
                        "slug": "WuWen-Ou",
                        "structuredName": {
                            "firstName": "WuWen",
                            "lastName": "Ou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WuWen Ou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844464"
                        ],
                        "name": "L. Todoran",
                        "slug": "L.-Todoran",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Todoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Todoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46520089"
                        ],
                        "name": "Xiaofan Lin",
                        "slug": "Xiaofan-Lin",
                        "structuredName": {
                            "firstName": "Xiaofan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofan Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "The competition dates back to 2003[1] [2] [3], and was substantially revised in 2011 and 2013 [4] [5] [6], creating a comprehensive reference framework for robust reading pipelines evaluation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2250003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a01deac56a81646e8d84cb7bf2d905714ff00808",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper describes the robust reading competitions for ICDAR 2003. With the rapid growth in research over the last few years on recognizing text in natural scenes, there is an urgent need to establish some common benchmark datasets and gain a clear understanding of the current state of the art. We use the term \u2018robust reading\u2019 to refer to text images that are beyond the capabilities of current commercial OCR packages. We chose to break down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system. The subproblems we chose were text locating, character recognition and word recognition. By breaking down the problem in this way, we hoped to gain a better understanding of the state of the art in each of the subproblems. Furthermore, our methodology involved storing detailed results of applying each algorithm to each image in the datasets, allowing researchers to study in depth the strengths and weaknesses of each algorithm. The text-locating contest was the only one to have any entries. We give a brief description of each entry and present the results of this contest, showing cases where the leading entries succeed and fail. We also describe an algorithm for combining the outputs of the individual text locators and show how the combination scheme improves on any of the individual systems."
            },
            "slug": "ICDAR-2003-robust-reading-competitions:-entries,-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions: entries, results, and future directions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper broke down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system, and described an algorithm for combining the outputs of the individual text locators and showed how the combination scheme improves on any of theindividual systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19]) \u2022 Weakly Contextualised: a vocabulary of all words in the training/test set \u2022 Generic: a generic vocabulary of about 90K words derived from the dataset4 of Jaderberg et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14136313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b8f58a038df83138435b12a499c8bf0de13811",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition."
            },
            "slug": "End-to-end-scene-text-recognition-Wang-Babenko",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "While scene text recognition has generally been treated with highly domain-specific methods, the results demonstrate the suitability of applying generic computer vision methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "The competition dates back to 2003[1] [2] [3], and was substantially revised in 2011 and 2013 [4] [5] [6], creating a comprehensive reference framework for robust reading pipelines evaluation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "Method MOTP MOTA ATA Baseline (TextSpotter)[21] 69."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "\u201cBaseline (TextSpotter)\u201d is an unconstrained realtime end-to-end text localization and recognition method [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10585219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6bb2d54b5d87c19607b7dc14e8aba7f51a62205",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The three main novel features are: (i) keeping multiple segmentations of each character until the very last stage of the processing when the context of each character in a text line is known, (ii) an efficient algorithm for selection of character segmentations minimizing a global criterion, and (iii) showing that, despite using theoretically scale-invariant methods, operating on a coarse Gaussian scale space pyramid yields improved results as many typographical artifacts are eliminated. The method runs in real time and achieves state-of-the-art text localization results on the ICDAR 2011 Robust Reading dataset. Results are also reported for end-to-end text recognition on the ICDAR 2011 dataset."
            },
            "slug": "On-Combining-Multiple-Segmentations-in-Scene-Text-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "On Combining Multiple Segmentations in Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An end-to-end real-time scene text localization and recognition method that achieves state-of-the-art text localization results on the ICDAR 2011 Robust Reading dataset and shows that, despite using theoretically scale-invariant methods, operating on a coarse Gaussian scale space pyramid yields improved results as many typographical artifacts are eliminated."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206591895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b595c9e969e5605f62da51b6c16dad8aad3e0e",
            "isKey": false,
            "numCitedBy": 790,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by \u201cfalse positives\u201d caused by detected watermark text in the dataset."
            },
            "slug": "Real-time-scene-text-localization-and-recognition-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "Real-time scene text localization and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The proposed end-to-end real-time scene text localization and recognition method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end- to-end text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207252270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616b246e332573af1f4859aa91440280774c183a",
            "isKey": false,
            "numCitedBy": 3766,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "slug": "The-Pascal-Visual-Object-Classes-Challenge:-A-Everingham-Eslami",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes Challenge: A Retrospective"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A review of the Pascal Visual Object Classes challenge from 2008-2012 and an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18738539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "928d2eda05c1a70b26a28806f76a82fe09adf0a1",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a set of on-line software tools for creating ground truth and calculating performance evaluation metrics for text extraction tasks such as localization, segmentation and recognition. The platform supports the definition of comprehensive ground truth information at different text representation levels while it offers centralised management and quality control of the ground truthing effort. It implements a range of state of the art performance evaluation algorithms and offers functionality for the definition of evaluation scenarios, on-line calculation of various performance metrics and visualisation of the results. The presented platform, which comprises the backbone of the ICDAR 2011 (challenge 1) and 2013 (challenges 1 and 2) Robust Reading competitions, is now made available for public use."
            },
            "slug": "An-on-line-platform-for-ground-truthing-and-of-text-Karatzas-Mestre",
            "title": {
                "fragments": [],
                "text": "An on-line platform for ground truthing and performance evaluation of text extraction systems"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The presented platform implements a range of state of the art performance evaluation algorithms and offers functionality for the definition of evaluation scenarios, on-line calculation of various performance metrics and visualisation of the results."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "\u201cBaseline (OpenCV + Tesseract)\u201d makes use of the publicly available pipeline6 proposed in [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18726451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d797247c4f46c1eea1d7a841537aea22b9cfedfe",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "It is a generally accepted fact that Off-the-shelf OCR engines do not perform well in unconstrained scenarios like natural scene imagery, where text appears among the clutter of the scene. However, recent research demonstrates that a conventional shape-based OCR engine would be able to produce competitive results in the end-to-end scene text recognition task when provided with a conveniently preprocessed image. In this paper we confirm this finding with a set of experiments where two off-the-shelf OCR engines are combined with an open implementation of a state-of-the-art scene text detection framework. The obtained results demonstrate that in such pipeline, conventional OCR solutions still perform competitively compared to other solutions specifically designed for scene text recognition."
            },
            "slug": "Scene-Text-Recognition:-No-Country-for-Old-Men-Bigorda-Karatzas",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition: No Country for Old Men?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper confirms that a conventional shape-based OCR engine would be able to produce competitive results in the end-to-end scene text recognition task when provided with a conveniently preprocessed image."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV Workshops"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8135633"
                        ],
                        "name": "Xuwang Yin",
                        "slug": "Xuwang-Yin",
                        "structuredName": {
                            "firstName": "Xuwang",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuwang Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5380819"
                        ],
                        "name": "Kaizhu Huang",
                        "slug": "Kaizhu-Huang",
                        "structuredName": {
                            "firstName": "Kaizhu",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhu Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 439,
                                "start": 409
                            }
                        ],
                        "text": "He, Z. Chen, F. Yin, and C. L. Liu, CASIA NLPR [16, 17] \u2022 \u201cRTST Lucas-Kanade-2\u201d Y. Zhou, and H. Lai, NJUCS Nanjing University [18] \u2022 \u201cStradvision-1\u201d H. Cho, M. Sung, and B. Jun, Stradvision \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u201cStradvision-2\u201d \u2013//\u2013 \u2022 \u2022 \u2022 \u201cTextCatcher-1\u201d J. Fabrizio, M. Robert-Seidowsky, LRDE \u2022 \u201cTextCatcher-2\u201d J. Fabrizio, M. Robert-Seidowsky, E. Carlinet, T. Geraud, LRDE \u2022 \u2022 \u201cUSTB-TexVideo\u201d X. Yin, S. Tian, Z. Y. Zuo, W. Y. Pei, and C. Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022 \u201cUSTB TexVideo-II-1\u201d \u2013//\u2013 \u2022 \u2022 \u201cUSTB texVideo-II-2\u201d \u2013//\u2013 \u2022 \u2022 \u201cVGGMaxBBNet\u201d A. Gupta, M. Jaderberg, A. Zisserman, Visual Geometry Group, University of Oxford [12] \u2022 \u201cCNN MSER\u201d W."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 431,
                                "start": 374
                            }
                        ],
                        "text": "The\n1http://rrc.cvc.uab.es\n\u201dMethod\u201d, Authors, Affiliation Challenge Task 1.4 2.4 3.1 3.4 4.1 4.3 4.4 \u201cDSM\u201d S. Kim, Qualcomm \u2022 \u201cAJOU\u201d H. I. Koo, and Y. G. Kim, Ajou University [8, 9] \u2022 \u2022 \u201cBeam Search CUNI\u201d J. Libovicky\u0301, and P. Pecina, Charles University in Prague \u2022 \u2022 \u201cBeam Search CUNI +S\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-I\u201d X. C. Yin, C. Yang, J. B. Hou, W. Y. Pei, X. Yin, and K. Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022 \u201cDeep2Text-II\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-MO\u201d X. C. Yin, W. Y. Pei, C. Yang, Y. Zheng, Q. Gao, G. Ji, and X. Yin, University of Science and Technology Beijing \u2022 \u2022 \u201cHUST MCLAB\u201d B. Shi, C. Zhang, C. Yao, X. Bai, and Z. Zhang, Huazhong University of Science and Technology \u2022 \u201cMAPS\u201d D. Kumar and A. G. Ramakrishnan, Dayananda Sagar Institutions and Indian Institute of Science [14] \u2022 \u201cNESP\u201d \u2013//\u2013 [15] \u2022 \u201cMSER MRF\u201d X. Liu, NanJing University \u2022 \u201cNJU Text\u201d F. Su, H. Xu, and T. Lu, Nanjing University \u2022 \u2022 \u2022 \u2022 \u201cPAL\u201d Y. C. Wu, K. Chen, X."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9621884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ac1c5f094be1e16512a9a7fd817e0b414632027",
            "isKey": true,
            "numCitedBy": 380,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks. In this paper, we propose an accurate and robust method for detecting texts in natural scene images. A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations. Character candidates are grouped into text candidates by the single-link clustering algorithm, where distance weights and clustering threshold are learned automatically by a novel self-training distance metric learning algorithm. The posterior probabilities of text candidates corresponding to non-text are estimated with a character classifier; text candidates with high non-text probabilities are eliminated and texts are identified with a text classifier. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition database; the f-measure is over 76%, much better than the state-of-the-art performance of 71%. Experiments on multilingual, street view, multi-orientation and even born-digital databases also demonstrate the effectiveness of the proposed method."
            },
            "slug": "Robust-Text-Detection-in-Natural-Scene-Images-Yin-Yin",
            "title": {
                "fragments": [],
                "text": "Robust Text Detection in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An accurate and robust method for detecting texts in natural scene images using a fast and effective pruning algorithm to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384553906"
                        ],
                        "name": "D. Kumar",
                        "slug": "D.-Kumar",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143825718"
                        ],
                        "name": "M. Prasad",
                        "slug": "M.-Prasad",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Prasad",
                            "middleNames": [
                                "N.",
                                "Anil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145677714"
                        ],
                        "name": "A. Ramakrishnan",
                        "slug": "A.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ramakrishnan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ramakrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13848101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "098a688414de687c872f5754621516202083efc1",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we report a breakthrough result on the difficult task of segmentation and recognition of coloured text from the word image dataset of ICDAR robust reading competition challenge 2: reading text in scene images. We split the word image into individual colour, gray and lightness planes and enhance the contrast of each of these planes independently by a power-law transform. The discrimination factor of each plane is computed as the maximum between-class variance used in Otsu thresholding. The plane that has maximum discrimination factor is selected for segmentation. The trial version of Omnipage OCR is then used on the binarized words for recognition. Our recognition results on ICDAR 2011 and ICDAR 2003 word datasets are compared with those reported in the literature. As baseline, the images binarized by simple global and local thresholding techniques were also recognized. The word recognition rate obtained by our non-linear enhancement and selection of plance method is 72.8% and 66.2% for ICDAR 2011 and 2003 word datasets, respectively. We have created ground-truth for each image at the pixel level to benchmark these datasets using a toolkit developed by us. The recognition rate of benchmarked images is 86.7% and 83.9% for ICDAR 2011 and 2003 datasets, respectively."
            },
            "slug": "NESP:-Nonlinear-enhancement-and-selection-of-plane-Kumar-Prasad",
            "title": {
                "fragments": [],
                "text": "NESP: Nonlinear enhancement and selection of plane for optimal segmentation and recognition of scene word images"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A breakthrough result is reported on the difficult task of segmentation and recognition of coloured text from the word image dataset of ICDAR robust reading competition challenge 2: reading text in scene images."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8982801"
                        ],
                        "name": "Van Pham",
                        "slug": "Van-Pham",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Pham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096223"
                        ],
                        "name": "Gueesang Lee",
                        "slug": "Gueesang-Lee",
                        "structuredName": {
                            "firstName": "Gueesang",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gueesang Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33616412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d4b7940e821a9c9d3eda0934a36dbbed9fb0d96",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural scenes of blurred text images are challenges to current text recognition field. In our paper, a novel method for text detection in natural scene image is suggested using edge detection, maximally stable extremal region (MSER) and tensor voting. Edge detection and MSER methods are combined to find the greatest character candidates from stable areas which are extracted from an input image. These text candidates are used to extract the text line information using tensor voting that creates normal vectors and curve saliency values in characters along the text lines. Therefore, the text line information is used to eliminate non-text areas. Our method is evaluated on the ICDAR2013 datasets and experiment results show that the proposed result is compared to the previous methods."
            },
            "slug": "Robust-Text-Detection-in-Natural-Scene-Images-Pham-Lee",
            "title": {
                "fragments": [],
                "text": "Robust Text Detection in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method for text detection in natural scene image is suggested using edge detection, maximally stable extremal region (MSER), and tensor voting to eliminate non-text areas."
            },
            "venue": {
                "fragments": [],
                "text": "Australasian Conference on Artificial Intelligence"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384553906"
                        ],
                        "name": "D. Kumar",
                        "slug": "D.-Kumar",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143825718"
                        ],
                        "name": "M. Prasad",
                        "slug": "M.-Prasad",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Prasad",
                            "middleNames": [
                                "N.",
                                "Anil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145677714"
                        ],
                        "name": "A. Ramakrishnan",
                        "slug": "A.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ramakrishnan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ramakrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13303734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3f82c4e95fd2be884e5ce045bd6b8a70f0a680a",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Scenic word images undergo degradations due to motion blur, uneven illumination, shadows and defocussing, which lead to difficulty in segmentation. As a result, the recognition results reported on the scenic word image datasets of ICDAR have been low. We introduce a novel technique, where we choose the middle row of the image as a sub-image and segment it first. Then, the labels from this segmented sub-image are used to propagate labels to other pixels in the image. This approach, which is unique and distinct from the existing methods, results in improved segmentation. Bayesian classification and Max-flow methods have been independently used for label propagation. This midline based approach limits the impact of degradations that happens to the image. The segmented text image is recognized using the trial version of Omnipage OCR. We have tested our method on ICDAR 2003 and ICDAR 2011 datasets. Our word recognition results of 64.5% and 71.6% are better than those of methods in the literature and also methods that competed in the Robust reading competition. Our method makes an implicit assumption that degradation is not present in the middle row."
            },
            "slug": "MAPS:-midline-analysis-and-propagation-of-Kumar-Prasad",
            "title": {
                "fragments": [],
                "text": "MAPS: midline analysis and propagation of segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel technique, where the middle row of the image is chosen as a sub-image and segmented first and the labels from this segmented sub- image are used to propagate labels to other pixels in the image, which results in improved segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "ICVGIP '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "Concretely, we use the OpenCV Class Specific Extremal Regions (CSER) and Exhaustive Search algorithms initially proposed by Neumann and Matas [18] along with the perceptual grouping approach of Gomez and Karatzas [23] for text localisation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18726109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49eb053d79a823aea3994b329670f08d838a338c",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text extraction methodologies are usually based in classification of individual regions or patches, using a priori knowledge for a given script or language. Human perception of text, on the other hand, is based on perceptual organisation through which text emerges as a perceptually significant group of atomic objects. Therefore humans are able to detect text even in languages and scripts never seen before. In this paper, we argue that the text extraction problem could be posed as the detection of meaningful groups of regions. We present a method built around a perceptual organisation framework that exploits collaboration of proximity and similarity laws to create text-group hypotheses. Experiments demonstrate that our algorithm is competitive with state of the art approaches on a standard dataset covering text in variable orientations and two languages."
            },
            "slug": "Multi-script-Text-Extraction-from-Natural-Scenes-Bigorda-Karatzas",
            "title": {
                "fragments": [],
                "text": "Multi-script Text Extraction from Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a method built around a perceptual organisation framework that exploits collaboration of proximity and similarity laws to create text-group hypotheses and demonstrates that the algorithm is competitive with state of the art approaches on a standard dataset covering text in variable orientations and two languages."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109321773"
                        ],
                        "name": "Xiaolong Liu",
                        "slug": "Xiaolong-Liu",
                        "structuredName": {
                            "firstName": "Xiaolong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144720255"
                        ],
                        "name": "Tong Lu",
                        "slug": "Tong-Lu",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Lu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MSER-MRF[26] 84."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "899b11aca4b16468cc2c36570912878e9fbd5fd2",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Text recognition in natural scenes is challenging due to the variations on text font, font size, color and scene background. In this paper, we propose a novel method to recognize scene characters based on born-digital template matching, which is formulated by a Morkov Random Field framework by combining both local point features and global spatial structures of scene characters to improve the accuracy. Then the character recognition result of the proposed MRF framework is determined by the votes from multiple one-versus-one classifiers. The experiments on two benchmark datasets show that the effectiveness of the proposed method, which outperforms the recent state-of-the-art natural scene recognition methods."
            },
            "slug": "Natural-Scene-character-recognition-using-Markov-Liu-Lu",
            "title": {
                "fragments": [],
                "text": "Natural Scene character recognition using Markov Random Field"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method to recognize scene characters based on born-digital template matching, which is formulated by a Morkov Random Field framework by combining both local point features and global spatial structures of scene characters to improve the accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775890"
                        ],
                        "name": "Ze-Yu Zuo",
                        "slug": "Ze-Yu-Zuo",
                        "structuredName": {
                            "firstName": "Ze-Yu",
                            "lastName": "Zuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ze-Yu Zuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143943924"
                        ],
                        "name": "Shu Tian",
                        "slug": "Shu-Tian",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041795"
                        ],
                        "name": "Wei-Yi Pei",
                        "slug": "Wei-Yi-Pei",
                        "structuredName": {
                            "firstName": "Wei-Yi",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Yi Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21166571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b0507d7c1c7a4d60a68b48ed0bc5fbf55925393",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection and tracking in scene videos are important prerequisites for content-based video analysis and retrieval, wearable camera systems and mobile devices augmented reality translators. Here, we present a novel multi-strategy tracking based text detection approach in scene videos. In this approach, a state-of-the-art scene text detection module [1] is first used to detect text in each video frame. Then a multi-strategy text tracking technique is proposed, which uses tracking by detection, spatio-temporal context learning, and linear prediction to predict the candidate text location sequentially, and adaptively integrates and selects the best matching text block from the candidate blocks with a rule-based method. This multi-strategy tracking technique can combine the advantages of the three different tracking techniques and afterwards make remedies to the disadvantages of them. Experiments on a variety of scene videos show that our proposed approach is effective and robust to reduce false alarm and improve the accuracy of detection."
            },
            "slug": "Multi-strategy-tracking-based-text-detection-in-Zuo-Tian",
            "title": {
                "fragments": [],
                "text": "Multi-strategy tracking based text detection in scene videos"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel multi-strategy tracking based text detection approach in scene videos that can combine the advantages of the three different tracking techniques and afterwards make remedies to the disadvantages of them is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041795"
                        ],
                        "name": "Wei-Yi Pei",
                        "slug": "Wei-Yi-Pei",
                        "structuredName": {
                            "firstName": "Wei-Yi",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Yi Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155661682"
                        ],
                        "name": "Jun Zhang",
                        "slug": "Jun-Zhang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680014"
                        ],
                        "name": "Hongwei Hao",
                        "slug": "Hongwei-Hao",
                        "structuredName": {
                            "firstName": "Hongwei",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongwei Hao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 439,
                                "start": 409
                            }
                        ],
                        "text": "He, Z. Chen, F. Yin, and C. L. Liu, CASIA NLPR [16, 17] \u2022 \u201cRTST Lucas-Kanade-2\u201d Y. Zhou, and H. Lai, NJUCS Nanjing University [18] \u2022 \u201cStradvision-1\u201d H. Cho, M. Sung, and B. Jun, Stradvision \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u201cStradvision-2\u201d \u2013//\u2013 \u2022 \u2022 \u2022 \u201cTextCatcher-1\u201d J. Fabrizio, M. Robert-Seidowsky, LRDE \u2022 \u201cTextCatcher-2\u201d J. Fabrizio, M. Robert-Seidowsky, E. Carlinet, T. Geraud, LRDE \u2022 \u2022 \u201cUSTB-TexVideo\u201d X. Yin, S. Tian, Z. Y. Zuo, W. Y. Pei, and C. Yang, University of Science and Technology Beijing [10, 11, 12] \u2022 \u2022 \u201cUSTB TexVideo-II-1\u201d \u2013//\u2013 \u2022 \u2022 \u201cUSTB texVideo-II-2\u201d \u2013//\u2013 \u2022 \u2022 \u201cVGGMaxBBNet\u201d A. Gupta, M. Jaderberg, A. Zisserman, Visual Geometry Group, University of Oxford [12] \u2022 \u201cCNN MSER\u201d W."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 431,
                                "start": 374
                            }
                        ],
                        "text": "The\n1http://rrc.cvc.uab.es\n\u201dMethod\u201d, Authors, Affiliation Challenge Task 1.4 2.4 3.1 3.4 4.1 4.3 4.4 \u201cDSM\u201d S. Kim, Qualcomm \u2022 \u201cAJOU\u201d H. I. Koo, and Y. G. Kim, Ajou University [8, 9] \u2022 \u2022 \u201cBeam Search CUNI\u201d J. Libovicky\u0301, and P. Pecina, Charles University in Prague \u2022 \u2022 \u201cBeam Search CUNI +S\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-I\u201d X. C. Yin, C. Yang, J. B. Hou, W. Y. Pei, X. Yin, and K. Huang, University of Science and Technology Beijing and Xi\u2019an Jiaotong-Liverpool University [10, 11, 12, 13] \u2022 \u2022 \u2022 \u2022 \u201cDeep2Text-II\u201d \u2013//\u2013 \u2022 \u2022 \u201cDeep2Text-MO\u201d X. C. Yin, W. Y. Pei, C. Yang, Y. Zheng, Q. Gao, G. Ji, and X. Yin, University of Science and Technology Beijing \u2022 \u2022 \u201cHUST MCLAB\u201d B. Shi, C. Zhang, C. Yao, X. Bai, and Z. Zhang, Huazhong University of Science and Technology \u2022 \u201cMAPS\u201d D. Kumar and A. G. Ramakrishnan, Dayananda Sagar Institutions and Indian Institute of Science [14] \u2022 \u201cNESP\u201d \u2013//\u2013 [15] \u2022 \u201cMSER MRF\u201d X. Liu, NanJing University \u2022 \u201cNJU Text\u201d F. Su, H. Xu, and T. Lu, Nanjing University \u2022 \u2022 \u2022 \u2022 \u201cPAL\u201d Y. C. Wu, K. Chen, X."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18268089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f5b946ea4016f1c4813d926e917d3b3cb2c22de",
            "isKey": true,
            "numCitedBy": 216,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks, while most current research efforts only focus on horizontal or near horizontal scene text. In this paper, first we present a unified distance metric learning framework for adaptive hierarchical clustering, which can simultaneously learn similarity weights (to adaptively combine different feature similarities) and the clustering threshold (to automatically determine the number of clusters). Then, we propose an effective multi-orientation scene text detection system, which constructs text candidates by grouping characters based on this adaptive clustering. Our text candidates construction method consists of several sequential coarse-to-fine grouping steps: morphology-based grouping via single-link clustering, orientation-based grouping via divisive hierarchical clustering, and projection-based grouping also via divisive clustering. The effectiveness of our proposed system is evaluated on several public scene text databases, e.g., ICDAR Robust Reading Competition data sets (2011 and 2013), MSRA-TD500 and NEOCR. Specifically, on the multi-orientation text data set MSRA-TD500, the <inline-formula><tex-math>$f$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"yin-ieq1-2388210.gif\"/></alternatives></inline-formula> measure of our system is <inline-formula><tex-math>$71$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"yin-ieq2-2388210.gif\"/> </alternatives></inline-formula> percent, much better than the state-of-the-art performance. We also construct and release a practical challenging multi-orientation scene text data set (USTB-SV1K), which is available at http://prir.ustb.edu.cn/TexStar/MOMV-text-detection/."
            },
            "slug": "Multi-Orientation-Scene-Text-Detection-with-Yin-Pei",
            "title": {
                "fragments": [],
                "text": "Multi-Orientation Scene Text Detection with Adaptive Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A unified distance metric learning framework for adaptive hierarchical clustering, which can simultaneously learn similarity weights and the clustering threshold, and an effective multi-orientation scene text detection system, which constructs text candidates by grouping characters based on this adaptive clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140087937"
                        ],
                        "name": "Qiu-Feng Wang",
                        "slug": "Qiu-Feng-Wang",
                        "structuredName": {
                            "firstName": "Qiu-Feng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiu-Feng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820427"
                        ],
                        "name": "Fei Yin",
                        "slug": "Fei-Yin",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14340566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "134820d1731bacdd19876d0eb04f27765229ca55",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an effective approach for the offline recognition of unconstrained handwritten Chinese texts. Under the general integrated segmentation-and-recognition framework with character oversegmentation, we investigate three important issues: candidate path evaluation, path search, and parameter estimation. For path evaluation, we combine multiple contexts (character recognition scores, geometric and linguistic contexts) from the Bayesian decision view, and convert the classifier outputs to posterior probabilities via confidence transformation. In path search, we use a refined beam search algorithm to improve the search efficiency and, meanwhile, use a candidate character augmentation strategy to improve the recognition accuracy. The combining weights of the path evaluation function are optimized by supervised learning using a Maximum Character Accuracy criterion. We evaluated the recognition performance on a Chinese handwriting database CASIA-HWDB, which contains nearly four million character samples of 7,356 classes and 5,091 pages of unconstrained handwritten texts. The experimental results show that confidence transformation and combining multiple contexts improve the text line recognition performance significantly. On a test set of 1,015 handwritten pages, the proposed approach achieved character-level accurate rate of 90.75 percent and correct rate of 91.39 percent, which are superior by far to the best results reported in the literature."
            },
            "slug": "Handwritten-Chinese-Text-Recognition-by-Integrating-Wang-Yin",
            "title": {
                "fragments": [],
                "text": "Handwritten Chinese Text Recognition by Integrating Multiple Contexts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experimental results show that confidence transformation and combining multiple contexts improve the text line recognition performance significantly, and are superior by far to the best results reported in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463454"
                        ],
                        "name": "H. Koo",
                        "slug": "H.-Koo",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Koo",
                            "middleNames": [
                                "Il"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111426782"
                        ],
                        "name": "Duck Hoon Kim",
                        "slug": "Duck-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Duck",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duck Hoon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kim, Ajou University [8, 9] \u2022 \u2022"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "87 AJOU[8] 47."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7486022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b45fdcdfb642262cf5b2b2b9e574ba47f471d8a",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new scene text detection algorithm based on two machine learning classifiers: one allows us to generate candidate word regions and the other filters out nontext ones. To be precise, we extract connected components (CCs) in images by using the maximally stable extremal region algorithm. These extracted CCs are partitioned into clusters so that we can generate candidate regions. Unlike conventional methods relying on heuristic rules in clustering, we train an AdaBoost classifier that determines the adjacency relationship and cluster CCs by using their pairwise relations. Then we normalize candidate word regions and determine whether each region contains text or not. Since the scale, skew, and color of each candidate can be estimated from CCs, we develop a text/nontext classifier for normalized images. This classifier is based on multilayer perceptrons and we can control recall and precision rates with a single free parameter. Finally, we extend our approach to exploit multichannel information. Experimental results on ICDAR 2005 and 2011 robust reading competition datasets show that our method yields the state-of-the-art performance both in speed and accuracy."
            },
            "slug": "Scene-Text-Detection-via-Connected-Component-and-Koo-Kim",
            "title": {
                "fragments": [],
                "text": "Scene Text Detection via Connected Component Clustering and Nontext Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new scene text detection algorithm based on two machine learning classifiers that allows us to generate candidate word regions and the other filters out nontext ones, and extends the approach to exploit multichannel information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143848064"
                        ],
                        "name": "Jo\u00e3o F. Henriques",
                        "slug": "Jo\u00e3o-F.-Henriques",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Henriques",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o F. Henriques"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144489408"
                        ],
                        "name": "Rui Caseiro",
                        "slug": "Rui-Caseiro",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Caseiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Caseiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145784436"
                        ],
                        "name": "P. Martins",
                        "slug": "P.-Martins",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Martins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Martins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182210"
                        ],
                        "name": "Jorge P. Batista",
                        "slug": "Jorge-P.-Batista",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Batista",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge P. Batista"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14017201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b4e50860d61095bb5fb65eaa367b131923917be",
            "isKey": false,
            "numCitedBy": 1902,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have seen greater interest in the use of discriminative classifiers in tracking systems, owing to their success in object detection. They are trained online with samples collected during tracking. Unfortunately, the potentially large number of samples becomes a computational burden, which directly conflicts with real-time requirements. On the other hand, limiting the samples may sacrifice performance. \n \nInterestingly, we observed that, as we add more and more samples, the problem acquires circulant structure. Using the well-established theory of Circulant matrices, we provide a link to Fourier analysis that opens up the possibility of extremely fast learning and detection with the Fast Fourier Transform. This can be done in the dual space of kernel machines as fast as with linear classifiers. We derive closed-form solutions for training and detection with several types of kernels, including the popular Gaussian and polynomial kernels. The resulting tracker achieves performance competitive with the state-of-the-art, can be implemented with only a few lines of code and runs at hundreds of frames-per-second. MATLAB code is provided in the paper (see Algorithm 1)."
            },
            "slug": "Exploiting-the-Circulant-Structure-of-with-Kernels-Henriques-Caseiro",
            "title": {
                "fragments": [],
                "text": "Exploiting the Circulant Structure of Tracking-by-Detection with Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the well-established theory of Circulant matrices, this work provides a link to Fourier analysis that opens up the possibility of extremely fast learning and detection with the Fast Fourier Transform, which can be done in the dual space of kernel machines as fast as with linear classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887371"
                        ],
                        "name": "Masashi Koga",
                        "slug": "Masashi-Koga",
                        "structuredName": {
                            "firstName": "Masashi",
                            "lastName": "Koga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masashi Koga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34878566"
                        ],
                        "name": "H. Fujisawa",
                        "slug": "H.-Fujisawa",
                        "structuredName": {
                            "firstName": "Hiromichi",
                            "lastName": "Fujisawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fujisawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42476557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e22b8123da0b35a4afa8ec6730a00991dc76e02",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a handwritten character string recognition system for Japanese mail address reading on a very large vocabulary. The address phrases are recognized as a whole because there is no extra space between words. The lexicon contains 111,349 address phrases, which are stored in a trie structure. In recognition, the text line image is matched with the lexicon entries (phrases) to obtain reliable segmentation and retrieve valid address phrases. The paper first introduces some effective techniques for text line image preprocessing and presegmentation. In presegmentation, the text line image is separated into primitive segments by connected component analysis and touching pattern splitting based on contour shape analysis. In lexicon matching, consecutive segments are dynamically combined into candidate character patterns. An accurate character classifier is embedded in lexicon matching to select characters matched with a candidate pattern from a dynamic category set. A beam search strategy is used to control the lexicon matching so as to achieve real-time recognition. In experiments on 3,589 live mail images, the proposed method achieved correct rate of 83.68 percent while the error rate is less than 1 percent."
            },
            "slug": "Lexicon-Driven-Segmentation-and-Recognition-of-for-Liu-Koga",
            "title": {
                "fragments": [],
                "text": "Lexicon-Driven Segmentation and Recognition of Handwritten Character Strings for Japanese Address Reading"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A handwritten character string recognition system for Japanese mail address reading on a very large vocabulary because there is no extra space between words to achieve real-time recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701229"
                        ],
                        "name": "Keni Bernardin",
                        "slug": "Keni-Bernardin",
                        "structuredName": {
                            "firstName": "Keni",
                            "lastName": "Bernardin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keni Bernardin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742325"
                        ],
                        "name": "R. Stiefelhagen",
                        "slug": "R.-Stiefelhagen",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Stiefelhagen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stiefelhagen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13567980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2258e01865367018ed6f4262c880df85b94959f8",
            "isKey": false,
            "numCitedBy": 1674,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Simultaneous tracking of multiple persons in real-world environments is an active research field and several approaches have been proposed, based on a variety of features and algorithms. Recently, there has been a growing interest in organizing systematic evaluations to compare the various techniques. Unfortunately, the lack of common metrics for measuring the performance of multiple object trackers still makes it hard to compare their results. In this work, we introduce two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time. These metrics have been extensively used in two large-scale international evaluations, the 2006 and 2007 CLEAR evaluations, to measure and compare the performance of multiple object trackers for a wide variety of tracking tasks. Selected performance results are presented and the advantages and drawbacks of the presented metrics are discussed based on the experience gained during the evaluations."
            },
            "slug": "Evaluating-Multiple-Object-Tracking-Performance:-Bernardin-Stiefelhagen",
            "title": {
                "fragments": [],
                "text": "Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time."
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP J. Image Video Process."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349120"
                        ],
                        "name": "Tom\u00e1s Voj\u00edr",
                        "slug": "Tom\u00e1s-Voj\u00edr",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Voj\u00edr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1s Voj\u00edr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The baseline offered is based on the TextSpotter framework for frame-by-frame detection (see section III-C), combined with the FoT tracker8 of Tomas Vojir et al [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2499902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34e7741cc3026f81c564c41b449541a97a26e8ba",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents contributions to the design of the Flock of Trackers (FoT). The FoT estimates the pose of the tracked object by robustly combining displacement estimates from a subset of local trackers that cover the object and has been. The enhancements of the Flock of Trackers are: (i) new reliability predictors for the local trackers - the Neighbourhood consistency predictor and the Markov predictor, (ii) new rules for combining the predictions and (iii) introduction of a RANSAC-based estimator of object motion. The enhanced FoT was extensively tested on 62 sequences.Most of the sequences are standard and used in the literature. The improved FoT showed performance superior to the reference method. For all 62 sequences, the ground truth is made publicly available."
            },
            "slug": "The-Enhanced-Flock-of-Trackers-Voj\u00edr-Matas",
            "title": {
                "fragments": [],
                "text": "The Enhanced Flock of Trackers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The enhancements of the Flock of Trackers are new reliability predictors for the local trackers - the Neighbourhood consistency predictor and the Markov predictor, new rules for combining the predictions and introduction of a RANSAC-based estimator of object motion."
            },
            "venue": {
                "fragments": [],
                "text": "Registration and Recognition in Images and Videos"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 20 I 3 robust reading competition,\" in Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on. IEEE,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 20 II robust reading competition challenge 2: Reading text in scene images,\" in Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "(ICDAR), 2011 International Conference on. IEEE,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 20 II robust reading competition challenge 2 : Reading text in scene images , \" in Document Analysis and Recognition ( ICDAR ) , 2011 International Conference on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 201 I robust reading competition-challenge I: reading text in born-digital images (web and email),\" in Document Analysis and Recognition (ICDAR)"
            },
            "venue": {
                "fragments": [],
                "text": "Inlernational Conference on. IEEE,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision (ICCV),"
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/ICDAR-2015-competition-on-Robust-Reading-Karatzas-Bigorda/9b02f729e6d442f6872078f599fc9da5c3605cee?sort=total-citations"
}