{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 82
                            }
                        ],
                        "text": "For several other experimental observations and comparisons on this data set, see Sindhwani et al. (2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 135
                            }
                        ],
                        "text": "For further experimental benchmark studies and comparisons with numerous other methods, we refer the reader to Chapelle et al. (2006); Sindhwani et al. (2006, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6035769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks."
            },
            "slug": "Beyond-the-point-cloud:-from-transductive-to-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "Beyond the point cloud: from transductive to semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper constructs a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS) that allow the structure of the RKHS to reflect the underlying geometry of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 189
                            }
                        ],
                        "text": "\u2026Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 268
                            }
                        ],
                        "text": "\u2026(Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi, 2003b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "A case of particular recent interest (for example, see Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003a; Donoho and Grimes, 2003; Coifman et al., 2005, for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "(13)\nNote that to avoid degenerate solutions we need to impose some additional conditions (cf. Belkin and Niyogi, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 77
                            }
                        ],
                        "text": "These algorithms are related to spectral clustering and Laplacian Eigenmaps (Belkin and Niyogi, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "It is also worth noting that while Bousquet et al. (2004) use the gradient \u2207 f (x) in the ambient space, we use the gradient over a submanifold \u2207M f for penalizing the function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 85
                            }
                        ],
                        "text": "In addition, several recently proposed transductive methods (e.g., Zhu et al., 2003; Belkin and Niyogi, 2003b) are also seen to be special cases of this general approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 268
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6789724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a49f2d906b48a36ab4baca448298666a9ec259",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the sub-manifold in question rather than the total ambient space. Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once a basis is obtained, training can be performed using the labeled data set. Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian. Practical applications to image and text classification are considered."
            },
            "slug": "Using-manifold-structure-for-partially-labelled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using manifold structure for partially labelled classification"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner under the assumption that the data lie on a submanifold in a high dimensional space is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 169
                            }
                        ],
                        "text": "In Table 5, we report the precision and error rates at the precision-recall break-even point averaged over 100 realizations of the data, and include results reported in Joachims (2003) for spectral graph transduction, and the cotraining algorithm (Blum and Mitchell, 1998) for comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 172
                            }
                        ],
                        "text": "\u2026Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", see Roweis and Saul (2000); Tenenbaum et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 80
                            }
                        ],
                        "text": "This additional information is known to improve performance, as demonstrated in Joachims (2003) and Blum and Mitchell (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 148
                            }
                        ],
                        "text": "\u2026made: (a) transductive categorization with LapSVM and LapRLS leads to significant improvements over inductive categorization with SVM and RLS. (b) Joachims (2003) reports 91.4% precision-recall break-even point, and 4.6% error rate for TSVM. Results for TSVM reported in the table were obtained\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 95
                            }
                        ],
                        "text": "The failure of TSVM in producing reasonable results on this data set has also been observed in Joachims (2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6027413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49b8dff62cccc26023c876460234bf29084a382f",
            "isKey": true,
            "numCitedBy": 734,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "slug": "Transductive-Learning-via-Spectral-Graph-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Learning via Spectral Graph Partitioning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an algorithm that robustly achieves good generalization performance and that can be trained efficiently, and shows a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7513025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor. The advantage of the extension is that it allows predicting the label for a new example without having to solve again a linear system of dimension 'n' (the number of unlabeled and labeled training examples), which can cost O(n^3). Experiments show that the extension works well, in the sense of predicting a label close to the one that would have been obtained if the test example had been included in the unlabeled set. This relatively efficient function induction procedure can also be used when 'n' is large to approximate the solution by writing it only in terms of a kernel expansion with 'm' Keywords: non-parametric models, classification, regression, semi-supervised learning, modeles non parametriques, classification, regression, apprentissage semi-supervise"
            },
            "slug": "Efficient-Non-Parametric-Function-Induction-in-Delalleau-Bengio",
            "title": {
                "fragments": [],
                "text": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experiments show that the proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples are extended to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1761253,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de4569c7353030fec21bbb38c06323dd69f777c",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples."
            },
            "slug": "On-Information-Regularization-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "On Information Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The work extends Szummer and Jaakkola's information regularization to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation, and shows in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2720935"
                        ],
                        "name": "Jihun Ham",
                        "slug": "Jihun-Ham",
                        "structuredName": {
                            "firstName": "Jihun",
                            "lastName": "Ham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jihun Ham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675444"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2913820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "087a090a83e01b0c741b53ec76ffad95ab31e010",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study a family of semisupervised learning algorithms for \u201caligning\u201d different data sets that are characterized by the same underlying manifold. The optimizations of these algorithms are based on graphs that provide a discretized approximation to the manifold. Partial alignments of the data sets\u2014obtained from prior knowledge of their manifold structure or from pairwise correspondences of subsets of labeled examples\u2014 are completed by integrating supervised signals with unsupervised frameworks for manifold learning. As an illustration of this semisupervised setting, we show how to learn mappings between different data sets of images that are parameterized by the same underlying modes of variability (e.g., pose and viewing angle). The curse of dimensionality in these problems is overcome by exploiting the low dimensional structure of image manifolds."
            },
            "slug": "Semisupervised-alignment-of-manifolds-Ham-Lee",
            "title": {
                "fragments": [],
                "text": "Semisupervised alignment of manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A family of semisupervised learning algorithms for \u201caligning\u201d different data sets that are characterized by the same underlying manifold, based on graphs that provide a discretized approximation to the manifold is studied."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848317"
                        ],
                        "name": "Glenn Fung",
                        "slug": "Glenn-Fung",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Fung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn Fung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 56
                            }
                        ],
                        "text": "Semi-Supervised SVMs (S3VM) (Bennett and Demiriz, 1999; Fung and Mangasarian, 2001): S3VM incorporate unlabeled data by including the minimum hinge-loss for the two choices of\nlabels for each unlabeled example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Fung and Mangasarian (2001) reformulate this approach as a concave minimization problem which is solved by a successive linear approximation algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5320604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fced0bfd90bd624876762dd6bfacb992e5ed3b27",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A concave minimization approach is proposed for classifying unlabeled data based on the following ideas: (i) A small representative percentage (5% to 10%) of the unlabeled data is chosen by a clustering algorithm and given to an expert or oracle to label, (ii) A linear support vector machine is trained using the small labeled sample while simultaneously assigning the remaining bulk of the unlabeled dataset to one of two classes so as to maximize the margin (distance) between the two bounding planes that determine the separating plane midway between them. This latter problem is formulated as a concave minimization problem on a polyhedral set for which a stationary point is quickly obtained by solving a few (5 to 7) linear programs. Such stationary points turn out to be very effective as evidenced by our computational results which show that clustered concave minimization yields: (a) Test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert. (b) Test set correctness averaged to within 5.1% when compared to that of a completely supervised linear support vector machine trained on the entire dataset which has been labeled by an expert."
            },
            "slug": "Semi-superyised-support-vector-machines-for-data-Fung-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Semi-superyised support vector machines for unlabeled data classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computational results show that clustered concave minimization yields test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66191239"
                        ],
                        "name": "Bernhard Schlkopf",
                        "slug": "Bernhard-Schlkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schlkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schlkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60860751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee8a371fc5adc5469435020a52fb815f3b57a71",
            "isKey": false,
            "numCitedBy": 2539,
            "numCiting": 473,
            "paperAbstract": {
                "fragments": [],
                "text": "In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series"
            },
            "slug": "Semi-Supervised-Learning-Chapelle-Schlkopf",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This first comprehensive overview of semi-supervised learning presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 248
                            }
                        ],
                        "text": "In Table 5, we report the precision and error rates at the precision-recall break-even point averaged over 100 realizations of the data, and include results reported in Joachims (2003) for spectral graph transduction, and the cotraining algorithm (Blum and Mitchell, 1998) for comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 100
                            }
                        ],
                        "text": "This additional information is known to improve performance, as demonstrated in Joachims (2003) and Blum and Mitchell (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 100
                            }
                        ],
                        "text": "Some recently proposed methods\nc\u00a92006 Mikhail Belkin, Partha Niyogi and Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "\u2026proposed methods\nc\u00a92006 Mikhail Belkin, Partha Niyogi and Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 12
                            }
                        ],
                        "text": "Cotraining (Blum and Mitchell, 1998): The cotraining algorithm was developed to integrate abundance of unlabeled data with availability of multiple sources of information in domains like web-page classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": true,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3711,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2602565"
                        ],
                        "name": "Sean Stromsten",
                        "slug": "Sean-Stromsten",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Stromsten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Stromsten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1303107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a8bed1f13ff4b7b3ae4eedee25a17f7ad2583eb",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples. We test our approach on eight real-world datasets."
            },
            "slug": "Semi-Supervised-Learning-with-Trees-Kemp-Griffiths",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning with Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2283419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96fe05eae94127593ec36858ea4f5d12af28ff93",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion. Unlike previous work using diffusion kernels and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization. This results in flexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results."
            },
            "slug": "Nonparametric-Transforms-of-Graph-Kernels-for-Zhu-Kandola",
            "title": {
                "fragments": [],
                "text": "Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An algorithm based on convex optimization for constructing kernels for semi-supervised learning that incorporates order constraints during optimization results in flexible kernels and avoids the need to choose among different parametric forms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712692"
                        ],
                        "name": "F. Odone",
                        "slug": "F.-Odone",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Odone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Odone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 153
                            }
                        ],
                        "text": "The work presented here is based on the University of Chicago Technical Report TR-2004-05, a short version in the Proceedings of AI and Statistics 2005, Belkin et al. (2005) and Sindhwani (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10368334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f666be963a921837340b63867637b39fb28141b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this lecture we introduce a class of learning algorithms, collectively called manifold regularization algorithms, suited for predicting/classifying data embedded in high-dimensional spaces. We introduce manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which our training set may consist of unlabeled as well as labeled examples."
            },
            "slug": "Manifold-Regularization-Odone-Rosasco",
            "title": {
                "fragments": [],
                "text": "Manifold Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This lecture introduces manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which the training set may consist of unlabeled as well as labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "See the Remarks below and Belkin (2003); Lafon (2004); Belkin and Niyogi (2005); Coifman et al. (2005); Hein et al. (2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "See the Remarks below and (Belkin, 2003; Lafon, 2004; Belkin and Niyogi, 2005; Coifman and Lafon, 2005; Hein et al., 2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10271267,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "40cd779cb417c9e665ec29fdccc73a6499c5ae5e",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-a-theoretical-foundation-for-manifold-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Towards a theoretical foundation for Laplacian-based manifold methods"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 29
                            }
                        ],
                        "text": "Semi-Supervised SVMs (S3VM) (Bennett and Demiriz, 1999; Fung and Mangasarian, 2001): S3VM incorporate unlabeled data by including the minimum hinge-loss for the two choices of\nlabels for each unlabeled example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 65
                            }
                        ],
                        "text": "This is formulated as a mixed-integer program for linear SVMs in Bennett and Demiriz (1999) and is found to be intractable for large amounts of unlabeled data. Fung and Mangasarian (2001) reformulate this approach as a concave minimization problem which is solved by a successive linear approximation algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 65
                            }
                        ],
                        "text": "This is formulated as a mixed-integer program for linear SVMs in Bennett and Demiriz (1999) and is found to be intractable for large amounts of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 28
                            }
                        ],
                        "text": "Semi-Supervised SVMs (S3VM) (Bennett and Demiriz, 1999; Fung and Mangasarian, 2001): S3VM incorporate unlabeled data by including the minimum hinge-loss for the two choices of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7635678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach."
            },
            "slug": "Semi-Supervised-Support-Vector-Machines-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general S3VM model is proposed that minimizes both the misclassification error and the function capacity based on all the available data that can be converted to a mixed-integer program and then solved exactly using integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 136
                            }
                        ],
                        "text": "We also performed one-vs-rest multiclass experiments on the USPS test set with l = 50 and u = 1957 with 10 random splits as provided by Chapelle and Zien (2005). The mean error rates"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 136
                            }
                        ],
                        "text": "We also performed one-vs-rest multiclass experiments on the USPS test set with l = 50 and u = 1957 with 10 random splits as provided by Chapelle and Zien (2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14283441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c5d2cafc35856832f2b478790f0af119baab92",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the first two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods."
            },
            "slug": "Semi-Supervised-Classification-by-Low-Density-Chapelle-Zien",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification by Low Density Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Three semi-supervised algorithms are proposed: deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM, and optimizing the Transductive SVM objective function by gradient descent."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u20261998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 89
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 132
                            }
                        ],
                        "text": "include transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi, 2003b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144500070"
                        ],
                        "name": "Shuchi Chawla",
                        "slug": "Shuchi-Chawla",
                        "structuredName": {
                            "firstName": "Shuchi",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchi Chawla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 142
                            }
                        ],
                        "text": "\u2026methods\nc\u00a92006 Mikhail Belkin, Partha Niyogi and Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5892518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eedbab3ae55fd6a4e7bbc75fcc261293384f883",
            "isKey": false,
            "numCitedBy": 1057,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."
            },
            "slug": "Learning-from-Labeled-and-Unlabeled-Data-using-Blum-Chawla",
            "title": {
                "fragments": [],
                "text": "Learning from Labeled and Unlabeled Data using Graph Mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data is considered."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The work presented here is based on the University of Chicago Technical Report TR-2004-05, a short version in the Proceedings of AI & Statistics 2005, Belkin et al. (2005) and Sindhwani (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Belkin and Niyogi (2003b))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some very preliminary steps in that direction have been taken in Belkin et al. (2004). 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "4 The Representer Theorem for the Empirical Case In the case when M is unknown and sampled via labeled and unlabeled examples, the LaplaceBeltrami operator on M may be approximated by the Laplacian of the data adjacency graph (see Belkin (2003); Bousquet et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 2003a). We also note the regularization based techniques of Corduneanu and Jaakkola (2003) and Bousquet et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "See the Remarks below and (Belkin, 2003; Lafon, 2004; Belkin and Niyogi, 2005; Coifman and Lafon, 2005; Hein et al., 2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(2000); Belkin and Niyogi (2003b); Donoho and Grimes (2003); Coifman et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118947079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0b74dd2397001588673891771de6c221fb91a894",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis discusses the general problem of learning a function on a manifold given by data points. The space of functions on a Riemannian manifold has a family of smoothness functionals and a canonical basis associated to the Laplace-Beltrami operator. Moreover, the Laplace-Beltrami operator can be reconstructed with certain convergence guarantees when the manifold is only known through the sampled data points. This allows the techniques of regularization and Fourier analysis to be applied to functions defined on data. A convergence result is proved for the case when data is sampled from a compact submanifold of R\u2227k . Several applications are considered."
            },
            "slug": "Problems-of-learning-on-manifolds-Niyogi-Belkin",
            "title": {
                "fragments": [],
                "text": "Problems of learning on manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684296"
                        ],
                        "name": "C. Grimes",
                        "slug": "C.-Grimes",
                        "structuredName": {
                            "firstName": "Carrie",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Grimes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 128
                            }
                        ],
                        "text": "A case of particular recent interest (for example, see Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003a; Donoho and Grimes, 2003; Coifman et al., 2005, for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 33
                            }
                        ],
                        "text": "Squared norm of the Hessian (cf. Donoho and Grimes, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1810410,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "57a66ac4a4e0a00d2cdee8711ce0a18b49e9f7a2",
            "isKey": false,
            "numCitedBy": 1589,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space \u211dn, is locally isometric to an open, connected subset \u0398 of Euclidean space \u211dd. Because \u0398 does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form \u210b(f) = \u222bM\u2009\u2225Hf(m)\u2225\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{equation*}{\\mathrm{_{{\\mathit{F}}}^{2}}}\\end{equation*}\\end{document}dm defined on functions f : M \u21a6 \u211d. Here Hf denotes the Hessian of f, and \u210b(f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of \u211dd, then \u210b(f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian."
            },
            "slug": "Hessian-eigenmaps:-Locally-linear-embedding-for-Donoho-Grimes",
            "title": {
                "fragments": [],
                "text": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Hessian-based locally linear embedding method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space is described, where the isometric coordinates can be recovered up to a linear isometry."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": false,
            "numCitedBy": 3073,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "This leads to the class of kernel based algorithms for classification and regression (e.g., Scholkopf and Smola, 2002; Wahba, 1990; Evgeniou et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 55
                            }
                        ],
                        "text": "In many cases, a small amount of feedback is sufficient to allow the child to master the acoustic-to-phonetic mapping of any language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 120
                            }
                        ],
                        "text": "Regularization is a key idea in the theory of splines (e.g., Wahba, 1990) and is widely used in machine learning (e.g., Evgeniou et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 70866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d13bc44e15fd93480e16305d37c025bc0818c2",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case."
            },
            "slug": "Regularization-Networks-and-Support-Vector-Machines-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Both formulations of regularization and Support Vector Machines are reviewed in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 189
                            }
                        ],
                        "text": "\u2026Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 268
                            }
                        ],
                        "text": "\u2026(Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi, 2003b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "A case of particular recent interest (for example, see Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003a; Donoho and Grimes, 2003; Coifman et al., 2005, for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al. (2004); Zhu et al. (2003, 2005); Kemp et al. (2004); Joachims (2003); Belkin and Niyogi (2003b): A variety of graph-based methods have been proposed for transductive inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "(13)\nNote that to avoid degenerate solutions we need to impose some additional conditions (cf. Belkin and Niyogi, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 77
                            }
                        ],
                        "text": "These algorithms are related to spectral clustering and Laplacian Eigenmaps (Belkin and Niyogi, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "It is also worth noting that while Bousquet et al. (2004) use the gradient \u2207 f (x) in the ambient space, we use the gradient over a submanifold \u2207M f for penalizing the function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 85
                            }
                        ],
                        "text": "In addition, several recently proposed transductive methods (e.g., Zhu et al., 2003; Belkin and Niyogi, 2003b) are also seen to be special cases of this general approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 268
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 41
                            }
                        ],
                        "text": "Graph-Based Approaches See, for example, Blum and Chawla (2001); Chapelle et al. (2003); Szummer and Jaakkola (2002); Zhou et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14879317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88816ae492956f3004daa41357166f1181c0c1bf",
            "isKey": false,
            "numCitedBy": 7047,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed."
            },
            "slug": "Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a geometrically motivated algorithm for representing the high-dimensional data that provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458509"
                        ],
                        "name": "Irina Matveeva",
                        "slug": "Irina-Matveeva",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Matveeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irina Matveeva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "Some very preliminary steps in that direction have been taken in Belkin et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "For graph regularization and label propagation see (Smola and Kondor, 2003; Belkin et al., 2004; Zhu et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44352521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5c6ea2f23fe8d3e986c4c99e83a90c204538619",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of labeling a partially labeled graph. This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings. It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance."
            },
            "slug": "Regularization-and-Semi-supervised-Learning-on-Belkin-Matveeva",
            "title": {
                "fragments": [],
                "text": "Regularization and Semi-supervised Learning on Large Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work considers the problem of labeling a partially labeled graph, which may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12184,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 252
                            }
                        ],
                        "text": "\u2026(Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi, 2003b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 41
                            }
                        ],
                        "text": "The joint optimization is implemented in Joachims (1999) by first using an inductive SVM to label the unlabeled data and then iteratively solving SVM quadratic programs, at each step switching labels to improve the objective function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "Some recently proposed methods\nc\u00a92006 Mikhail Belkin, Partha Niyogi and Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "Transductive SVM (TSVM) (Vapnik, 1998; Joachims, 1999): TSVMs are based on the following optimization principle:\nf \u2217 = argmin f\u2208HKyl+1,...yl+u\nC l\n\u2211 i=1\n(1\u2212 yi f (xi))+ +C\u2217 l+u\n\u2211 i=l+1\n(1\u2212 yi f (xi))+ +\u2016 f\u20162K ,\nwhich proposes a joint optimization of the SVM objective function over binary-valued\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": true,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Method PRBEP Error k-NN Joachims (2003) 73."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(b) Joachims (2003) reports 91."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 104
                            }
                        ],
                        "text": "See the Remarks below and Belkin (2003); Lafon (2004); Belkin and Niyogi (2005); Coifman et al. (2005); Hein et al. (2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "See the Remarks below and (Belkin, 2003; Lafon, 2004; Belkin and Niyogi, 2005; Coifman and Lafon, 2005; Hein et al., 2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2789515,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7edde67273c2ad09458d73328628f3385d0df837",
            "isKey": true,
            "numCitedBy": 317,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In the machine learning community it is generally believed that graph Laplacians corresponding to a finite sample of data points converge to a continuous Laplace operator if the sample size increases. Even though this assertion serves as a justification for many Laplacian-based algorithms, so far only some aspects of this claim have been rigorously proved. In this paper we close this gap by establishing the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator. Our investigation also includes the important case where the data lies on a submanifold of R d ."
            },
            "slug": "From-Graphs-to-Manifolds-Weak-and-Strong-Pointwise-Hein-Audibert",
            "title": {
                "fragments": [],
                "text": "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper establishes the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "Also see Bengio et al. (2004) and Brand (2003) for some recent related work on out-of-sample extensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122865377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99cd988b104202887ad9657b8a61baa7ff0581c1",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We equate nonlinear dimensionality reduction (NLDR) to graph embedding with side information about the vertices, and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space. Unlike most spectral NLDR methods, the central eigenproblem can be made relatively small, and the result is a continuous mapping defined over the entire space, not just the datapoints. A demonstration is made to visualizing the distribution of word usages (as a proxy to word meanings) in a sample of the machine learning literature."
            },
            "slug": "Continuous-nonlinear-dimensionality-reduction-by-Brand",
            "title": {
                "fragments": [],
                "text": "Continuous nonlinear dimensionality reduction by kernel Eigenmaps"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This work equate nonlinear dimensionality reduction (NLDR) to graph embedding with side information about the vertices, and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755208"
                        ],
                        "name": "F. Cucker",
                        "slug": "F.-Cucker",
                        "structuredName": {
                            "firstName": "Felipe",
                            "lastName": "Cucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34911188"
                        ],
                        "name": "S. Smale",
                        "slug": "S.-Smale",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8188805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b2dd79083a74699e4e0509ac3f0a8a302b4eabe",
            "isKey": false,
            "numCitedBy": 1461,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In"
            },
            "slug": "On-the-mathematical-foundations-of-learning-Cucker-Smale",
            "title": {
                "fragments": [],
                "text": "On the mathematical foundations of learning"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference) and relations of the theory of learning to the mainstream of mathematics are emphasized."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 331378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bbc0c752570c46a772f2982728f9ad4191f25dd",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach."
            },
            "slug": "Cluster-Kernels-for-Semi-Supervised-Learning-Chapelle-Weston",
            "title": {
                "fragments": [],
                "text": "Cluster Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label is proposed by modifying the eigenspectrum of the kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For additional details on the derivation and alternative formulations of SVMs, see Scholkopf and Smola (2002), Rifkin (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": "A case of particular recent interest (for example, see Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003a; Donoho and Grimes, 2003; Coifman et al., 2005, for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13983,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 30
                            }
                        ],
                        "text": "Measure-Based Regularization (Bousquet et al., 2004): The conceptual framework of this work is closest to our approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 87
                            }
                        ],
                        "text": "We also note the regularization based techniques of Corduneanu and Jaakkola (2003) and Bousquet et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 196
                            }
                        ],
                        "text": "In the case when M is unknown and sampled via labeled and unlabeled examples, the LaplaceBeltrami operator on M may be approximated by the Laplacian of the data adjacency graph (see Belkin, 2003; Bousquet et al., 2004, for some discussion)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Regularization is a key idea in the theory of splines (e.g., Wahba, 1990) and is widely used in machine learning (e.g., Evgeniou et al., 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 35
                            }
                        ],
                        "text": "It is also worth noting that while Bousquet et al. (2004) use the gradient \u2207 f (x) in the ambient space, we use the gradient over a submanifold \u2207M f for penalizing the function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3118640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e0d11533c411e3c0559762e7cfc6790c28ccf2b",
            "isKey": true,
            "numCitedBy": 129,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations."
            },
            "slug": "Measure-Based-Regularization-Bousquet-Chapelle",
            "title": {
                "fragments": [],
                "text": "Measure Based Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes three theoretical methods for taking into account this distribution P(x) for regularization and provides links to existing graph-based semi-supervised learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39651651"
                        ],
                        "name": "Jean-Fran\u00e7ois Paiement",
                        "slug": "Jean-Fran\u00e7ois-Paiement",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Paiement",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Paiement"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 19
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 20
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps. This leads to new method for dimensionality reduction and data representation. Further study of this approach is a direction of future research. We note that a similar algorithm has been independently proposed in Vert and Yamanishi (2005) in the context of supervised graph inference."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 635,
                                "start": 20
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps. This leads to new method for dimensionality reduction and data representation. Further study of this approach is a direction of future research. We note that a similar algorithm has been independently proposed in Vert and Yamanishi (2005) in the context of supervised graph inference. A relevant discussion is also presented in Ham et al. (2005) on the interpretation of several geometric dimensionality reduction techniques as kernel methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 9
                            }
                        ],
                        "text": "Also see Bengio et al. (2004) and Brand (2003) for some recent related work on out-of-sample extensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 8
                            }
                        ],
                        "text": "(2005); Bengio et al. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 20
                            }
                        ],
                        "text": "Unlike recent work (Bengio et al., 2004; Brand, 2003) on out-of-sample extensions, our method is based on a Representer theorem for RKHS. Remark 2: By taking multiple eigenvectors of the system in Equation 14 we obtain a natural regularized out-of-sample extension of Laplacian Eigenmaps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6894357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f39fe2659603f4194fd638d1a2e17985415c3bb",
            "isKey": true,
            "numCitedBy": 1067,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data."
            },
            "slug": "Out-of-Sample-Extensions-for-LLE,-Isomap,-MDS,-and-Bengio-Paiement",
            "title": {
                "fragments": [],
                "text": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for extending Local Linear Embedding, Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling as well as for Spectral Clustering is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724293"
                        ],
                        "name": "Yoshihiro Yamanishi",
                        "slug": "Yoshihiro-Yamanishi",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Yamanishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Yamanishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 68
                            }
                        ],
                        "text": "We note that a similar algorithm has been independently proposed in Vert and Yamanishi (2005) in the context of supervised graph inference."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 153
                            }
                        ],
                        "text": "We also note that a method similar to our regularized spectral clustering algorithm has been independently proposed in the context of graph inference in Vert and Yamanishi (2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11678258,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "addc878150c33c34147ae367c75be736e2b7f1ff",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the problem of graph inference where part of the graph is known as a supervised learning problem, and propose an algorithm to solve it. The method involves the learning of a mapping of the vertices to a Euclidean space where the graph is easy to infer, and can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of metabolic network reconstruction from genomic data."
            },
            "slug": "Supervised-Graph-Inference-Vert-Yamanishi",
            "title": {
                "fragments": [],
                "text": "Supervised Graph Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work forms the problem of graph inference where part of the graph is known as a supervised learning problem, and proposes an algorithm to solve it that can be formulated as an optimization problem in a reproducing kernel Hilbert space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 107
                            }
                        ],
                        "text": "We start by recalling some basic properties of reproducing kernel Hilbert spaces (see the original work of Aronszajn, 1950; Cucker and Smale, 2002, for a nice discussion in the context of learning theory) and their connections to integral operators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "See Aronszajn (1950) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "We give a convergence argument similar to the one found in Aronszajn (1950)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 17
                            }
                        ],
                        "text": "It can be shown (Aronszajn, 1950, p. 350) that the space (HK)M of functions fromHK restricted toM is an RKHS with the kernel KM , in other words (HK)M = HKM ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": true,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37805393"
                        ],
                        "name": "S. Lafon",
                        "slug": "S.-Lafon",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Lafon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lafon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107734495"
                        ],
                        "name": "A. B. Lee",
                        "slug": "A.-B.-Lee",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Lee",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. B. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34207023"
                        ],
                        "name": "M. Maggioni",
                        "slug": "M.-Maggioni",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Maggioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maggioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786884"
                        ],
                        "name": "B. Nadler",
                        "slug": "B.-Nadler",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Nadler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Nadler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49818480"
                        ],
                        "name": "F. Warner",
                        "slug": "F.-Warner",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Warner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Warner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 81
                            }
                        ],
                        "text": "See the Remarks below and Belkin (2003); Lafon (2004); Belkin and Niyogi (2005); Coifman et al. (2005); Hein et al. (2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 153
                            }
                        ],
                        "text": "A case of particular recent interest (for example, see Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003a; Donoho and Grimes, 2003; Coifman et al., 2005, for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15926341,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01b24de15cf337c55b9866c4b534596ca3d93abe",
            "isKey": false,
            "numCitedBy": 1370,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis."
            },
            "slug": "Geometric-diffusions-as-a-tool-for-harmonic-and-of-Coifman-Lafon",
            "title": {
                "fragments": [],
                "text": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 140
                            }
                        ],
                        "text": "\u2026(Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5525836,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6320770fe216ebbba769b9f0a006669b616a03d0",
            "isKey": false,
            "numCitedBy": 889,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-Kernels-on-Graphs-and-Other-Discrete-Kondor-Lafferty",
            "title": {
                "fragments": [],
                "text": "Diffusion Kernels on Graphs and Other Discrete Input Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea, and focuses on generating kernels on graphs, for which a special class of exponential kernels called diffusion kernels are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5945304"
                        ],
                        "name": "I. Holopainen",
                        "slug": "I.-Holopainen",
                        "structuredName": {
                            "firstName": "Ilkka",
                            "lastName": "Holopainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Holopainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "In that case, one natural choice for \u2016 f\u2016I is R x\u2208M \u2016\u2207M f\u20162 dPX(x), where \u2207M is the gradient (see, for example Do Carmo, 1992, for an introduction to differential geometry) of f along the manifold M and the integral is taken over the marginal distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4067798,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "727db858887077fba3a710626e1ff50d679f799c",
            "isKey": false,
            "numCitedBy": 6085,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "THE recent physical interpretation of intrinsic differential geometry of spaces has stimulated the study of this subject. Riemann proposed the generalisation, to spaces of any order, of Gauss's theory of surfaces, and introduced certain fundamental ideas in this general theory. Bianchi, Beltrami, and others made substantial contributions to the subject, which was extended by Ricci with the use of tensor analysis and his absolute calculus. Recently there has been an extensive study and development of Riemannian geometry, and the book before us aims at presenting the existing theory.Riemannian Geometry.By Prof. L. P. Eisenhart. Pp. vii + 262. (Princeton: Princeton University Press; London: Oxford University Press, 1926.) 13s. 6d. net."
            },
            "slug": "Riemannian-Geometry-Holopainen",
            "title": {
                "fragments": [],
                "text": "Riemannian Geometry"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1927
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 88517984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6214fc0672de4ccfce1cef8b2d1875e6ea7a3db7",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Consistency is a key property of all statistical procedures analyzing randomly sampled data. Surprisingly, despite decades of work, little is known about consistency of most clustering algorithms. In this paper we investigate consistency of the popular family of spectral clustering algorithms, which clusters the data with the help of eigenvectors of graph Laplacian matrices. We develop new methods to establish that, for increasing sample size, those eigenvectors converge to the eigenvectors of certain limit operators. As a result, we can prove that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other (unnormalized clustering) is only consistent under strong additional assumptions, which are not always satisfied in real data. We conclude that our analysis provides strong evidence for the superiority of normalized spectral clustering."
            },
            "slug": "Consistency-of-spectral-clustering-Luxburg-Belkin",
            "title": {
                "fragments": [],
                "text": "Consistency of spectral clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other is only consistent under strong additional assumptions, which are not always satisfied in real data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 111
                            }
                        ],
                        "text": "For further experimental benchmark studies and comparisons with numerous other methods, we refer the reader to Chapelle et al. (2006); Sindhwani et al. (2006, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7326173,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "60de4b6068407defa3c88f5feeb8b74d8e55fe9c",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators."
            },
            "slug": "Kernels-and-Regularization-on-Graphs-Smola-Kondor",
            "title": {
                "fragments": [],
                "text": "Kernels and Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators and can be found as a special case of the reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 10
                            }
                        ],
                        "text": "Following Scholkopf et al. (1995), we chose to train classifiers with polynomial kernels of degree 3, and set the weight on the regularization term for inductive methods as \u03b3l = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "Following Scholkopf et al. (1995), we chose to train classifiers with polynomial kernels of degree 3, and set the weight on the regularization term for inductive methods as \u03b3l = 0.05(C = 10)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 83
                            }
                        ],
                        "text": "For additional details on the derivation and alternative formulations of SVMs, see Scholkopf and Smola (2002); Rifkin (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": true,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "(c) Also shown, in the rightmost scatter plot in the bottom row of Figure 4, are standard deviation of error rates obtained by LapSVM and TSVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "The following comments can be made: (a) LapSVM and LapRLS make significant performance improvements over inductive methods and TSVM, for predictions on unlabeled speakers that come from the same group as the labeled speaker, over all choices of the labeled speaker."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 25
                            }
                        ],
                        "text": "Transductive SVM (TSVM) (Vapnik, 1998; Joachims, 1999): TSVMs are based on the following optimization principle:\nf \u2217 = argmin f\u2208HKyl+1,...yl+u\nC l\n\u2211 i=1\n(1\u2212 yi f (xi))+ +C\u2217 l+u\n\u2211 i=l+1\n(1\u2212 yi f (xi))+ +\u2016 f\u20162K ,\nwhich proposes a joint optimization of the SVM objective function over binary-valued\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "The following comments can be made: (a) manifold regularization results in significant improvements over inductive classification, for both RLS and SVM, and either compares well or significantly outperforms TSVM across the 45 classification problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Transductive SVM (TSVM) (Vapnik, 1998; Joachims, 1999): TSVMs are based on the following optimization principle:\nf \u2217 = argmin f\u2208HKyl+1,...yl+u\nC l\n\u2211 i=1\n(1\u2212 yi f (xi))+ +C\u2217 l+u\n\u2211 i=l+1\n(1\u2212 yi f (xi))+ +\u2016 f\u20162K ,\nwhich proposes a joint optimization of the SVM objective function over binary-valued labels on the unlabeled data and functions in the RKHS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "In Figure 4, we compare the error rates of manifold regularization algorithms, inductive classifiers and TSVM, at the break-even points in the precision-recall curves for the 45 binary classi-\nfication problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "Some recently proposed methods\nc\u00a92006 Mikhail Belkin, Partha Niyogi and Vikas Sindhwani.\ninclude transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "In this experiment, TSVM actually performs worse than the SVM baseline probably since local minima problems become severe in a multi-class setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The average training time for TSVMwas found to be more than 10 times slower than for LapSVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 266
                            }
                        ],
                        "text": "The following comments can be made: (a) transductive categorization with LapSVM and LapRLS leads to significant improvements over inductive categorization with SVM and RLS. (b) Joachims (2003) reports 91.4% precision-recall break-even point, and 4.6% error rate for TSVM. Results for TSVM reported in the table were obtained when we ran the TSVM implementation using SVM-Light software on this particular data set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "The failure of TSVM in producing reasonable results on this data set has also been observed in Joachims (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Note that even though TSVM were inspired by transductive inference, they do provide an out-of-sample extension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Note that TSVM solves multiple quadratic programs in the size of the labeled and unlabeled sets whereas LapSVM solves a single QP (Equation 11) in the size of the labeled set, followed by a linear system (Equation 10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "We found LapSVM to be significantly more stable than the inductive methods and TSVM, with respect to choice of the labeled data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "In Figure 3, the best decision surfaces across a wide range of parameter settings are also shown for SVM, transductive SVM and Laplacian SVM. Figure 3 demonstrates how TSVM fails to find the optimal solution, probably since it gets stuck in a local minimum."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1934343"
                        ],
                        "name": "David Hecherman",
                        "slug": "David-Hecherman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hecherman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Hecherman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 617436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02adea3455cd7b09e1dac9ddf2637a1e7ae84005",
            "isKey": false,
            "numCitedBy": 1291,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "1. ABSTRACT Text categorization \u2013 the assignment of natural language texts to one or more predefined categories based on their content \u2013 is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1"
            },
            "slug": "Inductive-learning-algorithms-and-representations-Dumais-Platt",
            "title": {
                "fragments": [],
                "text": "Inductive learning algorithms and representations for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A comparison of the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 119
                            }
                        ],
                        "text": "This leads to the class of kernel based algorithms for classification and regression (e.g., Scholkopf and Smola, 2002; Wahba, 1990; Evgeniou et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "In many cases, a small amount of feedback is sufficient to allow the child to master the acoustic-to-phonetic mapping of any language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 61
                            }
                        ],
                        "text": "Regularization is a key idea in the theory of splines (e.g., Wahba, 1990) and is widely used in machine learning (e.g., Evgeniou et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 92
                            }
                        ],
                        "text": "This leads to the class of kernel based algorithms for classification and regression (e.g., Scholkopf and Smola, 2002; Wahba, 1990; Evgeniou et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 61
                            }
                        ],
                        "text": "The proof is based on a simple orthogonality argument (e.g., Scholkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 8
                            }
                        ],
                        "text": "In many cases, a small amount of feedback is sufficient to allow the child to master the acoustic-to-phonetic mapping of any language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 83
                            }
                        ],
                        "text": "For additional details on the derivation and alternative formulations of SVMs, see Scholkopf and Smola (2002); Rifkin (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": true,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 421,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper. We pos tpone the discussion of related algorithms and various connections until Section 4.5. The idea of regularization has a rich mathematical history going back to Tikhon ov (1963), where it is used for solving ill-posed inverse problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 553,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper. We pos tpone the discussion of related algorithms and various connections until Section 4.5. The idea of regularization has a rich mathematical history going back to Tikhon ov (1963), where it is used for solving ill-posed inverse problems. Regularization is a key idea in the theory of splines (e.g., Wahba (1990)) and is widely used in machine learning (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2231,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper. We pos tpone the discussion of related algorithms and various connections until Section 4.5. The idea of regularization has a rich mathematical history going back to Tikhon ov (1963), where it is used for solving ill-posed inverse problems. Regularization is a key idea in the theory of splines (e.g., Wahba (1990)) and is widely used in machine learning (e.g., Evgeniou t al. (2000)). Many machine learning algorithms, including Support Vector Machines, can be inte rpre d as instances of regularization. Our framework exploits the geometry of the probability distribution that genera tes the data and incorporates it as an additional regularization term. Hence, there are two r egula ization terms \u2014 one controlling the complexity of the classifier in the ambient spaceand the other controlling the complexity as measured by the g ometryof the distribution. We consider in some detail the special case where this probability distribution is supported on a submanifold of the amb ient space. The points below highlight several aspects of the current paper: 1. Our general framework brings together three distinct concepts that h ave received some independent recent attention in machine learning: i. The first of these is the technology of spectral graph theory(e.g., see Chung (1997)) that has been applied to a wide range of clustering and classification tasks over the la s two decades. Such methods typically reduce to certain eigenvalue problems. ii. The second is the geometric point of view embodied in a class of algorithms that can be termed asmanifold learning1. These methods attempt to use the geometry of the probability distribution by assuming that its support has the geometric structure of a Rieman nian manifold. iii. The third important conceptual framework is the set of ideas surroundin g regularization in Reproducing Kernel Hilbert Spaces (RKHS). This leads to the class of kernel based algorithmsfor classification and regression (e.g., see Scholkopf and Smola (2002) ; Wahba (1990); Evgeniou et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1524,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper. We pos tpone the discussion of related algorithms and various connections until Section 4.5. The idea of regularization has a rich mathematical history going back to Tikhon ov (1963), where it is used for solving ill-posed inverse problems. Regularization is a key idea in the theory of splines (e.g., Wahba (1990)) and is widely used in machine learning (e.g., Evgeniou t al. (2000)). Many machine learning algorithms, including Support Vector Machines, can be inte rpre d as instances of regularization. Our framework exploits the geometry of the probability distribution that genera tes the data and incorporates it as an additional regularization term. Hence, there are two r egula ization terms \u2014 one controlling the complexity of the classifier in the ambient spaceand the other controlling the complexity as measured by the g ometryof the distribution. We consider in some detail the special case where this probability distribution is supported on a submanifold of the amb ient space. The points below highlight several aspects of the current paper: 1. Our general framework brings together three distinct concepts that h ave received some independent recent attention in machine learning: i. The first of these is the technology of spectral graph theory(e.g., see Chung (1997)) that has been applied to a wide range of clustering and classification tasks over the la s two decades."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 8
                            }
                        ],
                        "text": "(2000); Belkin and Niyogi (2003b); Donoho and Grimes (2003); Coifman et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "See the Remarks below and Belkin (2003); Lafon (2004); Belkin and Niyogi (2005); Coifman et al. (2005); Hein et al. (2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3009,
                                "start": 24
                            }
                        ],
                        "text": ", 2004; Joachims, 1999; Belkin and Niyogi, 20 03a). We also note the regularization based techniques of Corduneanu and Jaakkola (200 3) and Bousquet et al. (2004). The latter reference is closest in spirit to the intuitions of our paper. We pos tpone the discussion of related algorithms and various connections until Section 4.5. The idea of regularization has a rich mathematical history going back to Tikhon ov (1963), where it is used for solving ill-posed inverse problems. Regularization is a key idea in the theory of splines (e.g., Wahba (1990)) and is widely used in machine learning (e.g., Evgeniou t al. (2000)). Many machine learning algorithms, including Support Vector Machines, can be inte rpre d as instances of regularization. Our framework exploits the geometry of the probability distribution that genera tes the data and incorporates it as an additional regularization term. Hence, there are two r egula ization terms \u2014 one controlling the complexity of the classifier in the ambient spaceand the other controlling the complexity as measured by the g ometryof the distribution. We consider in some detail the special case where this probability distribution is supported on a submanifold of the amb ient space. The points below highlight several aspects of the current paper: 1. Our general framework brings together three distinct concepts that h ave received some independent recent attention in machine learning: i. The first of these is the technology of spectral graph theory(e.g., see Chung (1997)) that has been applied to a wide range of clustering and classification tasks over the la s two decades. Such methods typically reduce to certain eigenvalue problems. ii. The second is the geometric point of view embodied in a class of algorithms that can be termed asmanifold learning1. These methods attempt to use the geometry of the probability distribution by assuming that its support has the geometric structure of a Rieman nian manifold. iii. The third important conceptual framework is the set of ideas surroundin g regularization in Reproducing Kernel Hilbert Spaces (RKHS). This leads to the class of kernel based algorithmsfor classification and regression (e.g., see Scholkopf and Smola (2002) ; Wahba (1990); Evgeniou et al. (2000)). We show how these ideas can be brought together in a coherent and natu ral w y to incorporate geometric structure in a kernel based regularization framework. As far a s we know, these ideas have not been unified in a similar fashion before. 2. This general framework allows us to develop algorithms spanning the ran g from unsupervised to fully supervised learning. In this paper we primarily focus on the semi-supervised setting and present two families of algorithms: the Laplacian Regularized Least Squares (hereafter LapRL S) and the Laplacian Support Vector Machines (hereafter LapSVM). These are natural e xtensions of RLS and SVM respectively. In addition, several recently proposed transductive me thods (e.g., Zhu et al. (2003); Belkin and Niyogi (2003a)) are also seen to be special cases of thi general approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 8
                            }
                        ],
                        "text": "(2000); Belkin and Niyogi (2003b); Donoho and Grimes (2003); Coifman et al. (2 005) for a discussion on dimensionality reduction) is when the support of PX is a compact submanifold M \u2282 Rn. In that case, one natural choice for \u2016f\u2016I is \u222b x\u2208M \u2016\u2207Mf\u2016(2) dPX(x), where\u2207M is thegradient(see, e.g., Do Carmo (1992) for an introduction to differential geometry) of f along the manifoldM and the integral is taken over the marginal distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Laplacian eigenmaps for dimensionality redu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 82
                            }
                        ],
                        "text": "For several other experimental observations and comparisons on this data set, see Sindhwani et al. (2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 172
                            }
                        ],
                        "text": "For highly sparse data sets, for example, in text categorization problems, effective conjugate gradient schemes can be used in a large scale implementation, as outlined in Sindhwani et al. (2006). For the non-linear case, one may obtain approximate solutions (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 28
                            }
                        ],
                        "text": "Some ideas are presented in Sindhwani (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 178
                            }
                        ],
                        "text": "The work presented here is based on the University of Chicago Technical Report TR-2004-05, a short version in the Proceedings of AI and Statistics 2005, Belkin et al. (2005) and Sindhwani (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel machines for semi-supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Master\u2019s thesis, The University of Chicago,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 77
                            }
                        ],
                        "text": "The relation of L\u0303 to the weighted Laplace-Beltrami operator was discussed in Lafon (2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "See the Remarks below and Belkin (2003); Lafon (2004); Belkin and Niyogi (2005); Coifman et al. (2005); Hein et al. (2005) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 38
                            }
                        ],
                        "text": "Bayesian Techniques See, for example, Nigam et al. (2000); Seeger (2001); Corduneanu and Jaakkola (2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 78
                            }
                        ],
                        "text": "The relation of L\u0303 to the weighted Laplace-Beltrami operator was discussed in Lafon (2004). Remark 4: Note that a global kernel K restricted toM (denoted by KM ) is also a kernel defined on M with an associated RKHS HM of functions M \u2192 R."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Diffusion Maps and Geometric Harmonics"
            },
            "venue": {
                "fragments": [],
                "text": "Diffusion Maps and Geometric Harmonics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "The idea of regularization has a rich mathematical history going back to Tikhonov (1963), where it is used for solving ill-posed inverse problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 62
                            }
                        ],
                        "text": "scalability issues in semi-supervised learning, see, example, Tsang and Kwok. (2005); Bengio et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118021014,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8009c64e9b1fd3354afbf192b8a495978db17683",
            "isKey": false,
            "numCitedBy": 1031,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "REGULARIZATION-OF-INCORRECTLY-POSED-PROBLEMS-Tikhonov",
            "title": {
                "fragments": [],
                "text": "REGULARIZATION OF INCORRECTLY POSED PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 111
                            }
                        ],
                        "text": "For additional details on the derivation and alternative formulations of SVMs, see Scholkopf and Smola (2002); Rifkin (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60488836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5a433764320146b8c8496c4172dec6c32e506bf",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Everything-old-is-new-again:-a-fresh-look-at-in-Rifkin-Poggio",
            "title": {
                "fragments": [],
                "text": "Everything old is new again: a fresh look at historical approaches in machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12671141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11b324fe84f6aba94af668086812a83b19494c3b",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Manifold-Stucture-for-Partially-Labeled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using Manifold Stucture for Partially Labeled Classification"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 172
                            }
                        ],
                        "text": "For highly sparse data sets, for example, in text categorization problems, effective conjugate gradient schemes can be used in a large scale implementation, as outlined in Sindhwani et al. (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 135
                            }
                        ],
                        "text": "For further experimental benchmark studies and comparisons with numerous other methods, we refer the reader to Chapelle et al. (2006); Sindhwani et al. (2006, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63624102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b61c3cdc807820e34e022540ed92ad892cfd4fc0",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Geometric-Basis-of-Semi-Supervised-Learning-Sindhwani-Belkin",
            "title": {
                "fragments": [],
                "text": "The Geometric Basis of Semi-Supervised Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 111
                            }
                        ],
                        "text": "For further experimental benchmark studies and comparisons with numerous other methods, we refer the reader to Chapelle et al. (2006); Sindhwani et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 132
                            }
                        ],
                        "text": "include transductive SVM (Vapnik, 1998; Joachims, 1999), cotraining (Blum and Mitchell, 1998), and a variety of graph-based methods (Blum and Chawla, 2001; Chapelle et al., 2003; Szummer and Jaakkola, 2002; Kondor and Lafferty, 2002; Smola and Kondor, 2003; Zhou et al., 2004; Zhu et al., 2003, 2005; Kemp et al., 2004; Joachims, 1999; Belkin and Niyogi, 2003b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scholkopf. Cluster kernels for semi-supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Very large scale manifold regularization using core vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2005 Workshop on Large Scale Kernel Machines"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Very large scale manifold regularization using core vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2005 Workshop on Large Scale Kernel Machines"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 28
                            }
                        ],
                        "text": "Some ideas are presented in Sindhwani (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 178
                            }
                        ],
                        "text": "The work presented here is based on the University of Chicago Technical Report TR-2004-05, a short version in the Proceedings of AI and Statistics 2005, Belkin et al. (2005) and Sindhwani (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel Machines for Semi-supervised Learning. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Kernel Machines for Semi-supervised Learning. Master's thesis"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Heat kernels on weighted manifolds and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Cont. Math"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Seeger (2001) provides a detailed overview of Bayesian frameworks for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "Bayesian Techniques See, for example, Nigam et al. (2000); Seeger (2001); Corduneanu and Jaakkola (2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data. Inst. for Adaptive and Neural Computation, technical report"
            },
            "venue": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data. Inst. for Adaptive and Neural Computation, technical report"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient non-parametric func"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 70
                            }
                        ],
                        "text": "Using L\u0303 instead of L provides certain theoretical guarantees (see von Luxburg et al., 2004) and seems to perform as well or better in many practical tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistency of spectral clustering. Max Planck Institute for"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics Technical Report TR"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 27,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 61,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Manifold-Regularization:-A-Geometric-Framework-for-Belkin-Niyogi/19bb0dce99466077e9bc5a2ad4941607fc28b40c?sort=total-citations"
}