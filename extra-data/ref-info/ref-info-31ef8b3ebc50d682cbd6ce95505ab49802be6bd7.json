{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380801"
                        ],
                        "name": "M. McIlroy",
                        "slug": "M.-McIlroy",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "McIlroy",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McIlroy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62418519,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "e08c8a4c17f23c41616649ca73a908d06828d67f",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The word list used by the UNIX spelling checker, SPELL, was developed from many sources over several years. As the spelling checker may be used on minicomputers, it is important to make the list as compact as possible. Stripping prefixes and suffixes reduces the list below one third of its original size, hashing discards 60 percent of the bits that remain, and data compression halves it once again. This paper tells how the spelling checker works, how the words were chosen, how the spelling checker was used to improve itself, and how the (reduced) list of 30000 English words was squeezed into 26000 16-bit machine words."
            },
            "slug": "Development-of-a-Spelling-List-McIlroy",
            "title": {
                "fragments": [],
                "text": "Development of a Spelling List"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How the spelling checker works, how the words were chosen, how it was used to improve itself, and how the (reduced) list of 30000 English words was squeezed into 26000 16-bit machine words are told."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 200
                            }
                        ],
                        "text": "Perhaps our program could take advantage of these contextual cues by adopting very simple language modeling techniques such as trigrams, that have proven effective for speech recognition applications (Jelinek, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 105
                            }
                        ],
                        "text": "Using a classic Bayesian argument of the kind that is very popular in the speech recognition literature (Jelinek, 1985), one can often recover the intended correction, c, from a typo, t, by finding the correction c that maximizes Pr(c) Pr(t c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "Using a classic Bayesian argument of the kind that is popular in the speech recognition literature (Jelinek, 1985), one can often recover the intended correction, c, from a typo, t, by finding the correction c that maximizes P r ( c ) P r ( t l c ) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "Perhaps our program could take advantage of these contextual cues by adopting very simple language modeling techniques such as\ntrigrams, that have proven effective for speech recognition applications (Jelinek, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organized Language Modeling for Speech Recognition,\" IBM Report"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 56
                            }
                        ],
                        "text": "The matrices are smoothed using the Good-Turing method (Good, 1953)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11945361,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2986b25f50babd536dd0ecf2237d9eabf5843c2",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-POPULATION-FREQUENCIES-OF-SPECIES-AND-THE-OF-Good",
            "title": {
                "fragments": [],
                "text": "THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 77
                            }
                        ],
                        "text": "There have been a number of spelling correction programs in the past such as Kucera (1988) that generated a list of candidates by looking for insertions, deletions, substitutions and reversals, rauch as we have been doing here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "There have been a number of spelling correction programs in the past such as Kucera (1988) that generated a list of candidates by looking for insertions, deletions, substitutions and reversals, much as we have been doing here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 77
                            }
                        ],
                        "text": "There have been a number of spelling correction programs in the past such as Kucera (1988) that generated a list of candidates by looking for insertions, deletions, substitutions and reversals, rauch as we have been doing here. Our contribution is the emphasis on scoring. Mcllroy, the author of the Unix spell program (1982), intentionally focused on the spelling detection problem, and argued (private communication) that spelling correction was a bad idea so long as the corrector couldn't separate the plausible candidates from the implausible ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated Word Substitution Using Numerical Rankings of Structural Disparity Between Misspelled Words & Candidate Substitution Words,\" Patent Number: 4,783,758"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "There have been a number of spelling correction programs in the past such as Kucera (1988) that generated a list of candidates by looking for insertions, deletions, substitutions and reversals, much as we have been doing here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated Word Substitution Using Numerical Rankings of Structural Disparity Between Misspelled Words & Candidate Substitution Words,\u2019"
            },
            "venue": {
                "fragments": [],
                "text": "Patent Number:"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "submitted), \"What's Wrong with Adding One?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 10
                            }
                        ],
                        "text": "Following Box and Tiao (1973), we can assume an uninformative prior and reach a posterior distribution for p. The expectation of this distribution amounts to using r+ .5 instead of r. We call this the expected likelihood estimate. See Gale and Church (1990) for a discussion of the shortcomings of this method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 10
                            }
                        ],
                        "text": "Following Box and Tiao (1973), we can assume an uninformative prior and reach a posterior distribution for p."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Inference in Statistical Analysis, Addison-Wesley, Reading, Massachusetts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 8,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Spelling-Correction-Program-Based-on-a-Noisy-Kernighan-Church/31ef8b3ebc50d682cbd6ce95505ab49802be6bd7?sort=total-citations"
}