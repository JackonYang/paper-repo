{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077193775"
                        ],
                        "name": "Carlo",
                        "slug": "Carlo",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Carlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115707359"
                        ],
                        "name": "MethodRadford",
                        "slug": "MethodRadford",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "MethodRadford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MethodRadford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114079706"
                        ],
                        "name": "NealTechnical",
                        "slug": "NealTechnical",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "NealTechnical",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "NealTechnical"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 33
                            }
                        ],
                        "text": "Markov chain MonteCarlo methods (Neal 1992b, 1993b, and Chapter 3 of this thesis) produce the correct answereventually, but may sometimes fail to reach the true posterior distribution in a reasonablelength of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 51
                            }
                        ],
                        "text": "Use of simulatedannealing, as in my previous work (Neal 1992b), might help in this respect, though therewill still be no guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 29
                            }
                        ],
                        "text": "In previous implementations (Neal1992a, 1993a), I replaced them with equivalent scale factors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "I have applied mixture models with in nitenumbers of components to small data sets (Neal 1992a); the in nite model can in this casebe implemented with nite resources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 37
                            }
                        ],
                        "text": "In my earliest work on this problem (Neal 1992b), Ifelt that use of \\simulated annealing\" (Kirkpatrick, Gelatt, and Vecchi 1983) was desirable,in order to overcome the potential problem that the simulation could be trapped for a longtime in a local minimum of the energy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "This problem wasalso used in my tests of earlier hybrid Monte Carlo implementations (Neal 1992b, 1993a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17720618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38b7a4d7d9646c4474c893fc53a606dee3264fec",
            "isKey": true,
            "numCitedBy": 2,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \\Hybrid Monte Carlo\" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \\free energy\" diierences, it should also be possible to compare the merits of diierent network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks."
            },
            "slug": "Bayesian-Training-of-Backpropagation-Networks-by-Carlo-MethodRadford",
            "title": {
                "fragments": [],
                "text": "Bayesian Training of Backpropagation Networks by theHybrid Monte"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the Hybrid Monte Carlo method, and the method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": true,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052393"
                        ],
                        "name": "H. H. Thodberg",
                        "slug": "H.-H.-Thodberg",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Thodberg",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Thodberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 168
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 155
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such asrecognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat(Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Thodberg (1993), for example, predicts the fat content of meat fromspectral information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 19
                            }
                        ],
                        "text": "Such problems lead Thodberg (1993) to use the estimated probability mass only toselect a \\committee\" based on the better modes (perhaps from di erent models), to eachof which he assigns equal weight."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 156
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such as recognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat (Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "Such approximate Bayesianmethods have proven useful in some practical applications (MacKay 1993, Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15593225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3fb617767f9e500e84ed03fb48acdcf088f33dc",
            "isKey": true,
            "numCitedBy": 49,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "MacKay's Bayesian framework for backpropagation is a practical and powerful means of improving the generalisation ability of neural networks. The framework is reviewed and extended in a pedagogical way. The notation is simpliied using the ordinary weight decay parameter, and the noise parameter is shown to be nothing more than an overall scale. A detailed and explicit procedure for adjusting several weight decay parameters is given. Pruning is incorporated into the Bayesian framework. Appropriate symmetry factors on sparse architectures are deduced. Bayesian weight decay is demonstrated using artiicial data generated by a sparsely connected network. Pruning yields computational advantages: by removing unimportant weights the posterior weight distribution becomes Gaussian, and pruning removes zero-modes of the Hessian and redundant hidden units. In addition, pruning improves generalisation. The Bayesian evidence is used as a stop criterion for pruning. Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra. It outperforms \\early stopping\" as well as quadratic regression. The evidence of a committee of diierently trained networks is computed and the corresponding improved generalisation is veriied. The error bars on the predictions of the fat content are computed. There are three contributors: The random noise, the uncertainty in the weights, and the deviation among the committee members. Finally the Bayesian framework is compared to Moody's GPE."
            },
            "slug": "Ace-of-Bayes-:-Application-of-Neural-Thodberg",
            "title": {
                "fragments": [],
                "text": "Ace of Bayes : Application of Neural"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra and outperforms \\early stopping\" as well as quadratic regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": true,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Wolpert (1993) criticizes the use of this procedure forneural networks on the grounds that by analytically integrating over the hyperparameters,in the manner of Buntine and Weigend, one can obtain the relative posterior probabilitydensities for di erent values of the network parameters exactly,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7748868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d565fb42892f20c52b9fc615cc537835f30d094",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian \"evidence\" approximation has recently been employed to determine the noise and weight-penalty terms used in back-propagation. This paper shows that for neural nets it is far easier to use the exact result than it is to use the evidence approximation. Moreover, unlike the evidence approximation, the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code (the exact result is closed form). In addition, it turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error. Another advantage of the exact analysis is that it does not lead one to incorrect intuition, like the claim that using evidence one can \"evaluate different priors in light of the data\". This paper also discusses sufficiency conditions for the evidence approximation to hold, why it can sometimes give \"reasonable\" results, etc."
            },
            "slug": "On-the-Use-of-Evidence-in-Neural-Networks-Wolpert",
            "title": {
                "fragments": [],
                "text": "On the Use of Evidence in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error, and the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 33
                            }
                        ],
                        "text": "Markov chain MonteCarlo methods (Neal 1992b, 1993b, and Chapter 3 of this thesis) produce the correct answereventually, but may sometimes fail to reach the true posterior distribution in a reasonablelength of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 204
                            }
                        ],
                        "text": "The e ect of dependencies on the accuracy of a Monte Carlo estimate can be quanti edin terms of the autocorrelations between the values of a( (t)) once equilibrium is reach(see, for example (Ripley 1987, Neal 1993b))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 29
                            }
                        ],
                        "text": "In previous implementations (Neal1992a, 1993a), I replaced them with equivalent scale factors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 56
                            }
                        ],
                        "text": "I have reviewed these methods in more detail elsewhere (Neal 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 135
                            }
                        ],
                        "text": "The hybrid Monte Carlo algorithm itself, and methods related to it, have been reviewed byToussaint (1989), Kennedy (1990), and myself (Neal 1993b).3.1.1 Formulating the problem in terms of energyThe hybrid Monte Carlo algorithm is expressed in terms of sampling from the canonical(or Boltzmann)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 174
                            }
                        ],
                        "text": "Evaluation of Neural Network Models units and for the ARD networks with 6 hidden units are also not signi cantly di erent from that of the network with six hidden units that Ripley (1994a) trained with an approximate Bayesian method based on Gaussian approximations to several modes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 25
                            }
                        ],
                        "text": "In an earlier comparison(Neal 1993a), I found (in one case, at least) that weight decay can give results not muchworse than are obtained using Bayesian learning, provided the right degree of weight decay isused."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "This problem wasalso used in my tests of earlier hybrid Monte Carlo implementations (Neal 1992b, 1993a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "I had previously obtained similar results with an ear-lier implementation (Neal 1993a)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 88
                            }
                        ],
                        "text": "Uncorrected stochastic dynamics (see Section 3.1.2) can also be appliedto this problem (Neal 1993a), but as this raises the possibility of unrecognized systematicerror, the hybrid Monte Carlo method appears to be the safer choice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16302605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d275cf94e620bf5b3776bba8a88acccdcfcd9a19",
            "isKey": true,
            "numCitedBy": 213,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The attempt to find a single \"optimal\" weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "slug": "Bayesian-Learning-via-Stochastic-Dynamics-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning via Stochastic Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Bayesian methods avoid overfitting and poor generalization by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data, by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 33
                            }
                        ],
                        "text": "Markov chain MonteCarlo methods (Neal 1992b, 1993b, and Chapter 3 of this thesis) produce the correct answereventually, but may sometimes fail to reach the true posterior distribution in a reasonablelength of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 56
                            }
                        ],
                        "text": "I have reviewed these methods in more detail elsewhere (Neal 1993b). Tierney (1991) and Smith and Roberts (1993) also review recent work on Markov chain Monte Carlo methods and their applications in statistics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 51
                            }
                        ],
                        "text": "Use of simulatedannealing, as in my previous work (Neal 1992b), might help in this respect, though therewill still be no guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 29
                            }
                        ],
                        "text": "In previous implementations (Neal1992a, 1993a), I replaced them with equivalent scale factors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "I have applied mixture models with in nitenumbers of components to small data sets (Neal 1992a); the in nite model can in this casebe implemented with nite resources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 37
                            }
                        ],
                        "text": "In my earliest work on this problem (Neal 1992b), Ifelt that use of \\simulated annealing\" (Kirkpatrick, Gelatt, and Vecchi 1983) was desirable,in order to overcome the potential problem that the simulation could be trapped for a longtime in a local minimum of the energy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "This problem wasalso used in my tests of earlier hybrid Monte Carlo implementations (Neal 1992b, 1993a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61929344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc278353721406a248bf733e40cdecbda8ff3a48",
            "isKey": true,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation. This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space. An infinite number of mixture components can be accommodated without difficulty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any finite training set. The need to decide on a \u201ccorrect\u201d number of components is thereby avoided. The feasibility of the method is shown empirically for a simple classification task."
            },
            "slug": "Bayesian-Mixture-Modeling-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Mixture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation, and the true Bayesian predictive distribution is exhibited, implicitly integrating over the entire underlying parameter space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Hinton and van Camp (1993) use a Gaussian approximation of a di erent sort."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Hinton and van Camp (1993) take a rather di erent approach to approximat-ing the posterior weight distribution by a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9346534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
            },
            "slug": "Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp",
            "title": {
                "fragments": [],
                "text": "Keeping the neural networks simple by minimizing the description length of the weights"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units without time-consuming Monte Carlo simulations is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189281"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "Bayesian methods for this are emphasized by MacKay (1992a); frequentist methods such as cross-validation can also be used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 34
                            }
                        ],
                        "text": "This is a commonapproach, used by Liu (1994), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2646746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "906e33843520fa2395c72d71f8d20a1a5d9cd989",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, it is shown that the conventional back-propagation (BPP) algorithm for neural network regression is robust to leverages (data with x corrupted), but not to outliers (data with y corrupted). A robust model is to model the error as a mixture of normal distribution. The influence function for this mixture model is calculated and the condition for the model to be robust to outliers is given. EM algorithm [5] is used to estimate the parameter. The usefulness of model selection criteria is also discussed. Illustrative simulations are performed."
            },
            "slug": "Robust-Parameter-Estimation-and-Model-Selection-for-Liu",
            "title": {
                "fragments": [],
                "text": "Robust Parameter Estimation and Model Selection for Neural Network Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that the conventional back-propagation (BPP) algorithm for neural network regression is robust to leverages, but not to outliers, and a robust model is to model the error as a mixture of normal distribution."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 55
                            }
                        ],
                        "text": "It is used in the \\Boltzmann machine\" neuralnetwork of Ackley, Hinton, and Sejnowski (1985) to sample from distributions over hiddenunits, and is now widely used for statistical problems, following its exposition by Gemanand Geman (1984) and by Gelfand and Smith (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3393,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47973288"
                        ],
                        "name": "A. Young",
                        "slug": "A.-Young",
                        "structuredName": {
                            "firstName": "Alyssa",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Young (1977), for example, usespolynomial models of inde nitely high order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123102505,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5104689e412832ea5c3af39e86321e93f298d849",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We have an unknown function h(x) which we want to estimate within a finite interval. The observed values of h(x) are independent observations of a random variable y whose mean is to be approximated by a polynomial of unknown degree. The problem of estimating h(x) then translates into that of predicting y. We assume that the mean of y is a polynomial of an arbitrarily large degree and derive a prior distribution for its coefficients which expresses the belief that these coefficients will tend to decrease in absolute value as the power of x increases. A prior-posterior analysis for the coefficients is carried out from which we obtain modal estimates of them. We derive the predictive distribution of y for the case when all parameters other than the mean are known. Two examples comparing the performance of this procedure with some of the usual least squares ones are presented."
            },
            "slug": "A-Bayesian-approach-to-prediction-using-polynomials-Young",
            "title": {
                "fragments": [],
                "text": "A Bayesian approach to prediction using polynomials"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 263
                            }
                        ],
                        "text": "\u2026that it is valid, it will be applied automatically anyway.1.2 Bayesian neural networksWorkers in the eld of \\neural networks\" have diverse backgrounds and motivations, someof which can be seen in the collection of Rumelhart and McClelland (1986b) and the textby Hertz, Krogh, and Palmer (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": true,
            "numCitedBy": 6501,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "For classi cation models such as the \\softmax\" modelof Bridle (1989), the relationship is less straightforward."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 130
                            }
                        ],
                        "text": "For a classi cation task, where the target, y, is a single discrete value indicating one ofK possible classes, the softmax model (Bridle 1989) can be used to de ne the conditionalprobabilities of the various classes using a network with K output units, as follows"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "In applying a neural network to this ten-way classi cation problem, it is appropriate touse the \\softmax\" model (Bridle 1989), which corresponds to the generalized logistic regres-sion model of statistics (see Section 1.2.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 78
                            }
                        ],
                        "text": "All networks were used in conjunction with the softmax model for the targets (Bridle 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": true,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20331,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 242
                            }
                        ],
                        "text": "The problem of over tting can sometimes be alleviated by \\early stopping\" | halting training sometime before the maximum is reached, based on performance on a validation set separate from the training set (this is discussed, for instance, by Baldi and Chauvin (1991))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 240
                            }
                        ],
                        "text": "The problem of over tting can sometimes be alleviated by \\early stopping\" | haltingtraining sometime before the maximum is reached, based on performance on a validation setseparate from the training set (this is discussed, for instance, by Baldi and Chauvin (1991))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8300824,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81d6b69cad04080414dc08ba1d2fccf0ac71b999",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We study generalization in a simple framework of feedforward linear networks with n inputs and n outputs, trained from examples by gradient descent on the usual quadratic error function. We derive analytical results on the behavior of the validation function corresponding to the LMS error function calculated on a set of validation patterns. We show that the behavior of the validation function depends critically on the initial conditions and on the characteristics of the noise. Under certain simple assumptions, if the initial weights are sufficiently small, the validation function has a unique minimum corresponding to an optimal stopping time for training for which simple bounds can be calculated. There exists also situations where the validation function can have more complicated and somewhat unexpected behavior such as multiple local minima (at most n) of variable depth and long but finite plateau effects. Additional results and possible extensions are briefly discussed."
            },
            "slug": "Temporal-Evolution-of-Generalization-during-in-Baldi-Chauvin",
            "title": {
                "fragments": [],
                "text": "Temporal Evolution of Generalization during Learning in Linear Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the behavior of the validation function depends critically on the initial conditions and on the characteristics of the noise, and that under certain simple assumptions, if the initial weights are sufficiently small, the validate function has a unique minimum corresponding to an optimal stopping time for training."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204972"
                        ],
                        "name": "M. V. Rossum",
                        "slug": "M.-V.-Rossum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rossum",
                            "middleNames": [
                                "C.",
                                "W.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Rossum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2281536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6",
            "isKey": true,
            "numCitedBy": 1657,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Lecture Notes for the MSc/DTC module. The brain is a complex computing machine which has evolved to give the ttest output to a given input. Neural computation has as goal to describe the function of the nervous system in mathematical and computational terms. By analysing or simulating the resulting equations, one can better understand its function, research how changes in parameters would eect the function, and try to mimic the nervous system in hardware or software implementations. Neural Computation is a bit like physics, that has been successful in describing numerous physical phenomena. However, approaches developed in those elds not always work for neural computation, because: 1. Physical systems are best studied in reduced, simplied circumstances, but the nervous system is hard to study in isolation. Neurons require a narrow range of operating conditions (temperature, oxygen, presence of other neurons, ion concentrations, ...) under which they work as they should. These conditions are hard to reproduce outside the body. Secondly, the neurons form a highly interconnected network. The function of the nervous systems depends on this connectivity and interaction, by trying to isolate the components, you are likely to alter the function. 2. It is not clear how much detail one needs to describe the computations in the brain. In these lectures we shall see various description levels. 3. Neural signals and neural connectivity are hard to measure, especially, if disturbance and damage to the nervous system is to be kept minimal. Perhaps Neural Computation has more in common with trying to gure out how a complicated machine, such as a computer or car works. Knowledge of the basic physics helps, but is not sucient. Luckily there are factors which perhaps make understanding the brain easier than understanding an arbitrary complicated machine: 1. There is a high degree of conservation across species. This means that animal studies can be used to gain information about the human brain. Furthermore, study of, say, the visual system might help to understand the auditory system. 2. The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives. Therefore it might be possible to nd the organising principles and develop a brain from there. This would be easier than guring out the complete 'wiring diagram'. 3. The nervous system is exible and robust, neurons die everyday. This stands \u2026"
            },
            "slug": "Neural-Computation-Rossum",
            "title": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives, and it might be possible to develop a brain from there."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371112"
                        ],
                        "name": "L. Tierney",
                        "slug": "L.-Tierney",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Tierney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tierney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Tierney(1991) and Smith and Roberts (1993) also review recent work on Markov chain Monte Carlomethods and their applications in statistics.1.3.1 Monte Carlo integration using Markov chainsThe objective of Bayesian learning is to produce predictions for test cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3033166,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d5ae04ca51e76d69f5ad15ba40a3eea520d3860d",
            "isKey": true,
            "numCitedBy": 104,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Several Markov chain-based methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the strategies that are available, and discusses some theoretical and practical issues in the use of these strategies. In addition, some preliminary efforts to use Markov chains to control dynamic graphics for exploring higher-dimensional posterior distributions are outlined."
            },
            "slug": "Exploring-Posterior-Distributions-Using-Markov-Tierney",
            "title": {
                "fragments": [],
                "text": "Exploring Posterior Distributions Using Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2454055"
                        ],
                        "name": "A. Gelfand",
                        "slug": "A.-Gelfand",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Gelfand",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelfand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109352679"
                        ],
                        "name": "Adrian F. M. Smith",
                        "slug": "Adrian-F.-M.-Smith",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian F. M. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 245
                            }
                        ],
                        "text": "It is used in the \\Boltzmann machine\" neuralnetwork of Ackley, Hinton, and Sejnowski (1985) to sample from distributions over hiddenunits, and is now widely used for statistical problems, following its exposition by Gemanand Geman (1984) and by Gelfand and Smith (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53446269,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d990deca66c9afefbe042f95e41ada0c7227877",
            "isKey": false,
            "numCitedBy": 7054,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated."
            },
            "slug": "Sampling-Based-Approaches-to-Calculating-Marginal-Gelfand-Smith",
            "title": {
                "fragments": [],
                "text": "Sampling-Based Approaches to Calculating Marginal Densities"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 111
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989,Hornik, Stinchcombe, and White 1989) have shown that in this limit a network with onelayer of hidden units can approximate any continuous function de ned on a compact domainarbitrarily closely."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989, Hornik, Stinchcombe, and White 1989)have shown that a multilayer perceptron network with one hidden layer can approximateany function de ned on a compact domain arbitrarily closely, if su cient numbers of hiddenunits are used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 16
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989,Hornik, Stinchcombe, and White 1989) have shown that in this limit a network with onelayer of hidden units can approximate any continuous function de ned on a compact domainarbitrarily closely."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 16
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989, Hornik, Stinchcombe, and White 1989)have shown that a multilayer perceptron network with one hidden layer can approximateany function de ned on a compact domain arbitrarily closely, if su cient numbers of hiddenunits are used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 138
                            }
                        ],
                        "text": "Introduction Introductions to Bayesian inference are provided by Press (1989) and Schmitt (1969); Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1418,
                                "start": 16
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989, Hornik, Stinchcombe, and White 1989) have shown that a multilayer perceptron network with one hidden layer can approximate any function de ned on a compact domain arbitrarily closely, if su cient numbers of hidden units are used. Nevertheless, more complex network architectures may have advantages, and are commonly used. Possibilities include using more layers of hidden units, providing direct connections from inputs to outputs, and using di erent activation functions. However, in \\feedforward\" networks such as I consider here, the connections never form cycles, in order that the values of the outputs can be computed in a single forward pass, in time proportional to the number of network parameters. Multilayer perceptron networks can be used to de ne probabilistic models for regression and classi cation tasks by using the network outputs to de ne the conditional distribution for one or more targets, yk , given the various possible values of an input vector, x. The distribution of x itself is not modeled; it may not even be meaningful, since the input values might simply be chosen by the user. Models based on multilayer perceptrons have been applied to a great variety of problems. One typical class of applications are those that take as input sensory information of some type and from that predict some characteristic of what is sensed. Thodberg (1993), for example, predicts the fat content of meat from spectral information."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": true,
            "numCitedBy": 6369,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90984496"
                        ],
                        "name": "W. Jefferys",
                        "slug": "W.-Jefferys",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Jefferys",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Jefferys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118853158,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "5a9452773340d30c7043304884bf9ba5590093c5",
            "isKey": false,
            "numCitedBy": 478,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "'Ockham's razor', the ad hoc principle enjoining the greatest possible simplicity in theoretical explanations, is presently shown to be justifiable as a consequence of Bayesian inference; Bayesian analysis can, moreover, clarify the nature of the 'simplest' hypothesis consistent with the given data. By choosing the prior probabilities of hypotheses, it becomes possible to quantify the scientific judgment that simpler hypotheses are more likely to be correct. Bayesian analysis also shows that a hypothesis with fewer adjustable parameters intrinsically possesses an enhanced posterior probability, due to the clarity of its predictions."
            },
            "slug": "Ockham's-Razor-and-Bayesian-Analysis-Berger-Jefferys",
            "title": {
                "fragments": [],
                "text": "Ockham's Razor and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "'Ockham's razor', the ad hoc principle enjoining the greatest possible simplicity in theoretical explanations, is presently shown to be justifiable as a consequence of Bayesian inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "The e ect of dependencies on the accuracy of a Monte Carlo estimate can be quanti edin terms of the autocorrelations between the values of a( (t)) once equilibrium is reach(see, for example (Ripley 1987, Neal 1993b))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6151205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4180abb06c3e6931474c4a3e0ff079d2d5a0382",
            "isKey": false,
            "numCitedBy": 2209,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "One fifth (4 of 20) of the research articles published in the Journal of Educational Statistics in 1988 include simulation studies that justify or illustrate the authors' conclusions. A similar fraction (6 of 33) of the articles in the 1988 volume of Psychometrika include simulations; comparable proportions could be expected in other journals at the boundary of theoretical statistics and social/psychological applications. Due in part to the complexity of the problems tackled today and in part to the availability of cheap, powerful computing\u2014by no means independent influences\u2014simulation and Monte Carlo methods have become both necessary and practical tools for statisticians and applied workers in quantitative areas of education and psychology. Simulation has become popular\u2014not only in the quantitative social sciences, but in all of the mathematical sciences from physics to operations research to number theory\u2014because it is almost always easy to do. This ease of use makes the simulation experimenter vulnerable to two common pitfalls. Selection of the basic source of \"random numbers\" is often passive: Whatever is available in the computer's standard subroutine library is used. However, the fact that a pseudo-random number generator appears in a popular software package or operating system is hardly reason to trust it, as is shown by the infamous RANDU generator, once popular on IBM mainframes and PDP mini-computers, and by the generators burned into RAM on today's PCs. Simulation design and reporting also deserve special care. Some attempt must be made to assess the accuracy of the simulation estimates: One should accurately estimate and report SE (6) as well as 6. In addition, enough detail should be reported that the interested reader can replicate the study and check the results, just as with other experiments. Yet these considerations are also easy to overlook. Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques. Three themes run throughout the book. First, one shoud not take basic simulation subroutines for granted, especially on minior microcomputers where they tend to be poor implementations, implementations of poor algorithms, or both. Second, design of experiments, or variance reduction as it is known in this field, deserves greater consideration. Third, modern methods make it possible to simulate and analyze processes that are dependent over time, and using such processes opens the door to new simulation techniques, such as simulated annealing in optimization. Ripley intends this book to be a \"comprehensive guide,\" and it is indeed most accurately described as a researcher's handbook with examples and"
            },
            "slug": "Stochastic-simulation-Ripley",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques, and three themes run throughout the book."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley series in probability and mathematical statistics : applied probability and statistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18706,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109352679"
                        ],
                        "name": "Adrian F. M. Smith",
                        "slug": "Adrian-F.-M.-Smith",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian F. M. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3264000"
                        ],
                        "name": "G. Roberts",
                        "slug": "G.-Roberts",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Roberts",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Roberts"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "Tierney(1991) and Smith and Roberts (1993) also review recent work on Markov chain Monte Carlomethods and their applications in statistics.1.3.1 Monte Carlo integration using Markov chainsThe objective of Bayesian learning is to produce predictions for test cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115772206,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aed20302f003a751b1af4e40ace379b9fd255cc7",
            "isKey": true,
            "numCitedBy": 1708,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries"
            },
            "slug": "Bayesian-computation-via-the-gibbs-sampler-and-Smith-Roberts",
            "title": {
                "fragments": [],
                "text": "Bayesian computation via the gibbs sampler and related markov chain monte carlo methods (with discus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401345"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Kennedy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1031,
                                "start": 78
                            }
                        ],
                        "text": "1 The hybrid Monte Carlo algorithm The hybrid Monte Carlo algorithm of Duane, Kennedy, Pendleton, and Roweth (1987) merges the Metropolis algorithm with sampling techniques based on dynamical simulation. The output of the algorithm is a sample of points drawn from some speci ed distribution, which can then be used to form Monte Carlo estimates for the expectations of various functions with respect to this distribution (see equation (1.13)). For Bayesian learning, we wish to sample from the posterior distribution given the training data, and are interested in estimating the expectations needed to make predictions for test cases, such as in equation (1.11). One way of viewing the hybrid Monte Carlo algorithm is as a combination of Gibbs sampling and a particularly elaborate version of the Metropolis algorithm. I assume here that the reader is familiar with these two methods, which were reviewed in Section 1.3.1. The hybrid Monte Carlo algorithm itself, and methods related to it, have been reviewed by Toussaint (1989), Kennedy (1990), and myself (Neal 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 70
                            }
                        ],
                        "text": "1 The hybrid Monte Carlo algorithmThe hybrid Monte Carlo algorithm of Duane, Kennedy, Pendleton, and Roweth (1987)merges the Metropolis algorithm with sampling techniques based on dynamical simulation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987) \\HybridMonte Carlo\", Physics Letters B, vol. 195, pp. 216-222.DeGroot, M. H. (1970) Optimal Statistical Decisions, New York: McGraw-Hill."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 78
                            }
                        ],
                        "text": "1 The hybrid Monte Carlo algorithm The hybrid Monte Carlo algorithm of Duane, Kennedy, Pendleton, and Roweth (1987) merges the Metropolis algorithm with sampling techniques based on dynamical simulation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 191
                            }
                        ],
                        "text": "Another major contribution of this thesis is the development of a Markov chain MonteCarlo implementation of Bayesian learning for neural networks, based on the hybrid MonteCarlo algorithm of Duane, Kennedy, Pendleton, and Roweth (1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 200
                            }
                        ],
                        "text": "Another major contribution of this thesis is the development of a Markov chain Monte Carlo implementation of Bayesian learning for neural networks, based on the hybrid Monte Carlo algorithm of Duane, Kennedy, Pendleton, and Roweth (1987). I demonstrated in Chapter 3 that hybrid Monte Carlo can be many times faster at sampling the posterior distribution for network weights than simpler forms of the Metropolis algorithm; other methods, such as Gibbs sampling, cannot be applied to this problem at all."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 107
                            }
                        ],
                        "text": "The hybrid Monte Carlo algorithm itself, and methods related to it, have been reviewed byToussaint (1989), Kennedy (1990), and myself (Neal 1993b).3.1.1 Formulating the problem in terms of energyThe hybrid Monte Carlo algorithm is expressed in terms of sampling from the canonical(or Boltzmann)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 329
                            }
                        ],
                        "text": "Because of this,Monte Carlo estimates found using the stochastic dynamics method will su er from somesystematic error, which will go to zero only as the stepsize, , is reduced to zero (with thenumber of steps needed to compute each trajectory then going to in nity).3.1.3 Hybrid Monte CarloIn the hybrid Monte Carlo algorithm of Duane, et al (1987), the systematic error of thestochastic dynamics method is eliminated by merging it with the Metropolis algorithm."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118097224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f44e4580d7ec92da2779c194ba11ff6371f9473",
            "isKey": true,
            "numCitedBy": 35,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "These lectures introduce the family of Hybrid Stochastic Algorithms for performing Monte Carlo calculations in Quantum Field Theory. After explaining the basic concepts of Monte Carlo integration we discuss the properties of Markov processes and one particularly useful example of them: the Metropolis algorithm. Building upon this framework we consider the Hybrid and Langevin algorithms from the viewpoint that they are approximate versions of the Hybrid Monte Carlo method; and thus we are led to consider Molecular Dynamics using the Leapfrog algorithm. The lectures conclude by reviewing recent progress in these areas, explaining higher-order integration schemes, the asymptotic large-volume behaviour of the various algorithms, and some simple exact results obtained by applying them to free field theory. It is attempted throughout to give simple yet correct proofs of the various results encountered. 38 refs."
            },
            "slug": "The-theory-of-hybrid-stochastic-algorithms-Kennedy",
            "title": {
                "fragments": [],
                "text": "The theory of hybrid stochastic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "These lectures introduce the family of Hybrid Stochastic Algorithms for performing Monte Carlo calculations in Quantum Field Theory, and considers the Hybrid and Langevin algorithms from the viewpoint that they are approximate versions of the Hybrid Monte Carlo method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144999249"
                        ],
                        "name": "G. Kane",
                        "slug": "G.-Kane",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kane",
                            "middleNames": [
                                "Stanley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kane"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71267845,
            "fieldsOfStudy": [
                "Computer Science",
                "History"
            ],
            "id": "78f6f0ac3d501cb0073a7d94edde5267044a59ae",
            "isKey": false,
            "numCitedBy": 2756,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Computing: Theory and Practice , by Philip Wasserman, 230 pp, $41.95, with illus, ISBN 0-442-20743-3, New York, NY, Van Nostrand Reinhold, 1989. Neural Networks: A Tutorial , by Michael Chester, 182 pp, $38, with illus, ISBN 0-13-368903-4, Englewood Cliffs, NJ, Prentice Hall, 1993. Neural Networks: Algorithms, Applications, and Programming Techniques , by James Freeman and David Skapura, 401 pp, $50.50, with illus, ISBN 0-201-51376-5, Reading, Mass, Addison-Wesley, 1991. Understanding Neural Networks: Computer Explorations , vol 1: Basic Networks , vol 2: Advanced Networks , 309, 367 pp, by Maureen Caudill and Charles Butler, paper, with illus, spiral-bound, with 1 diskette/vol, $39.95/vol, vol 1: ISBN0-262-53102-X (Macintosh), 0-262-53099-6 (IBM), vol 2: ISBN 0-262-53103-8 (Macintosh), 0-262-53100-3 (IBM), Cambridge, Mass, The MIT Press, 1992. Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons , in which they proved that neural networks, as then conceived, can"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Kane",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol 1: Foundations, vol 2: Psychological and Biological Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons, in which they proved that neural networks, as then conceived, can be proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 38
                            }
                        ],
                        "text": "The objective of the assessments that Quinlan (1993) reports was to evaluate whetherhis scheme for combining \\instance-based\" and \\model-based\" learning was bene cial.147\n4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Quinlan (1993) assesses the performance of various learning procedures on this problemusing ten-way cross validation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 195
                            }
                        ],
                        "text": "Following these preliminary runs, I decided to do a cross-validation assessment of thenetwork with two hidden layers (each with six hidden units), in order to compare with theresults reported by Quinlan (1993)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "However,Quinlan (1993) uses only a linear transformation of the variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "This is a common procedure, used by Quinlan(1993), for example.From a Bayesian viewpoint, this normalization of inputs may appear to make little sense."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 53
                            }
                        ],
                        "text": "The results in the bottom section are as reported by Quinlan (1993). of 0."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 127
                            }
                        ],
                        "text": "I did linearly transform the input variables and targets to normalize them to have meanzero and standard deviation one, as did Quinlan (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "This is a common procedure, used by Quinlan (1993), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 286
                            }
                        ],
                        "text": "\u2026Tests on the Boston housing dataThe Boston housing data originates with Harrison and Rubinfeld (1978), who were inter-ested in the e ect of air pollution on housing prices.12 The data set was used to evaluatea method for combining instance-based and model-based learning procedures by Quinlan(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1343,
                                "start": 197
                            }
                        ],
                        "text": "Following these preliminary runs, I decided to do a cross-validation assessment of the network with two hidden layers (each with six hidden units), in order to compare with the results reported by Quinlan (1993). Technically speaking, this is cheating | this network architecture was chosen with knowledge of results involving all the data, whereas training for each component of the cross-validation assessment is supposed to be based solely on the nine-tenths of the data allocated to training for that component. There are two reasons why this does not invalidate the results. First, one could apply the same methodology of selecting an architecture (using preliminary runs trained with a subset of the data) within each component of the cross-validation assessment. Since the training and test sets for these runs would be only slightly smaller than for the preliminary runs done here, the results would likely be similar. (This was not done because it would have required considerably more computation time.) Second, the network architecture selected is that which is the most complex, the one that would be selected a priori under the philosophy of modeling that I am advocating. The preliminary runs simply con rm that, as expected, using a simpler architecture is not advantageous. The objective of the assessments that Quinlan (1993) reports was to evaluate whether his scheme for combining \\instance-based\" and \\model-based\" learning was bene cial."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 197
                            }
                        ],
                        "text": "Following these preliminary runs, I decided to do a cross-validation assessment of the network with two hidden layers (each with six hidden units), in order to compare with the results reported by Quinlan (1993). Technically speaking, this is cheating | this network architecture was chosen with knowledge of results involving all the data, whereas training for each component of the cross-validation assessment is supposed to be based solely on the nine-tenths of the data allocated to training for that component."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 53
                            }
                        ],
                        "text": "The results in the bottom section are as reported by Quinlan (1993).of 0.5."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2327460,
            "fieldsOfStudy": [
                "Computer Science",
                "Education",
                "Economics"
            ],
            "id": "4f9e6b13e22ae8f3d77b1f5d1c946179e3abfd64",
            "isKey": true,
            "numCitedBy": 653,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-Instance-Based-and-Model-Based-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "Combining Instance-Based and Model-Based Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 46
                            }
                        ],
                        "text": "Rejection sampling in general is discussed by Devroye (1986).) This method produces a sample of independent values from the posterior given the training data, from which Monte Carlo estimates can 82"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "The result is a Gamma distribution that canbe sampled from by standard methods (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "Rejection sampling ingeneral is discussed by Devroye (1986).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 80
                            }
                        ],
                        "text": "The result is a Gamma distribution that can be sampled from by standard methods (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 74
                            }
                        ],
                        "text": "E cient methods of generating Gamma-distributedrandom variates are known (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "This algorithm| a form of rejection sampling (Devroye 1986) | directly embodies thede nition of the posterior given by equation (1.2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 187
                            }
                        ],
                        "text": "Producing the left half of the gure was easy, since generating values for the networkweights and biases from independent Gaussian distributions can be done quickly usingstandard methods (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "This algorithm| a form of rejection sampling (Devroye 1986) | directly embodies the de nition of the posterior given by equation (1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 74
                            }
                        ],
                        "text": "E cient methods of generating Gamma-distributed random variates are known (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 188
                            }
                        ],
                        "text": "Producing the left half of the gure was easy, since generating values for the network weights and biases from independent Gaussian distributions can be done quickly using standard methods (Devroye 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6898695,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6733e22b52d8a86988a36be7404cd38fd5b8a66f",
            "isKey": true,
            "numCitedBy": 3704,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey of the main methods in non-uniform random variate generation, and highlights recent research on the subject. Classical paradigms such as inversion, rejection, guide tables, and transformations are reviewed. We provide information on the expected time complexity of various algorithms, before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods. Authors\u2019 address: School of Computer Science, McGill University, 3480 University Street, Montreal, Canada H3A 2K6. The authors\u2019 research was sponsored by NSERC Grant A3456 and FCAR Grant 90-ER-0291. 1. The main paradigms The purpose of this chapter is to review the main methods for generating random variables, vectors and processes. Classical workhorses such as the inversion method, the rejection method and table methods are reviewed in section 1. In section 2, we discuss the expected time complexity of various algorithms, and give a few examples of the design of generators that are uniformly fast over entire families of distributions. In section 3, we develop a few universal generators, such as generators for all log concave distributions on the real line. Section 4 deals with random variate generation when distributions are indirectly specified, e.g, via Fourier coefficients, characteristic functions, the moments, the moment generating function, distributional identities, infinite series or Kolmogorov measures. Random processes are briefly touched upon in section 5. Finally, the latest developments in Markov chain methods are discussed in section 6. Some of this work grew from Devroye (1986a), and we are carefully documenting work that was done since 1986. More recent references can be found in the book by H\u00f6rmann, Leydold and Derflinger (2004). Non-uniform random variate generation is concerned with the generation of random variables with certain distributions. Such random variables are often discrete, taking values in a countable set, or absolutely continuous, and thus described by a density. The methods used for generating them depend upon the computational model one is working with, and upon the demands on the part of the output. For example, in a ram (random access memory) model, one accepts that real numbers can be stored and operated upon (compared, added, multiplied, and so forth) in one time unit. Furthermore, this model assumes that a source capable of producing an i.i.d. (independent identically distributed) sequence of uniform [0, 1] random variables is available. This model is of course unrealistic, but designing random variate generators based on it has several advantages: first of all, it allows one to disconnect the theory of non-uniform random variate generation from that of uniform random variate generation, and secondly, it permits one to plan for the future, as more powerful computers will be developed that permit ever better approximations of the model. Algorithms designed under finite approximation limitations will have to be redesigned when the next generation of computers arrives. For the generation of discrete or integer-valued random variables, which includes the vast area of the generation of random combinatorial structures, one can adhere to a clean model, the pure bit model, in which each bit operation takes one time unit, and storage can be reported in terms of bits. Typically, one now assumes that an i.i.d. sequence of independent perfect bits is available. In this model, an elegant information-theoretic theory can be derived. For example, Knuth and Yao (1976) showed that to generate a random integer X described by the probability distribution {X = n} = pn, n \u2265 1, any method must use an expected number of bits greater than the binary entropy of the distribution, \u2211"
            },
            "slug": "Non-Uniform-Random-Variate-Generation-Devroye",
            "title": {
                "fragments": [],
                "text": "Non-Uniform Random Variate Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This chapter reviews the main methods for generating random variables, vectors and processes in non-uniform random variate generation, and provides information on the expected time complexity of various algorithms before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103338087"
                        ],
                        "name": "Paul B. Mackenze",
                        "slug": "Paul-B.-Mackenze",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Mackenze",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul B. Mackenze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "Some random variation may also be needed to avoid periodicitiesthat could interfere with ergodicity (Mackenzie 1989), though this is not expected to be aproblem for an irregular distribution such as a neural network posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 459,
                                "start": 102
                            }
                        ],
                        "text": "Some random variation may also be needed to avoid periodicities that could interfere with ergodicity (Mackenzie 1989), though this is not expected to be a problem for an irregular distribution such as a neural network posterior. The name Langevin Monte Carlo is given to hybrid Monte Carlo with L = 1, that is, in which candidate states are generated using only a single leapfrog iteration. The \\smart Monte Carlo\" method of Rossky, Doll, and Friedman (1978) is equivalent to this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 101
                            }
                        ],
                        "text": "Some random variation may also be needed to avoid periodicities that could interfere with ergodicity (Mackenzie 1989), though this is not expected to be a problem for an irregular distribution such as a neural network posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122624633,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5d58565e5873e6ca4b30c14751e7cd322105179c",
            "isKey": true,
            "numCitedBy": 92,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Improved-Hybrid-Monte-Carlo-Method-Mackenze",
            "title": {
                "fragments": [],
                "text": "An Improved Hybrid Monte Carlo Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660076"
                        ],
                        "name": "T. Masters",
                        "slug": "T.-Masters",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Masters",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Masters"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 45
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989,Hornik, Stinchcombe, and White 1989) have shown that in this limit a network with onelayer of hidden units can approximate any continuous function de ned on a compact domainarbitrarily closely."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 46
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989, Hornik, Stinchcombe, and White 1989)have shown that a multilayer perceptron network with one hidden layer can approximateany function de ned on a compact domain arbitrarily closely, if su cient numbers of hiddenunits are used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59694196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ed59488ce16f7b34fb1686a4570f72108fa32b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-Feedforward-Networks-Masters",
            "title": {
                "fragments": [],
                "text": "Multilayer Feedforward Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 79
                            }
                        ],
                        "text": "The size of this range should generally increase with the number of parameters (Neal 1994), so the windowed algorithm might be more useful with larger networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 229
                            }
                        ],
                        "text": "5.2 The windowed hybrid Monte Carlo algorithmI have developed a variant of hybrid Monte Carlo in which transitions take place between\\windows\" of states at the beginning and end of a trajectory, rather than between singlestates (Neal 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 229
                            }
                        ],
                        "text": "2 The windowed hybrid Monte Carlo algorithm I have developed a variant of hybrid Monte Carlo in which transitions take place between \\windows\" of states at the beginning and end of a trajectory, rather than between single states (Neal 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 79
                            }
                        ],
                        "text": "The size of this range should generally increase withthe number of parameters (Neal 1994), so the windowed algorithm might be more usefulwith larger networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119507994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8b729ec5ab0e4442e2801fde133ef12775c0010",
            "isKey": true,
            "numCitedBy": 156,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted."
            },
            "slug": "An-improved-acceptance-procedure-for-the-hybrid-Neal",
            "title": {
                "fragments": [],
                "text": "An improved acceptance procedure for the hybrid Monte Carlo algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators and allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30399419"
                        ],
                        "name": "A. Horowitz",
                        "slug": "A.-Horowitz",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Horowitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 72
                            }
                        ],
                        "text": "However, this is not the case for the hybrid Monte Carlo variant due to Horowitz (1991), in which the momentum is not completely replaced."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 248
                            }
                        ],
                        "text": "Amongthose that might be useful in this application, but that I have not yet evaluated, are theuse of discretizations of the dynamics other than the leapfrog method (Creutz and Gocksch1989), and the alternative approach to avoiding random walks of Horowitz (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "However, this is not the case for the hybrid Monte Carlovariant due to Horowitz (1991), in which the momentum is not completely replaced."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121631021,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "23faf79a6469016307cf33ca0dcec0fa4c59bf24",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-generalized-guided-Monte-Carlo-algorithm-Horowitz",
            "title": {
                "fragments": [],
                "text": "A generalized guided Monte Carlo algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799557"
                        ],
                        "name": "D. Toussaint",
                        "slug": "D.-Toussaint",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Toussaint",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Toussaint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 265
                            }
                        ],
                        "text": "In su ciently large problems, we might expect that stochastic dynamics willhave an advantage, since the error in the energy that controls the rejection rate will growwith system size, but the systematic error may perhaps not (for more on this, see thediscussion by Toussaint (1989))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120405210,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "65807867388d6658e854616a5f63afda1842a98e",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-algorithms-for-Monte-Carlo-and-to-Toussaint",
            "title": {
                "fragments": [],
                "text": "Introduction to algorithms for Monte Carlo simulations and their application to QCD"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72184075"
                        ],
                        "name": "F. William",
                        "slug": "F.-William",
                        "structuredName": {
                            "firstName": "Feller",
                            "lastName": "William",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. William"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Priors for In nite Networksdistributions of index (Feller, 1966, Sections IX.8 and XVII.5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 156
                            }
                        ],
                        "text": "\u2026for the weights from hidden to outputunits that do not have nite variance.2.2.1 Limits for priors with in nite varianceThe theory of stable distributions (Feller, 1966, Section VI.1) provides the basis for analysingthe convergence of priors in which hidden-to-output weights have in nite variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 113
                            }
                        ],
                        "text": "The theory behind this scaling concerns the convergence of sums of independent random variables to \\stable laws\" (Feller 1966), as discussed in Chapter 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "The theory behind this scaling concerns the convergence of sums of independent ran-dom variables to \\stable laws\" (Feller 1966), as discussed in Chapter 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 65112646,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "65ba8fd8ef9c2a70cee99d2e5cab9302d0307a1e",
            "isKey": true,
            "numCitedBy": 12374,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Office hours: MWF, immediately after class or early afternoon (time TBA). We will cover the mathematical foundations of probability theory. The basic terminology and concepts of probability theory include: random experiments, sample or outcome spaces (discrete and continuous case), events and their algebra, probability measures, conditional probability A First Course in Probability (8th ed.) by S. Ross. This is a lively text that covers the basic ideas of probability theory including those needed in statistics. Theoretical concepts are introduced via interesting concrete examples. In 394 I will begin my lectures with the basics of probability theory in Chapter 2. However, your first assignment is to review Chapter 1, which treats elementary counting methods. They are used in applications in Chapter 2. I expect to cover Chapters 2-5 plus portions of 6 and 7. You are encouraged to read ahead. In lectures I will not be able to cover every topic and example in Ross, and conversely, I may cover some topics/examples in lectures that are not treated in Ross. You will be responsible for all material in my lectures, assigned reading, and homework, including supplementary handouts if any."
            },
            "slug": "An-Introduction-To-Probability-Theory-And-Its-William",
            "title": {
                "fragments": [],
                "text": "An Introduction To Probability Theory And Its Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A First Course in Probability (8th ed.) by S. Ross is a lively text that covers the basic ideas of probability theory including those needed in statistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6691406"
                        ],
                        "name": "P. Rossky",
                        "slug": "P.-Rossky",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rossky",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rossky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49443651"
                        ],
                        "name": "J. Doll",
                        "slug": "J.-Doll",
                        "structuredName": {
                            "firstName": "Jimmie",
                            "lastName": "Doll",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Doll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48535628"
                        ],
                        "name": "H. Friedman",
                        "slug": "H.-Friedman",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Friedman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 33
                            }
                        ],
                        "text": "The \\smartMonte Carlo\" method of Rossky, Doll, and Friedman (1978) is equivalent to this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14967940,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "987312496fb243bb56518fb9ecfa051e9df8f35d",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A new Monte Carlo simulation procedure is developed which is expected to produce more rapid convergence than the standard Metropolis method. The trial particle moves are chosen in accord with a Brownian dynamics algorithm rather than at random. For two model systems, a string of point masses joined by harmonic springs and a cluster of charged soft spheres, the new procedure is compared to the standard one and shown to manifest a more rapid convergence rate for some important energetic and structural properties."
            },
            "slug": "Brownian-dynamics-as-smart-Monte-Carlo-simulation-Rossky-Doll",
            "title": {
                "fragments": [],
                "text": "Brownian dynamics as smart Monte Carlo simulation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16085713"
                        ],
                        "name": "Creutz",
                        "slug": "Creutz",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Creutz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Creutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30025792"
                        ],
                        "name": "Gocksch",
                        "slug": "Gocksch",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Gocksch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gocksch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 168
                            }
                        ],
                        "text": "Among those that might be useful in this application, but that I have not yet evaluated, are the use of discretizations of the dynamics other than the leapfrog method (Creutz and Gocksch 1989), and the alternative approach to avoiding random walks of Horowitz (1991). I have made preliminary investigations into two other variants | one in which trajectories are computed using \\partial gradients\", another in which a \\windowed\" acceptance procedure is used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 167
                            }
                        ],
                        "text": "Among those that might be useful in this application, but that I have not yet evaluated, are the use of discretizations of the dynamics other than the leapfrog method (Creutz and Gocksch 1989), and the alternative approach to avoiding random walks of Horowitz (1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 166
                            }
                        ],
                        "text": "Amongthose that might be useful in this application, but that I have not yet evaluated, are theuse of discretizations of the dynamics other than the leapfrog method (Creutz and Gocksch1989), and the alternative approach to avoiding random walks of Horowitz (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46609541,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9804b411dfb3d5d96610f8306df33d611bbe0603",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple recursive iteration of the leapfrog discretization of Newton's equations which leads to a removal of the finite-step-size error to any desired order. This is done in a manner that preserves phase-space areas and reversibility, as required for use in the hybrid Monte Carlo method for simulating fermionic fields. The resulting asymptotic volume dependence is exp((ln/ital V/)/sup 1/2/). We test the scheme on the (2+1)-dimensional Hubbard model."
            },
            "slug": "Higher-order-hybrid-Monte-Carlo-algorithms.-Creutz-Gocksch",
            "title": {
                "fragments": [],
                "text": "Higher-order hybrid Monte Carlo algorithms."
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A simple recursive iteration of the leapfrog discretization of Newton's equations leads to a removal of the finite-step-size error to any desired order in a manner that preserves phase-space areas and reversibility."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2929,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96253820"
                        ],
                        "name": "C. Litton",
                        "slug": "C.-Litton",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Litton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Litton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Barnett (1982) presents a comparative view of di erent approaches to statistical inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61875585,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b2a64b8525173506798dbceb98de0c1d48219e23",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction: Statistical Inference and Decision-Making. An Illustration of the Different Approaches. Probability. Utility, and Decision-Making. Classical Inference. Bayesian Inference. Decision Theory. Some Other Approaches. Perspective. Bibliography. Subject and Author Index."
            },
            "slug": "Comparative-Statistical-Inference.-Litton",
            "title": {
                "fragments": [],
                "text": "Comparative Statistical Inference."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This book discusses Statistical Inference and Decision-Making in the context of classical and Bayesian inference, utility, and decision-Making."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29936138"
                        ],
                        "name": "E. Marinari",
                        "slug": "E.-Marinari",
                        "structuredName": {
                            "firstName": "Enzo",
                            "lastName": "Marinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marinari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145052965"
                        ],
                        "name": "G. Parisi",
                        "slug": "G.-Parisi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Parisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 75
                            }
                        ],
                        "text": "It would also be interesting to apply a method such as simulated tempering (Marinari and Parisi 1992), in order to sample e ciently in cases where the posterior distribution has widely separated modes, which is one possible explanation for the divergence seen here between the two non-ARD runs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 93
                            }
                        ],
                        "text": "Finally, one could try applyingmethods for escaping local modes such as simulated tempering (Marinari and Parisi 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Finally, one could try applying methods for escaping local modes such as simulated tempering (Marinari and Parisi 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 75
                            }
                        ],
                        "text": "It would also be interesting to apply a method such as simulatedtempering (Marinari and Parisi 1992), in order to sample e ciently in cases where theposterior distribution has widely separated modes, which is one possible explanation for thedivergence seen here between the two non-ARD runs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12321327,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f909aeaef41c781f8b4e27d5c3cc854dd3ddb3cc",
            "isKey": true,
            "numCitedBy": 1513,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new global optimization method (Simulated Tempering) for simulating effectively a system with a rough free-energy landscape (i.e., many coexisting states) at finite nonzero temperature. This method is related to simulated annealing, but here the temperature becomes a dynamic variable, and the system is always kept at equilibrium. We analyse the method on the Random Field Ising Model, and we find a dramatic improvement over conventional Metropolis and cluster methods. We analyse and discuss the conditions under which the method has optimal performances."
            },
            "slug": "Simulated-tempering:-a-new-Monte-Carlo-scheme-Marinari-Parisi",
            "title": {
                "fragments": [],
                "text": "Simulated tempering: a new Monte Carlo scheme"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32922277"
                        ],
                        "name": "N. Metropolis",
                        "slug": "N.-Metropolis",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Metropolis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Metropolis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91743329"
                        ],
                        "name": "A. W. Rosenbluth",
                        "slug": "A.-W.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Arianna",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991661"
                        ],
                        "name": "M. Rosenbluth",
                        "slug": "M.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46516796"
                        ],
                        "name": "A. H. Teller",
                        "slug": "A.-H.-Teller",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Teller",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Teller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3840350"
                        ],
                        "name": "E. Teller",
                        "slug": "E.-Teller",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Teller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 127
                            }
                        ],
                        "text": "3 The Metropolis algorithm The Metropolis algorithm was introduced in the classic paper of Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller (1953), and has since seen extensive use in statistical physics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 224
                            }
                        ],
                        "text": "In the implementation of Chapter 3, it will also be used to update hyperparameters.1.3.3 The Metropolis algorithmThe Metropolis algorithm was introduced in the classic paper of Metropolis, Rosenbluth,Rosenbluth, Teller, and Teller (1953), and has since seen extensive use in statistical physics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1046577,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f6a13f116e270dde9d67848495f801cdb8efa25d",
            "isKey": true,
            "numCitedBy": 32409,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two\u2010dimensional rigid\u2010sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four\u2010term virial coefficient expansion."
            },
            "slug": "Equation-of-state-calculations-by-fast-computing-Metropolis-Rosenbluth",
            "title": {
                "fragments": [],
                "text": "Equation of state calculations by fast computing machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100550660"
                        ],
                        "name": "H. C. Andersen",
                        "slug": "H.-C.-Andersen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Christian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. C. Andersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of random walk behaviour.3.1.2 The stochastic dynamics methodHybrid Monte Carlo can be viewed as an elaboration of the stochastic dynamics method(Andersen 1980), in which the task of sampling from the canonical distribution for q and pgiven by equation (3.2) is split into two sub-tasks |\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34820304,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9b2c7cfba54c9dc301e51d950ebe7b517b53a5a0",
            "isKey": true,
            "numCitedBy": 4300,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In the molecular dynamics simulation method for fluids, the equations of motion for a collection of particles in a fixed volume are solved numerically. The energy, volume, and number of particles are constant for a particular simulation, and it is assumed that time averages of properties of the simulated fluid are equal to microcanonical ensemble averages of the same properties. In some situations, it is desirable to perform simulations of a fluid for particular values of temperature and/or pressure or under conditions in which the energy and volume of the fluid can fluctuate. This paper proposes and discusses three methods for performing molecular dynamics simulations under conditions of constant temperature and/or pressure, rather than constant energy and volume. For these three methods, it is shown that time averages of properties of the simulated fluid are equal to averages over the isoenthalpic\u2013isobaric, canonical, and isothermal\u2013isobaric ensembles. Each method is a way of describing the dynamics of a certain number of particles in a volume element of a fluid while taking into account the influence of surrounding particles in changing the energy and/or density of the simulated volume element. The influence of the surroundings is taken into account without introducing unwanted surface effects. Examples of situations where these methods may be useful are discussed."
            },
            "slug": "Molecular-dynamics-simulations-at-constant-pressure-Andersen",
            "title": {
                "fragments": [],
                "text": "Molecular dynamics simulations at constant pressure and/or temperature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91275845"
                        ],
                        "name": "R. Winkler",
                        "slug": "R.-Winkler",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Winkler",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Winkler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93799782"
                        ],
                        "name": "H. Morawitz",
                        "slug": "H.-Morawitz",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Morawitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Morawitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46624590"
                        ],
                        "name": "D. Y. Yoon",
                        "slug": "D.-Y.-Yoon",
                        "structuredName": {
                            "firstName": "Do",
                            "lastName": "Yoon",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Y. Yoon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of random walk behaviour.3.1.2 The stochastic dynamics methodHybrid Monte Carlo can be viewed as an elaboration of the stochastic dynamics method(Andersen 1980), in which the task of sampling from the canonical distribution for q and pgiven by equation (3.2) is split into two sub-tasks |\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 118
                            }
                        ],
                        "text": "2 The stochastic dynamics method Hybrid Monte Carlo can be viewed as an elaboration of the stochastic dynamics method (Andersen 1980), in which the task of sampling from the canonical distribution for q and p given by equation (3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 98731939,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "668b2322616efb27024e9347120430c858b5943c",
            "isKey": true,
            "numCitedBy": 45,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Molecular dynamics simulations are usually performed with a fixed number of particles in a periodic box of constant size and shape, however, there are situations in which it is desirable to carry out simulations, where the volume of the periodic box is allowed to fluctuate and the external pressure is fixed. The external pressure and the volume are related by a stress tensor. In this paper a novel instantaneous external stress tensor is derived, which allows the volume and the internal pressure to fluctuate in a constant external pressure simulation. Compared to other constant pressure simulation methods, the method proposed contains no artificial parameter and no extension of the phase space. It is shown that time averages of system properties are equal to isoenthalpic-isobaric ensemble averages. The implementation of the proposed method in a constant pressure molecule dynamics simulation is outlined and result of simulations of Lennard-Jones particles are presented."
            },
            "slug": "Novel-molecular-dynamics-simulations-at-constant-Winkler-Morawitz",
            "title": {
                "fragments": [],
                "text": "Novel molecular dynamics simulations at constant pressure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39416713"
                        ],
                        "name": "M. Degroot",
                        "slug": "M.-Degroot",
                        "structuredName": {
                            "firstName": "Morris",
                            "lastName": "Degroot",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Degroot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 136
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "One reason for optimism in this regard is that the posteriordistribution for many models becomes increasingly Gaussian as the amount of training dataincreases (DeGroot 1970, Chapter 10)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119884967,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81c8a5823de21d98ea395081cbfe647bfb456cd6",
            "isKey": false,
            "numCitedBy": 4235,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword.Preface.PART ONE. SURVEY OF PROBABILITY THEORY.Chapter 1. Introduction.Chapter 2. Experiments, Sample Spaces, and Probability.2.1 Experiments and Sample Spaces.2.2 Set Theory.2.3 Events and Probability.2.4 Conditional Probability.2.5 Binomial Coefficients.Exercises.Chapter 3. Random Variables, Random Vectors, and Distributions Functions.3.1 Random Variables and Their Distributions.3.2 Multivariate Distributions.3.3 Sums and Integrals.3.4 Marginal Distributions and Independence.3.5 Vectors and Matrices.3.6 Expectations, Moments, and Characteristic Functions.3.7 Transformations of Random Variables.3.8 Conditional Distributions.Exercises.Chapter 4. Some Special Univariate Distributions.4.1 Introduction.4.2 The Bernoulli Distributions.4.3 The Binomial Distribution.4.4 The Poisson Distribution.4.5 The Negative Binomial Distribution.4.6 The Hypergeometric Distribution.4.7 The Normal Distribution.4.8 The Gamma Distribution.4.9 The Beta Distribution.4.10 The Uniform Distribution.4.11 The Pareto Distribution.4.12 The t Distribution.4.13 The F Distribution.Exercises.Chapter 5. Some Special Multivariate Distributions.5.1 Introduction.5.2 The Multinomial Distribution.5.3 The Dirichlet Distribution.5.4 The Multivariate Normal Distribution.5.5 The Wishart Distribution.5.6 The Multivariate t Distribution.5.7 The Bilateral Bivariate Pareto Distribution.Exercises.PART TWO. SUBJECTIVE PROBABILITY AND UTILITY.Chapter 6. Subjective Probability.6.1 Introduction.6.2 Relative Likelihood.6.3 The Auxiliary Experiment.6.4 Construction of the Probability Distribution.6.5 Verification of the Properties of a Probability Distribution.6.6 Conditional Likelihoods.Exercises.Chapter 7. Utility.7.1 Preferences Among Rewards.7.2 Preferences Among Probability Distributions.7.3 The Definitions of a Utility Function.7.4 Some Properties of Utility Functions.7.5 The Utility of Monetary Rewards.7.6 Convex and Concave Utility Functions.7.7 The Anxiomatic Development of Utility.7.8 Construction of the Utility Function.7.9 Verification of the Properties of a Utility Function.7.10 Extension of the Properties of a Utility Function to the Class ?E.Exercises.PART THREE. STATISTICAL DECISION PROBLEMS.Chapter 8. Decision Problems.8.1 Elements of a Decision Problem.8.2 Bayes Risk and Bayes Decisions.8.3 Nonnegative Loss Functions.8.4 Concavity of the Bayes Risk.8.5 Randomization and Mixed Decisions.8.6 Convex Sets.8.7 Decision Problems in Which ~2 and D Are Finite.8.8 Decision Problems with Observations.8.9 Construction of Bayes Decision Functions.8.10 The Cost of Observation.8.11 Statistical Decision Problems in Which Both ? and D contains Two Points.8.12 Computation of the Posterior Distribution When the Observations Are Made in More Than One Stage.Exercises.Chapter 9. Conjugate Prior Distributions.9.1 Sufficient Statistics.9.2 Conjugate Families of Distributions.9.3 Construction of the Conjugate Family.9.4 Conjugate Families for Samples from Various Standard Distributions.9.5 Conjugate Families for Samples from a Normal Distribution.9.6 Sampling from a Normal Distribution with Unknown Mean and Unknown Precision.9.7 Sampling from a Uniform Distribution.9.8 A Conjugate Family for Multinomial Observations.9.9 Conjugate Families for Samples from a Multivariate Normal Distribution.9.10 Multivariate Normal Distributions with Unknown Mean Vector and Unknown Precision matrix.9.11 The Marginal Distribution of the Mean Vector.9.12 The Distribution of a Correlation.9.13 Precision Matrices Having an Unknown Factor.Exercises.Chapter 10. Limiting Posterior Distributions.10.1 Improper Prior Distributions.10.2 Improper Prior Distributions for Samples from a Normal Distribution.10.3 Improper Prior Distributions for Samples from a Multivariate Normal Distribution.10.4 Precise Measurement.10.5 Convergence of Posterior Distributions.10.6 Supercontinuity.10.7 Solutions of the Likelihood Equation.10.8 Convergence of Supercontinuous Functions.10.9 Limiting Properties of the Likelihood Function.10.10 Normal Approximation to the Posterior Distribution.10.11 Approximation for Vector Parameters.10.12 Posterior Ratios.Exercises.Chapter 11. Estimation, Testing Hypotheses, and linear Statistical Models.11.1 Estimation.11.2 Quadratic Loss.11.3 Loss Proportional to the Absolute Value of the Error.11.4 Estimation of a Vector.11.5 Problems of Testing Hypotheses.11.6 Testing a Simple Hypothesis About the Mean of a Normal Distribution.11.7 Testing Hypotheses about the Mean of a Normal Distribution.11.8 Deciding Whether a Parameter Is Smaller or larger Than a Specific Value.11.9 Deciding Whether the Mean of a Normal Distribution Is Smaller or larger Than a Specific Value.11.10 Linear Models.11.11 Testing Hypotheses in Linear Models.11.12 Investigating the Hypothesis That Certain Regression Coefficients Vanish.11.13 One-Way Analysis of Variance.Exercises.PART FOUR. SEQUENTIAL DECISIONS.Chapter 12. Sequential Sampling.12.1 Gains from Sequential Sampling.12.2 Sequential Decision Procedures.12.3 The Risk of a Sequential Decision Procedure.12.4 Backward Induction.12.5 Optimal Bounded Sequential Decision procedures.12.6 Illustrative Examples.12.7 Unbounded Sequential Decision Procedures.12.8 Regular Sequential Decision Procedures.12.9 Existence of an Optimal Procedure.12.10 Approximating an Optimal Procedure by Bounded Procedures.12.11 Regions for Continuing or Terminating Sampling.12.12 The Functional Equation.12.13 Approximations and Bounds for the Bayes Risk.12.14 The Sequential Probability-ratio Test.12.15 Characteristics of Sequential Probability-ratio Tests.12.16 Approximating the Expected Number of Observations.Exercises.Chapter 13. Optimal Stopping.13.1 Introduction.13.2 The Statistician's Reward.13.3 Choice of the Utility Function.13.4 Sampling Without Recall.13.5 Further Problems of Sampling with Recall and Sampling without Recall.13.6 Sampling without Recall from a Normal Distribution with Unknown Mean.13.7 Sampling with Recall from a Normal Distribution with Unknown Mean.13.8 Existence of Optimal Stopping Rules.13.9 Existence of Optimal Stopping Rules for Problems of Sampling with Recall and Sampling without Recall.13.10 Martingales.13.11 Stopping Rules for Martingales.13.12 Uniformly Integrable Sequences of Random Variables.13.13 Martingales Formed from Sums and Products of Random Variables.13.14 Regular Supermartingales.13.15 Supermartingales and General Problems of Optimal Stopping.13.16 Markov Processes.13.17 Stationary Stopping Rules for Markov Processes.13.18 Entrance-fee Problems.13.19 The Functional Equation for a Markov Process.Exercises.Chapter 14. Sequential Choice of Experiments.14.1 Introduction.14.2 Markovian Decision Processes with a Finite Number of Stages.14.3 Markovian Decision Processes with an Infinite Number of Stages.14.4 Some Betting Problems.14.5 Two-armed-bandit Problems.14.6 Two-armed-bandit Problems When the Value of One Parameter Is Known.14.7 Two-armed-bandit Problems When the Parameters Are Dependent.14.8 Inventory Problems.14.9 Inventory Problems with an Infinite Number of Stages.14.10 Control Problems.14.11 Optimal Control When the Process Cannot Be Observed without Error.14.12 Multidimensional Control Problems.14.13 Control Problems with Actuation Errors.14.14 Search Problems.14.15 Search Problems with Equal Costs.14.16 Uncertainty Functions and Statistical Decision Problems.14.17 Sufficient Experiments.14.18 Examples of Sufficient Experiments.Exercises.References.Supplementary Bibliography.Name Index.Subject Index."
            },
            "slug": "Optimal-Statistical-Decisions-Degroot",
            "title": {
                "fragments": [],
                "text": "Optimal Statistical Decisions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 32
                            }
                        ],
                        "text": "The method of cross validation (Stone 1974) is sometimes used to nd an appropriate14\n1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 268
                            }
                        ],
                        "text": "In su ciently large problems, we might expect that stochastic dynamics will have an advantage, since the error in the energy that controls the rejection rate will grow with system size, but the systematic error may perhaps not (for more on this, see the discussion by Toussaint (1989))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 121
                            }
                        ],
                        "text": "2 Tests of ARD on the noisy LED display problem The noisy LED display problem was used by Breiman, Friedman, Olshen, and Stone (1984) to evaluate their Classi cation and Regression Tree (CART) system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 31
                            }
                        ],
                        "text": "The method of cross validation (Stone 1974) is sometimes used to nd an appropriate 14"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62698647,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7b28610d2d681a11398eb614de0d70d7de41c20c",
            "isKey": true,
            "numCitedBy": 7500,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance."
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "They employ an elaboration of theMinimum Description Length framework (Rissanen 1986) that is equivalent to Bayesianinference using an approximation to the posterior distribution chosen so as to minimizethe Kullback-Leibler divergence with the true posterior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "They employ an elaboration of the Minimum Description Length framework (Rissanen 1986) that is equivalent to Bayesian inference using an approximation to the posterior distribution chosen so as to minimize the Kullback-Leibler divergence with the true posterior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120741100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre un theoreme fondamental qui donne une borne inferieure pour la longueur de code et donc, pour les erreurs de prediction. On definit les notions \u00abd'information a priori\u00bb et \u00abd'information utile\u00bb dans les donnees"
            },
            "slug": "Stochastic-Complexity-and-Modeling-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84664926"
                        ],
                        "name": "H. Peitgen",
                        "slug": "H.-Peitgen",
                        "structuredName": {
                            "firstName": "Heinz-Otto",
                            "lastName": "Peitgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Peitgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1877119"
                        ],
                        "name": "M. Barnsley",
                        "slug": "M.-Barnsley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Barnsley",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Barnsley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 276
                            }
                        ],
                        "text": "\u2026which the joint distribution of the values of the function at any nite number of points is multivariate Gaussian, are known as Gaussian processes ; theyarise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski1989), and computer graphics (Peitgen and Saupe 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 122
                            }
                        ],
                        "text": "One way to achieve these e ects would be to change the hidden unit activation func-tion from tanh(z) to sign(z)jzj( 1)=2 (Peitgen and Saupe 1988, Sections 1.4.1 and 1.6.11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28842816,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "51d2fdec558e7f71bf2843b02ed1bbd65ebd4c05",
            "isKey": true,
            "numCitedBy": 1771,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Foreword: People and Events Behind the \"Science of Fractal Images\".- Fractals in Nature: From Characterization to Simulation.- Algorithms for Random Fractals.- Color Plates and Captions.- Fractal Patterns Arising in Chaotic Dynamical Systems.- Fantastic Deterministic Fractals.- Fractal Modelling of Real World Images.- Fractal Landscapes Without Creases and with Rivers.- An Eye for Fractals.- A Unified Approach to Fractal Curves and Plants.- Exploring the Mandelbrot Set.- Bibliography.- Index."
            },
            "slug": "The-science-of-fractal-images-Peitgen-Barnsley",
            "title": {
                "fragments": [],
                "text": "The science of fractal images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Fractal Modelling of Real World Images and a Unified Approach to Fractal Curves and Plants are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146497700"
                        ],
                        "name": "D. Harrison",
                        "slug": "D.-Harrison",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harrison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66865765"
                        ],
                        "name": "D. Rubinfeld",
                        "slug": "D.-Rubinfeld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Rubinfeld",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubinfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 507,
                                "start": 198
                            }
                        ],
                        "text": "For this data set, one way in which the prior knowledge of the original investigators may appear in the distribution of the input variables is through their selection of the study area | presumably Harrison and Rubinfeld believed that the range of variation in input variables seen over the Boston area was similar to the range over which these variables might be relevant, as otherwise they might have chosen to study housing prices in all of Massachusetts, or in just the suburb of Newton. Quinlan (1993) assesses the performance of various learning procedures on this problem using ten-way cross validation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 0
                            }
                        ],
                        "text": "Harrison and Rubinfeld (1978) consider various non-linear transformations (e.g. logarithmic) of the target and input variables as the basis for their linear model. However, Quinlan (1993) uses only a linear transformation of the variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 75
                            }
                        ],
                        "text": "2 Tests on the Boston housing data The Boston housing data originates with Harrison and Rubinfeld (1978), who were interested in the e ect of air pollution on housing prices."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Harrison and Rubinfeld (1978) consider various non-linear transformations (e.g. loga-rithmic) of the target and input variables as the basis for their linear model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 75
                            }
                        ],
                        "text": "2 Tests on the Boston housing data The Boston housing data originates with Harrison and Rubinfeld (1978), who were interested in the e ect of air pollution on housing prices.12 The data set was used to evaluate a method for combining instance-based and model-based learning procedures by Quinlan (1993). Although the original objective of Harrison and Rubinfeld was to obtain insight into factors a ecting price, rather than to make accurate predictions, my goal here (and that of Quinlan) is to predict the housing prices based on the attributes given, with performance measured by either squared error loss or absolute error loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Harrison and Rubinfeld (1978) consider various non-linear transformations (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 147
                            }
                        ],
                        "text": "\u2026indeed have been achieved in a real application of a similar nature.4.4.2 Tests on the Boston housing dataThe Boston housing data originates with Harrison and Rubinfeld (1978), who were inter-ested in the e ect of air pollution on housing prices.12 The data set was used to evaluatea method for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Harrison and Rubinfeld (1978) do not mention any censoring.141\n4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1668,
                                "start": 75
                            }
                        ],
                        "text": "2 Tests on the Boston housing data The Boston housing data originates with Harrison and Rubinfeld (1978), who were interested in the e ect of air pollution on housing prices.12 The data set was used to evaluate a method for combining instance-based and model-based learning procedures by Quinlan (1993). Although the original objective of Harrison and Rubinfeld was to obtain insight into factors a ecting price, rather than to make accurate predictions, my goal here (and that of Quinlan) is to predict the housing prices based on the attributes given, with performance measured by either squared error loss or absolute error loss. The data concerns the median price in 1970 of owner-occupied houses in 506 census tracts within the Boston metropolitan area. Thirteen attributes pertaining to each census tract are available for use in predicting the median price, as shown in Figure 4.9. The data is messy in several respects. Some of the attributes are not actually measured on a per-tract basis, but only for larger regions. The median prices for the highest-priced tracts appear to be censored.13 Considering these potential problems, it seems unreasonable to expect that the distribution of the target variable (median price), given the input variables, will be nicely 12The original data is in StatLib, available by anonymous ftp to lib.stat.cmu.edu, directory datasets. 13Censoring is suggested by the fact that the highest median price of exactly $50,000 is reported for sixteen of the tracts, while fteen tracts are reported to have median prices above $40,000 and below $50,000, with prices rounded only to the nearest hundred. Harrison and Rubinfeld (1978) do not mention any censoring."
                    },
                    "intents": []
                }
            ],
            "corpusId": 55571328,
            "fieldsOfStudy": [
                "Economics",
                "Environmental Science"
            ],
            "id": "d4a440776b76332ccb8b9c1c38e1222abaa70c03",
            "isKey": true,
            "numCitedBy": 1546,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hedonic-housing-prices-and-the-demand-for-clean-air-Harrison-Rubinfeld",
            "title": {
                "fragments": [],
                "text": "Hedonic housing prices and the demand for clean air"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35600903"
                        ],
                        "name": "K. Falconer",
                        "slug": "K.-Falconer",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Falconer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Falconer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026prior on the weights and biases going into hidden unitscan be found for which the resulting prior over functions has fractional Brownian properties(Falconer 1990, Section 16.2), characterized byD(x(p); x(q)) jx(p) x(q)j (2.12)As above, values of = 2 and = 1 correspond to smooth and Brownian\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45391913,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b3b630825210016960d9684fa78e924b372aff5e",
            "isKey": true,
            "numCitedBy": 6222,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I Foundations: mathematical background Hausdorff measure and dimension alternative definitions of dimension techniques for calculating dimensions local structure of fractals projections of fractals products of fractals intersections of fractals. Part II Applications and examples: fractals defined by transformations examples from number theory graphs of functions examples from pure mathematics dynamical systems iteration of complex functions-Julia sets random fractals Brownian motion and Brownian surfaces multifractal measures physical applications."
            },
            "slug": "Fractal-geometry-mathematical-foundations-and-Falconer",
            "title": {
                "fragments": [],
                "text": "Fractal geometry - mathematical foundations and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Applications and examples: fractals defined by transformations examples from number theory graphs of functions examples from pure mathematics dynamical systems iteration of complex functions-Julia sets random fractals Brownian motion and Brownian surfaces multifractal measures physical applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 113
                            }
                        ],
                        "text": "In applying a neural network to this ten-way classi cation problem, it is appropriate to use the \\softmax\" model (Bridle 1989), which corresponds to the generalized logistic regression model of statistics (see Section 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 78
                            }
                        ],
                        "text": "All networks were used in conjunction with the softmax model for the targets (Bridle 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "In applying a neural network to this ten-way classi cation problem, it is appropriate touse the \\softmax\" model (Bridle 1989), which corresponds to the generalized logistic regres-sion model of statistics (see Section 1.2.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "For classi cation models such as the \\softmax\" model of Bridle (1989), the relationship is less straightforward."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 130
                            }
                        ],
                        "text": "For a classi cation task, where the target, y, is a single discrete value indicating one ofK possible classes, the softmax model (Bridle 1989) can be used to de ne the conditionalprobabilities of the various classes using a network with K output units, as follows"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "For classi cation models such as the \\softmax\" modelof Bridle (1989), the relationship is less straightforward."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "For a classi cation task, where the target, y, is a single discrete value indicating one of K possible classes, the softmax model (Bridle 1989) can be used to de ne the conditional probabilities of the various classes using a network with K output units, as follows: P (y = k j x) = exp(fk(x)) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic interpretation of feedforward classi cation network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 696,
                                "start": 31
                            }
                        ],
                        "text": "The same point is discussed by MacKay (1992a) in the context of more complex models, where \\simplicity\" cannot necessarily be determined by merely counting parameters. Viewed in one way, these results explain Occam's Razor, and point to the appropriate de nition of simplicity. Viewed another way, however, they say that Bayesians needn't concern themselves with Occam's Razor, since to the extent that it is valid, it will be applied automatically anyway. 1.2 Bayesian neural networks Workers in the eld of \\neural networks\" have diverse backgrounds and motivations, some of which can be seen in the collection of Rumelhart and McClelland (1986b) and the text by Hertz, Krogh, and Palmer (1991). In this thesis, I focus on the potential for neural networks to learn models for complex relationships that are interesting from the viewpoint of arti cial intelligence or useful in engineering applications."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 137
                            }
                        ],
                        "text": "Here, I will discuss implementations based on Gaussian approximations to modes, which have been described by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg (1993). Hinton and van Camp (1993) use a Gaussian approximation of a di erent sort."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1312,
                                "start": 144
                            }
                        ],
                        "text": "Eliminating the hyperparameters in this way may appear to be an obviously bene cial simpli cation of the problem, but this is not the case | as MacKay (1994) explains, integrating out such hyperparameters can sometimes produce a posterior parameter distribution in which the mode is entirely unrepresentative of the distribution as a whole. Basing an approximation on the location of the mode will then give drastically incorrect results. In MacKay's implementation (1991, 1992b, 1992c), he assumes only that the Gaussian approximation can be used to represent the posterior distribution of the parameters for given values of the hyperparameters. He xes the hyperparameters to the values that maximize the probability of the data (what he calls the \\evidence\" for these values of the hyperparameters). In nding these values, he makes use of the Gaussian approximation to integrate over the network parameters. MacKay's approach to handling the hyperparameters is computationally equivalent to the \\ML-II\" method of prior selection (Berger 1982, Section 3.5.4). From a fully Bayesian viewpoint, it is only an approximation to the true answer, which would be obtained by integrating over the hyperparameters as well as the parameters, but experience has shown that it is often a good approximation. Wolpert (1993) criticizes the use of this procedure for neural networks on the grounds that by analytically integrating over the hyperparameters, in the manner of Buntine and Weigend, one can obtain the relative posterior probability densities for di erent values of the network parameters exactly, without the need for any approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 648,
                                "start": 31
                            }
                        ],
                        "text": "The same point is discussed by MacKay (1992a) in the context of more complex models, where \\simplicity\" cannot necessarily be determined by merely counting parameters. Viewed in one way, these results explain Occam's Razor, and point to the appropriate de nition of simplicity. Viewed another way, however, they say that Bayesians needn't concern themselves with Occam's Razor, since to the extent that it is valid, it will be applied automatically anyway. 1.2 Bayesian neural networks Workers in the eld of \\neural networks\" have diverse backgrounds and motivations, some of which can be seen in the collection of Rumelhart and McClelland (1986b) and the text by Hertz, Krogh, and Palmer (1991)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 144
                            }
                        ],
                        "text": "Eliminating the hyperparameters in this way may appear to be an obviously bene cial simpli cation of the problem, but this is not the case | as MacKay (1994) explains, integrating out such hyperparameters can sometimes produce a posterior parameter distribution in which the mode is entirely unrepresentative of the distribution as a whole."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1902,
                                "start": 144
                            }
                        ],
                        "text": "Eliminating the hyperparameters in this way may appear to be an obviously bene cial simpli cation of the problem, but this is not the case | as MacKay (1994) explains, integrating out such hyperparameters can sometimes produce a posterior parameter distribution in which the mode is entirely unrepresentative of the distribution as a whole. Basing an approximation on the location of the mode will then give drastically incorrect results. In MacKay's implementation (1991, 1992b, 1992c), he assumes only that the Gaussian approximation can be used to represent the posterior distribution of the parameters for given values of the hyperparameters. He xes the hyperparameters to the values that maximize the probability of the data (what he calls the \\evidence\" for these values of the hyperparameters). In nding these values, he makes use of the Gaussian approximation to integrate over the network parameters. MacKay's approach to handling the hyperparameters is computationally equivalent to the \\ML-II\" method of prior selection (Berger 1982, Section 3.5.4). From a fully Bayesian viewpoint, it is only an approximation to the true answer, which would be obtained by integrating over the hyperparameters as well as the parameters, but experience has shown that it is often a good approximation. Wolpert (1993) criticizes the use of this procedure for neural networks on the grounds that by analytically integrating over the hyperparameters, in the manner of Buntine and Weigend, one can obtain the relative posterior probability densities for di erent values of the network parameters exactly, without the need for any approximation. This criticism is based on a failure to appreciate the nature of the task. The posterior probability densities for di erent parameter values are, in themselves, of no interest | all that matters is how well the predictive distribution is approximated. MacKay (1994) shows that in approximating this predictive distribution, it is more important to integrate over the large number of parameters in the network than over the typically small number of hyperparameters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 65
                            }
                        ],
                        "text": "These quantities also handled similarly in the implementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991). In the simplest cases, a hyperparameter of the rst type controls the standard deviation for all parameters in a certain group."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1115,
                                "start": 137
                            }
                        ],
                        "text": "Here, I will discuss implementations based on Gaussian approximations to modes, which have been described by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg (1993). Hinton and van Camp (1993) use a Gaussian approximation of a di erent sort. Schemes based on Gaussian approximations to modes operate as follows: 1) Find one or more modes of the posterior parameter distribution. 2) Approximate the posterior distribution in the vicinity of each such mode by a Gaussian whose covariance matrix is chosen to match the second derivatives of the log posterior probability at the mode. 3) If more than one mode is being used, decide how much weight to give to each. 4) Approximate the predictive distribution of equation (1.9) by the corresponding integral with respect to the Gaussian about the mode, or the weighted mixture of Gaussians about the various modes. (For models that are linear in the vicinity of a mode, this is easy; simple approximations may su ce in some other cases (MacKay 1992c); at worst, it can be done reasonably e ciently using simple Monte Carlo methods, as Ripley (1994a) does."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 31
                            }
                        ],
                        "text": "The same point is discussed by MacKay (1992a) in the context of more complex models, where \\simplicity\" cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Adaptive Models, Ph.D thesis, California"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1992b) \\A practical Bayesian framework for backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1992a) \\Bayesian interpolation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 47
                            }
                        ],
                        "text": "The last two sections give results reported by Ripley (1994a), rst for neural networks trained with \\weight decay\" (maximum penalized likelihood) or by an approximate Bayesian method, second for various other statistical procedures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 252
                            }
                        ],
                        "text": "Distributions over functions of this sort, in which the joint distribution of the values of the function at any nite number of points is multivariate Gaussian, are known as Gaussian processes ; they arise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski 1989), and computer graphics (Peitgen and Saupe 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "13, along with the results that Ripley (1994a) reports for neural networks and other methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1095,
                                "start": 32
                            }
                        ],
                        "text": "13, along with the results that Ripley (1994a) reports for neural networks and other methods. Performance is judged here by three criteria | mis-classi cation rate, mis-classi cation rate with the two types of window glass not distinguished, and average log probability assigned to the correct class. The rst two criteria are also used by Ripley. The mis-classi cation rate is the fraction of test cases for which the best guess produced by the model is not the correct class, the best guess being the class whose predictive probability is the highest. When the two categories of window glass are combined, the predictive probabilities for each are summed for the purpose of determining the best guess. In a forensic application, a guess without any indication of reliability is perhaps not useful. To test the accuracy of the full predictive distribution produced by the models, I report minus the log of the predictive probability of the correct class, averaged over the test cases.19 Note that the test set on which these performance gures are based is quite small (96 cases). Ripley (1994a) considers di erences of 4% or less in mis-classi cation rate to not 19For this problem, it may in fact be inappropriate to use predictive probabilities in any of these ways, since such probabilities take no account of other information that may be available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 207
                            }
                        ],
                        "text": "\u2026which the joint distribution of the values of the function at any nite number of points is multivariate Gaussian, are known as Gaussian processes ; theyarise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski1989), and computer graphics (Peitgen and Saupe 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "Since the same test set was used for all the neural network gures, comparisons of di erent neural network models may be signi cant with di erences less than this (as discussed by Ripley (1994a))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatial Statistics, New York: John Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The same point isdiscussed by MacKay (1992a) in the context of more complex models, where \\simplicity\"cannot necessarily be determined by merely counting parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Methods based on makinga Gaussian approximation to the posterior (MacKay 1991, 1992b; Buntine and Weigend1991) may break down as the number of hidden units becomes large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 135
                            }
                        ],
                        "text": "Here, I willdiscuss implementations based on Gaussian approximations to modes, which have beendescribed by Buntine and Weigend (1991), MacKay (1991, 1992b, 1992c), and Thodberg(1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "It is tempting to regard this as an in-dication that the guesses found using hybrid Monte Carlo are closer to the true Bayesian2See Figure 11 of (MacKay 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026based on hybrid Monte Carlo, and providean idea of its performance, I will show here how it can be applied to learning a neuralnetwork model for the \\robot arm\" problem used by Mackay (1991, 1992b) to illustrate hisimplementation of Bayesian inference based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "To test this,I generated new versions of the training set of 200 cases used before by MacKay (1991,1992b) and for the demonstration in Section 3.3, and of the test set of 10 000 cases used in135\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "\u2026and the actual targets in testcases is the subject of Chapter 4, but it is of interest here to compare the test error for therobot arm problem using the hybrid Monte Carlo implementation with the test error foundby MacKay (1991, 1992b) using his implementation based on Gaussian approximations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "2.2 Tests of large networks on the robot arm problemI have tested the behaviour of Bayesian learning with large networks on the robot armproblem of MacKay (1991, 1992b), a regression problem with two input variables and twotarget variables, described in Section 3.3.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "One bene t of a hierarchical model is that the degree of \\regularization\" that is appro-priate for the task can be determined automatically from the data (MacKay 1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Bayesian methods for this areemphasized by MacKay (1992a); frequentist methods such as cross-validation can also beused."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "These quantities also handled similarly in theimplementation of MacKay (1991, 1992b) and in the work of Buntine and Weigend (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Note, by the way, that this should not be aproblem when single-valued estimates for the hyperparameters are used that maximize theprobability of the data (the \\evidence\"), as is done by MacKay (1991, 1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "These results are di erent from those reported by MacKay (1991, 1992b), who found aslight decline in the \\evidence\" for larger networks (up to twenty hidden units), applied tothe robot arm problem with a training set of 200 cases."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "MacKay (1991, 1992b) has tried the most obvious possibility of giving theweights and biases Gaussian prior distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "I demonstrate the use of thisimplementation on the \\robot arm\" problem of MacKay (1991, 1992b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1992c) \\The evidence framework applied to classi cation networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 217
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 172
                            }
                        ],
                        "text": "Evaluation of Neural Network Modelsunits and for the ARD networks with 6 hidden units are also not signi cantly di erent fromthat of the network with six hidden units that Ripley (1994a) trained with an approximateBayesian method based on Gaussian approximations to several modes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 177
                            }
                        ],
                        "text": "Since the same test set was usedfor all the neural network gures, comparisons of di erent neural network models may besigni cant with di erences less than this (as discussed by Ripley (1994a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "4.3 Tests on the forensic glass dataThe forensic glass data was used by Ripley (1994a, 1994b) to test several non-linear clas-si ers, including various neural network models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 97
                            }
                        ],
                        "text": "The predictive performance of these networks is shown in Figure 4.13, along with theresults that Ripley (1994a) reports for neural networks and other methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 274
                            }
                        ],
                        "text": "One possibility is to weight each mode by anestimate of the total probability mass in its vicinity, obtained from the relative probabilitydensity at the mode and the determinant of the covariance matrix of the Gaussian used toapproximate the mode (Buntine and Weigend 1991, Ripley 1994a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 47
                            }
                        ],
                        "text": "The last two sections give results reported by Ripley(1994a), rst for neural networks trained with \\weight decay\" (maximum penalized likelihood) orby an approximate Bayesian method, second for various other statistical procedures.156\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Ripley (1994a) considers di erences of 4% or less in mis-classi cation rate to not19For this problem, it may in fact be inappropriate to use predictive probabilities in any of these ways,since such probabilities take no account of other information that may be available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994b) \\Neural networks and related methods for classi cation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 217
                            }
                        ],
                        "text": "(For models that are linear in thevicinity of a mode, this is easy; simple approximations may su ce in someother cases (MacKay 1992c); at worst, it can be done reasonably e cientlyusing simple Monte Carlo methods, as Ripley (1994a) does.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 172
                            }
                        ],
                        "text": "Evaluation of Neural Network Modelsunits and for the ARD networks with 6 hidden units are also not signi cantly di erent fromthat of the network with six hidden units that Ripley (1994a) trained with an approximateBayesian method based on Gaussian approximations to several modes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 177
                            }
                        ],
                        "text": "Since the same test set was usedfor all the neural network gures, comparisons of di erent neural network models may besigni cant with di erences less than this (as discussed by Ripley (1994a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "4.3 Tests on the forensic glass dataThe forensic glass data was used by Ripley (1994a, 1994b) to test several non-linear clas-si ers, including various neural network models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 97
                            }
                        ],
                        "text": "The predictive performance of these networks is shown in Figure 4.13, along with theresults that Ripley (1994a) reports for neural networks and other methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 274
                            }
                        ],
                        "text": "One possibility is to weight each mode by anestimate of the total probability mass in its vicinity, obtained from the relative probabilitydensity at the mode and the determinant of the covariance matrix of the Gaussian used toapproximate the mode (Buntine and Weigend 1991, Ripley 1994a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 47
                            }
                        ],
                        "text": "The last two sections give results reported by Ripley(1994a), rst for neural networks trained with \\weight decay\" (maximum penalized likelihood) orby an approximate Bayesian method, second for various other statistical procedures.156\n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Ripley (1994a) considers di erences of 4% or less in mis-classi cation rate to not19For this problem, it may in fact be inappropriate to use predictive probabilities in any of these ways,since such probabilities take no account of other information that may be available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994a) \\Flexible non-linear approaches to classi cation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "Tierney(1991) and Smith and Roberts (1993) also review recent work on Markov chain Monte Carlomethods and their applications in statistics.1.3.1 Monte Carlo integration using Markov chainsThe objective of Bayesian learning is to produce predictions for test cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian computation via the Gibbs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 84
                            }
                        ],
                        "text": "Such approximate Bayesianmethods have proven useful in some practical applications (MacKay 1993, Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 214
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such as recognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat (Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 213
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such asrecognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat(Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 153
                            }
                        ],
                        "text": "Such a model has been developed by David MacKay and myself (MacKay and Neal,in preparation), and used by MacKay in a model of energy usage in buildings (Mackay1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian non-linear modeling for the energy prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "Xk0 exp(fk0(x)) (1.8)This method of de ning class probabilities is also used in generalized linear models instatistics (McCullagh and Nelder, 1983, Section 5.1.3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models, London: Chap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 221923757,
            "fieldsOfStudy": [],
            "id": "8398090fa627cd00eaa6f4afde6a4e6458c9070e",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bibliography.",
            "title": {
                "fragments": [],
                "text": "Bibliography."
            },
            "venue": {
                "fragments": [],
                "text": "The British Journal for the History of Science"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1886445"
                        ],
                        "name": "F. Guess",
                        "slug": "F.-Guess",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Guess",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Guess"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123401507,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d2dcf782737482b7655dc4797582b19ca4ce7448",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-Statistics:-Principles,-Models,-and-Guess",
            "title": {
                "fragments": [],
                "text": "Bayesian Statistics: Principles, Models, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102805647"
                        ],
                        "name": "Samuel A. Schmitt",
                        "slug": "Samuel-A.-Schmitt",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Schmitt",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel A. Schmitt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122032671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ef4fc481c2f951cae5a6b8ba4e74454f66f044f",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measuring-Uncertainty:-An-Elementary-Introduction-Schmitt",
            "title": {
                "fragments": [],
                "text": "Measuring Uncertainty: An Elementary Introduction to Bayesian Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 32
                            }
                        ],
                        "text": "The method of cross validation (Stone 1974) is sometimes used to nd an appropriate14\n1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116210394,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c61149de2d5aca78932729a16c657f811edc63b5",
            "isKey": false,
            "numCitedBy": 827,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-(With-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions (With Discussion)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114788213"
                        ],
                        "name": "D. Signorini",
                        "slug": "D.-Signorini",
                        "structuredName": {
                            "firstName": "DavidF.",
                            "lastName": "Signorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Signorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4137433"
                        ],
                        "name": "J. Slattery",
                        "slug": "J.-Slattery",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058057862"
                        ],
                        "name": "S. Dodds",
                        "slug": "S.-Dodds",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Dodds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dodds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51934565"
                        ],
                        "name": "V. Lane",
                        "slug": "V.-Lane",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095059657"
                        ],
                        "name": "P. Littlejohns",
                        "slug": "P.-Littlejohns",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Littlejohns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Littlejohns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 45
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989,Hornik, Stinchcombe, and White 1989) have shown that in this limit a network with onelayer of hidden units can approximate any continuous function de ned on a compact domainarbitrarily closely."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 46
                            }
                        ],
                        "text": "Several people (Cybenko 1989, Funahashi 1989, Hornik, Stinchcombe, and White 1989)have shown that a multilayer perceptron network with one hidden layer can approximateany function de ned on a compact domain arbitrarily closely, if su cient numbers of hiddenunits are used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2878979,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "20b844e395355b40fa5940c61362ec40e56027aa",
            "isKey": false,
            "numCitedBy": 4703,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-Signorini-Slattery",
            "title": {
                "fragments": [],
                "text": "Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 28
                            }
                        ],
                        "text": "In an optimization context, Szu and Hartley (1987) advocate using a multivariateCauchy rather than a Gaussian as the Metropolis proposal distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4219843,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8fcbcf065153215e931ce502383dda8290f583a1",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Physics-Letters",
            "title": {
                "fragments": [],
                "text": "Physics Letters"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Bayesian mixture modeling Neudorfer (editors) Maximum Entropy and Bayesian Methods: Proceedings of the 11th International Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ") \\Bayesian training of backpropagation networks by the hybrid Monte Carlo method"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 136
                            }
                        ],
                        "text": "IntroductionIntroductions to Bayesian inference are provided by Press (1989) and Schmitt (1969);Berger (1985), Box and Tiao (1973), and DeGroot (1970) o er more advanced treatments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "One reason for optimism in this regard is that the posteriordistribution for many models becomes increasingly Gaussian as the amount of training dataincreases (DeGroot 1970, Chapter 10)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal Statistical Decisions, New York: McGraw-Hill"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 248
                            }
                        ],
                        "text": "It is used in the \\Boltzmann machine\" neural network of Ackley, Hinton, and Sejnowski (1985) to sample from distributions over hidden units, and is now widely used for statistical problems, following its exposition by Geman and Geman (1984) and by Gelfand and Smith (1990). The Gibbs sampler is applicable when we wish to sample from a distribution over a multi-dimensional parameter, = f 1; ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 245
                            }
                        ],
                        "text": "It is used in the \\Boltzmann machine\" neuralnetwork of Ackley, Hinton, and Sejnowski (1985) to sample from distributions over hiddenunits, and is now widely used for statistical problems, following its exposition by Gemanand Geman (1984) and by Gelfand and Smith (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sampling-based approaches to calculating"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 283
                            }
                        ],
                        "text": "Distributions over functions of this sort, in which the joint distribution of the values of the function at any nite number of points is multivariate Gaussian, are known as Gaussian processes ; they arise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski 1989), and computer graphics (Peitgen and Saupe 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 238
                            }
                        ],
                        "text": "\u2026which the joint distribution of the values of the function at any nite number of points is multivariate Gaussian, are known as Gaussian processes ; theyarise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski1989), and computer graphics (Peitgen and Saupe 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling of Uncertainty in Low-level Vision, Boston: Kluwer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(1994b) \\Neural networks and related methods for classiication\" (with discussion) \\Stochastic complexity and modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B Bibliography Rissanen, J. Annals of Statistics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Physics Letters B"
            },
            "venue": {
                "fragments": [],
                "text": "Physics Letters B"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(1993b) \\Probabilistic inference using Markov Chain Monte Carlo methods"
            },
            "venue": {
                "fragments": [],
                "text": "(1993b) \\Probabilistic inference using Markov Chain Monte Carlo methods"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(1994a) \\Flexible non-linear approaches to classiication"
            },
            "venue": {
                "fragments": [],
                "text": "From Statistics to Neural Networks: Theory and Pattern Recognition Applications, ASI Proceedings, subseries F, Computer and Systems Sciences"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Bayesian back-propagation \\Higher-order hybrid Monte Carlo algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems Physical Review Letters"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 84
                            }
                        ],
                        "text": "Such approximate Bayesianmethods have proven useful in some practical applications (MacKay 1993, Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 213
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such asrecognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat(Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 153
                            }
                        ],
                        "text": "Such a model has been developed by David MacKay and myself (MacKay and Neal,in preparation), and used by MacKay in a model of energy usage in buildings (Mackay1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Hyperparameters: Optimise, or integrate out?"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 84
                            }
                        ],
                        "text": "Such approximate Bayesianmethods have proven useful in some practical applications (MacKay 1993, Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 213
                            }
                        ],
                        "text": "Such models have been applied to a wide variety of tasks, such asrecognizing hand-written digits (Le Cun, et al 1990), determining the fat content of meat(Thodberg 1993), and predicting energy usage in buildings (MacKay 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 153
                            }
                        ],
                        "text": "Such a model has been developed by David MacKay and myself (MacKay and Neal,in preparation), and used by MacKay in a model of energy usage in buildings (Mackay1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Bayesian non-linear modeling for the energy prediction competition"
            },
            "venue": {
                "fragments": [],
                "text": "\\Bayesian non-linear modeling for the energy prediction competition"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 988,
                                "start": 277
                            }
                        ],
                        "text": "One possibility is to weight each mode by an estimate of the total probability mass in its vicinity, obtained from the relative probability density at the mode and the determinant of the covariance matrix of the Gaussian used to approximate the mode (Buntine and Weigend 1991, Ripley 1994a). This is not a fully correct procedure, however | the weight a mode receives ought really to be adjusted according to the probability of the mode being found by the optimization procedure, with the easily found modes being given less weight than they would otherwise have had, since they occur more often. For large problems this will not be possible, however, since each mode will typically be seen only once, making the probabilities of nding the modes impossible to determine. Another problem is that if the Gaussian approximation is not very accurate, one mode may receive most of the weight simply because it happened to be favoured by approximation error. Such problems lead Thodberg (1993) to use the estimated probability mass only to select a \\committee\" based on the better modes (perhaps from di erent models), to each of which he assigns equal weight."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic Simulation, New York: John Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(1993a) \\Bayesian learning via stochastic dynamics"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 5"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and\\structural risk minimization\" (Vapnik 1982)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abstract Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Abstract Inference"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "It is used in the \\Boltzmann machine\" neural network of Ackley, Hinton, and Sejnowski (1985) to sample from distributions over hidden units, and is now widely used for statistical problems, following its exposition by Geman and Geman (1984) and by Gelfand and Smith (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Keeping neural networks simple by minimizing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "(in preparation) \\Automatic relevance determination for neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "(in preparation) \\Automatic relevance determination for neural networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and\\structural risk minimization\" (Vapnik 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1208,
                                "start": 68
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and \\structural risk minimization\" (Vapnik 1982). From a Bayesian perspective, adjusting the complexity of the model based on the amount of training data makes no sense. A Bayesian de nes a model, selects a prior, collects data, computes the posterior, and then makes predictions. There is no provision in the Bayesian framework for changing the model or the prior depending on how much data was collected. If the model and prior are correct for a thousand observations, they are correct for ten observations as well (though the impact of using an incorrect prior might be more serious with fewer observations). In practice, we might sometimes switch to a simpler model if it turns out that we have little data, and we feel that we will consequently derive little bene t from using a complex, computationally expensive model, but this is a matter of pragmatism rather than principle. For problems where we do not expect a simple solution, the proper Bayesian approach is therefore to use a model of a suitable type that is as complex as we can a ord computationally, regardless of the size of the training set. Young (1977), for example, uses polynomial models of inde nitely high order."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 67
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and \\structural risk minimization\" (Vapnik 1982)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abstract Inference, New York: John Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Barnett (1982) presents a comparative view of di erent approaches to statistical inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparative Statistical Inference, Second Edition"
            },
            "venue": {
                "fragments": [],
                "text": "Comparative Statistical Inference, Second Edition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 33
                            }
                        ],
                        "text": "The \\smartMonte Carlo\" method of Rossky, Doll, and Friedman (1978) is equivalent to this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Brownian dynamics as smart"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 119
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and\\structural risk minimization\" (Vapnik 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 119
                            }
                        ],
                        "text": "More formal approaches of this sort include the \\method of sieves\" (Grenander 1981) and \\structural risk minimization\" (Vapnik 1982)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependencies"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 35
                            }
                        ],
                        "text": "This is a common approach, used by Liu (1994), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a back-propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 47,
            "methodology": 52,
            "result": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 92,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-Learning-for-Neural-Networks-Neal/db869fa192a3222ae4f2d766674a378e47013b1b?sort=total-citations"
}