{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692339"
                        ],
                        "name": "B. Yeo",
                        "slug": "B.-Yeo",
                        "structuredName": {
                            "firstName": "Boon-Lock",
                            "lastName": "Yeo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yeo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108661679"
                        ],
                        "name": "Bede Liu",
                        "slug": "Bede-Liu",
                        "structuredName": {
                            "firstName": "Bede",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bede Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 62189337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac83336e50496b9a75714f1024531fe7d698d33b",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Embedded captions in TV programs such as news broadcasts, documentaries and coverage of sports events provide important information on the underlying events. In digital video libraries, such captions represent a highly condensed form of key information on the contents of the video. In this paper we propose a scheme to automatically detect the presence of captions embedded in video frames. The proposed method operates on reduced image sequences which are efficiently reconstructed from compressed MPEG video and thus does not require full frame decompression. The detection, extraction and analysis of embedded captions help to capture the highlights of visual contents in video documents for better organization of video, to present succinctly the important messages embedded in the images, and to facilitate browsing, searching and retrieval of relevant clips."
            },
            "slug": "Visual-content-highlighting-via-automatic-of-on-Yeo-Liu",
            "title": {
                "fragments": [],
                "text": "Visual content highlighting via automatic extraction of embedded captions on MPEG compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A scheme to automatically detect the presence of captions embedded in video frames which operates on reduced image sequences which are efficiently reconstructed from compressed MPEG video and thus does not require full frame decompression."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "Jain and Yu [5] decomposed the video frames into subimages of different colors and then examined if each subimage contained text components that satisfy some prespecified heuristics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702517"
                        ],
                        "name": "I. Sethi",
                        "slug": "I.-Sethi",
                        "structuredName": {
                            "firstName": "Ishwar",
                            "lastName": "Sethi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20747085"
                        ],
                        "name": "Nilesh V. Patel",
                        "slug": "Nilesh-V.-Patel",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Patel",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh V. Patel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Therefore, there is an emerging trend to extract features directly in the compressed domain [11], [12], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41185787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "741cc5fb54d426fc48498e360e7b0b75b8222a38",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the challenging problems in video databases is the organization of video information. Segmenting a video into a number of clips and characterizing each clip has been suggested as one mechanism for organizing video information. This approach requires a suitable method to automatically locate cut points in a video. One way of finding such cut points is to determine the boundaries between consecutive camera shots. In this paper, we address this as a statistical hypothesis testing problem and present three tests to determine cut locations. All the three tests are such that they can be applied directly to the compressed video. This avoids an unnecessary decompression-compression cycle, since it is common to store and transmit digital video in compressed form. As our experimental results indicate, the statistical approach permits accurate detection of scene changes induced through straight as well as optical cuts."
            },
            "slug": "Statistical-approach-to-scene-change-detection-Sethi-Patel",
            "title": {
                "fragments": [],
                "text": "Statistical approach to scene change detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The statistical approach permits accurate detection of scene changes induced through straight as well as optical cuts and avoids an unnecessary decompression-compression cycle since it is common to store and transmit digital video in compressed form."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114337658"
                        ],
                        "name": "B. Shen",
                        "slug": "B.-Shen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702517"
                        ],
                        "name": "I. Sethi",
                        "slug": "I.-Sethi",
                        "structuredName": {
                            "firstName": "Ishwar",
                            "lastName": "Sethi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sethi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14629644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "249fee848f5273dad29a6343bd151690a859c9bf",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper presents a scheme for performing convolution operation directly on compressed images without decompressing them first. The use of such a scheme is demonstrated and discussed by showing the implementation of the Laplacian-of-Gaussian operator for edge detection. We present a complete evaluation of the different parameters involved in this process and show edge detection results on several real images through our proposed scheme. In each case, it is shown that the proposed scheme of directly performing convolution on the compressed data leads to not only a significant computation speedup but also yields better edges."
            },
            "slug": "Convolution-Based-Edge-Detection-for-Image/Video-in-Shen-Sethi",
            "title": {
                "fragments": [],
                "text": "Convolution-Based Edge Detection for Image/Video in Block DCT Domain"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "In each case, it is shown that the proposed scheme of directly performing convolution on the compressed data leads to not only a significant computation speedup but also yields better edges."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2211839"
                        ],
                        "name": "D. L. Gall",
                        "slug": "D.-L.-Gall",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Gall",
                            "middleNames": [
                                "J.",
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Gall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 652286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bafe1e3aba293f978ec0f44599d7d62eca9206c",
            "isKey": false,
            "numCitedBy": 2458,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The Moving Picture Experts Group (MPEG) standard addresses compression of video signals at approximately 1.5M-bits. MPEG is a generic standard and is independent of any particular applications. Applications of compressed video on digital storage media include asymmetric applications such as electronic publishing, games and entertainment. Symmetric applications of digital video include video mail, video conferencing, videotelephone and production of electronic publishing. Design of the MPEG algorithm presents a difficult challenge since quality requirements demand high compression that cannot be achieved with only intraframe coding. The algorithm\u2019s random access requirement, however, is best satisfied with pure intraframe coding. MPEG uses predictive and interpolative coding techniques to answer this challenge. Extensive details are presented."
            },
            "slug": "MPEG:-a-video-compression-standard-for-multimedia-Gall",
            "title": {
                "fragments": [],
                "text": "MPEG: a video compression standard for multimedia applications"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Design of the MPEG algorithm presents a difficult challenge since quality requirements demand high compression that cannot be achieved with only intraframe coding, and the algorithm\u2019s random access requirement is best satisfied with pure intraframes coding."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 30086203,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7e3ed958f25974dab09b445fbd6f297852b761fc",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Like shot changes, the presence of text in digital video is an important event that can be used to index digital video and provide extremely useful semantic information about the scene content. The special characteristics of digital video compared to document images both require and allow new robust approaches to recognition of text in video. We discuss the characteristics and special challenges of text in video and present a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem. Preliminary results from our approach are presented."
            },
            "slug": "Indexing-text-events-in-digital-video-databases-Gargi-Antani",
            "title": {
                "fragments": [],
                "text": "Indexing text events in digital video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The characteristics and special challenges of text in video are discussed and a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 29853292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4af75831ed098d9fea02507f36cdbc38852fe6",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a substantial interest in retrieving images from a large database using the textual information contained in the images. An algorithm which will automatically locate the textual regions in the input image will facilitate this task; the optical character recognizer can then be applied to only those regions of the image which contain text. We present a method for automatically locating text in complex color images. The algorithm first finds the approximate locations of text lines using horizontal spatial variance, and then extracts text components in these boxes using color segmentation. The proposed method has been used to locate text in compact disc (CD) and book cover images, as well as in the images of traffic scenes captured by a video camera. Initial results are encouraging and suggest that these algorithms can be used in image retrieval applications."
            },
            "slug": "Locating-text-in-complex-color-images-Zhong-Karu",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed algorithm has been used to locate text in compact disc and book cover images, as well as in the images of traffic scenes captured by a video camera, and initial results suggest that these algorithms can be used in image retrieval applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645662"
                        ],
                        "name": "Michael Smith",
                        "slug": "Michael-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 36119549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02e95ad680fc3b216a11181638ef5d31f66f423a",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe three technologies involved in creating a digital video library suitable for fullcontent search and retrieval. Image processing analyzes scenes, speech processing transcribes the audio signal, and natural language processing determines word relevance. The integration of these technologies enables us to include vast amounts of video data in the library."
            },
            "slug": "Text,-Speech,-and-Vision-for-Video-Segmentation:-Hauptmann-Smith",
            "title": {
                "fragments": [],
                "text": "Text, Speech, and Vision for Video Segmentation: The InformediaTM Project"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Three technologies involved in creating a digital video library suitable for fullcontent search and retrieval are described: image processing analyzes scenes, speech processing transcribes the audio signal, and natural language processing determines word relevance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "In addition, character components form text lines with approximately the same spacing between them [4], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Jain and Zhong [4], [6] have used the distinguishing texture present in text to determine and separate text, graphics, and halftone image regions in scanned grayscale document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39821816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14371d83ac90edc11b3604d4bf64915f99e45678",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Page-segmentation-using-tecture-analysis-Jain-Zhong",
            "title": {
                "fragments": [],
                "text": "Page segmentation using tecture analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784312"
                        ],
                        "name": "C. Low",
                        "slug": "C.-Low",
                        "structuredName": {
                            "firstName": "Chien",
                            "lastName": "Low",
                            "middleNames": [
                                "Yong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Low"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588714"
                        ],
                        "name": "D. Zhong",
                        "slug": "D.-Zhong",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zhong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "In response to such needs, various video content analysis techniques using one or a combination of image, audio, and textual information present in video have been proposed to parse, index, and abstract massive amounts of data [1], [3], [15], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "With hundreds of thousands of hours of archival videos, there is an urgent demand for tools that will allow efficient browsing and retrieving of video data [1], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2469411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16daa3e4fd7fb7c3b8fa843ca79ef27a34ebce1f",
            "isKey": false,
            "numCitedBy": 399,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an integrated solution for computer assisted video parsing and content-based video retrieval and browsing. The uniqueness and effectiveness of this solution lies in its use of video content information provided by a parsing process driven by visual feature analysis. More specifically, parsing will temporally segment and abstract a video source, based on low-level image analyses; then retrieval and browsing of video will be based on key-frames selected during abstraction and spatial-temporal variations of visual features, as well as some shot-level semantics derived from camera operation and motion analysis. These processes, as well as video retrieval and browsing tools, are presented in detail as functions of an integrated system. Also, experimental results on automatic key-frame detection are given."
            },
            "slug": "Video-parsing,-retrieval-and-browsing:-an-and-Zhang-Low",
            "title": {
                "fragments": [],
                "text": "Video parsing, retrieval and browsing: an integrated and content-based solution"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper presents an integrated solution for computer assisted video parsing and content-based video retrieval and browsing that uses video content information provided by a parsing process driven by visual feature analysis."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [ 17 ], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 36502813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b00e70c64f2cc19cd893067c6e209ea47f37ee6",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer-assisted content-based indexing is a critical enabling technology and currently a bottleneck in productive use of video resources. This paper presents the Video Classification Project, an effort toward automating content-based video indexing and retrieval, at the Institute of Systems Science of the National University of Singapore. We discuss in detail three goals of the project: image processing tools for video parsing, feature extraction and retrieval; a knowledge-based approach to representing video content; and stratified tools which allow greater flexibility in browsing a video resource, either before or after performing specific retrieval operations."
            },
            "slug": "Developing-power-tools-for-video-indexing-and-Zhang-Smoliar",
            "title": {
                "fragments": [],
                "text": "Developing power tools for video indexing and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Video Classification Project is presented, an effort toward automating content-based video indexing and retrieval, at the Institute of Systems Science of the National University of Singapore."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In response to such needs, various video content analysis techniques using one or a combination of image, audio, and textual information present in video have been proposed to parse, index, and abstract massive amounts of data [1], [3], [ 15 ], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8345108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4d170b81ffc895e5b7960040a3f44a044d6bb8",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources."
            },
            "slug": "Intelligent-Access-to-Digital-Video:-Informedia-Wactlar-Kanade",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video: Informedia Project"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval, and focused the work on two corpuses."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50121148"
                        ],
                        "name": "G. Wallace",
                        "slug": "G.-Wallace",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Wallace",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wallace"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7051992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e190281cc49c07bce64d037fb24705bb9296b9b",
            "isKey": false,
            "numCitedBy": 3525,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG\u2019s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for \u201clossy\u2019\u2019 compression, and a predictive method for \u201clossless\u2019\u2019 compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method."
            },
            "slug": "The-JPEG-still-picture-compression-standard-Wallace",
            "title": {
                "fragments": [],
                "text": "The JPEG still picture compression standard"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772930"
                        ],
                        "name": "Michael G. Christel",
                        "slug": "Michael-G.-Christel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Christel",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael G. Christel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "With hundreds of thousands of hours of archival videos, there is an urgent demand for tools that will allow efficient browsing and retrieving of video data [ 1 ], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In response to such needs, various video content analysis techniques using one or a combination of image, audio, and textual information present in video have been proposed to parse, index, and abstract massive amounts of data [ 1 ], [3], [15], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35858397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265d13a97e85d6409015205d1aae8e10259ae2e6",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Vastdigital libraries of information will soon be available on the nation\u2019s Information Superhighway as a result of emerging technologies for multimedia computing. These libraries wiU profoundly impact the conduct of business, professional, andpersonal activity. However, it is not enough to simply store and play back video (ss in currently envisioned commercial video-on-demand services); to be most effective, new technology is needed for searching through these vast data collections and retrieving the most relevant selections."
            },
            "slug": "Informedia-digital-video-library-Christel-Stevens",
            "title": {
                "fragments": [],
                "text": "Informedia digital video library"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "To be most effective, new technology is needed for searching through these vast data collections and retrieving the most relevant selections."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114660236"
                        ],
                        "name": "H. Schneider",
                        "slug": "H.-Schneider",
                        "structuredName": {
                            "firstName": "Hans-Jochen",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 39944136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26f2e89f5585cf2ec4c0cdcd4ec4264d5726b07e",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "research and development in information retrieval is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the research and development in information retrieval is universally compatible with any devices to read."
            },
            "slug": "Research-and-Development-in-Information-Retrieval-Salton-Schneider",
            "title": {
                "fragments": [],
                "text": "Research and Development in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The research and development in information retrieval is universally compatible with any devices to read, and can be downloaded instantly from the authors' digital library."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Jain and Zhong [4], [6] have used the distinguishing texture present in text to determine and separate text, graphics, and halftone image regions in scanned grayscale document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "Y. Zhong is with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [22] further utilized the texture characteristics of text lines to extract text in grayscale images with complex backgrounds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "In addition, character components form text lines with approximately the same spacing between them [4], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [22] extracted text as those connected components of monotonous color which follow certain size constraints and horizontal alignment constraints."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aText Segmentation Using Gabor Filters for Automatic Document Processing,o"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141915"
                        ],
                        "name": "W. Niblack",
                        "slug": "W.-Niblack",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Niblack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Niblack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60957367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a39dd6665306deed780261aee45fc79d2433062",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Storage-and-Retrieval-for-Image-and-Video-Databases-Niblack-Jain",
            "title": {
                "fragments": [],
                "text": "Storage and Retrieval for Image and Video Databases III"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784312"
                        ],
                        "name": "C. Low",
                        "slug": "C.-Low",
                        "structuredName": {
                            "firstName": "Chien",
                            "lastName": "Low",
                            "middleNames": [
                                "Yong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Low"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115902487"
                        ],
                        "name": "J. H. Wu",
                        "slug": "J.-H.-Wu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Wu",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60663552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07bf92ff321df9a27deada97b758d67574aea4c6",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Video-parsing,-retrieval-and-browsing:-an-and-Zhang-Low",
            "title": {
                "fragments": [],
                "text": "Video parsing, retrieval and browsing: an integrated and content-based solution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 34993677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73bf7b2fdb26498c05896d99fee4b8f3608c3bd6",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] extracted text as those connected components of monotonous color which follow certain size constraints and horizontal alignment constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] further utilized the texture characteristics of text lines to extract text in grayscale images with complex backgrounds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "corpusId": 34435085,
            "fieldsOfStudy": [],
            "id": "29a98da1ded1b7d8e849b95c0490013490b1b306",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A New Methodology for Gray-Scale Character Segmentation and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skimming and Characterization through Language and Image Understanding Techniques,\u00ba technical report"
            },
            "venue": {
                "fragments": [],
                "text": "Skimming and Characterization through Language and Image Understanding Techniques,\u00ba technical report"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "With hundreds of thousands of hours of archival videos, there is an urgent demand for tools that will allow efficient browsing and retrieving of video data [1], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aDeveloping Power Tools for Video Indexing and Retrieval,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. SPIE Conf. Storage and Retrieval for Image and Video Databases,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaIntelligent Access to Digital Video: The Informedia Project,\u00ba IEEE Computer"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaIntelligent Access to Digital Video: The Informedia Project,\u00ba IEEE Computer"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] proposed to filter the video stream for the textappearance events using the number of Intracoded blocks in P- or B-frames based on the assumption that when captions appear and disappear, the corresponding blocks are usually Intracoded."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aIndexing Text Events in Digital Video Databaseso"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 14th Int'l Conf. Pattern Recognition (ICPR), pp. 916\u00b1918"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Therefore, there is an emerging trend to extract features directly in the compressed domain [11], [12], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aVideo Parsing and Browsing Using Compressed Data,o"
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Tools and Applications, pp"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] used chaincodes to segment text components from video images and used temporal information to refine the extraction based on the assumption that captions stay static in a video sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and R"
            },
            "venue": {
                "fragments": [],
                "text": "Bolle, aAutomatic Text Extraction from Video for Content-Based Annotation and Retrieval,o Proc. 14th Int'l Conf. Pattern Recognition, pp. 618\u00b1620"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and E"
            },
            "venue": {
                "fragments": [],
                "text": "Riseman, aFinding Text in Images,o 20th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pp. 3\u00b112"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "The DCT coefficients in JPEG images [16] or MPEG video [8], which capture the directionality and periodicity of local image blocks are used as texture measures to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aThe JPEG Still Picture Compression Standard,o"
            },
            "venue": {
                "fragments": [],
                "text": "Comm. ACM,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "In response to such needs, various video content analysis techniques using one or a combination of image, audio, and textual information present in video have been proposed to parse, index, and abstract massive amounts of data [1], [3], [15], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aText"
            },
            "venue": {
                "fragments": [],
                "text": "Speech, and Vision for Video Segmentation: The Informedia Project,o AAAI Symp. Computational Models for Integrating Language and Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "For example, Smith and Kanade [14] located text as horizontal rectangular structures of clustered sharp edges."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aVideo Skimming and Characterization through Language and Image Understanding Techniques,o technical report"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaPage Segmentation Using Texture Analysis,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaPage Segmentation Using Texture Analysis,\u00ba Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 11
                            }
                        ],
                        "text": "Therefore, Yeo and Liu's method would not be able to handle captions that gradually enter or disappear from the frames."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Yeo and Liu [18] proposed to extract embedded captions in partially uncompressed MPEG video, where reduced resolution video frames were reconstructed from original MPEG sequences using either the DC components or the DC components plus two AC components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aVisual Content Highlighting via Automatic Extraction of Embedded Captions on MPEG Compressed Video,o"
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Digital Video Compression: Algorithms and Technologies,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 252
                            }
                        ],
                        "text": "For example, the MPEG video compression standard applies DCT coding to reduce spatial redundancies within a video frame (same as in the JPEG image compression standard) and motion compensation to reduce temporal redundancies between consecutive frames [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "The DCT coefficients in JPEG images [16] or MPEG video [8], which capture the directionality and periodicity of local image blocks are used as texture measures to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aMPEG: A Video Compression Standard for Multimedia Applications,o"
            },
            "venue": {
                "fragments": [],
                "text": "Comm. ACM,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaInformedia Digital Video Library,\u00ba Proc. ACM Multimedia Conf"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaInformedia Digital Video Library,\u00ba Proc. ACM Multimedia Conf"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automatic Caption Localization in Compressed Video"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smoliar, \u00aaVideo Parsing and Browsing Using Compressed Data,\u00ba Multimedia Tools and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Smoliar, \u00aaVideo Parsing and Browsing Using Compressed Data,\u00ba Multimedia Tools and Applications"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Therefore, there is an emerging trend to extract features directly in the compressed domain [11], [12], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aConvolution-Based Edge-Detection for Image/ Video in Block DCT Domain,o"
            },
            "venue": {
                "fragments": [],
                "text": "J. Visual Comm. and Image Representation,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "In a similar manner, Lienhart and Stuber [9] identified text as connected components which are of the same color, fall in some specified size range, and have corresponding matching components in consecutive video frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 127
                            }
                        ],
                        "text": "A number of algorithms to extract caption texts from still images and video have been published in recent years [2], [5], [7], [9], [10], [13], [14], [17], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 208
                            }
                        ],
                        "text": "We also used 1) a CNN video clip (3897 total frames, 268 I-frames), which covered a variety of events including outdoor and newsroom news programs, and weather forecast and 2) 6 video clips that were used in [9] (267 I-frames total), which included home video, movie clips, and commercial advertisements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "aAutomatic Text Recognition in Digital Videos,o"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Praktische Informatic IV, pp"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-caption-localization-in-compressed-video-Zhong-Zhang/f7721138e41d82fedabca59c9a66e67d9b7053f3?sort=total-citations"
}