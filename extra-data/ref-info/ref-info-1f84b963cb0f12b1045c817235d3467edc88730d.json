{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109366306"
                        ],
                        "name": "Xiaoqian Liu",
                        "slug": "Xiaoqian-Liu",
                        "structuredName": {
                            "firstName": "Xiaoqian",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqian Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109080279"
                        ],
                        "name": "Weiqiang Wang",
                        "slug": "Weiqiang-Wang",
                        "structuredName": {
                            "firstName": "Weiqiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqiang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127808"
                        ],
                        "name": "T. Zhu",
                        "slug": "T.-Zhu",
                        "structuredName": {
                            "firstName": "Tingshao",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10594614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff4d3d02065db0ff8c77e27d4f79f81204d04041",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Captions in videos play a significant role for automatically understanding and indexing video content, since much semantic information is associated with them. This paper presents an effective approach to extracting captions from videos, in which multiple different categories of features (edge, color, stroke etc.) are utilized, and the spatio-temporal characteristics of captions are considered. First, our method exploits the distribution of gradient directions to decompose a video into a sequence of clips temporally, so that each clip contains a caption at most, which makes the successive extraction computation more efficient and accurate. For each clip, the edge and corner information are then utilized to locate text regions. Further, text pixels are extracted based on the assumption that text pixels in text regions always have homogeneous color, and their quantity dominates the region relative to non-text pixels with different colors. Finally, the segmentation results are further refined. The encouraging experimental results on 2565 characters have preliminarily validated our approach."
            },
            "slug": "Extracting-Captions-in-Complex-Background-from-Liu-Wang",
            "title": {
                "fragments": [],
                "text": "Extracting Captions in Complex Background from Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an effective approach to extracting captions from videos, in which multiple different categories of features are utilized, and the spatio-temporal characteristics of captions are considered."
            },
            "venue": {
                "fragments": [],
                "text": "2010 20th International Conference on Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717886"
                        ],
                        "name": "Wen Wu",
                        "slug": "Wen-Wu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10007203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f5ff7a415810fa61c1894cb10fd0b9ca0c8a44d",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast and robust framework for incrementally detecting text on road signs from video is presented in this paper. This new framework makes two main contributions. 1) The framework applies a divide-and-conquer strategy to decompose the original task into two subtasks, that is, the localization of road signs and the detection of text on the signs. The algorithms for the two subtasks are naturally incorporated into a unified framework through a feature-based tracking algorithm. 2) The framework provides a novel way to detect text from video by integrating two-dimensional (2-D) image features in each video frame (e.g., color, edges, texture) with the three-dimensional (3-D) geometric structure information of objects extracted from video sequence (such as the vertical plane property of road signs). The feasibility of the proposed framework has been evaluated using 22 video sequences captured from a moving vehicle. This new framework gives an overall text detection rate of 88.9% and a false hit rate of 9.2%. It can easily be applied to other tasks of text detection from video and potentially be embedded in a driver assistance system."
            },
            "slug": "Detection-of-text-on-road-signs-from-video-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "Detection of text on road signs from video"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A fast and robust framework for incrementally detecting text on road signs from video by integrating two-dimensional image features in each video frame with the three-dimensional geometric structure information of objects extracted from video sequence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Intell. Transp. Syst."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10699750"
                        ],
                        "name": "Xinbo Gao",
                        "slug": "Xinbo-Gao",
                        "structuredName": {
                            "firstName": "Xinbo",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinbo Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7137861"
                        ],
                        "name": "Jianzhuang Liu",
                        "slug": "Jianzhuang-Liu",
                        "structuredName": {
                            "firstName": "Jianzhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "We compare the proposed method with two temporal localization methods [23], [25] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] can detect shot boundaries perfectly (no missing and no errors); 2) all captions crossing shot boundaries are assumed to be detected perfectly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "In Table II, the experimental results about [23] are obtained based on two facts: 1) we assume that the system of Tang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [23], shot boundaries were first detected to decompose a video into a sequence of shots and then temporal segmentation computation was carried out on each shot."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] utilize the temporal feature ( frame difference) to locate the frames containing the same caption, and their approach is more suitably applied in news videos, due to the assumption that the scene changes gradually within a shot."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 287
                            }
                        ],
                        "text": "Since labeling the boundaries is a tedious and time-consuming work, we evaluate the performance only on the longest video, which contains 12 888 frames with 1024*576 resolution, and involves Chinese, puncTABLE II PERFORMANCE COMPARISON OF OUR TEMPORAL LOCALIZATION METHOD, THE METHOD OF [23], AND THE METHOD OF [25]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] exploited the quantized spatial difference density (QSDD) to detect the caption transition frames while [25] was based on comparing the histogram of edge gradient directions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 383,
                                "start": 379
                            }
                        ],
                        "text": "Compared with the existing methods, our contributions are summarized as follows: 1) we describe a framework of extracting video captions, where the conception of temporal localization is used to cluster frames with the same captions together, and this enables us to more effectively extract captions based on multiple frames, instead of many independent frames; 2) compared with [23], our usage of temporal features is multi-facets: not only in localization, but also in segmentation and post-processing, and the related techniques can be used to general video types; 3) a dedicated stroke-like edge detector is proposed to effectively removing the interference of non-stroke edges in complex background to facilitate the detection and localization of captions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15021030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1697bfbb2c701fba6032d63309a904d21b4f0d09",
            "isKey": true,
            "numCitedBy": 133,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier. Using a novel caption-transition detection scheme we locate both spatial and temporal positions of video captions with high precision and efficiency. Then employing several new character segmentation and binarization techniques, we improve the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions. As the first attempt on Chinese video-caption recognition, our experiment results are very encouraging."
            },
            "slug": "A-spatial-temporal-approach-for-video-caption-and-Tang-Gao",
            "title": {
                "fragments": [],
                "text": "A spatial-temporal approach for video caption detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier that improves the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694936"
                        ],
                        "name": "Qixiang Ye",
                        "slug": "Qixiang-Ye",
                        "structuredName": {
                            "firstName": "Qixiang",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixiang Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153706048"
                        ],
                        "name": "W. Gao",
                        "slug": "W.-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725937"
                        ],
                        "name": "Debin Zhao",
                        "slug": "Debin-Zhao",
                        "structuredName": {
                            "firstName": "Debin",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debin Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17956059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcb8cd892adbfded8373716a53787f55da89180a",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-robust-text-detection-in-images-and-video-Ye-Huang",
            "title": {
                "fragments": [],
                "text": "Fast and robust text detection in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152803133"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143663454"
                        ],
                        "name": "Hong Lu",
                        "slug": "Hong-Lu",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689805"
                        ],
                        "name": "Yap-Peng Tan",
                        "slug": "Yap-Peng-Tan",
                        "structuredName": {
                            "firstName": "Yap-Peng",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yap-Peng Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "The edge-based methods [5], [6], [7], [8] locate potential text regions by dense edges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10219325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "300a13b816a303ee0f498d702a4b997cb377d44a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Text superimposed on video frames provides synoptic or supplemental information on video semantics. In this paper, we propose a novel method to detect superimposed text effectively. First, we detect edges by an improved Canny edge detector. Then, a line-feature vector graph is generated based on the edge map and the stroke information is extracted. Finally text regions are generated and filtered according to line features. Experimental results show that, without much increasing the computational cost, our proposed method could suppress the false alarms notably. Furthermore, our method can be easily customized to applications with different tradeoffs in recall and precision."
            },
            "slug": "Effective-video-text-detection-using-line-features-Liu-Lu",
            "title": {
                "fragments": [],
                "text": "Effective video text detection using line features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that, without much increasing the computational cost, the proposed method could suppress the false alarms notably and can be easily customized to applications with different tradeoffs in recall and precision."
            },
            "venue": {
                "fragments": [],
                "text": "ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714626"
                        ],
                        "name": "Sang-Kyun Kim",
                        "slug": "Sang-Kyun-Kim",
                        "structuredName": {
                            "firstName": "Sang-Kyun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Kyun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884493"
                        ],
                        "name": "Young-Su Moon",
                        "slug": "Young-Su-Moon",
                        "structuredName": {
                            "firstName": "Young-Su",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young-Su Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110811374"
                        ],
                        "name": "Ji Yeun Kim",
                        "slug": "Ji-Yeun-Kim",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Kim",
                            "middleNames": [
                                "Yeun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Yeun Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[17], [18] design a stroke filter which is based on the assumption that strokes have certain width and specific spatial distribution, in which the strokes"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9994292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0294b9126760fac88ea270e97701174c23629688",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose stroke filter for text localization in video images. First, we give the definition of text, which is based on the analysis of previous methods and intrinsic characteristics of text. Secondly, the definition is realized in the form of stroke filter, which is elaborately designed based on local region analysis. We also discuss the relationship between the proposed stroke filter and the other related filters. Furthermore, stroke filter can be implemented in a fast way without convolution operation. The effectiveness and efficiency of stroke filter is validated by extensive experiments on a challenging database."
            },
            "slug": "Stroke-Filter-for-Text-Localization-in-Video-Images-Liu-Jung",
            "title": {
                "fragments": [],
                "text": "Stroke Filter for Text Localization in Video Images"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The definition of text is given, which is based on the analysis of previous methods and intrinsic characteristics of text, and the definition is realized in the form of stroke filter, which was elaborately designed based on local region analysis."
            },
            "venue": {
                "fragments": [],
                "text": "2006 International Conference on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "The edge-based methods [5], [6], [7], [8] locate potential text regions by dense edges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13916038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebcf53211f6ef8e64facfa32811f7647a55b3782",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection plays a vital role in retrieving and browsing video data efficiently and accurately. In this paper, we propose a method for detecting both graphics and scene text in video images by proposing initial text block identification, text portion segmentation and new edge features for false positive elimination. The heuristic rules based on filters and edge analysis are formed to identify the initial text block and to segment the complete text portion from the image. The new edge features such as straightness and cursiveness are explored to eliminate false positives. To evaluate the performance of the proposed method, we introduce misdetection rate and processing time in addition to detection rate and false positive rate. The experimental results show that the proposed method outperforms existing methods in terms of the above metrics."
            },
            "slug": "Video-text-detection-based-on-filters-and-edge-Shivakumara-Phan",
            "title": {
                "fragments": [],
                "text": "Video text detection based on filters and edge features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method for detecting both graphics and scene text in video images by proposing initial text block identification, text portion segmentation and new edge features for false positive elimination is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "The edge-based methods [5], [6], [7], [8] locate potential text regions by dense edges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12862847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f64a1d2e366eb476be69cc431f053dcaa22935a",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection is fundamental to video information retrieval and indexing. Existing methods cannot handle well those texts with different contrast or embedded in a complex background. To handle these difficulties, this paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution. First, it applies edge detection and uses a low threshold to filter out definitely non-text edges. Then, a local threshold is selected to both keep low-contrast text and simplify complex background of high-contrast text. Next, two text-area enhancement operators are proposed to highlight those areas with either high edge strength or high edge density. Finally, coarse-to-fine detection locates text regions efficiently. Experimental results show that this approach is robust for contrast, font-size, font-color, language, and background complexity."
            },
            "slug": "A-new-approach-for-video-text-detection-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A new approach for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution, and it applies edge detection and uses a low threshold to filter out definitely non-text edges."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. International Conference on Image Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2198635"
                        ],
                        "name": "Youngsu Moon",
                        "slug": "Youngsu-Moon",
                        "structuredName": {
                            "firstName": "Youngsu",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngsu Moon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17], [18] design a stroke filter which is based on the assumption that strokes have certain width and specific spatial distribution, in which the strokes"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18522972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e9a4a9ebdff7fdbf8119241bd62d144a426f31",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Most existing methods of text segmentation in video images are not robust because they do not consider the intrinsic characteristics of text. In this paper, we propose a novel method of text segmentation based on stroke filter (SF). First, we give the definition of text, which is realized in the form of stroke filter based on local region analysis. Based on stroke filter response, text polarity determination and local region growing modules are performed successively. The effectiveness of our method is validated by experiments on a challenging database."
            },
            "slug": "Text-segmentation-based-on-stroke-filter-Liu-Jung",
            "title": {
                "fragments": [],
                "text": "Text segmentation based on stroke filter"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper gives the definition of text, which is realized in the form of stroke filter based on local region analysis, and proposes a novel method of text segmentation based on stroke filter (SF)."
            },
            "venue": {
                "fragments": [],
                "text": "MM '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 309,
                                "start": 306
                            }
                        ],
                        "text": "Since captions in movies or TV programs can supply rich semantic information which is very useful in video content analysis, indexing, and retrieval, research efforts have been made toward extracting video captions in various applications, such as traffic monitoring [1], [2], video indexing and retrieval [3], and computerized aid for visually impaired [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "After localizing a text region, text segmentation aims to segment out text pixels, in order to produce a binary image subsequently used for character recognition [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] deal with video frames independently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Corner-based methods [9] identify candidate text regions by seeking densely distributed corners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13999848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24591ec88e706697bffa18f36728f192e0d797b6",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new automatic text location approach for videos is proposed. First of all, the corner points of the selected video frames are detected. After deleting some isolate corners, we merge the remaining corners to form candidate text regions. The regions are then decomposed vertically and horizontally using edge maps of the video frames to get candidate text lines. Finally, a text box verification step based on the feature derived from edge maps is taken to significantly reduce false alarms. Experimental results show that the new text location scheme proposed in this paper is accurate."
            },
            "slug": "Automatic-location-of-text-in-video-frames-Hua-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic location of text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the new text location scheme proposed in this paper is accurate and can be used to significantly reduce false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107824420"
                        ],
                        "name": "C. Garcia",
                        "slug": "C.-Garcia",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346037"
                        ],
                        "name": "X. Apostolidis",
                        "slug": "X.-Apostolidis",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Apostolidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Apostolidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46620452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57930a675de539c59bc33f56d9894c999d264f72",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints related to the texture of words. Then, classification and binarization of potential text areas are achieved in a single scheme performing color quantization and characters periodicity analysis. We report a high rate of good detection results with very few false alarms and reliable text binarization."
            },
            "slug": "Text-detection-and-segmentation-in-complex-color-Garcia-Apostolidis",
            "title": {
                "fragments": [],
                "text": "Text detection and segmentation in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background and to binarize efficiently the detected text areas so that they can be processed by standard OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061097"
                        ],
                        "name": "N. Ezaki",
                        "slug": "N.-Ezaki",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Ezaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ezaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806816"
                        ],
                        "name": "M. Bulacu",
                        "slug": "M.-Bulacu",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Bulacu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bulacu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799278"
                        ],
                        "name": "Lambert Schomaker",
                        "slug": "Lambert-Schomaker",
                        "structuredName": {
                            "firstName": "Lambert",
                            "lastName": "Schomaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lambert Schomaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 357,
                                "start": 354
                            }
                        ],
                        "text": "Since captions in movies or TV programs can supply rich semantic information which is very useful in video content analysis, indexing, and retrieval, research efforts have been made toward extracting video captions in various applications, such as traffic monitoring [1], [2], video indexing and retrieval [3], and computerized aid for visually impaired [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2561294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95e6599c7ac506446c4feefbf5a22841d24b08b0",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons. This paper describes the system design and evaluates several character extraction methods. Automatic text recognition from natural images receives a growing attention because of potential applications in image retrieval, robotics and intelligent transport system. Camera-based document analysis becomes a real possibility with the increasing resolution and availability of digital cameras. However, in the case of a blind person, finding the text region is the first important problem that must be addressed, because it cannot be assumed that the acquired image contains only characters. At first, our system tries to find in the image areas with small characters. Then it zooms into the found areas to retake higher resolution images necessary for character recognition. In the present paper, we propose four character-extraction methods based on connected components. We tested the effectiveness of our methods on the ICDAR 2003 Robust Reading Competition data. The performance of the different methods depends on character size. In the data, bigger characters are more prevalent and the most effective extraction method proves to be the sequence: Sobel edge detection, Otsu binarization, connected component extraction and rule-based connected component filtering."
            },
            "slug": "Text-detection-from-natural-scene-images:-towards-a-Ezaki-Bulacu",
            "title": {
                "fragments": [],
                "text": "Text detection from natural scene images: towards a system for visually impaired persons"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons and evaluates several character extraction methods based on connected components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34713082"
                        ],
                        "name": "P. Dubey",
                        "slug": "P.-Dubey",
                        "structuredName": {
                            "firstName": "Premnath",
                            "lastName": "Dubey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dubey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The edge-based methods [5], [6], [7], [8] locate potential text regions by dense edges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14062660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2ba6a11b3b257136c63443f41c39e4d7d833cd7",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection plays a crucial role in various applications. In this paper we present an edge based text detection technique in the complex images for multi purpose application. The technique applied vertical Sobel edge detection and a newly proposed morphological technique that used to connect the edges to form the candidate regions. The technique has special advantage, by providing a distinguishable texture on the text area over the others. The connected components are then extracted using a purposed segmentation algorithm. Later all the candidate regions are verified to specify the text region. The propose techniques has been tested with different types of image acquired from different input sources and environment. The experimental result shows highly successful rate"
            },
            "slug": "Edge-Based-Text-Detection-for-Multi-purpose-Dubey",
            "title": {
                "fragments": [],
                "text": "Edge Based Text Detection for Multi-purpose Application"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An edge based text detection technique in the complex images for multi purpose application using vertical Sobel edge detection and a newly proposed morphological technique that used to connect the edges to form the candidate regions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2006 8th international Conference on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31459400"
                        ],
                        "name": "W. Mao",
                        "slug": "W.-Mao",
                        "structuredName": {
                            "firstName": "Weng",
                            "lastName": "Mao",
                            "middleNames": [
                                "Zu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145288211"
                        ],
                        "name": "K. Chung",
                        "slug": "K.-Chung",
                        "structuredName": {
                            "firstName": "Korris",
                            "lastName": "Chung",
                            "middleNames": [
                                "Fu-Lai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595159"
                        ],
                        "name": "Kenneth K. M. Lam",
                        "slug": "Kenneth-K.-M.-Lam",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lam",
                            "middleNames": [
                                "K.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth K. M. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144250750"
                        ],
                        "name": "W. Siu",
                        "slug": "W.-Siu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Siu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Siu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5893059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c2098f5478256b50f8e1917ae3b60d5b69bb969",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames. Local energy analysis has been shown to work well in text detection, where remarkable local energy variations of pixels correspond to text region or boundary of other objects and lower local energy variations of pixels correspond to background or the interior of non-text objects. Local energy variation is calculated in a local region based on the wavelet transform coefficients of images. Hybrid Chinese/English text in images and video frames can be detected whether it is aligned horizontally or vertically. The font size of text to be detected may vary in a wide range of values. The proposed method has been tested on 321 frame images obtained from local TV programs and a tested dataset with low missed rate and false alarm rate."
            },
            "slug": "Hybrid-Chinese/English-text-detection-in-images-and-Mao-Chung",
            "title": {
                "fragments": [],
                "text": "Hybrid Chinese/English text detection in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames and a tested dataset with low missed rate and false alarm rate is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115290599"
                        ],
                        "name": "Se Hyun Park",
                        "slug": "Se-Hyun-Park",
                        "structuredName": {
                            "firstName": "Se",
                            "lastName": "Park",
                            "middleNames": [
                                "Hyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se Hyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28448043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "198a090d5a34dbbbc3fc7b3d74b26e0938665606",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual data within video frames are very useful for describing the contents of the video frames, as they enable both keyword and free-text-based searching. In this paper, we pose the problem of text location in digital video as an example of supervised texture classification and use a support vector machine (SVM) as the texture classifier. Unlike other text detection methods, we do not incorporate any explicit texture feature extraction scheme. Instead, the gray-level values of the raw pixels are directly fed to the classifier. This is based on the observation that a SVM has the capability of learning in a high-dimensional space and of incorporating a feature extraction scheme in its own architecture. In comparison with a neural network-based text detection method, the SVM classifier illustrates the excellence of the proposed method."
            },
            "slug": "Support-vector-machine-based-text-detection-in-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Support vector machine-based text detection in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper poses the problem of text location in digital video as an example of supervised texture classification and uses a support vector machine (SVM) as the texture classifier, which illustrates the excellence of the proposed method."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] propose a stroke width transform (SWT) to decide whether the adjacent edges have stroke-like width."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738703"
                        ],
                        "name": "R. Ewerth",
                        "slug": "R.-Ewerth",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Ewerth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ewerth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920919"
                        ],
                        "name": "Teuta Stefi",
                        "slug": "Teuta-Stefi",
                        "structuredName": {
                            "firstName": "Teuta",
                            "lastName": "Stefi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Teuta Stefi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] utilize a slightly modified K-means algorithm to extract text pixels from background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5770812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e305d04ac524113b6b9a4567914c8e1fc4d9c5e",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the number of digital multimedia libraries is growing rapidly, the need to efficiently index, browse and retrieve this information is also increased. In this context, text appearing in images represents an important entity for indexing and retrieval purposes. Often, text is superimposed over complex image background and its recognition by a commercial optical character recognition (OCR) engine is difficult. Thus, there is the need for a text segmentation process, including background removal and binarization, in order to achieve a satisfactory recognition rate by OCR. In this paper, an unsupervised learning method for text segmentation in images with complex backgrounds is presented. First, the color of the text and background is determined based on a color quantizer. Then, the pixel color and the standard deviation of the wavelet transformed image are used to distinguish between text and non-text pixels. To classify pixels into text and background, a slightly modified k-means algorithm is applied which is used to produce a binarized text image. The segmentation result is fed into a commercial OCR software to investigate the segmentation quality. The performance of our approach is demonstrated by presenting experimental results for a set of video frames."
            },
            "slug": "Unsupervised-Text-Segmentation-Using-Color-and-Gllavata-Ewerth",
            "title": {
                "fragments": [],
                "text": "Unsupervised Text Segmentation Using Color and Wavelet Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An unsupervised learning method for text segmentation in images with complex backgrounds is presented and a slightly modified k-means algorithm is applied which is used to produce a binarized text image."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2849465"
                        ],
                        "name": "I. Ar",
                        "slug": "I.-Ar",
                        "structuredName": {
                            "firstName": "Ilktan",
                            "lastName": "Ar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145665200"
                        ],
                        "name": "M. E. Karsligil",
                        "slug": "M.-E.-Karsligil",
                        "structuredName": {
                            "firstName": "Mine",
                            "lastName": "Karsligil",
                            "middleNames": [
                                "Elif"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Karsligil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36524034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ac30ff0567f85cee6f490b26bb4e013a09549f",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a new texture-based method for extraction of text areas in a complex document image. Gabor filter, motivated by the multi-channel filtering approach of Human Visual System (HVS), has been employed to create energy map of the document. In this energy map we assumed that text areas were rich in high frequency components. Connected components (probable text characters) were extracted by binarization of the energy map with Otsu's adaptive threshold method. First non-text components such as pictures, lines, frames etc. were eliminated by Gabor filtering. As a novel approach, remaining non-text components were then eliminated by using character component interval tracing. Elimination that formed in two stage, enhanced the success of detecting text area on different kinds of digital documents."
            },
            "slug": "Text-Area-Detection-in-Digital-Documents-Images-Ar-Karsligil",
            "title": {
                "fragments": [],
                "text": "Text Area Detection in Digital Documents Images Using Textural Features"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new texture-based method for extraction of text areas in a complex document image by eliminating non-text components by using character component interval tracing, which enhanced the success of detecting text area on different kinds of digital documents."
            },
            "venue": {
                "fragments": [],
                "text": "CAIP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2530942"
                        ],
                        "name": "H. Hase",
                        "slug": "H.-Hase",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Hase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188516"
                        ],
                        "name": "T. Shinokawa",
                        "slug": "T.-Shinokawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Shinokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71114145"
                        ],
                        "name": "M. Yoneda",
                        "slug": "M.-Yoneda",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoneda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[21] segment the text regions with similar color based on CCA technique, and Gllavata et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40333068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b24ea59f58374750894ad050dbafe88c81ed54f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Character-string-extraction-from-color-documents-Hase-Shinokawa",
            "title": {
                "fragments": [],
                "text": "Character string extraction from color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757655"
                        ],
                        "name": "S. Bhattacharjee",
                        "slug": "S.-Bhattacharjee",
                        "structuredName": {
                            "firstName": "Sushil",
                            "lastName": "Bhattacharjee",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhattacharjee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13639250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "808f205f834cfdfa823b97fbf1f20bf82ddaa8d7",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a considerable interest in designing automatic systems that will scan a given paper document and store it on electronic media for easier storage, manipulation, and access. Most documents contain graphics and images in addition to text. Thus, the document image has to be segmented to identify the text regions, so that OCR techniques may be applied only to those regions. In this paper, we present a simple method for document image segmentation in which text regions in a given document image are automatically identified. The proposed segmentation method for document images is based on a multichannel filtering approach to texture segmentation. The text in the document is considered as a textured region. Nontext contents in the document, such as blank spaces, graphics, and pictures, are considered as regions with different textures. Thus, the problem of segmenting document images into text and nontext regions can be posed as a texture segmentation problem. Two-dimensional Gabor filters are used to extract texture features for each of these regions. These filters have been extensively used earlier for a variety of texture segmentation tasks. Here we apply the same filters to the document image segmentation problem. Our segmentation method does not assume any a priori knowledge about the content or font styles of the document, and is shown to work even for skewed images and handwritten text. Results of the proposed segmentation method are presented for several test images which demonstrate the robustness of this technique."
            },
            "slug": "Text-segmentation-using-gabor-filters-for-automatic-Jain-Bhattacharjee",
            "title": {
                "fragments": [],
                "text": "Text segmentation using gabor filters for automatic document processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a simple method for document image segmentation in which text regions in a given document image are automatically identified and is shown to work even for skewed images and handwritten text."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "We adopt themethod presented by [24] to detect corner pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 778478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab46391005cea85fa5c204b6e77a9c870fdbaed",
            "isKey": false,
            "numCitedBy": 8402,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments.<<ETX>>"
            },
            "slug": "Good-features-to-track-Shi-Tomasi",
            "title": {
                "fragments": [],
                "text": "Good features to track"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115290599"
                        ],
                        "name": "Se Hyun Park",
                        "slug": "Se-Hyun-Park",
                        "structuredName": {
                            "firstName": "Se",
                            "lastName": "Park",
                            "middleNames": [
                                "Hyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se Hyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "Since captions in movies or TV programs can supply rich semantic information which is very useful in video content analysis, indexing, and retrieval, research efforts have been made toward extracting video captions in various applications, such as traffic monitoring [1], [2], video indexing and retrieval [3], and computerized aid for visually impaired [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 213
                            }
                        ],
                        "text": "Texture-based methods [10], [11], [12] utilize various texture descriptors, such as Gabor filter [13] and wavelet transform [14], [15], and then the machine learning techniques such as SVM [16] or neural networks [1] are applied to classify the text and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123666990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1b5738d6d86f53cae7cc12c00db68ea81de0b1e",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a method for locating car license plates using neural networks. Neural networks are used as filters for analysing small windows of an image and deciding whether each window contains a license plate. A post-processor combines these filtered images and gives the final location of the license plates. The method offers robustness when dealing with noisy images. Tests with car images travelling on the road and at the entrance of a car park showed extraction rates of 99 and 97.5%, respectively. These results suggest that the proposed method works well with real-world situations."
            },
            "slug": "Locating-car-license-plates-using-neural-networks-Park-Kim",
            "title": {
                "fragments": [],
                "text": "Locating car license plates using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests with car images travelling on the road and at the entrance of a car park showed extraction rates of 99 and 97.5%, respectively, suggesting that the proposed method works well with real-world situations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 17
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Robustly-Extracting-Captions-in-Videos-Based-on-and-Liu-Wang/1f84b963cb0f12b1045c817235d3467edc88730d?sort=total-citations"
}