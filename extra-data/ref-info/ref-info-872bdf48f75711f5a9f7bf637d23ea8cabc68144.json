{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409052480"
                        ],
                        "name": "S. C. Hinds",
                        "slug": "S.-C.-Hinds",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Hinds",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Hinds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168553590"
                        ],
                        "name": "James L. Fisher",
                        "slug": "James-L.-Fisher",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402307078"
                        ],
                        "name": "D. D'Amato",
                        "slug": "D.-D'Amato",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "D'Amato",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D'Amato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61919721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b27f45bebdec8e8d8503f7314533674c51de0e84",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "As part of the development of a document image analysis system, a method, based on the Hough transform, was devised for the detection of document skew and interline spacing-necessary parameters for the automatic segmentation of text from graphics. Because the Hough transform is computationally expensive, the amount of data within a document image is reduced through the computation of its horizontal and vertical black runlengths. Histograms of these runlengths are used to determine whether the document is in portrait or landscape orientation. A gray scale burst image is created from the black runlengths that are perpendicular to the text lines by placing the length of the run in the run's bottom-most pixel. By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images. Because only small runlengths are input to the Hough transform and because the accumulator array is incremented by the runlength associated with a pixel rather than by a factor of 1, the negative effects of noise, black margins, and figures are avoided. Consequently, interline spacing can be determined more accurately.<<ETX>>"
            },
            "slug": "A-document-skew-detection-method-using-run-length-Hinds-Fisher",
            "title": {
                "fragments": [],
                "text": "A document skew detection method using run-length encoding and the Hough transform"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images and interline spacing can be determined more accurately."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69485492"
                        ],
                        "name": "D. R. Ferguson",
                        "slug": "D.-R.-Ferguson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ferguson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Ferguson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "Some variations, however, which require reduced amount of computation have at times been introduced [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29472483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45bdf2e5368886fd77f7542d797f58909ce3a169",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic reading of optically scanned forms consists of two major components: extraction of the data image from the form and interpretation of the image as coded alphanumerics. The second component is also known as optical character recognition, or OCR. We have implemented a method for entry of a wide variety of forms that contain machine-printed data and that are often produced in business environments. The function, called Intelligent Forms Processing (IFP), accepts conventional forms that call for information to be printed in designated blank areas, but in which the information may exceed boundaries due to poor registration during printing. The human eye easily accommodates data that impinge on form boundaries or on background text; however, the same powers of discrimination applied to machine processing pose a technical challenge. The IFP system uses a setup phase to create a model of each form that is to be read. Scanned forms containing data are compared against the matching form model. Special algorithms are employed to extract data fields while removing background printing (e.g., form lines) intersecting the data. The extracted data images are interpreted by an OCR process that reads typical monospace fonts. New fonts may be added easily in a separate design mode. If the data are alphabetic, a lexicon may be assembled to define the possible entries."
            },
            "slug": "Intelligent-Forms-Processing-Casey-Ferguson",
            "title": {
                "fragments": [],
                "text": "Intelligent Forms Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The automatic reading of optically scanned forms consists of two major components: extraction of the data image from the form and interpretation of the image as coded alphanumerics, also known as optical character recognition, or OCR."
            },
            "venue": {
                "fragments": [],
                "text": "IBM Syst. J."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[10,11]) to obtain the power spectral density of the horizontal projection: The algorithm as it applies in this case can be summarized in the following steps:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120068581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6839013b5dd6bcc30d6aeef615c5f034103af090",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-vectorizer-and-feature-extractor-for-document-Pavlidis",
            "title": {
                "fragments": [],
                "text": "A vectorizer and feature extractor for document recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "In the case where the angle is large we use the connected-components [9] algorithm for an initial determination of the rotation angle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "A variety of skew detection methods have been proposed in the literature; they include Projection Pro\"le [8], Hough Transform [4], Nearest}Neighbor Clustering [3], Bounding Box Detection [6] or A combination of the above methods [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22995244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d85097da36118fbccfeb7802abf89bf4b4c63a3e",
            "isKey": false,
            "numCitedBy": 728,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independence from skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods. >"
            },
            "slug": "The-Document-Spectrum-for-Page-Layout-Analysis-O'Gorman",
            "title": {
                "fragments": [],
                "text": "The Document Spectrum for Page Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components, yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977841"
                        ],
                        "name": "M. Wesley",
                        "slug": "M.-Wesley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wesley",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wesley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896180"
                        ],
                        "name": "G. Markowsky",
                        "slug": "G.-Markowsky",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Markowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Markowsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[10,11]) to obtain the power spectral density of the horizontal projection: The algorithm as it applies in this case can be summarized in the following steps:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123092243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d3f41e387f3931173568ec0beddaf09e6fa81ad",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, the authors presented an algorithm for finding all polyhedral solid objects with a given set of vertices and straight line edges (its wire frame). This paper extends the Wire Frame algorithm to find all solid polyhedral objects with a given set of two dimensional projections. These projections may contain depth information in the form of dashed and solid lines, may represent cross sections, and may be overall or detail views. The choice of labeling conventions in the projections determines the difficulty of the problem. It is shown that with certain conventions and projections the problem of fleshing out projections essentially reduces to the problem of fleshing out wire frames. Even if no labeling is used, the Projections algorithm presented here finds all solutions even though it is possible to construct simple examples with a very large number of solutions. Such examples have a large amount of symmetry and various accidental coincidences which typically do not occur in objects of practical interest. Because of its generality, the algorithm can handle pathological cases if they arise. This Projections algorithm, which has applications in the conversion of engineering drawings in a Computer Aided Design, Computer Aided Manufacturing (CADCAM) system, has been implemented. The algorithm has successfully found solutions to problems that are rather complex in terms of either the number of possible solutions or the inherent complexity of projections of objects of engineering interest."
            },
            "slug": "Fleshing-out-projections-Wesley-Markowsky",
            "title": {
                "fragments": [],
                "text": "Fleshing out projections"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Projections algorithm, which has applications in the conversion of engineering drawings in a Computer Aided Design, computer Aided Manufacturing (CADCAM) system, has been implemented and has successfully found solutions to problems that are rather complex."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830195"
                        ],
                        "name": "R. Bleisinger",
                        "slug": "R.-Bleisinger",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Bleisinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bleisinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36422303"
                        ],
                        "name": "R. Hoch",
                        "slug": "R.-Hoch",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Hoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20501775"
                        ],
                        "name": "Frank Fein",
                        "slug": "Frank-Fein",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Fein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083596961"
                        ],
                        "name": "Frank H\u00f6nes",
                        "slug": "Frank-H\u00f6nes",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00f6nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank H\u00f6nes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1584642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a263aa246f730ea1c28817eafd0d8e5f899e61c",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of the model-based document analysis system called Pi ODA (paper interface to office document architecture), which was developed as a prototype for the analysis of single-sided business letters in German, are presented. Initially, Pi ODA extracts a part-of hierarchy of nested layout objects such as text-blocks, lines, and words based on their presentation on the page. Subsequently, in a step called logical labeling, the layout objects and their compositions are geometrically analyzed to identify corresponding logical objects that can be related to a human perceptible meaning, such as sender, recipient, and date in a letter. A context-sensitive text recognition for logical objects is then applied using logical vocabularies and syntactic knowledge. As a result, Pi ODA produces a document representation that conforms to the ODA international standard.<<ETX>>"
            },
            "slug": "From-paper-to-office-document-standard-Dengel-Bleisinger",
            "title": {
                "fragments": [],
                "text": "From paper to office document standard representation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The principles of the model-based document analysis system called Pi ODA (paper interface to office document architecture), which was developed as a prototype for the analysis of single-sided business letters in German, are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12874899"
                        ],
                        "name": "J. Sammon",
                        "slug": "J.-Sammon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sammon",
                            "middleNames": [
                                "W."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sammon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43151050,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "154f8a9906bcc99fca9b17aa521649b1c3734093",
            "isKey": false,
            "numCitedBy": 3461,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results. The algorithm is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "slug": "A-Nonlinear-Mapping-for-Data-Structure-Analysis-Sammon",
            "title": {
                "fragments": [],
                "text": "A Nonlinear Mapping for Data Structure Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results that is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153312185"
                        ],
                        "name": "S. Mori",
                        "slug": "S.-Mori",
                        "structuredName": {
                            "firstName": "Shunji",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686714"
                        ],
                        "name": "Kazuhiko Yamamoto",
                        "slug": "Kazuhiko-Yamamoto",
                        "structuredName": {
                            "firstName": "Kazuhiko",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuhiko Yamamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Methods and tools have been developed in the past [2] that to a satisfactory degree solve the problem of printed character recognition and to a less than satisfactory degree the problem of handwritten character recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58021636,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0627cb9872115ea4c4d3484538a2f440923d8f13",
            "isKey": false,
            "numCitedBy": 940,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Research and development of OCR systems are considered from a historical point of view. The historical development of commercial systems is included. Both template matching and structure analysis approaches to R&D are considered. It is noted that the two approaches are coming closer and tending to merge. Commercial products are divided into three generations, for each of which some representative OCR systems are chosen and described in some detail. Some comments are made on recent techniques applied to OCR, such as expert systems and neural networks, and some open problems are indicated. The authors' views and hopes regarding future trends are presented. >"
            },
            "slug": "Historical-review-of-OCR-research-and-development-Mori-Suen",
            "title": {
                "fragments": [],
                "text": "Historical review of OCR research and development"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Both template matching and structure analysis approaches to R&D are considered and it is noted that the two approaches are coming closer and tending to merge."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Learning Vector Quantization (LVQ) was used to train the system on all the KL-transformed PSDs in the training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Using Learning Vector Quantization [4] (LVQ) a total of 375 distributions were obtained, the centroids of which form the actual code-book."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "A variety of skew detection methods have been proposed in the literature; they include Projection Pro\"le [8], Hough Transform [4], Nearest}Neighbor Clustering [3], Bounding Box Detection [6] or A combination of the above methods [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "Several generic algorithms have been presented in the past that can be used to detect skew [3,4] and shift [5}7], but all of them are either expensive, as far as the computation time is concerned, or they do not work for every type of form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "D'Amato, A document skew detection method using run-length encoding and the hough transform"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 10th International Conference Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "A variety of skew detection methods have been proposed in the literature; they include Projection Pro\"le [8], Hough Transform [4], Nearest}Neighbor Clustering [3], Bounding Box Detection [6] or A combination of the above methods [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "Several generic algorithms have been presented in the past that can be used to detect skew [3,4] and shift [5}7], but all of them are either expensive, as far as the computation time is concerned, or they do not work for every type of form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60023835,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "ee1decd6020b5cf63d1d6a4ea7cfde1ab5309279",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-skew-angle-of-printed-documents-Baird",
            "title": {
                "fragments": [],
                "text": "The skew angle of printed documents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intelli - gent forms processing system , Mach"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Appl ."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 261
                            }
                        ],
                        "text": "The most plausible methods proposed in the literature are summarized next: Use of special symbols or structures in the form: These symbols are initially located on the new form and then compared to a prototype to determine the degree of rotation or translation [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Detection of vertical and horizontal lines from which the skew and shift can be determined [7]: The projection pro\"le is used to determine the location of the lines that correspond to the highest peaks in the pro\"les."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NIST handprinted forms and characters database, National Institute of Standards and Technology, Advanced Systems Division, Visual Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 171
                            }
                        ],
                        "text": "Even the companies that are in the process of becoming fully o$ce-automated have a need for a system that will convert the old documents into a suitable electronic format [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hones, From paper to o$ce document standard representation, Comput"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-generalization-of-the-form-identification-Liolios-Fakotakis/872bdf48f75711f5a9f7bf637d23ea8cabc68144?sort=total-citations"
}