{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Jelinek et al. (1992) provide a tutorial introduction covering the standard algorithms for the four tasks mentioned in the introduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62304080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "617241818e8ddd6edcb4ee7682992673c18c6f3d",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In automatic speech recognition, language models can be represented by Probabilistic Context Free Grammars (PCFGs). In this lecture we review some known algorithms which handle PCFGs; in particular an algorithm for the computation of the total probability that a PCFG generates a given sentence (Inside), an algorithm for finding the most probable parse tree (Viterbi), and an algorithm for the estimation of the probabilities of the rewriting rules of a PCFG given a corpus (Inside-Outside). Moreover, we introduce the Left-to-Right Inside algorithm, which computes the probability that successive applications of the grammar rewriting rules (beginning with the sentence start symbol s) produce a word string whose initial substring is a given one."
            },
            "slug": "Basic-Methods-of-Probabilistic-Context-Free-Jelinek-Lafferty",
            "title": {
                "fragments": [],
                "text": "Basic Methods of Probabilistic Context Free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Left-to-Right Inside algorithm is introduced, which computes the probability that successive applications of the grammar rewriting rules produce a word string whose initial substring is a given one."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2220955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ad8e98574a275930bf04a477ce3532fd13c503c",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence."
            },
            "slug": "Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The construction of a very wide-coverage probabilistic parsing system for natural language (NL) based on LR parsing techniques, intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62632209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "befbe8bd0a8aa0fc0a5a92a9679f87e81b8ba40b",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel algorithm for estimating the parameters of a hidden stochastic context-free grammar is presented. In contrast to the inside/outside (I/O) algorithm it does not require the grammar to be expressed in Chomsky normal form, and thus can operate directly on more natural representations of a grammar. The algorithm uses a trellis-based structure as opposed to the binary branching tree structure used by the I/O algorithm. The form of the trellis is an extension of that used by the forward/backward (F/B) algorithm, and as a result the algorithm reduces to the latter for components that can be modeled as finite-state networks. In the same way that a hidden Markov model (HMM) is a stochastic analog of a finite-state network, the representation used by the algorithm is a stochastic analog of a recursive transition network, in which a state may be simple or itself contain an underlying structure.<<ETX>>"
            },
            "slug": "Hidden-Markov-estimation-for-unrestricted-grammars-Kupiec",
            "title": {
                "fragments": [],
                "text": "Hidden Markov estimation for unrestricted stochastic context-free grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel algorithm for estimating the parameters of a hidden stochastic context-free grammar that does not require the grammar to be expressed in Chomsky normal form, and thus can operate directly on more natural representations of a grammar."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13328586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b23944bc96f26c1bec20312aea07b0acd3eb41f5",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition language models are based on probabilities P(Wk+1 = v | w1w2,...,wk) that the next word Wk+1 will be any particular word v of the vocabulary, given that the word sequence w1,w2,...,wk is hypothesized to have been uttered in the past. If probabilistic context-free grammars are to be used as the basis of the language model, it will be necessary to compute the probability that successive application of the grammar rewrite rules (beginning with the sentence start symbol s) produces a word string whose initial substring is an arbitrary sequence w1,w2,...,wk+1. In this paper we describe a new algorithm that achieves the required computation in at most a constant times k3-steps."
            },
            "slug": "Computation-of-the-Probability-of-Initial-Substring-Jelinek-Lafferty",
            "title": {
                "fragments": [],
                "text": "Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A new algorithm is described that achieves the required computation in at most a constant times k3-steps of the grammar rewrite rules for probabilistic context-free grammars."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144954740"
                        ],
                        "name": "J. Earley",
                        "slug": "J.-Earley",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Earley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Earley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b",
            "isKey": false,
            "numCitedBy": 1339,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(<italic>k</italic>) algorithm and the familiar top-down algorithm. It has a time bound proportional to <italic>n</italic><supscrpt>3</supscrpt> (where <italic>n</italic> is the length of the string being parsed) in general; it has an <italic>n</italic><supscrpt>2</supscrpt> bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "slug": "An-efficient-context-free-parsing-algorithm-Earley",
            "title": {
                "fragments": [],
                "text": "An efficient context-free parsing algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described and appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2376935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da838db79e7593018894ada44db35eee670941d6",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar."
            },
            "slug": "Pearl:-A-Probabilistic-Chart-Parser-Magerman-Marcus",
            "title": {
                "fragments": [],
                "text": "Pearl: A Probabilistic Chart Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence and provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069154640"
                        ],
                        "name": "Jonathan Segal",
                        "slug": "Jonathan-Segal",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Segal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5713988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "823233093025cdc6aec13dbc17d9e4116f686eba",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice."
            },
            "slug": "Precise-N-Gram-Probabilities-From-Stochastic-Stolcke-Segal",
            "title": {
                "fragments": [],
                "text": "Precise N-Gram Probabilities From Stochastic Context-Free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An algorithm for computing n-gram probabilities from stochastic context-free grammars, which operates via the computation of substring expectations, which is accomplished by solving systems of linear equations derived from the grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643813272"
                        ],
                        "name": "StolckeAndreas",
                        "slug": "StolckeAndreas",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "StolckeAndreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "StolckeAndreas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215863926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36a0308e21157b58a66f4485c55c733bfedec75f",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities..."
            },
            "slug": "An-efficient-probabilistic-context-free-parsing-StolckeAndreas",
            "title": {
                "fragments": [],
                "text": "An efficient probabilistic context-free parsing algorithm that computes prefix probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastically context- free grammar and an input string: a) probabilities..."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111035923"
                        ],
                        "name": "M. Jones",
                        "slug": "M.-Jones",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Jones",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 249
                            }
                        ],
                        "text": "In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa, 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus, 1991; Magerman and Weir, 1992; Jones and Eisner, 1992; Briscoe and Carroll, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 192
                            }
                        ],
                        "text": "\u2026over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al., 1991); to guide the rule choice efficiently during parsing (Jones and Eisner, 1992); to compute island probabilities for non-linear parsing (Corazza et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9754538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f6e1e510be6daea31e9f9c56fb7db714d375725",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general approach to the probabilistic parsing of context-free grammars. The method integrates context-sensitive statistical knowledge of various types (e.g., syntactic and semantic) and can be trained incrementally from a bracketed corpus. We introduce a variant of the GHR contextfree recognition algorithm, and explain how to adapt it for efficient probahilistic parsing. In splitcorpus testing on a real-world corpus of sentences from software testing documents, with 20 possible parses for a sentence of average length, the system finds and identifies the correct parse in 96% of the sentences for which it finds any parse, while producing only 1.03 parses per sentence for those sentences. Significantly, this success rate would be only 79% without the semantic statistics."
            },
            "slug": "A-Probabilistic-Parser-and-Its-Application-Jones-Eisner",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Parser and Its Application"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A variant of the GHR contextfree recognition algorithm is introduced, and how to adapt it for efficient probahilistic parsing of context-free grammars is explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49497622"
                        ],
                        "name": "K. Lari",
                        "slug": "K.-Lari",
                        "structuredName": {
                            "firstName": "Kaveh",
                            "lastName": "Lari",
                            "middleNames": [
                                "Sookhak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa, 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus, 1991; Magerman and Weir, 1992; Jones and Eisner, 1992; Briscoe and Carroll, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "Magerman and Marcus (1991) are interested primarily in scoring functions to guide a parser efficiently to the most promising parses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53736294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-stochastic-context-free-grammars-Lari-Young",
            "title": {
                "fragments": [],
                "text": "Applications of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705919"
                        ],
                        "name": "Chuck Wooters",
                        "slug": "Chuck-Wooters",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Wooters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuck Wooters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069154640"
                        ],
                        "name": "Jonathan Segal",
                        "slug": "Jonathan-Segal",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918411"
                        ],
                        "name": "G. Tajchman",
                        "slug": "G.-Tajchman",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Tajchman",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tajchman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 183008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2d00cf1be1f129c88d0471263b90a3f3f06e942",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a number of experiments in adding new grammatical knowledge to the Berkeley Restaurant Project (BeRP), our medium-vocabulary (1300 word), speaker-independent, spontaneous continuous-speech understanding system. We describe an algorithm for using a probabilistic Earley parser and a stochastic context-free grammar (SCFG) to generate word transition probabilities at each frame for a Viterbi decoder. We show that using an SCFG as a language model improves the word error rate from 34.6% (bigram) to 29.6% (SCFG), and the semantic sentence recognition error from from 39.0% (bigram) to 34.1% (SCFG). In addition, we get a further reduction to 28.8% word error by mixing the bigram and SCFG LMs. We also report on our preliminary results from using discourse-context information in the LM."
            },
            "slug": "Using-a-stochastic-context-free-grammar-as-a-model-Jurafsky-Wooters",
            "title": {
                "fragments": [],
                "text": "Using a stochastic context-free grammar as a language model for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm for using a probabilistic Earley parser and a stochastic context-free grammar (SCFG) to generate word transition probabilities at each frame for a Viterbi decoder and it is shown that using an SCFG as a language model improves the word error rate."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 151
                            }
                        ],
                        "text": "In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney, 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari and Young, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118252779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c531497e9467788127ad891719127e989a8ff03",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unifying framework of syntactic and statistical pattern recognition for one-dimensional observations and signals like speech. The syntactic constraints will be based upon stochastic extensions of the grammars in the Chomsky hierarchy. These extended stochastic grammars can be applied to both discrete and continuous observations. Neglecting the mathematical details and complications, we can convert a grammar of the Chomsky hierarchy to a stochastic grammar by attaching probabilities to the grammar rules and, for continuous observations, attaching probability density functions to the terminals of the grammar. In such a framework, a consistent integration of syntactic pattern recognition and statistical pattern recognition, which is typically based upon Bayes\u2019 decision rule for minimum error rate, can be achieved such that no error correction or postprocessing after the recognition phase is required. Efficient algorithms and closed-form solutions for the parsing and recognition problem will be presented for the following types of stochastic grammars: regular, linear and context-free. It will be shown how these techniques can be applied to the task of continuous speech recognition."
            },
            "slug": "Stochastic-Grammars-and-Pattern-Recognition-Ney",
            "title": {
                "fragments": [],
                "text": "Stochastic Grammars and Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A unifying framework of syntactic and statistical pattern recognition for one-dimensional observations and signals like speech is presented and it will be shown how these techniques can be applied to the task of continuous speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696761"
                        ],
                        "name": "F. Casacuberta",
                        "slug": "F.-Casacuberta",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Casacuberta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Casacuberta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143783065"
                        ],
                        "name": "E. Vidal",
                        "slug": "E.-Vidal",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Vidal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vidal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 61
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nCasacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not necessarily probabilistic) CFGs and performs a computation that is isomorphic to that of inside probabilities shown here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\n(move the dot over the current symbol)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 124
                            }
                        ],
                        "text": "Context-free forward probabilities include all available probabilistic information (subject to assumptions implicit\nAndreas Stolcke Efficient Probabilistic Context-Free Parsing\nin the SCFG formalism) available from an input prefix, whereas the usual inside probabilities do not take into account the nonterminal prior probabilities that result from the top-down relation to the start state."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 80
                            }
                        ],
                        "text": "The particular way in which and were defined turns out to be convenient\nAndreas Stolcke Efficient Probabilistic Context-Free Parsing\nhere, as no reference to the production probabilities themselves needs to be made in the computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nThis would leave the forward probabilities at 0(0S !"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nE-step: Compute expectations for how often each grammar rule is used, given the corpus D and the current grammar parameters (rule probabilities)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nB.3.1 Speeding up matrix inversions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nc) G has no useless nonterminals iff all nonterminals X appear in at least one derivation of some string x 2 with non-zero probability, i.e.,P (S ) X ) x) > 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not necessarily probabilistic) CFGs and performs a computation that is isomorphic to that of inside probabilities shown here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nEarley\u2019s parser (and hence ours) also deals with any context-free rule format in a seamless way, without requiring conversions to Chomsky Normal Form (CNF), as is often assumed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nThe worst-case complexity for Earley\u2019s parser is dominated by the completion step, which takes O(l2) for each input position, l being the length of the current prefix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nfor all Y; Z such that R(Z ) Y ) is non-zero, and Y ! is not a unit production (j j> 1 or 2 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nDefinition 1 The following quantities are defined relative to a SCFGG, a nonterminalX, and a stringx over the alphabet of G.\na) The probability of a (partial) derivation 1 ) 2 ) : : : k is inductively defined by\n1) P ( 1) = 1 2) P ( 1 ) : :"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 0
                            }
                        ],
                        "text": "Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not necessarily probabilistic) CFGs and performs a computation that is isomorphic to that of inside probabilities shown here. Schabes (1991) adds both inner and outer probabilities to Earley's algorithm, with the purpose of obtaining a generalized estimation algorithm for SCFGs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 157
                            }
                        ],
                        "text": "Definition 3 The probability P (P) of a path P is the product of the probabilities of all rules used in the predicted states occurring in P. Lemma 2\nAndreas Stolcke Efficient Probabilistic Context-Free Parsing\na) For all paths P starting with a nonterminal X, P (P) gives the probability of the (partial) derivation represented by P."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nComputation of P (X ) ) for all X can be cast as a system of non-linear equations, as follows."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 8
                            }
                        ],
                        "text": "Andreas Stolcke Efficient Probabilistic Context-Free Parsing\nThe problem is best explained by studying an example."
                    },
                    "intents": []
                }
            ],
            "corpusId": 86854910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5c110656a5598cf054c4509a58cdac0f870cb18",
            "isKey": true,
            "numCitedBy": 2,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In Syntactic Pattern Recognition, the weighted grammars are very useful in the representation of the inexact nature of many problems. Basically, such grammars are defined as a conventional grammar and a function from the set of rules into some weighting space. Some algorithms are used for particular grammars and particular spaces. In this paper, a general parsing algorithm for weighted context-free grammars is presented. This algorithm parses a given string and supplies the corresponding weight. An extension of this algorithm whose main goal is to achieve, in parallel with the parsing, a segmentation of the input string into substrings which are recognized by the weighted grammars used is also presented."
            },
            "slug": "A-parsing-algorithm-for-weighted-grammars-and-Casacuberta-Vidal",
            "title": {
                "fragments": [],
                "text": "A parsing algorithm for weighted grammars and substring recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general parsing algorithm for weighted context-free grammars is presented, which parses a given string and supplies the corresponding weight and an extension of this algorithm whose main goal is to achieve, in parallel with the parsing, a segmentation of the input string into substrings which are recognized by the weightedgrammars used."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107485398"
                        ],
                        "name": "J. H. Wright",
                        "slug": "J.-H.-Wright",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 66
                            }
                        ],
                        "text": "15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p's, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 49
                            }
                        ],
                        "text": "6 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LR parser tables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p\u2019s, as shown in Stolcke (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "Probabilistic LR parsing (Wright, 1990) is based on LR items augmented with certain conditional probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 25
                            }
                        ],
                        "text": "Probabilistic LR parsing (Wright 1990) is based on LR items augmented with certain conditional probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "The solution used by Wright (1990) is to collapse items whose probabilities are within a small tolerance ~ and are otherwise identical."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p's, as shown in Stolcke (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 21
                            }
                        ],
                        "text": "The solution used by Wright (1990) is to collapse items whose probabilities are within a small tolerance and are otherwise identical."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61129334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c94cfb3ffab340e35c7e59130d40d82f161068",
            "isKey": true,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LR-parsing-of-probabilistic-grammars-with-input-for-Wright",
            "title": {
                "fragments": [],
                "text": "LR parsing of probabilistic grammars with input uncertainty for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13851143"
                        ],
                        "name": "A. Corazza",
                        "slug": "A.-Corazza",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Corazza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corazza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698769"
                        ],
                        "name": "R. Gretter",
                        "slug": "R.-Gretter",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Gretter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gretter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152299194"
                        ],
                        "name": "G. Satta",
                        "slug": "G.-Satta",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Satta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Satta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 273
                            }
                        ],
                        "text": "\u2026over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al., 1991); to guide the rule choice efficiently during parsing (Jones and Eisner, 1992); to compute island probabilities for non-linear parsing (Corazza et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28373730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7db2105e7df22c83cb7b44b2dc736027277042ac",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe an effort to adapt island-driven parsers to handle stochastic context-free grammars. These grammars could be used as language models (LMs) by a language processor (LP) to computer the probability of a linguistic interpretation. As different islands may compete for growth, it is important to compute the probability that an LM generates a sentence containing islands and gaps between them. Algorithms for computing these probabilities are introduced. The complexity of these algorithms is analyzed both from theoretical and practical points of view. It is shown that the computation of probabilities in the presence of gaps of unknown length requires the impractical solution of a nonlinear system of equations, whereas the computation of probabilities for cases with gaps containing a known number of unknown words has polynomial time complexity and is practically feasible. The use of the results obtained in automatic speech understanding systems is discussed. >"
            },
            "slug": "Computation-of-Probabilities-for-an-Island-Driven-Corazza-Mori",
            "title": {
                "fragments": [],
                "text": "Computation of Probabilities for an Island-Driven Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An effort to adapt island-driven parsers to handle stochastic context-free grammars could be used as language models by a language processor to computer the probability of a linguistic interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71486292"
                        ],
                        "name": "M. Tomita",
                        "slug": "M.-Tomita",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Tomita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tomita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60902655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f60a9c6f58eef2749b71f288b819410fc787f5d",
            "isKey": false,
            "numCitedBy": 472,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction.- 2. Informal Description of the Algorithm.- 3. Examples.- 4. Formal Specification of the Algorithm.- 5. Comparison with Other Algorithms.- 6. Empirical Results.- 7. Left-to-Right on-Line Parsing.- 8. Sentence Disambiguation by Asking.- 9. Interactive/Personal Machine Translation.- 10. Concluding Remarks.- Appendix A. The Parsing Table Constructor.- Appendix B. Earley's Algorithm.- Appendix C. Proof of Correctness of the Algorithm.- C.1. Introduction.- C.2. Soundness of the Algorithm.- C.3. Completeness of the Algorithm.- Appendix D. Raw Empirical Data.- Appendix E. Programs Used in the Experiments.- E.1. Tomita's Algorithm.- E.2. Earley's Algorithm.- E.3. Earley's Algorithm with an Improvement.- E.4. LR(0) Table Construction Algorithm.- E.5. Utility Functions.- Appendix F. Grammars Used in the Experiments.- Appendix G. Sentences Used in the Experiments.- Appendix H. Nishida and Doshita's System.- References.- Author Index."
            },
            "slug": "Efficient-Parsing-for-Natural-Language:-A-Fast-for-Tomita",
            "title": {
                "fragments": [],
                "text": "Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Parsing Table Constructor for Nishida and Doshita's System, with examples of left-to-Right on-Line Parsing and Interactive/Personal Machine Translation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 33
                            }
                        ],
                        "text": "The crucial notion introduced by Baker (1979) for this purpose is the \u201couter probability\u201d of a nonterminal, or the joint probability that the nonterminal is generated with a given prefix and suffix of terminals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al., 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 165
                            }
                        ],
                        "text": "This constitutes the main distinguishing feature of Earley parsing compared to the strict bottom-up computation used in the standard inside probability computation (Baker, 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 85
                            }
                        ],
                        "text": "Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) (Baker, 1979), as well as (2) (Jelinek, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) (Rabiner and Juang, 1986) and in Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "The name is motivated by the fact that reduces to the \u201couter probability\u201d of X as defined in Baker (1979) if the dot is in final position."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 97
                            }
                        ],
                        "text": "The inner probabilities as defined here correspond closely to the quantities of the same name in Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 206
                            }
                        ],
                        "text": "However, for fully parameterized grammars in CNF we can verify the scaling of the algorithm in terms of the number of nonterminals n, and verify that it has the sameO(n3) time and space requirements as the Inside/Outside (I/O) and LRI algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 121
                            }
                        ],
                        "text": "The total time is therefore O(l3) for an input of length l, which is also the complexity of the standard Inside/Outside (Baker, 1979) and LRI (Jelinek and Lafferty, 1991) algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "The literature on Earley-based probabilistic parsers is sparse, presumably because of the precedent set by the Inside/Outside algorithm, which is more naturally formulated as a bottom-up algorithm."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121084921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c79a9bb8f885050cad70b4c69e016b186ffa538",
            "isKey": true,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms which are based on modeling speech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component."
            },
            "slug": "Trainable-grammars-for-speech-recognition-Baker",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes that permits automatic training of the stochastic analog of an arbitrary context free grammar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) (Baker, 1979), as well as (2) (Jelinek, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60744956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d64cae6538bb3b8e595007585477a9fdef106602",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string. In speech recognition, a language model is used to direct the hypothesis search for the sentence that was spoken. In fact, its probability assignments are a factor in evaluating the likelihood that any word string was uttered, and in ordering the word string hypotheses to be examined (1). Language models are also useful in text encoding for compression or transmission, in character recognition of printed or handwritten text, etc. Ideally, the probability assigned by a language model to any word should depend on the entire past sequence of words. However, it is practically impossible to implement such a dependence: 1) The probability estimate involved would have to be extracted from a giant sample of training text containing billions of words; 2) If the probabilities were extractable, sufficient space in computer memory could not be found to store them."
            },
            "slug": "Markov-Source-Modeling-of-Text-Generation-Jelinek",
            "title": {
                "fragments": [],
                "text": "Markov Source Modeling of Text Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111178"
                        ],
                        "name": "S. Graham",
                        "slug": "S.-Graham",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Graham",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750403"
                        ],
                        "name": "M. Harrison",
                        "slug": "M.-Harrison",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Harrison",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719325"
                        ],
                        "name": "W. L. Ruzzo",
                        "slug": "W.-L.-Ruzzo",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Ruzzo",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. L. Ruzzo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1468978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9274889f719b8308a0e389b2251d2a2b64c03e84",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for recognizing and parsing arbitrary context-free languages is presented, and several new results are given on the computational complexity of these problems. The new algorithm is of both practical and theoretical interest. It is conceptually simple and allows a variety of efficient implementations, which are worked out in detail. Two versions are given which run in faster than cubic time. Surprisingly close connections between the Cocke-Kasami-Younger and Earley algorithms are established which reveal that the two algorithms are \u201calmost\u201d identical."
            },
            "slug": "An-Improved-Context-Free-Recognizer-Graham-Harrison",
            "title": {
                "fragments": [],
                "text": "An Improved Context-Free Recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Surprisingly close connections between the Cocke-Kasami-Younger and Earley algorithms are established which reveal that the two algorithms are \u201calmost\u201d identical."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Program. Lang. Syst."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143757098"
                        ],
                        "name": "C. Weir",
                        "slug": "C.-Weir",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7358857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe3788f2de079b08a6b2b130bba17ec0429c7f04",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser."
            },
            "slug": "Efficiency,-Robustness-and-Accuracy-in-Picky-Chart-Magerman-Weir",
            "title": {
                "fragments": [],
                "text": "Efficiency, Robustness and Accuracy in Picky Chart Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Plymouth's Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 696805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c0eab87d4855c42ae6395bf2e27eefe55003b4a",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus to achieve faster convergence and better modelling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60775129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40dbb25a15b63af3faccb81c8e64a3f5d659e07e",
            "isKey": false,
            "numCitedBy": 1856,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From volume 1 Preface (See Front Matter for full Preface) \n \nThis book is intended for a one or two semester course in compiling theory at the senior or graduate level. It is a theoretically oriented treatment of a practical subject. Our motivation for making it so is threefold. \n \n(1) In an area as rapidly changing as Computer Science, sound pedagogy demands that courses emphasize ideas, rather than implementation details. It is our hope that the algorithms and concepts presented in this book will survive the next generation of computers and programming languages, and that at least some of them will be applicable to fields other than compiler writing. \n \n(2) Compiler writing has progressed to the point where many portions of a compiler can be isolated and subjected to design optimization. It is important that appropriate mathematical tools be available to the person attempting this optimization. \n \n(3) Some of the most useful and most efficient compiler algorithms, e.g. LR(k) parsing, require a good deal of mathematical background for full understanding. We expect, therefore, that a good theoretical background will become essential for the compiler designer. \n \nWhile we have not omitted difficult theorems that are relevant to compiling, we have tried to make the book as readable as possible. Numerous examples are given, each based on a small grammar, rather than on the large grammars encountered in practice. It is hoped that these examples are sufficient to illustrate the basic ideas, even in cases where the theoretical developments are difficult to follow in isolation. \n \nFrom volume 2 Preface (See Front Matter for full Preface) \n \nCompiler design is one of the first major areas of systems programming for which a strong theoretical foundation is becoming available. Volume I of The Theory of Parsing, Translation, and Compiling developed the relevant parts of mathematics and language theory for this foundation and developed the principal methods of fast syntactic analysis. Volume II is a continuation of Volume I, but except for Chapters 7 and 8 it is oriented towards the nonsyntactic aspects of compiler design. \n \nThe treatment of the material in Volume II is much the same as in Volume I, although proofs have become a little more sketchy. We have tried to make the discussion as readable as possible by providing numerous examples, each illustrating one or two concepts. \n \nSince the text emphasizes concepts rather than language or machine details, a programming laboratory should accompany a course based on this book, so that a student can develop some facility in applying the concepts discussed to practical problems. The programming exercises appearing at the ends of sections can be used as recommended projects in such a laboratory. Part of the laboratory course should discuss the code to be generated for such programming language constructs as recursion, parameter passing, subroutine linkages, array references, loops, and so forth."
            },
            "slug": "The-Theory-of-Parsing,-Translation,-and-Compiling-Aho-Ullman",
            "title": {
                "fragments": [],
                "text": "The Theory of Parsing, Translation, and Compiling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is the hope that the algorithms and concepts presented in this book will survive the next generation of computers and programming languages, and that at least some of them will be applicable to fields other than compiler writing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144635215"
                        ],
                        "name": "S. Nakagawa",
                        "slug": "S.-Nakagawa",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Nakagawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nakagawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60865397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa82abfffd43b020dca0e8bdb4f9510cce5c5b6a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new continuous speech recognition method by phoneme-based word spotting and time-synchronous context-free parsing. The word pattern is composed of the concatenation of phoneme patterns. The knowledge of syntax is given in Backus Normal Form. Therefore, our method is task-independent in terms of reference patterns and task language. The system first spots word candidates in an input sentence, and then generates a word lattice. The word spotting is performed by a dynamic time warping method. Secondly, it selects the best word sequences found in the word lattice from all possible sentences which are defined by a context-free grammar."
            },
            "slug": "Spoken-sentence-recognition-by-time-synchronous-of-Nakagawa",
            "title": {
                "fragments": [],
                "text": "Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new continuous speech recognition method by phoneme-based word spotting and time-synchronous context-free parsing that is task-independent in terms of reference patterns and task language."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029039"
                        ],
                        "name": "T. Booth",
                        "slug": "T.-Booth",
                        "structuredName": {
                            "firstName": "Taylor",
                            "lastName": "Booth",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Booth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97813852"
                        ],
                        "name": "Richarda . Thompson",
                        "slug": "Richarda-.-Thompson",
                        "structuredName": {
                            "firstName": "Richarda",
                            "lastName": "Thompson",
                            "middleNames": [
                                "."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richarda . Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Booth and Thompson (1973) show that one can write a SCFG that satisfies (a) and (c) but generates derivations that do not terminate with probability 1, and give necessary and sufficient conditions for (b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 40
                            }
                        ],
                        "text": "The terminology used here is taken from Booth and Thompson (1973)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 101
                            }
                        ],
                        "text": "In the following, we assume that the probabilities in a SCFG are proper and consistent as defined in Booth and Thompson (1973), and that the grammar contains no useless nonterminals (ones that can never appear in a derivation)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Booth and Thompson (1973) show that the grammar is consistent if and only if the probability that stochastic rewriting of the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41410699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b",
            "isKey": true,
            "numCitedBy": 269,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of assigning a probability to each word of a language is considered. Two methods are discussed. One method assigns a probability to a word on the basis of particular measurable features of the language. The second method is applied to languages L(G) generated by a grammar G. A probability is associated with each production of G. These in turn define the word probabilities of each word in the language. The conditions for this assignment to be a probabilistic measure are derived."
            },
            "slug": "Applying-Probability-Measures-to-Abstract-Languages-Booth-Thompson",
            "title": {
                "fragments": [],
                "text": "Applying Probability Measures to Abstract Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The problem of assigning a probability to each word of a language is considered and two methods are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169600"
                        ],
                        "name": "T. Fujisaki",
                        "slug": "T.-Fujisaki",
                        "structuredName": {
                            "firstName": "Tetsunosuke",
                            "lastName": "Fujisaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fujisaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795982"
                        ],
                        "name": "T. Nishino",
                        "slug": "T.-Nishino",
                        "structuredName": {
                            "firstName": "Tetsuro",
                            "lastName": "Nishino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nishino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 145
                            }
                        ],
                        "text": "\u2026as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al., 1991); to guide the rule choice efficiently during parsing (Jones and Eisner, 1992); to compute island probabilities for non-linear\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59759783,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "e7470c416e13fcf91396cc29fa43a7903ea6d519",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Constructing a grammar which can parse sentences selected from a natural language corpus is a difficult task. One of the most serious problems is the unmanageably large number of ambiguities. Pure syntactic analysis based only on syntactic knowledge will sometimes result in hundreds of ambiguous parses."
            },
            "slug": "Probabilistic-Parsing-Method-for-Sentence-Fujisaki-Jelinek",
            "title": {
                "fragments": [],
                "text": "Probabilistic Parsing Method for Sentence Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A grammar which can parse sentences selected from a natural language corpus is a difficult task and pure syntactic analysis based only on syntactic knowledge will sometimes result in hundreds of ambiguous parses."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71486292"
                        ],
                        "name": "M. Tomita",
                        "slug": "M.-Tomita",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Tomita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tomita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62691315,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "321aeab25a35a813f8c8bb5f0b86470a2d769a5d",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Any books that you read, no matter how you got the sentences that have been read from the books, surely they will give you goodness. But, we will show you one of recommendation of the book that you need to read. This efficient parsing for natural language is what we surely mean. We will show you the reasonable reasons why you need to read this book. This book is a kind of precious book written by an experienced author."
            },
            "slug": "Efficient-parsing-for-natural-language-Tomita",
            "title": {
                "fragments": [],
                "text": "Efficient parsing for natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "One of recommendation of the book that you need to read is shown, which is a kind of precious book written by an experienced author and it will show the reasonable reasons why you should read this book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705097"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "Carlos",
                                "Neves"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19956410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "547d483ed1e80066693af561f63daa30ffa8e9fa",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nA concise and practical introduction to logic programming and the logic-programming language Prolog, both as vehicles for understanding elementary computational linguistics and as tools for implementing the basic components of natural-language-processing systems."
            },
            "slug": "Prolog-and-Natural-Language-Analysis-Pereira-Shieber",
            "title": {
                "fragments": [],
                "text": "Prolog and Natural-Language Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A concise and practical introduction to logic programming and the logic-programming language Prolog, both as vehicles for understanding elementary computational linguistics and as tools for implementing the basic components of natural-language-processing systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1968677"
                        ],
                        "name": "A. Paeseler",
                        "slug": "A.-Paeseler",
                        "structuredName": {
                            "firstName": "Annedore",
                            "lastName": "Paeseler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Paeseler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60561887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9c6786490ffb6e9d3dd18db8bf6a5dc2d6e6c04",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an adaptation of Earley\u2019s algorithm for the recognition of spoken sentences. Earley\u2019s algorithm is one of the most efficient parsing algorithms for written sentences. The modifications and extensions of Earley\u2019s algorithm required by the variability and ambiguity of the speech signal concern the scoring of word and sentence hypotheses and the application of a beam search or pruning technique."
            },
            "slug": "Modification-of-Earley's-algorithm-for-speech-Paeseler",
            "title": {
                "fragments": [],
                "text": "Modification of Earley's algorithm for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An adaptation of Earley\u2019s algorithm for the recognition of spoken sentences and the scoring of word and sentence hypotheses and the application of a beam search or pruning technique is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 176
                            }
                        ],
                        "text": "These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder (Bahl et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705919"
                        ],
                        "name": "Chuck Wooters",
                        "slug": "Chuck-Wooters",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Wooters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuck Wooters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918411"
                        ],
                        "name": "G. Tajchman",
                        "slug": "G.-Tajchman",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Tajchman",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tajchman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069154640"
                        ],
                        "name": "Jonathan Segal",
                        "slug": "Jonathan-Segal",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8241008,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6237bf81e36284ed74fca2803169964dca1b96b6",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the architecture and performance of the Berkeley Restaurant Project (BeRP), a medium-vocabulary, speaker-independent, spontaneous continuous speech understanding system currently under development at ICSI. BeRP serves as a testbed for a number of our speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiplepronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading. In addition, it has proved quite usable in its function as a database frontend, even though many of our subjects are non-native speakers of English."
            },
            "slug": "The-berkeley-restaurant-project-Jurafsky-Wooters",
            "title": {
                "fragments": [],
                "text": "The berkeley restaurant project"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "BeRP serves as a testbed for a number of speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiplepronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144230085"
                        ],
                        "name": "J. K. Skwirzynski",
                        "slug": "J.-K.-Skwirzynski",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Skwirzynski",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. K. Skwirzynski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57991610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc805fa91a874761253ee6e4b3500b44af227f13",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume contains the full proceedings of the Fourth Advanced Study Institute organised by myself and my colleagues in . * the field of Communication Theory and Allied Subjects. In the first Institute we associated the subject of signal processing in communication with that in control engineering. Then we concentrated on noise and random phenomena by bringing in as well the subject of stochastic calculus. The third time our subject was multi-user communication and associated with it, the important problem of assessing algorithmic complexity. This time we are concerned with the vast increase of computational power that is now available in communication systems processors and controllers. This forces a mathematical, algorithmic and structural approach to the solution of computational requirements and design problems, in contrast to previous heuristic and intuitive methods. We are also concerned with the interactions and trade-offs between the structure, speed, and complexity of a process, and between software and hardware implementations. At the previous Advanced Study Institute in this series, on Multi-User Communications, there was a session on computational complexity, applied particularly to network routing problems. It was the aim of this Institute to expand this topic and to link it with information theory, random processes, pattern analysis, and implementation aspects of communication processors. The first part of these proceedings concentrates on pattern and structure in communications processing. In organising this session I was greatly helped and guided by Professor P. G. Farrell and Professor J. L. Massey."
            },
            "slug": "The-impact-of-processing-techniques-on-Skwirzynski",
            "title": {
                "fragments": [],
                "text": "The impact of processing techniques on communications"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The aim of this Institute was to expand this topic and to link it with information theory, random processes, pattern analysis, and implementation aspects of communication processors, and the first part of these proceedings concentrates on pattern and structure in communications processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 141
                            }
                        ],
                        "text": "2 Rule Probability Estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (ExpectationMaximization) algorithm (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 112
                            }
                        ],
                        "text": "The rule probabilities in a SCFG can be iteratively estimated using the EM (ExpectationMaximization) algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102329511"
                        ],
                        "name": "George W. Soules",
                        "slug": "George-W.-Soules",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Soules",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George W. Soules"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063108982"
                        ],
                        "name": "Norman Weiss",
                        "slug": "Norman-Weiss",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norman Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 576,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus. After that, the Mstep consists of a simple normalization of these counts to yield the new production probabilities. In this section we examine the computation of production count expectations required for the E-step. The crucial notion introduced by Baker (1979) for this purpose is the \"outer probability\" of a nonterminal, or the joint probability that the nonterminal is generated with a given prefix and suffix of terminals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al., 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122568650,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4",
            "isKey": true,
            "numCitedBy": 4551,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Maximization-Technique-Occurring-in-the-Analysis-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 568,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus. After that, the M-step consists of a simple normalization of these counts to yield the new production probabilities. In this section we examine the computation of production count expectations required for the E-step. The crucial notion introduced by Baker (1979) for this purpose is the \u201couter probability\u201d of a nonterminal, or the joint probability that the nonterminal is generated with a given prefix and suffix of terminals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al., 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique occuring in the statistical analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 568,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus. After that, the M-step consists of a simple normalization of these counts to yield the new production probabilities. In this section we examine the computation of production count expectations required for the E-step. The crucial notion introduced by Baker (1979) for this purpose is the \\outer probability\" of a nonterminal, or the joint probability that the nonterminal is generated with a given pre x and su x of terminals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al., 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models (Rabiner and Juang 1986), where one computes the Viterbi path (state sequence) through an HMM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 126
                            }
                        ],
                        "text": "Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models (Rabiner and Juang, 1986), where one computes the Viterbi path (state sequence) through an HMM. Precisely the same approach can be used in the Earley parser, using the fact that each derivation corresponds to a path."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 128
                            }
                        ],
                        "text": "The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) (Rabiner and Juang, 1986) and in Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 148
                            }
                        ],
                        "text": "Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models (Rabiner and Juang, 1986), where one computes the Viterbi path (state sequence) through an HMM. Precisely the same approach can be used in the Earley\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 122
                            }
                        ],
                        "text": "The terminology is derived from analogous or similar quantities commonlyused in the literatureonHiddenMarkovModels (HMMs) (Rabiner and Juang 1986) and in Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to hiddenMarkov models"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402050148"
                        ],
                        "name": "P. Saint-Dizier",
                        "slug": "P.-Saint-Dizier",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Saint-Dizier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Saint-Dizier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60541696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "777570c140f6f995385f4d5a3b6bd902dfab3f5d",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review-of-Prolog-and-natural-language-analysis:-10-Saint-Dizier",
            "title": {
                "fragments": [],
                "text": "Review of Prolog and natural-language analysis: CSLI lecture notes 10 by Fernando C. N. Pereira and Stuart M. Shieber. Center for the Study of Language and Information 1987."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70373389"
                        ],
                        "name": "W. H. Offenhauser",
                        "slug": "W.-H.-Offenhauser",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Offenhauser",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H. Offenhauser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203949059,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "853d1fc0d05fa83d73906f2a71a5a7d15788fe4a",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "liams for their clarifi cation on state and national regulations aimed at reducing the risks of salmonellosis and their advice on conducting this survey. We also thank members and staff of the National Association of State Public Health Veterinarians and the National Resource Center for Health and Safety in Child Care and Early Education for their assistance. a population-based, case-control study.nas and Salmonella Marina infection in children: a refl ection of the increasing incidence of reptile-associated sal-monellosis in the United States. 6. Centers for Disease Control and Prevention. Multistate outbreak of human Salmonella Typhimurium infections associated with aquatic frogs\u2014United States, Wild Boars as Hosts of Human-Pathogenic Anaplasma phagocytophilum Variants To the Editor: Michalik et al. (1) reported a 12% prevalence of Ana-plasma phagocytophilum, the caus-ative agent of human granulocytic anaplasmosis and tick-borne fever of ruminants, in wild boars in Poland. A. phagocytophilum has been reported with low prevalence among wild boar in the Czech Republic, Slovenia (2), and Japan (3). In Spain and Missis-sippi, United States, A. phagocyto-philum in wild boars or feral pigs, respectively , has not been reported (4,5). Furthermore, in Slovenia and Poland, the A. phagocytophilum gene sequences found in samples from wild boars were identical to those found in samples from humans and the tick vector Ixodes ricinus (1). These results suggested , as pointed out by Michalik et al. (1), that wild boar might play a role in the epizootiology of A. phagocyto-philum by serving as a natural reservoir host, at least in some regions. To test this hypothesis, we conducted transcriptomics studies to characterize host response to A. phagocy-tophilum infection in naturally and experimentally infected boars (6,7). The results suggested that boars are susceptible to A. phagocytophilum, but are able to control infection, mainly through activation of innate immune responses and cytoskeleton rearrangement to promote phagocy-tosis and autophagy. Control of A. phagocytophilum infection in boars might result in infection levels below PCR detection or infection clearance, contributing to the low percentage of infection prevalence detected for this species in most regions. The low detection levels suggest that boars have a low or no impact as a reservoir host for A. phagocytophilum. Even if boars remain persistently infected with A. phagocytophilum at low levels by downregulating some adap-tive immune genes and delaying the apoptotic death of neutrophils through activation of the Jak-STAT pathway, among other mechanisms (6), their role as a source of infection for \u2026"
            },
            "slug": "In-References-to-References-Offenhauser",
            "title": {
                "fragments": [],
                "text": "In References to References"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is suggested that boars are susceptible to A. phagocytophilum infection, but are able to control infection, mainly through activation of innate immune responses and cytoskeleton rearrangement to promotephagocy-tosis and autophagy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18314894,
            "fieldsOfStudy": [],
            "id": "6c974a7d091913b486f51d983ead7023dd5e8de2",
            "isKey": false,
            "numCitedBy": 2281,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Center-for-the-Study-of-Language-and-Information-Pollard",
            "title": {
                "fragments": [],
                "text": "Center for the Study of Language and Information"
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705097"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "Carlos",
                                "Neves"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Pereira and Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63967455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e4843e2c55843b5c5b429f89dad3d99e801f02",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus to achieve faster convergence and better modeling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society, Series B"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society, Series B"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient Probabilistic Context-Free Parsing Earley, Jay"
            },
            "venue": {
                "fragments": [],
                "text": "Communications of the ACM,"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HiddenMarkov estimation for unrestricted stochastic context - free grammars \u201d"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Acoustics , Speech and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 151
                            }
                        ],
                        "text": "In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney, 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari and Young, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic grammars and pattern recognition In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO ASI Series"
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic grammars and pattern recognition In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO ASI Series"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applying probability measures to abstract"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prolog and Natural-Language Analysis . Number 10 in CSLI Lecture Notes Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "Prolog and Natural-Language Analysis . Number 10 in CSLI Lecture Notes Computational Linguistics"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 49
                            }
                        ],
                        "text": "A detailed account of this technique is given in Stolcke (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 161
                            }
                        ],
                        "text": "15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p\u2019s, as shown in Stolcke (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "Parsing bracketed inputs is described in more detail in Stolcke (1993), where it is also shown that bracketing gives the expected improved efficiency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient probabilistic context-free parsing algorithm that computes prefix Andreas Stolcke Efficient Probabilistic Context-Free Parsing probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "An efficient probabilistic context-free parsing algorithm that computes prefix Andreas Stolcke Efficient Probabilistic Context-Free Parsing probabilities"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation (Baum et al., 1970); the original formulation for the case of SCFGs is due to Baker (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique occuring in the statisti"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic grammars and pattern recognition.\" In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Las Cruces"
            },
            "venue": {
                "fragments": [],
                "text": "Las Cruces"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Jelinek et al. (1992) provide a tutorial introduction covering the standard algorithms for the four tasks mentioned in the introduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "initial substring generation by stochastic context-free grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic grammars and pattern recognition Speech Recognition and Understanding. Recent Advances , Trends, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "F75 of NATO ASI Series Proceedings of the NATO Advanced Study Institute"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 145
                            }
                        ],
                        "text": "\u2026as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al., 1991); to guide the rule choice efficiently during parsing (Jones and Eisner, 1992); to compute island probabilities for non-linear\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 200
                            }
                        ],
                        "text": "In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A probabilistic parsing method for sentence disambiguation.\" In Current Issues in Parsing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition.\" In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Jared"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computation of the probability"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the NATO Advanced Study Institute,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "De Mori, volume F75 of NATO ASI Series. Springer Verlag, Berlin, 345\u2013360"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the NATO Advanced Study Institute,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "De Mori, volume F75 of NATO ASI Series. Springer Verlag, Berlin, 319\u2013344"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the NATO Advanced Study Institute,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 112
                            }
                        ],
                        "text": "The rule probabilities in a SCFG can be iteratively estimated using the EM (ExpectationMaximization) algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 142
                            }
                        ],
                        "text": "2 Rule probability estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-Maximization) algorithm (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inside-outside algorithm for estimating the parameters of a hidden"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the NATO Advanced Study Institute, Cetraro"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Schabes (1991) adds both inner and outer probabilities to Earley\u2019s algorithm, with the purpose of obtaining a generalized estimation algorithm for SCFGs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Unpublished mss"
            },
            "venue": {
                "fragments": [],
                "text": "Presented at the Second Workshop on Mathematics of Language"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1987:chapter 6) for improving the e ciency of bottom-up parsers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Efficient-Probabilistic-Context-Free-Parsing-Stolcke/79fbfc1dc8846379074aaf4deb7fb0a96722eeed?sort=total-citations"
}