{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "These results are consistent with the more extended simulation studies in (Heskes, 1998, 2000)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1118769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ac52b7d8db223029388551b2db25657ed8c9852",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a machine-learning solution to problems consisting of many similar prediction tasks. Each of the individual tasks has a high risk of overrtting. We combine two types of knowledge transfer between tasks to reduce this risk: multi-task learning and hierarchical Bayesian modeling. Multi-task learning is based on the assumption that there exist features typical to the task at hand. To nd these features, we train a huge two-layered neural network. Each task has its own output, but shares the weights from the input to the hidden units with all other tasks. In this way a relatively large set of possible explanatory variables (the network inputs) is reduced to a smaller and easier to handle set of features (the hidden units). Given this set of features and after an appropriate scale transformation, we assume that the tasks are exchangeable. This assumption allows for a hierarchical Bayesian analysis in which the hyperparameters can be estimated from the data. EEectively, these hyperpa-rameters act as regularizers and prevent over-tting. We describe how to make the system robust against nonstationarities in the time series and give directions for further improvement. We illustrate our ideas on a database regarding the prediction of newspaper sales."
            },
            "slug": "Solving-a-Huge-Number-of-Similar-Tasks:-A-of-and-a-Heskes",
            "title": {
                "fragments": [],
                "text": "Solving a Huge Number of Similar Tasks: A Combination of Multi-Task Learning and a Hierarchical Bayesian Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A machine-learning solution to problems consisting of many similar prediction tasks, each of the individual tasks has a high risk of overrtting, that combines two types of knowledge transfer between tasks to reduce this risk: multi-task learning and hierarchical Bayesian modeling."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 29
                            }
                        ],
                        "text": "For finite\nnumbers of tasks, Baxter (1997) shows that the generalization error as a function of the number of tasksN and the dimension of the hyperparameters|\u039b| is proportional to|\u039b| and inversely proportional toN (see also Heskes, 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "In the present article we seek to combine insights that are obtained in the multilevel field with methods that have been designed in the neural network community to create a synergetic new approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Empirical Bayes; Multitask learning; Mixture of experts; Multilevel analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "A neural network model would use \u2018hard shared\u2019 parameters (the same for each of the parallel tasks) to detect \u2018features\u2019 in the covariatesx, and use these features for regression (Baxter, 1997, Caruana, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 30
                            }
                        ],
                        "text": "Our work has been inspired by Baxter (1997), Caruana (1997) and Thrun and O\u2019Sullivan (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "The bottom layer of this network creates that lower-dimensional representation (see e.g. Baxter, 1997) of the inputs, that is best suited for the second layer to perform regression on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 91
                            }
                        ],
                        "text": "This type of\nc\u00a92003 Bart Bakker and Tom Heskes.\noptimization has previously been studied by Baxter (1997): he showed that the risk of overfitting the shared parameters is an orderN (the number of tasks) smaller than overfitting the task-specific parameters (hiddento-output weights)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12565208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd",
            "isKey": true,
            "numCitedBy": 382,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks."
            },
            "slug": "A-Bayesian/Information-Theoretic-Model-of-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that for many common machine learning problems, although in general the authors do not know the true (objective) prior for the problem, they do have some idea of a set of possible priors to which the true prior belongs."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Empirical Bayes; Multitask learning; Mixture of experts; Multilevel analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "Our work has been inspired by Baxter (1997), Caruana (1997) and Thrun and O\u2019Sullivan (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 190
                            }
                        ],
                        "text": "A neural network model would use \u2018hard shared\u2019 parameters (the same for each of the parallel tasks) to detect \u2018features\u2019 in the covariatesx, and use these features for regression (Baxter, 1997, Caruana, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45998148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47aaeb6dc682162dfe5659c2cad64e5d825ad910",
            "isKey": false,
            "numCitedBy": 3260,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems."
            },
            "slug": "Multitask-Learning-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Prior work on MTL is reviewed, new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals is presented, and new results for MTL with k-nearest neighbor and kernel regression are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning and Data Mining"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 222
                            }
                        ],
                        "text": "For finite\nnumbers of tasks, Baxter (1997) shows that the generalization error as a function of the number of tasksN and the dimension of the hyperparameters|\u039b| is proportional to|\u039b| and inversely proportional toN (see also Heskes, 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 154
                            }
                        ],
                        "text": "In this case, sufficient statistics can be calculated beforehand, after which optimizing the shared parameters no longer scales with the number of tasks (Heskes, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 16
                            }
                        ],
                        "text": "Given the maximum likelihood parameters\u039b\u2217, we can easily computeP(A i |Di ,\u039b\u2217) to make predictions, compute error bars, and so on."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "These results are consistent with the more extended simulation studies in (Heskes, 1998, 2000)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1376989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "446d6b48f79fce24cb12f293e3b161112be261a6",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new model for studying mul-titask learning, linking theoretical results to practical simulations. In our model all tasks are combined in a single feedforward neu-ral network. Learning is implemented in a Bayesian fashion. In this Bayesian framework the hidden-to-output weights, being speciic to each task, play the role of model parameters. The input-to-hidden weights, which are shared between all tasks, are treated as hyperparameters. Other hyper-parameters describe error variance and correlations and priors for the model parameters. An important feature of our model is that the probability of these hyperparam-eters given the data can be computed ex-plicitely and only depends on a set of suu-cient statistics. None of these statistics scales with the number of tasks or patterns, which makes empirical Bayes for multitask learning a relatively straightforward optimization problem. Simulations on real-world data sets on single-copy newspaper and magazine sales illustrate properties of multitask learning. Most notably we derive experimental curves for \\learning to learn\" that can be linked to theoretical results obtained elsewhere."
            },
            "slug": "Empirical-Bayes-for-Learning-to-Learn-Heskes",
            "title": {
                "fragments": [],
                "text": "Empirical Bayes for Learning to Learn"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new model for studying mul-titask learning is presented, linking theoretical results to practical simulations, and experimental curves for \"learning to learn\" that can be linked to theoretical results obtained elsewhere are derived."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395754302"
                        ],
                        "name": "Joseph O'Sullivan",
                        "slug": "Joseph-O'Sullivan",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph O'Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10420876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f42a55da3a222184eee20c67d374a9134b77bdc",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been an increased interest in \u201clifelong\u201d machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "slug": "Discovering-Structure-in-Multiple-Learning-Tasks:-Thrun-O'Sullivan",
            "title": {
                "fragments": [],
                "text": "Discovering Structure in Multiple Learning Tasks: The TC Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The task-clustering algorithm TC clusters learning tasks into classes of mutually related tasks, and outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 12
                            }
                        ],
                        "text": "Here it is motivated by the fact that we can use the data of all tasks to optimize \u039b, the dimension of which is independent of and assumed to be much smaller than the number of tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 101
                            }
                        ],
                        "text": "This is referred to as empirical Bayes (Robert, 1994), and is similar to MacKay\u2019s evidence framework (MacKay, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14332165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks."
            },
            "slug": "Probable-networks-and-plausible-predictions-a-of-Mackay",
            "title": {
                "fragments": [],
                "text": "Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 119
                            }
                        ],
                        "text": "The gating in Section 4.3 yields an EM algorithm that is similar to the one for a mixture-of-experts architecture (see Jordan and Jacobs, 1994, Jiang and Tanner, 1999), but the application of it is quite different."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 83
                            }
                        ],
                        "text": "The task gating part of our model can be compared to the mixture of experts\nmodel (Jordan and Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": true,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 171
                            }
                        ],
                        "text": "For e.g. multitask classification problems, we would have to resort to appropriate approximations, perhaps similar to those used in Gaussian processes for classification (Williams and Barber, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18841569,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f0ddbcb32e50514de5c89c8ceca58345c5a43948",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by /spl sigma/(y(x)), where /spl sigma/(y)=1/(1+e/sup -y/). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m>2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets."
            },
            "slug": "Bayesian-Classification-With-Gaussian-Processes-Williams-Barber",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification With Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Bayesian treatment is provided, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation, and the method is generalized to multiclass problems (m>2) using the softmax function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692636"
                        ],
                        "name": "I. Cadez",
                        "slug": "I.-Cadez",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Cadez",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cadez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144824619"
                        ],
                        "name": "S. Gaffney",
                        "slug": "S.-Gaffney",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Gaffney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gaffney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 23
                            }
                        ],
                        "text": "Trajectory clustering (Cadez et al., 2000) can be derived as a special case of task clustering without hidden units and with all covariance matrices\u03a3\u03b1 set to zero."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1342934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dc74683c2c56c6b48bb88ba020704e0c3281a24",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unifying probabilisti framework for lustering individuals or systems into groups when the available data measurements are not multivariate ve tors of xed dimensionality. For example, one might have data from a set of medi al patients, where for ea h patient one has a set of of observed time-series, ea h time-series of potentially di erent length and di erent sampling rate. We propose a general model-based probabilisti framework for lustering data types of this form whi h are non-ve tor in nature and may vary in size from individual to individual. The Expe tation-Maximization (EM) pro edure for lustering within this framework is dis ussed and we dis uss how it be applied in a general manner to lustering of sequen es, time-series, traje tories, and other non-ve tor data. We show that a number of earlier algorithms an be viewed as spe ial ases within this unifying framework. The paper on ludes with several illustrations of the method, in luding lustering of red blood ell data in a medi al diagnosis ontext, lustering of proteins from urves of gene expression data, and lustering of individuals based on their sequen es of Web navigation. General Terms Clustering, Mixture Models, EM Algorithm"
            },
            "slug": "A-general-probabilistic-framework-for-clustering-Cadez-Gaffney",
            "title": {
                "fragments": [],
                "text": "A general probabilistic framework for clustering individuals and objects"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper presents a unifying probabilisti framework for lustering individuals or systems into groups when the available data measurements are not multivariate ve tors of xed dimensionality and shows that a number of earlier algorithms an be viewed as spe ial ases within this unifying framework."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89763589"
                        ],
                        "name": "M. Seltzer",
                        "slug": "M.-Seltzer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Seltzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seltzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725639"
                        ],
                        "name": "W. Wong",
                        "slug": "W.-Wong",
                        "structuredName": {
                            "firstName": "Wing",
                            "lastName": "Wong",
                            "middleNames": [
                                "Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39465526"
                        ],
                        "name": "A. Bryk",
                        "slug": "A.-Bryk",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bryk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bryk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "In thefull Bayesian approach (see e.g. Seltzer et al., 1996) further prior distributions are defined for these hyperparameters, which are chosena priori."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60449368,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ca04f8b03170ca74e8addee9df5375b32a703e3",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "In applications of hierarchical models (HMs), a potential weakness of empirical Bayes estimation approaches is that they do not to take into account uncertainty in the estimation of the variance components (see, e.g., Dempster, 1987). One possible solution entails employing a fully Bayesian approach, which involves specifying a prior probability distribution for the variance components and then integrating over the variance components as well as other unknowns in the HM to obtain a marginal posterior distribution of interest (see, e.g., Draper, 1995; Rubin, 1981). Though the required integrations are often exceedingly complex, Markov-chain Monte Carlo techniques (e.g., the Gibbs sampler) provide a viable means of obtaining marginal posteriors of interest in many complex settings. In this article, we fully generalize the Gibbs sampling algorithms presented in Seltzer (1993) to a broad range of settings in which vectors of random regression parameters in the HM (e.g., school means and slopes) are assumed multivariate normally or multivariate t distributed across groups. Through analyses of the data from an innovative mathematics curriculum, we examine when and why it becomes important to employ a fully Bayesian approach and discuss the need to study the sensitivity of results to alternative prior distributional assumptions for the variance components and for the random regression parameters."
            },
            "slug": "Bayesian-Analysis-in-Applications-of-Hierarchical-Seltzer-Wong",
            "title": {
                "fragments": [],
                "text": "Bayesian Analysis in Applications of Hierarchical Models: Issues and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Through analyses of the data from an innovative mathematics curriculum, it is examined when and why it becomes important to employ a fully Bayesian approach and the need to study the sensitivity of results to alternative prior distributional assumptions for the variance components and for the random regression parameters is discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "For these two non-Bayesian methods we applied early stopping (see e.g. Caruana et al., 2001) to prevent overfitting on the training data (the model parameters were optimized on a training set, and the optimization process stopped when no more improvement was found on a separate validation set)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7365231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "072d756c8b17a78018298e67ff29e6d3a4fe5770",
            "isKey": false,
            "numCitedBy": 854,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity."
            },
            "slug": "Overfitting-in-Neural-Nets:-Backpropagation,-and-Caruana-Lawrence",
            "title": {
                "fragments": [],
                "text": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is shown that nets with excess capacity generalize well when trained with backprop and early stopping, and that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non- linearity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157778213"
                        ],
                        "name": "X. Lin",
                        "slug": "X.-Lin",
                        "structuredName": {
                            "firstName": "X",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109904128"
                        ],
                        "name": "D. Zhang",
                        "slug": "D.-Zhang",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 125
                            }
                        ],
                        "text": "Over the past years many proposals have been made to incorporate nonlinearity into these models, through B-splines (see e.g. Lin and Zhang, 1999) and other methods (Brumback and Rice, 1998, Arora et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10964916,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "606cff654f13dec782ed723cc7ccb27b833c3aa3",
            "isKey": false,
            "numCitedBy": 442,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalized additive mixed models are proposed for overdispersed and correlated data, which arise frequently in studies involving clustered, hierarchical and spatial designs. This class of models allows flexible functional dependence of an outcome variable on covariates by using nonparametric regression, while accounting for correlation between observations by using random effects. We estimate nonparametric functions by using smoothing splines and jointly estimate smoothing parameters and variance components by using marginal quasi\u2010likelihood. Because numerical integration is often required by maximizing the objective functions, double penalized quasi\u2010likelihood is proposed to make approximate inference. Frequentist and Bayesian inferences are compared. A key feature of the method proposed is that it allows us to make systematic inference on all model components within a unified parametric mixed model framework and can be easily implemented by fitting a working generalized linear mixed model by using existing statistical software. A bias correction procedure is also proposed to improve the performance of double penalized quasi\u2010likelihood for sparse data. We illustrate the method with an application to infectious disease data and we evaluate its performance through simulation."
            },
            "slug": "Inference-in-generalized-additive-mixed-modelsby-Lin-Zhang",
            "title": {
                "fragments": [],
                "text": "Inference in generalized additive mixed modelsby using smoothing splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39367091"
                        ],
                        "name": "Wenxin Jiang",
                        "slug": "Wenxin-Jiang",
                        "structuredName": {
                            "firstName": "Wenxin",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenxin Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31781461"
                        ],
                        "name": "M. Tanner",
                        "slug": "M.-Tanner",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Tanner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "The gating in Section 4.3 yields an EM algorithm that is similar to the one for a mixture-of-experts architecture (see Jordan and Jacobs, 1994, Jiang and Tanner, 1999), but the application of it is quite different."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6142135,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8c7708a51ca83c18619eabccaa9007c2792bfd3c",
            "isKey": true,
            "numCitedBy": 58,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a class of hierarchical mixtures-of-experts (HME) models where generalized linear models with nonlinear mean functions of the form ( xT) are mixed. Here () is the inverse link function. It is shown that mixtures of such mean functions can approximate a class of smooth functions of the form (h(x)), where h() W2;k (a Sobolev class over [0, 1]s, as the number of experts m in the network increases. An upper bound of the approximation rate is given as O(m2/s) in Lp norm. This rate can be achieved within the family of HME structures with no more than s-layers, where s is the dimension of the predictor x."
            },
            "slug": "On-the-Approximation-Rate-of-Hierarchical-for-Jiang-Tanner",
            "title": {
                "fragments": [],
                "text": "On the Approximation Rate of Hierarchical Mixtures-of-Experts for Generalized Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that mixtures of such mean functions can approximate a class of smooth functions of the form h(x), where h() W2;k (a Sobolev class over [0, 1]s) is the inverse link function, as the number of experts m in the network increases."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "Figure 5 displays a Hinton diagram (Bishop, 1995) of the input-to-hidden weights typically and consistently (up to permutation and sign flips) found in all of the multitask learning approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144442133"
                        ],
                        "name": "L. Pratt",
                        "slug": "L.-Pratt",
                        "structuredName": {
                            "firstName": "Lorien",
                            "lastName": "Pratt",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pratt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "An alternative approach to multitask learning has been taken by Thrun and O\u2019Sullivan (1996) and Pratt (1992), who have devised elegant ways to transfer knowledge obtained by one network to another network learning a similar task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 147613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b66fcdaca4c9e789bd4fae5dfd08a325bbb9fa48",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm, called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly."
            },
            "slug": "Discriminability-Based-Transfer-between-Neural-Pratt",
            "title": {
                "fragments": [],
                "text": "Discriminability-Based Transfer between Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new algorithm, called Discriminability-Based Transfer (DBT), is presented, which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1878090"
                        ],
                        "name": "D. Gamerman",
                        "slug": "D.-Gamerman",
                        "structuredName": {
                            "firstName": "Dani",
                            "lastName": "Gamerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gamerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3019460"
                        ],
                        "name": "H. Migon",
                        "slug": "H.-Migon",
                        "structuredName": {
                            "firstName": "Helio",
                            "lastName": "Migon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Migon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "Here we hope to make a connection between multilevel analysis and dynamic hierarchical models (Gamerman and Migon, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125025953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c2847218abf845fb127f7f399a63bd61f32bb65",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An analysis of a time series of cross-sectional data is considered under a Bayesian perspective. Information is modelled in terms of prior distributions and stratified parametric linear models developed by Lindley and Smith and dynamic linear models developed by Harrison and Stevens are merged into a general framework. This is shown to include many models proposed in econometrics and experimental design. Properties of the model are derived and shrinkage estimators reassessed. Evolution, smoothing and passage of data information through the levels of the hierarchy are discussed. Inference with an unknown scalar observation variance is drawn and an extension to the non-linear case is proposed"
            },
            "slug": "Dynamic-Hierarchical-Models-Gamerman-Migon",
            "title": {
                "fragments": [],
                "text": "Dynamic Hierarchical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An analysis of a time series of cross-sectional data is considered under a Bayesian perspective and evolution, smoothing and passage of data information through the levels of the hierarchy are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48483340"
                        ],
                        "name": "Vipin Arora",
                        "slug": "Vipin-Arora",
                        "structuredName": {
                            "firstName": "Vipin",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vipin Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37673354"
                        ],
                        "name": "P. Lahiri",
                        "slug": "P.-Lahiri",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Lahiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47310763"
                        ],
                        "name": "K. Mukherjee",
                        "slug": "K.-Mukherjee",
                        "structuredName": {
                            "firstName": "Kanchan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mukherjee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 190
                            }
                        ],
                        "text": "Over the past years many proposals have been made to incorporate nonlinearity into these models, through B-splines (see e.g. Lin and Zhang, 1999) and other methods (Brumback and Rice, 1998, Arora et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119939356,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b3bb25096b91d456e93e0d8b645e57ad17878",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Estimation of finite population means is considered when samples are collected using a stratified sampling design. Finite populations for different strata are assumed to be realizations from different superpopulations. The true means of the observations lie on a regression surface with random intercepts for different strata. The true sampling variances are also different and random for different strata. The strata are connected through two common prior distributions, one for the intercepts and another for the sampling variances for all the strata. The model is appropriate in two important survey situations. First, it can be applied to repeated surveys where the physical characteristics of the sampling units change slowly over time. Second, the model is appropriate in small-area estimation problems where a very few samples are available for any particular area. Empirical Bayes estimators of the finite population means are shown to be asymptotically optimal in the sense of Robbins. The proposed emp..."
            },
            "slug": "Empirical-Bayes-Estimation-of-Finite-Population-Arora-Lahiri",
            "title": {
                "fragments": [],
                "text": "Empirical Bayes Estimation of Finite Population Means from Complex Surveys"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47141399"
                        ],
                        "name": "M. Daniels",
                        "slug": "M.-Daniels",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Daniels",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Daniels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3588784"
                        ],
                        "name": "C. Gatsonis",
                        "slug": "C.-Gatsonis",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Gatsonis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gatsonis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 114
                            }
                        ],
                        "text": "The model is tested (Section 5) both on an artificial data set, which consists of samples drawn from a mixture of Gaussians, and on two real-world data sets: the Junior School Problem (predicting test results for British school children) and the Telegraaf problem (predicting newspaper sales in The Netherlands)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 116
                            }
                        ],
                        "text": "The similar tasks in the other example can be the prediction of survival of patients in different clinics (see e.g. Daniels and Gatsonis, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 113
                            }
                        ],
                        "text": "More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 292
                            }
                        ],
                        "text": "In task gating these weights become task-dependent and the value forqi\u03b1 depends on the task-specific feature vectorf i .\nthis way, the posterior distribution effectively \u2018assigns\u2019 tasks to that cluster that is most compatible with the data within the task, in the sense that all other clusters (Gaussians) do contribute much less to (6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 50
                            }
                        ],
                        "text": "Then we could take as a prior a mixture ofnclusterGaussians,\nA i \u223c ncluster\n\u2211 \u03b1=1 q\u03b1N (m\u03b1,\u03a3\u03b1) (5)\ninstead of the single Gaussian (2)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121768372,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "5e4f8d2e1492ab0eeb3f13b5f60847914f887d88",
            "isKey": true,
            "numCitedBy": 106,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In recent years many studies have reported large differences in the use of medical treatments and procedures across geographic regions, hospitals, and individual health care providers. Beyond reporting on the extent of observed variations, these studies examine the role of contributing factors including patient, regional, and provider characteristics. In addition, they may assess the relation between health care processes and outcomes, such as patient mortality, morbidity, and functioning. Studies of variations in health care utilization and outcomes involve the analysis of multilevel clustered data; for example, data on patients clustered by hospital and/or geographic region. The goals of the analysis include the estimation of cluster-specific adjusted responses, covariate effects, and components of variance. The analytic strategy needs to account for correlations induced by clustering and to handle the presence of large variations in cluster size. In this article we formulate a broad class of h..."
            },
            "slug": "Hierarchical-Generalized-Linear-Models-in-the-of-in-Daniels-Gatsonis",
            "title": {
                "fragments": [],
                "text": "Hierarchical Generalized Linear Models in the Analysis of Variations in Health Care Utilization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A broad class of studies of variations in health care utilization and outcomes involve the analysis of multilevel clustered data and the analytic strategy needs to account for correlations induced by clustering and to handle the presence of large variations in cluster size."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2811424"
                        ],
                        "name": "M. Aitkin",
                        "slug": "M.-Aitkin",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Aitkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aitkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197313"
                        ],
                        "name": "N. Longford",
                        "slug": "N.-Longford",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Longford",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Longford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 43
                            }
                        ],
                        "text": "Examples are the school problems (see e.g. Aitkin and Longford, 1986), and clinical trials."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "In this article we extend the model by allowing more differentiation in the similarities between tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64585013,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5aabcbe2e50c1eca2799dc18507c293c50554328",
            "isKey": false,
            "numCitedBy": 827,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The assessment of school effectiveness in educational research studies is considered from the viewpoint of statistical modelling. A variety of models are applied to a set of data on 907 pupils in 18 schools from one Local Education Authority. We argue for the general use of variance component or \"random parameter\" models for the analysis of such studies involving clustered observations. For the data examined, the model which regresses school mean outcome on school mean intake (i.e. the \"means on means\" model) is shown to give estimated school effects considerably different from those produced by other models. In the light of our results, we comment on several recent large-scale British studies of school effectiveness."
            },
            "slug": "Statistical-Modelling-Issues-in-School-Studies-Aitkin-Longford",
            "title": {
                "fragments": [],
                "text": "Statistical Modelling Issues in School Effectiveness Studies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work argues for the general use of variance component or \"random parameter\" models for the analysis of such studies involving clustered observations, and comments on several recent large-scale British studies of school effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 220
                            }
                        ],
                        "text": "To optimize the likelihood of the shared parameters\u039b, which now include all cluster means and covariances as well as the prior assignment probabilitiesq, we can apply an expectation-maximization or EM-algorithm (see e.g. Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 40
                            }
                        ],
                        "text": "This is referred to as empirical Bayes (Robert, 1994), and is similar to MacKay\u2019s evidence framework (MacKay, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120559124,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "6d646a27be187bbb78ec2e76c6bcfaeef9971a3e",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Introduction.- Decision-Theoretic Foundations of Statistical Inference.- From Prior Information to Prior Distributions.- Bayesian Point Estimation.- Tests and Confidence Regions.- Admissibility and Complete Classes.- Invariance, Haar Measures, and Equivariant Estimators.- Hierarchical and Empirical Bayes Extensions.- Bayesian Calculations.- A Defense of the Bayesian Choice."
            },
            "slug": "The-Bayesian-choice-:-a-decision-theoretic-Robert",
            "title": {
                "fragments": [],
                "text": "The Bayesian choice : a decision-theoretic motivation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5527740"
                        ],
                        "name": "S. Raudenbush",
                        "slug": "S.-Raudenbush",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Raudenbush",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudenbush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39465526"
                        ],
                        "name": "A. Bryk",
                        "slug": "A.-Bryk",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bryk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bryk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 39
                            }
                        ],
                        "text": "Both approaches are described in e.g. (Bryk and Raudenbush, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62254677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c92d6fa1e30e12946c874e5a8b9aeee3c0155e29",
            "isKey": false,
            "numCitedBy": 24547,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction The Logic of Hierarchical Linear Models Principles of Estimation and Hypothesis Testing for Hierarchical Linear Models An Illustration Applications in Organizational Research Applications in the Study of Individual Change Applications in Meta-Analysis and Other Cases Where Level-1 Variances are Known Three-Level Models Assessing the Adequacy of Hierarchical Models Technical Appendix"
            },
            "slug": "Hierarchical-Linear-Models:-Applications-and-Data-Raudenbush-Bryk",
            "title": {
                "fragments": [],
                "text": "Hierarchical Linear Models: Applications and Data Analysis Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This chapter discusses Hierarchical Linear Models in Applications, Applications in Organizational Research, and Applications in the Study of Individual Change Applications in Meta-Analysis and Other Cases Where Level-1 Variances are Known."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35082751"
                        ],
                        "name": "R. Bosker",
                        "slug": "R.-Bosker",
                        "structuredName": {
                            "firstName": "Roel",
                            "lastName": "Bosker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bosker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58334305,
            "fieldsOfStudy": [
                "Psychology",
                "Mathematics"
            ],
            "id": "d22d8b80ca404425b37eb471e83061f943f4ec11",
            "isKey": false,
            "numCitedBy": 1573,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Boekbespreking-van-\"A.S.-Bryk-&-S.W.-Raudenbusch-:-Bosker",
            "title": {
                "fragments": [],
                "text": "Boekbespreking van \"A.S. Bryk & S.W. Raudenbusch - Hierarchical linear models: Applications and data analysis methods\" : Sage Publications, Newbury Parki, London/New Delhi 1992"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "These results are consistent with the more extended simulation studies in (Heskes, 1998, 2000)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving a huge number of similar tasks: a combination of multi-task learning and hierarchical Bayesian modeling"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 9
                            }
                        ],
                        "text": "See also Mortimore et al. (1988)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "School Matters. Wells: Open Books"
            },
            "venue": {
                "fragments": [],
                "text": "School Matters. Wells: Open Books"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 437,
                                "start": 65
                            }
                        ],
                        "text": "Lin and Zhang, 1999) and other methods (Brumback and Rice, 1998, Arora et al., 1997). To the best of our knowledge, the ideas of task clustering and gating are new to this field. An alternative approach to multitask learning has been taken by Thrun and O\u2019Sullivan (1996) and Pratt (1992), who have devised elegant ways to transfer knowledge obtained by one network to another network learning a similar task. Thrun and O\u2019Sullivan (1996) also suggest a task clustering algorithm, where a distance metric between parallel tasks is learned, and used to classify new learning tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 65
                            }
                        ],
                        "text": "Lin and Zhang, 1999) and other methods (Brumback and Rice, 1998, Arora et al., 1997). To the best of our knowledge, the ideas of task clustering and gating are new to this field. An alternative approach to multitask learning has been taken by Thrun and O\u2019Sullivan (1996) and Pratt (1992), who have devised elegant ways to transfer knowledge obtained by one network to another network learning a similar task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical Bayes estimation of finite population means from complex surveys.Journal of the American Statistical Association"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 17,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes/a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8?sort=total-citations"
}