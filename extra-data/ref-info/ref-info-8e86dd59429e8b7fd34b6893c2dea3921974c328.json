{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32562635"
                        ],
                        "name": "Liunian Harold Li",
                        "slug": "Liunian-Harold-Li",
                        "structuredName": {
                            "firstName": "Liunian",
                            "lastName": "Li",
                            "middleNames": [
                                "Harold"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liunian Harold Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144508458"
                        ],
                        "name": "Da Yin",
                        "slug": "Da-Yin",
                        "structuredName": {
                            "firstName": "Da",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Da Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "3 Table 2: Performance of VisualBERT (Li et al., 2019) and LXMERT (Tan and Bansal, 2019) on the three evaluation sets of NLVR2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 23
                            }
                        ],
                        "text": "We evaluate VisualBERT (Li et al., 2019) and LXMERT (Tan and Bansal, 2019) using the subset of the evaluation data and show that they largely maintain their reported performance, although there is a small drop in performance on the balanced set and a small increase in performance on the unbalanced set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "We evaluate VisualBERT (Li et al., 2019) and LXMERT (Tan and Bansal, 2019) using the subset of the evaluation data and show that they largely maintain their reported performance, although there is a small drop in performance on the balanced set and a small increase in performance on the unbalanced\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 48
                            }
                        ],
                        "text": "We show that the performance of existing models (Li et al., 2019; Tan and Bansal, 2019) is relatively robust to this potential bias."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 199528533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "isKey": true,
            "numCitedBy": 630,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "slug": "VisualBERT:-A-Simple-and-Performant-Baseline-for-Li-Yatskar",
            "title": {
                "fragments": [],
                "text": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32849969"
                        ],
                        "name": "Alane Suhr",
                        "slug": "Alane-Suhr",
                        "structuredName": {
                            "firstName": "Alane",
                            "lastName": "Suhr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alane Suhr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49219517"
                        ],
                        "name": "Stephanie Zhou",
                        "slug": "Stephanie-Zhou",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephanie Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83384205"
                        ],
                        "name": "Iris Zhang",
                        "slug": "Iris-Zhang",
                        "structuredName": {
                            "firstName": "Iris",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iris Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067571140"
                        ],
                        "name": "Huajun Bai",
                        "slug": "Huajun-Bai",
                        "structuredName": {
                            "firstName": "Huajun",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huajun Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Suhr et al. (2019) note that forcing the label assignment\n1License information for images in Figure 1b, from top left to bottom right: Charles Rondeau (CC0), Hagerty Ryan, USFWS (CC0), Andale (CC0), Charles Rondeau (CC0), George Hodan (CC0), Maksym Pyrizhok (PDP), Peter Griffin (CC0), Petr\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 7
                            }
                        ],
                        "text": "NLVR2 (Suhr et al., 2019) uses two paired real images and a natural language statement for a similar classification setup, where the goal is to decide if the statement is true or false with the regard to the pair of images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "(b) Examples from NLVR2 (Suhr et al., 2019)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 6
                            }
                        ],
                        "text": "NLVR2 (Suhr et al., 2019) was designed to be robust for language bias through a data collection process that resulted in each natural language sentence appearing with both true and false labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "NLVR (Suhr et al., 2017) and NLVR2 Suhr et al. (2019) are two recently proposed benchmarks for visual reasoning with natural language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53178856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf336d272a30d6ad6141db67faa64deb8791cd61",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge."
            },
            "slug": "A-Corpus-for-Reasoning-about-Natural-Language-in-Suhr-Zhou",
            "title": {
                "fragments": [],
                "text": "A Corpus for Reasoning about Natural Language Grounded in Photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work introduces a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges, and Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40060272"
                        ],
                        "name": "Omer Goldman",
                        "slug": "Omer-Goldman",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Goldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29995084"
                        ],
                        "name": "Veronica Latcinnik",
                        "slug": "Veronica-Latcinnik",
                        "structuredName": {
                            "firstName": "Veronica",
                            "lastName": "Latcinnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veronica Latcinnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51180269"
                        ],
                        "name": "Udi Naveh",
                        "slug": "Udi-Naveh",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Naveh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Udi Naveh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750652"
                        ],
                        "name": "Jonathan Berant",
                        "slug": "Jonathan-Berant",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Berant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Berant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "We only report accuracy on this subset of the data because consistency (Goldman et al., 2018) changes due to the example selection process and we want to avoid confusion with the consistency measures of the existing splits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1576593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c460bfcdd3cda8a787cbfe920dab41136f2dc129",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far."
            },
            "slug": "Weakly-Supervised-Semantic-Parsing-with-Abstract-Goldman-Latcinnik",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Semantic Parsing with Abstract Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes that in closed worlds with clear semantic types, one can substantially alleviate problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form and results in sharing between different examples that alleviates the difficulties in training."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32849969"
                        ],
                        "name": "Alane Suhr",
                        "slug": "Alane-Suhr",
                        "structuredName": {
                            "firstName": "Alane",
                            "lastName": "Suhr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alane Suhr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053174592"
                        ],
                        "name": "James Yeh",
                        "slug": "James-Yeh",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 6
                            }
                        ],
                        "text": "NLVR (Suhr et al., 2017) and NLVR2 Suhr et al. (2019) are two recently proposed benchmarks for visual reasoning with natural language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 45
                            }
                        ],
                        "text": "C L\n] 2\n3 Se\np 20\n19\n(a) Examples from NLVR (Suhr et al., 2017)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19435386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9e28863c7fb963b40a379c5a4e0da00eb031933",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research."
            },
            "slug": "A-Corpus-of-Natural-Language-for-Visual-Reasoning-Suhr-Lewis",
            "title": {
                "fragments": [],
                "text": "A Corpus of Natural Language for Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method of crowdsourcing linguistically-diverse data, and an analysis of the data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3218666"
                        ],
                        "name": "Hao Hao Tan",
                        "slug": "Hao-Hao-Tan",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Tan",
                            "middleNames": [
                                "Hao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Hao Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 48
                            }
                        ],
                        "text": "We show that the performance of existing models (Li et al., 2019; Tan and Bansal, 2019) is relatively robust to this potential bias."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 19
                            }
                        ],
                        "text": ", 2019) and LXMERT (Tan and Bansal, 2019) using the subset of the evaluation data and show that they largely maintain their reported performance, although there is a small drop in performance on the balanced set and a small increase in performance on the unbalanced set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 19
                            }
                        ],
                        "text": ", 2019) and LXMERT (Tan and Bansal, 2019) on the three evaluation sets of NLVR2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 53
                            }
                        ],
                        "text": "We evaluate VisualBERT (Li et al., 2019) and LXMERT (Tan and Bansal, 2019) using the subset of the evaluation data and show that they largely maintain their reported performance, although there is a small drop in performance on the balanced set and a small increase in performance on the unbalanced\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "We evaluate VisualBERT (Li et al., 2019) and LXMERT (Tan and Bansal, 2019) using the subset of the evaluation data and show that they largely maintain their reported performance, although there is a small drop in performance on the balanced set and a small increase in performance on the unbalanced set."
                    },
                    "intents": []
                }
            ],
            "corpusId": 201103729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79c93274429d6355959f1e4374c2147bb81ea649",
            "isKey": true,
            "numCitedBy": 916,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert"
            },
            "slug": "LXMERT:-Learning-Cross-Modality-Encoder-from-Tan-Bansal",
            "title": {
                "fragments": [],
                "text": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework, a large-scale Transformer model that consists of three encoders, achieves the state-of-the-art results on two visual question answering datasets and shows the generalizability of the pre-trained cross-modality model."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 5,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/NLVR2-Visual-Bias-Analysis-Suhr-Artzi/8e86dd59429e8b7fd34b6893c2dea3921974c328?sort=total-citations"
}