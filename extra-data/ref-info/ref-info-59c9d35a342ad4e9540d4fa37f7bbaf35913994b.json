{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059594591"
                        ],
                        "name": "Michal Roth",
                        "slug": "Michal-Roth",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13146480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2",
            "isKey": false,
            "numCitedBy": 614,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or \\dynamic gestures\", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,"
            },
            "slug": "Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth",
            "title": {
                "fragments": [],
                "text": "Orientation Histograms for Hand Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A method to recognize hand gestures, based on a pattern recognition technique developed by McConnell employing histograms of local orientation, which is simple and fast to compute, and which can distinguish a small vocabulary of about 10 hand gestures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109665"
                        ],
                        "name": "K. Kyuma",
                        "slug": "K.-Kyuma",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Kyuma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kyuma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50395926"
                        ],
                        "name": "E. Lange",
                        "slug": "E.-Lange",
                        "structuredName": {
                            "firstName": "Eberhard",
                            "lastName": "Lange",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145434530"
                        ],
                        "name": "J. Ohta",
                        "slug": "J.-Ohta",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40804177"
                        ],
                        "name": "A. Hermanns",
                        "slug": "A.-Hermanns",
                        "structuredName": {
                            "firstName": "Anno",
                            "lastName": "Hermanns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hermanns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50702668"
                        ],
                        "name": "B. Banish",
                        "slug": "B.-Banish",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Banish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Banish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48341311"
                        ],
                        "name": "M. Oita",
                        "slug": "M.-Oita",
                        "structuredName": {
                            "firstName": "Masaya",
                            "lastName": "Oita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12821254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e484db5b8a30e0eeb3f2e9f6181a9e07a4275e62",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial retinas combine video camera and image processing functions, allowing machines to function in their environment with unprecedented autonomy, or to augment quality control, surveillance and hazard monitoring. We review several retina devices and reveal how they execute basic manipulations of the image at processing speeds well beyond the capabilities of the human eye."
            },
            "slug": "Artificial-retinas-\u0097-fast,-versatile-image-Kyuma-Lange",
            "title": {
                "fragments": [],
                "text": "Artificial retinas \u0097 fast, versatile image processors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work reviews several retina devices and reveals how they execute basic manipulations of the image at processing speeds well beyond the capabilities of the human eye."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17525960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e88ca837b122a9c9e546db5395b451f27ea01f19",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe our work on 3-D model-based tracking and recognition of human movement from real images. Our system has two major components. The rst component takes real image sequences acquired from multiple views and recovers the 3-D body pose at each time instant. The pose-recovery problem is formulated as a search problem and entails nding the pose parameters of a graphical human model for which its synthesized appearance is most similar to the actual appearance of the real human in the multi-view images. Currently, we use a best-rst search technique and chamfer matching as a fast similarity measure between synthesized and real edge images. The second component of our system deals with the representation and recognition of human movement patterns. The recognition of human movement patterns is considered as a classiication problem involving the matching of a test sequence with several reference sequences representing prototypical activities. A variation of dynamic time-warping is used to match movement patterns using 3-D joint angles as features. We illustrate our approach on real data acquired simultaneously from three views and data derived from stereo Moving Light Displays with diierent types of hand-gestures."
            },
            "slug": "Towards-3-D-model-based-tracking-and-recognition-of-Gavrila-Davis",
            "title": {
                "fragments": [],
                "text": "Towards 3-D model-based tracking and recognition of human movement: a multi-view approach"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "3-D model-based tracking and recognition of human movement from real images, and a variation of dynamic time-warping is used to match movement patterns using 3-D joint angles as features."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The response time of the vision interface should be less than a video frame time and the interface should cost less than $50 U.S."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3175562,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "4ef915fa9e5b2260d4b45927c0033a7ba53bf66e",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of local parametrized models of image motion for recovering and recognizing the non-rigid and articulated motion of human faces. Parametric flow models (for example affine) are popular for estimating motion in rigid scenes. We observe that within local regions in space and time, such models not only accurately model non-rigid facial motions but also provide a concise description of the motion in terms of a small number of parameters. These parameters are intuitively related to the motion of facial features during facial expressions and we show how expressions such as anger, happiness, surprise, fear, disgust and sadness can be recognized from the local parametric motions in the presence of significant head motion. The motion tracking and expression recognition approach performs with high accuracy in extensive laboratory experiments involving 40 subjects as well as in television and movie sequences.<<ETX>>"
            },
            "slug": "Tracking-and-recognizing-rigid-and-non-rigid-facial-Black-Yacoob",
            "title": {
                "fragments": [],
                "text": "Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper explores the use of local parametrized models of image motion for recovering and recognizing the non-rigid and articulated motion of human faces and shows how expressions can be recognized from the local parametric motions in the presence of significant head motion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448449"
                        ],
                        "name": "Y. J. Tejwani",
                        "slug": "Y.-J.-Tejwani",
                        "structuredName": {
                            "firstName": "Yogendra",
                            "lastName": "Tejwani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. J. Tejwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The rst exploits the image projection capabilities of the AR module; the second uses its ability to quickly calculate x and y derivatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 713769,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3e5ad6a48fe0b8e97825e9ec1e831d18eeb5605a",
            "isKey": false,
            "numCitedBy": 2456,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A scheme is developed for classifying the types of motion perceived by a humanlike robot. It is assumed that the robot receives visual images of the scene using a perspective system model. Equations, theorems, concepts, clues, etc., relating the objects, their positions, and their motion to their images on the focal plane are presented.<<ETX>>"
            },
            "slug": "Robot-vision-Tejwani",
            "title": {
                "fragments": [],
                "text": "Robot vision"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A scheme is developed for classifying the types of motion perceived by a humanlike robot and equations, theorems, concepts, clues, etc., relating the objects, their positions, and their motion to their images on the focal plane are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Symposium on Circuits and Systems,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "The response time of the vision interface should be less than a video frame time and the interface should cost less than $50 U.S."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14814282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66505cb708b098a93331471f079965f6ded4ea7f",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of real example views at different poses. But what if we only have one real view available, such as a scanned passport photo-can we still recognize faces under different poses? Given one real view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We develop example-based techniques for applying the rotation seen in the prototypes to essentially \"rotate\" the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views for a view-based, pose-invariant face recognizer. Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques.<<ETX>>"
            },
            "slug": "Face-recognition-from-one-example-view-Beymer-Poggio",
            "title": {
                "fragments": [],
                "text": "Face recognition from one example view"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 30
                            }
                        ],
                        "text": "1 Image Moments Image moments [9, 1] provide useful summaries of global image information, and have been applied to shape analysis or other tasks, often for binary images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208784962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ff65ac698013cdd9d61326cab49a1d75404e001",
            "isKey": false,
            "numCitedBy": 18721,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editors",
            "title": {
                "fragments": [],
                "text": "Editors"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Research Bulletin"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Davis . Towards 3 - d model - based tracking and recognition of human movement : a multiview approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 184
                            }
                        ],
                        "text": "Figure 1 shows the elements of the AR chip: a 2-D array of variable sensitivity photodetection cells (VSPC), a random access scanner for sensitivity control, and an output multiplexer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPIE"
            },
            "venue": {
                "fragments": [],
                "text": "2597(283)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 64
                            }
                        ],
                        "text": "Others have developed algorithms to identify the pose of a gure [8, 12, 5], but those algorithms do not meet our speed and cost requirements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Posture estimation in reduced-model gesture input systems"
            },
            "venue": {
                "fragments": [],
                "text": "M. Bichsel, editor, Intl. Workshop on automatic face- and gesture-recognition, pages 290{ 295, Zurich, Switzerland"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robot vzsion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Vzszon"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 5th Intl. Conf. on Computer Vision,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "The response time of the vision interface should be less than a video frame time and the interface should cost less than $50 U.S."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Workshop on Automatic Face-and Gesture-Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "International Workshop on Automatic Face-and Gesture-Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Posture estimation in reduced-model gesture input systems Workshop on automatic face-and gesture-recognition, pages 290{ 295"
            },
            "venue": {
                "fragments": [],
                "text": "Posture estimation in reduced-model gesture input systems Workshop on automatic face-and gesture-recognition, pages 290{ 295"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Internatzonal Workshop on Automatic Face- and Gesture- Recognztion"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer Society,"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Computer-vision-for-computer-games-Freeman-Tanaka/59c9d35a342ad4e9540d4fa37f7bbaf35913994b?sort=total-citations"
}