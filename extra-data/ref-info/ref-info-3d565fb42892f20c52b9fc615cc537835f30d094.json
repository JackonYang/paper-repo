{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "{See (Wolpert 1992) and (Buntine and Weigend 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 6
                            }
                        ],
                        "text": "{See (Wolpert 1992) and (Buntine and Weigend 1991). and allow the output values in L to range\nOn the Use of Evidence in Neural Networks 541\nfrom -00 to +00.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16602772,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dadf60c10e4bc99672064adf1d42f07a754d446b",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper first reviews the reasoning behind the Bayesian \"evidence\" procedure for setting parameters in the probability distributions involved in inductive inference. This paper then proves that the evidence procedure is incorrect. More precisely, this paper proves that the assumptions going into the evidence procedure do not, as claimed, \"let the data determine the distributions\". Instead, those assumptions simply amount to an implicit replacement of the original distributions, containing free parameters, with new distributions, none of whose parameters are free. For example, as used by MacKay [1991] in the context of neural nets, the evidence procedure is a means for using the training set to determine the free parameter ex in the distribution P(Iwil) oc exp(ro:: 1 Wi2), where the N Wi are the N weights in the network. As this paper proves, in actuality the assumptions going into MacKay's use of the evidence procedure do not result in a distribution P(lwil) oc exp(ro::1 w?) for some ex, but rather result in a parameter-less distribution, P(lwil) oc (L:1 w?r CN!2 + 1). This paper goes on to prove that ifone makes the assumption of an \"entropic prior\" with unknown parameter value, in addition to the assumptions used in the evidence procedure, then the prior is completely fixed, but in a form which can not be entropic. (This calls into question the self-consistency of the numerous arguments purporting to derive an entropic prior \"from first principles\".) Finally, this paper goes on to investigate the Bayesian first-principles \"proof' of Occam's razor involving Occam factors. This paper proves that that \"proof' is flawed."
            },
            "slug": "A-Rigorous-Investigation-of-\u201cEvidence\u201d-and-\u201cOccam-Wolpert",
            "title": {
                "fragments": [],
                "text": "A Rigorous Investigation of \u201cEvidence\u201d and \u201cOccam Factors\u201d in Bayesian Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 281
                            }
                        ],
                        "text": "\u2026on the data L, it would appear that when the evidence approximation is valid, the data determines the prior, or as MacKay puts it, \"the modem Bayesian ... does not assign the priors - many different priors can be ... compared in the light of the data by evaluating the evidence\" (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 140
                            }
                        ],
                        "text": "For any other a in an interval extending three orders of magnitude about this a', test set error is essentially unchanged (see figure 5 of (MacKay 1992\u00bb."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 15
                            }
                        ],
                        "text": "In paper 2 of (MacKay 1992) the initial use of the evidence approximation is \"a failure of Bayesian prediction\"; P(y I L) doesn't correlate with test set error (see figure 7)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "compared in the light of the data by evaluating the evidence\u201d (MacKay 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 213
                            }
                        ],
                        "text": "MacKay has applied the evidence approximation to finding the posterior for the neural net pew I a.) and P(L I w.~) recounted above combined with a P{y) = P{a., ~) which is uniform over all a. and ~ from 0 to +00 (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For any other \u03b1 in an interval extending three orders of magnitude about this \u03b1', test set error is essentially unchanged (see figure 5 of (MacKay 1992))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The analysis in (MacKay 1992) mentions condition (ii) as well, but not condition (iii)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In paper 2 of (MacKay 1992) the initial use of the evidence approximation is \u201ca failure of Bayesian prediction\u201d; P(\u03b3 | L) doesn\u2019t correlate with test set error (see figure 7 of that paper)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "It has recently become popular to consider the problem of training neural nets from a Bayesian viewpoint (Buntine and Weigend 1991, MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 66
                            }
                        ],
                        "text": "In addition, the tests of the evidence approximation reported in (MacKay 1992) are not all that convincing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 91
                            }
                        ],
                        "text": "This is, of course, a silly answer, and reflects the poor choice of distributions made in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is not clear from the provided neural net data whether the test of theorem 4 is passed in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 178
                            }
                        ],
                        "text": "Indeed, consider the simple case where the noise is fixed, i.e., P(y) = P(YI) O(Y2 - Pt), so that the only term we must \"deal with\" is YI = a. Set all other distributions as in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "MacKay has applied the evidence approximation to finding the posterior for the neural net P(w | \u03b1) and P(L | w, \u03b2) recounted above combined with a P(\u03b3) = P(\u03b1, \u03b2) which is uniform over all \u03b1 and \u03b2 from 0 to +\u221e (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 84
                            }
                        ],
                        "text": "It is not clear from the provided neural net data whether this condition is met in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Set all other distributions as in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In any case, close scrutiny of the tests of the evidence approximation reported in (MacKay 1992) reveals those tests to be less than fully convincing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, given the poor choice of distributions in (MacKay 1992), one might argue that using an approximation which induces a large error is quite sensible, since doing so allows one to avoid the silly answers demanded by those poor distributions under the exact calculation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": true,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 281
                            }
                        ],
                        "text": "\u2026on the data L, it would appear that when the evidence approximation is valid, the data determines the prior, or as MacKay puts it, \"the modem Bayesian ... does not assign the priors - many different priors can be ... compared in the light of the data by evaluating the evidence\" (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 140
                            }
                        ],
                        "text": "For any other a in an interval extending three orders of magnitude about this a', test set error is essentially unchanged (see figure 5 of (MacKay 1992\u00bb."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 15
                            }
                        ],
                        "text": "In paper 2 of (MacKay 1992) the initial use of the evidence approximation is \"a failure of Bayesian prediction\"; P(y I L) doesn't correlate with test set error (see figure 7)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 213
                            }
                        ],
                        "text": "MacKay has applied the evidence approximation to finding the posterior for the neural net pew I a.) and P(L I w.~) recounted above combined with a P{y) = P{a., ~) which is uniform over all a. and ~ from 0 to +00 (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "It has recently become popular to consider the problem of training neural nets from a Bayesian viewpoint (Buntine and Weigend 1991, MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 66
                            }
                        ],
                        "text": "In addition, the tests of the evidence approximation reported in (MacKay 1992) are not all that convincing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 91
                            }
                        ],
                        "text": "This is, of course, a silly answer, and reflects the poor choice of distributions made in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 178
                            }
                        ],
                        "text": "Indeed, consider the simple case where the noise is fixed, i.e., P(y) = P(YI) O(Y2 - Pt), so that the only term we must \"deal with\" is YI = a. Set all other distributions as in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 84
                            }
                        ],
                        "text": "It is not clear from the provided neural net data whether this condition is met in (MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": true,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 106
                            }
                        ],
                        "text": "It has recently become popular to consider the problem of training neural nets from a Bayesian viewpoint (Buntine and Weigend 1991, MacKay 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 25
                            }
                        ],
                        "text": "{See (Wolpert 1992) and (Buntine and Weigend 1991). and allow the output values in L to range\nOn the Use of Evidence in Neural Networks 541\nfrom -00 to +00.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 95
                            }
                        ],
                        "text": "This Bayesian approach is the starting point for the \"evidence\" approximation created by Gull {Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118754484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82fa37d5be8e747131a5857992cc33bb95469ce3",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian derivation of \u201cClassic\u201d MaxEnt image processing (Skilling 1989a) shows that exp(\u03b1S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of \u201cClassic\u201d MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter \u03b1, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2\u03b1S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image."
            },
            "slug": "Developments-in-Maximum-Entropy-Data-Analysis-Gull",
            "title": {
                "fragments": [],
                "text": "Developments in Maximum Entropy Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", (Skilling 1989)) suffer from this problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118543247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b35e86ce2b9947c23a13fed84cb66ae84bc2f6d",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction. The argument repeatedly goes from the particular to the general, in that if there are general theories then they must apply to special cases. Two such special cases, formalised as the \u201cCox axioms \u201c, lead to the well-known fact that Bayesian probability theory is the only consistent language of inference. Further cases, formalised as the axioms of maximum entropy, show that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy. Finally, a quantified special case shows that this monotonic function must be the exponential, leaving only a single dimensional scaling factor to be determined a posteriori. Many types of distribution, including probability distributions themselves, are positive and additive, so the entropy exponential is very general."
            },
            "slug": "Classic-Maximum-Entropy-Skilling",
            "title": {
                "fragments": [],
                "text": "Classic Maximum Entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction, formalised as the axioms ofmaximum entropy, which shows that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144465303"
                        ],
                        "name": "C. Strauss",
                        "slug": "C.-Strauss",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Strauss",
                            "middleNames": [
                                "E.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Strauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36878862"
                        ],
                        "name": "D. R. Wolf",
                        "slug": "D.-R.-Wolf",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolf",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Wolf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117713287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d46f510f2419567204053ec5a4fb308e73c57fb",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "First, the correct entropic prior is computed by marginalization of alpha. This is followed by a discussion of improvements to the \u201cevidence\u201d approximation. Surprisingly, it appears that the approximations used to restore the famous \u201cSusie\u201d image may have questionable aspects."
            },
            "slug": "Alpha,-Evidence,-and-the-Entropic-Prior-Strauss-Wolpert",
            "title": {
                "fragments": [],
                "text": "Alpha, Evidence, and the Entropic Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The correct entropic prior is computed by marginalization of alpha, and the approximations used to restore the famous \u201cSusie\u201d image may have questionable aspects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150910777"
                        ],
                        "name": "A. R. Davies",
                        "slug": "A.-R.-Davies",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Davies",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. R. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143744122"
                        ],
                        "name": "R. Anderssen",
                        "slug": "R.-Anderssen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Anderssen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Anderssen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123565120,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "691edda4156c41f13c7f75411e941fa38ab0633e",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the role played by optimization in the choice of parameters for Tikhonov regularization of first-kind integral equations. Asymptotic analyses are presented for a selection of practical optimizing methods applied to a model deconvolution problem. These methods include the discrepancy principle, cross-validation and maximum likelihood. The relationship between optimality and regularity is emphasized. New bounds on the constants appearing in asymptotic estimates are presented."
            },
            "slug": "Optimisation-in-the-regularisation-ill-posed-Davies-Anderssen",
            "title": {
                "fragments": [],
                "text": "Optimisation in the regularisation ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Australian Mathematical Society. Series B. Applied Mathematics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acknowledgments This work was done at the SF! and was supported in part by NLM grant F37 LMOOOll. I would like to thank Charlie Strauss and Tim Wallstrom for stimulating discussion"
            },
            "venue": {
                "fragments": [],
                "text": "Acknowledgments This work was done at the SF! and was supported in part by NLM grant F37 LMOOOll. I would like to thank Charlie Strauss and Tim Wallstrom for stimulating discussion"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert et al. 1993).)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 27
                            }
                        ],
                        "text": "(See (Strauss et al. 1993, Wolpert et al. 1993).)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1993).On evidenceandthemarginalization of alpha in the entropic prior"
            },
            "venue": {
                "fragments": [],
                "text": "In preparation"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation . A Practical Framework for Backpropagation Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "< e + I y -o dy pew, y 1 L), this means that pew 1 L) < e(1 + 20 I r). So if pew I L) > c > e, r < 2el (c -e), and there must be a peak ofP(w, y 1 L)"
            },
            "venue": {
                "fragments": [],
                "text": "< e + I y -o dy pew, y 1 L), this means that pew 1 L) < e(1 + 20 I r). So if pew I L) > c > e, r < 2el (c -e), and there must be a peak ofP(w, y 1 L)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization in the regularization of illposed problems"
            },
            "venue": {
                "fragments": [],
                "text": "J . Australian Math . Soc . Ser . B"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classic maximum entropy . In \u201c Maximum - entropy and Bayesian meth"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 94
                            }
                        ],
                        "text": "This procedure is a close relative of non-Bayesian statistics' generalized maximum likelihood (Davies and Anderssen 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization in the regularization of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": "J. Australian Math. Soc. Ser. B, 28, 114."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert et al. 1993).)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 27
                            }
                        ],
                        "text": "(See (Strauss et al. 1993, Wolpert et al. 1993).)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On evidence and the marginalization of alpha in the entropic prior"
            },
            "venue": {
                "fragments": [],
                "text": "In preparation"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation. A Practical Framework for Backpropagation Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classic maximum entropy. 1n \"Maximum-entropy and Bayesian methods Kluwer Academics publishers"
            },
            "venue": {
                "fragments": [],
                "text": "J. Skilling"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 58
                            }
                        ],
                        "text": "Note that the proof would go through even if P(y I L) were not peaked about \"I, or if P(y I L) were peaked about some point far from the \"I for which (ii) and (iii) hold; nowhere in the proof is the definition of \"I from condition (i) used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classic maximum entropy Maximum-entropy and Bayesian methods Kluwer Academics publishers"
            },
            "venue": {
                "fragments": [],
                "text": "J. Skilling"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-Use-of-Evidence-in-Neural-Networks-Wolpert/3d565fb42892f20c52b9fc615cc537835f30d094?sort=total-citations"
}