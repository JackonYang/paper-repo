{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626983"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10436583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling a collection of similar regression or classification tasks can be improved by making the tasks 'learn from each other'. In machine learning, this subject is approached through 'multitask learning', where parallel tasks are modeled as multiple outputs of the same network. In multilevel analysis this is generally implemented through the mixed-effects linear model where a distinction is made between 'fixed effects', which are the same for all tasks, and 'random effects', which may vary between tasks. In the present article we will adopt a Bayesian approach in which some of the model parameters are shared (the same for all tasks) and others more loosely connected through a joint prior distribution that can be learned from the data. We seek in this way to combine the best parts of both the statistical multilevel approach and the neural network machinery. The standard assumption expressed in both approaches is that each task can learn equally well from any other task. In this article we extend the model by allowing more differentiation in the similarities between tasks. One such extension is to make the prior mean depend on higher-level task characteristics. More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians. This can be further generalized to a mixture of experts architecture with the gates depending on task characteristics. All three extensions are demonstrated through application both on an artificial data set and on two real-world problems, one a school problem and the other involving single-copy newspaper sales."
            },
            "slug": "Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes",
            "title": {
                "fragments": [],
                "text": "Task Clustering and Gating for Bayesian Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Bayesian approach is adopted in which some of the model parameters are shared and others more loosely connected through a joint prior distribution that can be learned from the data to combine the best parts of both the statistical multilevel approach and the neural network machinery."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395754302"
                        ],
                        "name": "Joseph O'Sullivan",
                        "slug": "Joseph-O'Sullivan",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph O'Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "There have also been various attempts to theoretically study multi\u2013task learning, see [4, 5, 6, 7, 8, 15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15352445,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "c92689ab1e100998231e8c8e35da21817f646f47",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been an increased interest in machine learning methods that transfer knowledge across multiple learning tasks and \u201clearn to learn.\u201d Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading."
            },
            "slug": "Clustering-Learning-Tasks-and-the-Selective-of-Thrun-O'Sullivan",
            "title": {
                "fragments": [],
                "text": "Clustering Learning Tasks and the Selective Cross-Task Transfer of Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "To increase robustness of machine learning approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading."
            },
            "venue": {
                "fragments": [],
                "text": "Learning to Learn"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "We assume that Pt is different for each task but that the Pt\u2019s are related \u2013 as, for example, considered in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "A statistical learning theory based approach to multi\u2013task learning has been developed in [5, 6] and [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "There have also been various attempts to theoretically study multi\u2013task learning, see [4, 5, 6, 7, 8, 15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "In particular, it may be possible to link the matrix\u2013valued kernels to the notion of relatedness between tasks discussed in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [8] the extended VC dimension was used to derive tighter bounds that hold for each task (not just the average error among tasks as considered in [6]) in the case that the learning tasks are related in a particular way defined."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13967968,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a4551508f84a3f5447d3490b2db95b4d87a7969",
            "isKey": true,
            "numCitedBy": 345,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The approach of learning of multiple \u201crelated\u201d tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow \u201calgorithmically related\u201d, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "slug": "Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely",
            "title": {
                "fragments": [],
                "text": "Exploiting Task Relatedness for Mulitple Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work offers an alternative approach to multiple task learning, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "We compared our method with Hierarchical Bayes (HB) [1, 2] which is considered to be a state of the art method for preference modeling of a population of individual consumers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 156
                            }
                        ],
                        "text": "There has been a lot of experimental work showing the benefits of such multi\u2013task learning relative to individual task learning when tasks are related, see [4, 11, 15, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "There have also been various attempts to theoretically study multi\u2013task learning, see [4, 5, 6, 7, 8, 15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "A Hierarchical Bayes Model of Primary and Secondary Demand."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 168
                            }
                        ],
                        "text": "In other words we assume that the tasks are related in a way that the true models are all close to some model w0 (playing the role of the mean of the Gaussian used for Hierarchical Bayes [1, 2])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 161
                            }
                        ],
                        "text": "The results a) show the strength of the proposed approach relative to other multi\u2013task learning methods, and b) verify, in agreement with past experimental work [4, 11, 15], the advantage of multi\u2013task learning relative to single task learning."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 43
                            }
                        ],
                        "text": "For example, hierarchical Bayesian methods [1, 2, 4, 15] assume that all functions wt come from a particular probability distribution such as a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 9
                            }
                        ],
                        "text": "Finally, [4, 15] suggest a similar hierarchical model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "We follow the intuition of Hierarchical Bayes [1, 2, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1376989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "446d6b48f79fce24cb12f293e3b161112be261a6",
            "isKey": true,
            "numCitedBy": 105,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new model for studying mul-titask learning, linking theoretical results to practical simulations. In our model all tasks are combined in a single feedforward neu-ral network. Learning is implemented in a Bayesian fashion. In this Bayesian framework the hidden-to-output weights, being speciic to each task, play the role of model parameters. The input-to-hidden weights, which are shared between all tasks, are treated as hyperparameters. Other hyper-parameters describe error variance and correlations and priors for the model parameters. An important feature of our model is that the probability of these hyperparam-eters given the data can be computed ex-plicitely and only depends on a set of suu-cient statistics. None of these statistics scales with the number of tasks or patterns, which makes empirical Bayes for multitask learning a relatively straightforward optimization problem. Simulations on real-world data sets on single-copy newspaper and magazine sales illustrate properties of multitask learning. Most notably we derive experimental curves for \\learning to learn\" that can be linked to theoretical results obtained elsewhere."
            },
            "slug": "Empirical-Bayes-for-Learning-to-Learn-Heskes",
            "title": {
                "fragments": [],
                "text": "Empirical Bayes for Learning to Learn"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new model for studying mul-titask learning is presented, linking theoretical results to practical simulations, and experimental curves for \"learning to learn\" that can be linked to theoretical results obtained elsewhere are derived."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49601276"
                        ],
                        "name": "D. Silver",
                        "slug": "D.-Silver",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Silver",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Silver"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 527678,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "cb48a7a46f9bc3ba74ecd728e91866d259a7982f",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "With a distinction made between two forms of task knowledge transfer, 'representational' and 'functional', eta MTL, a modified version of the multiple task learning (MTL) method of functional (parallel) transfer, is introduced. The eta MTL method employs a separate learning rate, etak, for each task output node k. etak varies as a function of a measure of relatedness, Rk, between the kth task and the primary task of interest. Results of experiments demonstrate the ability of eta MTL to dynamically select the most related source task(s) for the functional transfer of prior domain knowledge. The eta MTL method of learning is nearly equivalent to standard MTL when all parallel tasks are sufficiently related to the primary task, and is similar to single task learning when none of the parallel tasks are related to the primary task."
            },
            "slug": "The-Parallel-Transfer-of-Task-Knowledge-Using-Rates-Silver",
            "title": {
                "fragments": [],
                "text": "The Parallel Transfer of Task Knowledge Using Dynamic Learning Rates Based on a Measure of Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results of experiments demonstrate the ability of eta MTL to dynamically select the most related source task(s) for the functional transfer of prior domain knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 6 ] the problem of bias learning is considered, where the goal is to choose an optimal hypothesis space from a family of hypothesis spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [8] the extended VC dimension was used to derive tighter bounds that hold for each task (not just the average error among tasks as considered in [ 6 ]) in the case that the learning tasks are related in a particular way defined."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 6 ] the notion of the \u201cextended VC dimension\u201d (for a family of hypothesis spaces) is defined and it is used to derive generalization bounds on the average error of T tasks learned which is shown to decrease at best as 1"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have also been various attempts to theoretically study multi\u2013task learning, see [4, 5,  6 , 7, 8, 15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A statistical learning theory based approach to multi\u2013task learning has been developed in [5,  6 ] and [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": true,
            "numCitedBy": 973,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 90
                            }
                        ],
                        "text": "In many practical situations a number of statistical models need to be estimated from data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "\u2026i.e. a face, from\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Keywords Multi\u2013Task Learning, Support Vector Machines, Regularization, Kernel Methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12565208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd",
            "isKey": true,
            "numCitedBy": 382,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks."
            },
            "slug": "A-Bayesian/Information-Theoretic-Model-of-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that for many common machine learning problems, although in general the authors do not know the true (objective) prior for the problem, they do have some idea of a set of possible priors to which the true prior belongs."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12725766,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8b836f2797a8008da6477426490d4afef0e9335b",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this letter, we provide a study of learning in a Hilbert space of vector-valued functions. We motivate the need for extending learning theory of scalar-valued functions by practical considerations and establish some basic results for learning vector-valued functions that should prove useful in applications. Specifically, we allow an output space Y to be a Hilbert space, and we consider a reproducing kernel Hilbert space of functions whose values lie in Y. In this setting, we derive the form of the minimal norm interpolant to a finite set of data and apply it to study some regularization functionals that are important in learning theory. We consider specific examples of such functionals corresponding to multiple-output regularization networks and support vector machines, for both regression and classification. Finally, we provide classes of operator-valued kernels of the dot product and translation-invariant type."
            },
            "slug": "On-Learning-Vector-Valued-Functions-Micchelli-Pontil",
            "title": {
                "fragments": [],
                "text": "On Learning Vector-Valued Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This letter provides a study of learning in a Hilbert space of vector-valued functions and derives the form of the minimal norm interpolant to a finite set of data and applies it to study some regularization functionals that are important in learning theory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "As observed in [13] when the number of test data is large (for all experiments below we have more than 2800 test data in total for all the tasks) then model selection among only a small number of models leads to a choice that has actual test performance similar to the one observed on the test data used."
                    },
                    "intents": []
                }
            ],
            "corpusId": 70866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d13bc44e15fd93480e16305d37c025bc0818c2",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case."
            },
            "slug": "Regularization-Networks-and-Support-Vector-Machines-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Both formulations of regularization and Support Vector Machines are reviewed in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143614516"
                        ],
                        "name": "J. Gehrke",
                        "slug": "J.-Gehrke",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Gehrke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gehrke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "There have also been various attempts to theoretically study multi\u2013task learning, see [4, 5, 6, 7, 8, 15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "For example: a) the case of having the same output (\u201cy\u2019s\u201d) and different inputs (\u201cx\u2019s\u201d), which corresponds to the problem of integrating information from heterogeneous databases [7]; or, b) the case of multi\u2013modal learning or learning by components, where the (x, y) data for each of the tasks do not belong to the same space X \u00d7 Y but data for task t come from a space Xt\u00d7Yt \u2013 this is for example the machine vision case of learning to recognize a face by first learning to recognize parts of the face, such as eyes, mouth, and nose [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7881875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdd70e5a7fc771d49aa46389145a7805bb312183",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task. For example, aiming towards the design of an automated diagnostic tool for some disease, one may wish to integrate data gathered in many different hospitals. A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data. In practice, the problem is usually solved by a manual construction of semantic mappings and translations between the different sources. Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations.In this work we propose a theoretical framework for making classification predictions from a collection of different data sources, without creating explicit translations between them. Our framework allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of different learning algorithms. Our main objective, at this stage, is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework."
            },
            "slug": "A-theoretical-framework-for-learning-from-a-pool-of-Ben-David-Gehrke",
            "title": {
                "fragments": [],
                "text": "A theoretical framework for learning from a pool of disparate data sources"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A theoretical framework for making classification predictions from a collection of different data sources, without creating explicit translations between them is proposed, which allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of different learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684626"
                        ],
                        "name": "B. Heisele",
                        "slug": "B.-Heisele",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Heisele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Heisele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 538,
                                "start": 534
                            }
                        ],
                        "text": "For example: a) the case of having the same output (\u201cy\u2019s\u201d) and different inputs (\u201cx\u2019s\u201d), which corresponds to the problem of integrating information from heterogeneous databases [7]; or, b) the case of multi\u2013modal learning or learning by components, where the (x, y) data for each of the tasks do not belong to the same space X \u00d7 Y but data for task t come from a space Xt\u00d7Yt \u2013 this is for example the machine vision case of learning to recognize a face by first learning to recognize parts of the face, such as eyes, mouth, and nose [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 722328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fdc86b3b648449b89940f57f55ff23328c18b89",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algorithm for automatically learning discriminative components of objects with SVM classifiers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classifiers are then combined in a second stage to yield a hierarchical SVM classifier. Experimental results in face classification show considerable robustness against rotations in depth and suggest performance at significantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classification experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classifier which may be relevant for biological models of visual recognition."
            },
            "slug": "Categorization-by-Learning-and-Combining-Object-Heisele-Serre",
            "title": {
                "fragments": [],
                "text": "Categorization by Learning and Combining Object Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An algorithm for automatically learning discriminative components of objects with SVM classifiers based on growing image parts by minimizing theoretical bounds on the error probability of an SVM, which suggests performance at significantly better level than other face detection systems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95492690"
                        ],
                        "name": "C. Boussios",
                        "slug": "C.-Boussios",
                        "structuredName": {
                            "firstName": "Constantinos",
                            "lastName": "Boussios",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Boussios"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213789"
                        ],
                        "name": "G. Zacharia",
                        "slug": "G.-Zacharia",
                        "structuredName": {
                            "firstName": "Giorgos",
                            "lastName": "Zacharia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zacharia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 227
                            }
                        ],
                        "text": "We used two measures of performance: a) the Root Mean Square Error (RMSE) of the estimated utility functions relative to the true (supposedly unknown) utility functions \u2013 an error measure typically used for preference modeling [3, 12, 24]; b) the average hit errors (misclassification) of the estimated functions on a test set of 16 questions per individual \u2013 hence a total of 96 test data per individual for the corresponding classification problem solved leading to 2880 and 9600 test data for the 30 and 100 tasks cases, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 299
                            }
                        ],
                        "text": "Each question was subsequently transformed into 6 data points (twice the number of comparisons of the \u201cwinner\u201d product among the four and the remaining 3 \u201closer\u201d products) of 16 dimensions each that were used for the classification learning problem corresponding to this preference modeling problem [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "We note that the conclusions are qualitatively the same for both error measures, as also observed by [12, 24]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "It turns out [12] that this problem is equivalent to solving a classification problem, therefore the results we report below can be seen as results for a classification problem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 61
                            }
                        ],
                        "text": "In particular we simply replicated the experimental setup of [3, 12, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 16
                            }
                        ],
                        "text": "As discussed in [3, 12, 24] these parameters are chosen so that the range of average utility functions, noise, and similarities among the preferences of the individual consumers found in practice is covered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "As in [3, 12, 24] we used \u03b2 = 3 for low noise in the data and \u03b2 = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10044627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d75a252ea2b7674568c85c51ebb000ccff2ae1b",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce methods from statistical learning theory to the field of conjoint analysis for preference modeling. We present a method for estimating preference models that can be highly nonlinear and robust to noise. Like recently developed polyhedral methods for conjoint analysis, our method is based on computationally efficient optimization techniques. We compare our method with standard logistic regression, hierarchical Bayes, and the polyhedral methods using standard, widely used simulation data. The experiments show that the proposed method handles noise significantly better than both logistic regression and the recent polyhedral methods and is never worse than the best method among the three mentioned above. It can also be used for estimating nonlinearities in preference models faster and better than all other methods. Finally, a simple extension for handling heterogeneity shows promising results relative to hierarchical Bayes. The proposed method can therefore be useful, for example, for analyzing large amounts of data that are noisy or for estimating interactions among product features."
            },
            "slug": "Generalized-robust-conjoint-estimation-Evgeniou-Boussios",
            "title": {
                "fragments": [],
                "text": "Generalized robust conjoint estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method for estimating preference models that can be highly nonlinear and robust to noise and is based on computationally efficient optimization techniques, which can be useful for analyzing large amounts of data that are noisy or for estimating interactions among product features."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Breiman and Friedman [9] propose the curds&whey method, where the relations be-\ntween the various tasks are modeled in a post\u2013processing fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "[9] L. Breiman and J.H Friedman."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Breiman and Friedman [9] propose the curds&whey method, where the relations be-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121621272,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c5f1b46a306a486bcf91be71f2a726b11f462514",
            "isKey": true,
            "numCitedBy": 474,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We look at the problem of predicting several response variables from the same set of explanatory variables. The question is how to take advantage of correlations between the response variables to improve predictive accuracy compared with the usual procedure of doing individual regressions of each response variable on the common set of predictor variables. A new procedure is introduced called the curds and whey method. Its use can substantially reduce prediction errors when there are correlations between responses while maintaining accuracy even if the responses are uncorrelated. In extensive simulations, the new procedure is compared with several previously proposed methods for predicting multiple responses (including partial least squares) and exhibits superior accuracy. One version can be easily implemented in the context of standard statistical packages."
            },
            "slug": "Predicting-Multivariate-Responses-in-Multiple-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Predicting Multivariate Responses in Multiple Linear Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Since the goal is to predict the exam scores of the students we run regression using the SVM \"\u2013loss function [25] for the multi\u2013task learning method proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "It is a matter of choosing the appropriate parameter \u03bc, which may be possible to do for example using some form of cross\u2013validation or a validation set [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "The generalization to nonlinear models will then be done through the use of Reproducing Kernel Hilbert Spaces (RKHS), see for example [20, 25, 26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "In this paper we develop methods for multi\u2013task learning that are natural extensions of existing kernel based learning methods for single task learning, such as Support Vector Machines (SVMs) [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "An important characteristic of SVM is that they can be used to estimate highly non-linear functions through the use of kernels [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "For example, in the \u201cschool dataset\u201d each function is a regression function and we used for V the \"\u2013loss function of SVM regression [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6175764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79542629a0d6a2d69325ae2cd38a09a773cf35af",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. This paper describes a computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Each data set is represented via a kernel function, which defines generalized similarity relationships between pairs of entities, such as genes or proteins. The kernel representation is both flexible and efficient, and can be applied to many different types of data. Furthermore, kernel functions derived from different types of data can be combined in a straightforward fashion\u2014recent advances in the theory of kernel methods have provided efficient algorithms to perform such combinations in an optimal way. These methods formulate the problem of optimal kernel combination as a convex optimization problem that can be solved with semi-definite programming techniques. In this paper, we demonstrate the utility of these techniques by investigating the problem of predicting membrane proteins from heterogeneous data, including amino acid sequences, hydropathy profiles, gene expression data and known protein-protein interactions. A statistical learning algorithm trained from all of these data performs significantly better than the same algorithm trained on any single type of data and better than existing algorithms for membrane protein classification."
            },
            "slug": "A-Framework-for-Genomic-Data-Fusion-and-its-to-Lanckriet-Bie",
            "title": {
                "fragments": [],
                "text": "A Framework for Genomic Data Fusion and its Application to Membrane Protein Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A computational framework for integrating and drawing inferences from a collection of genome-wide measurements, which demonstrates the utility of predicting membrane proteins from heterogeneous data, including amino acid sequences, hydropathy profiles, gene expression data and known protein-protein interactions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065704414"
                        ],
                        "name": "Olivier Toubia",
                        "slug": "Olivier-Toubia",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Toubia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Toubia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082269983"
                        ],
                        "name": "Duncan Simester",
                        "slug": "Duncan-Simester",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Simester",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duncan Simester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168898437"
                        ],
                        "name": "John R. Hauser",
                        "slug": "John-R.-Hauser",
                        "structuredName": {
                            "firstName": "John R.",
                            "lastName": "Hauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Hauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075592770"
                        ],
                        "name": "Ely Dahan",
                        "slug": "Ely-Dahan",
                        "structuredName": {
                            "firstName": "Ely",
                            "lastName": "Dahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ely Dahan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 227
                            }
                        ],
                        "text": "We used two measures of performance: a) the Root Mean Square Error (RMSE) of the estimated utility functions relative to the true (supposedly unknown) utility functions \u2013 an error measure typically used for preference modeling [3, 12, 24]; b) the average hit errors (misclassification) of the estimated functions on a test set of 16 questions per individual \u2013 hence a total of 96 test data per individual for the corresponding classification problem solved leading to 2880 and 9600 test data for the 30 and 100 tasks cases, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "We note that the conclusions are qualitatively the same for both error measures, as also observed by [12, 24]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 61
                            }
                        ],
                        "text": "In particular we simply replicated the experimental setup of [3, 12, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 16
                            }
                        ],
                        "text": "As discussed in [3, 12, 24] these parameters are chosen so that the range of average utility functions, noise, and similarities among the preferences of the individual consumers found in practice is covered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "As in [3, 12, 24] we used \u03b2 = 3 for low noise in the data and \u03b2 = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1476155,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "a4b14d020a667acdf3f744c366faafbe7997fe06",
            "isKey": true,
            "numCitedBy": 228,
            "numCiting": 221,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and test new adaptive question design and estimation algorithms for partial profile conjoint analysis. Polyhedral question design focuses questions to reduce a feasible set of parameters as rapidly as possible. Analytic center estimation uses a centrality criterion based on consistency with respondents' answers. Both algorithms run with no noticeable delay between questions. \n \nWe evaluate the proposed methods relative to established benchmarks for question design random selection, D-efficient designs, adaptive conjoint analysis and estimation hierarchical Bayes. Monte Carlo simulations vary respondent heterogeneity and response errors. For low numbers of questions, polyhedral question design does best or is tied for best for all tested domains. For high numbers of questions, efficient fixed designs do better in some domains. Analytic center estimation shows promise for high heterogeneity and for low response errors; hierarchical Bayes for low heterogeneity and high response errors. Other simulations evaluate hybrid methods, which include self-explicated data. \n \nA field test 330 respondents compared methods on both internal validity holdout tasks and external validity actual choice of a laptop bag worth approximately $100. The field test is consistent with the simulation results and offers strong support for polyhedral question design. In addition, marketplace sales were consistent with conjoint-analysis predictions."
            },
            "slug": "Fast-Polyhedral-Adaptive-Conjoint-Estimation-Toubia-Simester",
            "title": {
                "fragments": [],
                "text": "Fast Polyhedral Adaptive Conjoint Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "New adaptive question design and estimation algorithms for partial profile conjoint analysis and estimation shows promise for high heterogeneity and for low response errors; hierarchical Bayes for low heterogeneity and high response errors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065685288"
                        ],
                        "name": "Neeraj Arora",
                        "slug": "Neeraj-Arora",
                        "structuredName": {
                            "firstName": "Neeraj",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neeraj Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47688333"
                        ],
                        "name": "Joel Huber",
                        "slug": "Joel-Huber",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Huber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Huber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 61
                            }
                        ],
                        "text": "In particular we simply replicated the experimental setup of [3, 12, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "As in [3, 12, 24] we used \u03b2 = 3 for low noise in the data and \u03b2 = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 227
                            }
                        ],
                        "text": "We used two measures of performance: a) the Root Mean Square Error (RMSE) of the estimated utility functions relative to the true (supposedly unknown) utility functions \u2013 an error measure typically used for preference modeling [3, 12, 24]; b) the average hit errors (misclassification) of the estimated functions on a test set of 16 questions per individual \u2013 hence a total of 96 test data per individual for the corresponding classification problem solved leading to 2880 and 9600 test data for the 30 and 100 tasks cases, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 16
                            }
                        ],
                        "text": "As discussed in [3, 12, 24] these parameters are chosen so that the range of average utility functions, noise, and similarities among the preferences of the individual consumers found in practice is covered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 168131081,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "9b129662c0c5394fd4a932d3dd811af30d0cfba0",
            "isKey": true,
            "numCitedBy": 142,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose aggregate customization as an approach to improve individual estimates using a hierarchical Bayes choice model. Our approach involves the use of prior estimates to build a common design customized for the average respondent. We conduct two simulation studies to investigate conditions that are most conducive to aggregate customization. The simulations are validated by a field study showing that aggregate customization results in better estimates of individual parameters and more accurate predictions of individuals' choices. The proposed approach is easy to use, and a simulation study can assess the expected benefit from aggregate customization prior to its implementation. Copyright 2001 by the University of Chicago."
            },
            "slug": "Improving-Parameter-Estimates-and-Model-Prediction-Arora-Huber",
            "title": {
                "fragments": [],
                "text": "Improving Parameter Estimates and Model Prediction by Aggregate Customization in Choice Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work proposes aggregate customization as an approach to improve individual estimates using a hierarchical Bayes choice model and conducts two simulation studies to investigate conditions that are most conducive to aggregate customization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065685288"
                        ],
                        "name": "Neeraj Arora",
                        "slug": "Neeraj-Arora",
                        "structuredName": {
                            "firstName": "Neeraj",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neeraj Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976827"
                        ],
                        "name": "Greg M. Allenby",
                        "slug": "Greg-M.-Allenby",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Allenby",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg M. Allenby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113295360"
                        ],
                        "name": "James L. Ginter",
                        "slug": "James-L.-Ginter",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ginter",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. Ginter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We follow the intuition of Hierarchical Bayes [1, 2, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [1, 2] a hierarchical Bayes model is estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "face, from a pool of similar objects; in finance forecasting models for predicting the value of many possibly related indicators simultaneously is often required; in marketing modeling the preferences of many individuals simultaneously is common practice [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is the standard problem of conjoint analysis [1, 2] for preference modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We compared our method with Hierarchical Bayes (HB) [1, 2] which, as mentioned, is considered to be a state of the art method for preference modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the response accuracy of the individual consumers) which is modeled according to the assumptions of Hierarchial Bayes (HB) [1, 2], considered a state of the art approach for preference modeling for a population of individual consumers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is for example the standard setup in marketing applications of preference modeling [1, 2] where the same choice panel questions (the same \u201cx\u2019s\u201d) are given to many individual consumers, each individual provides his/her own preferences (the \u201cy\u2019s\u201d), and we assume that there is some commonality among the preferences of the individuals (the \u201cft\u2019s\u201d)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In other words we assume that the tasks are related in a way that the true models are all close to some model w0 (playing the role of the mean of the Gaussian used for Hierarchical Bayes [1, 2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, hierarchical Bayesian methods [1, 2, 14, 4] assume that all functions wt come from a particular probability distribution such as a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 154307865,
            "fieldsOfStudy": [
                "Business",
                "Economics"
            ],
            "id": "e9fee6573c06c664c3b53e4776d42c5ce3be2e12",
            "isKey": true,
            "numCitedBy": 205,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Product design, pricing policies, and promotional activities influence the primary and secondary demand for goods and services. Brand managers need to develop an understanding of the relationships between marketing mix decisions and consumer decisions of whether to purchase in the product category, which brand to buy, and how much to consume. Knowledge about factors most effective in influencing primary and secondary demand of a product allows firms to grow by enhancing their market share as well their market size. The purpose of this paper is to develop an individual level model that allows an investigation of both the primary and secondary aspects of consumer demand. Unlike models of only primary demand or only secondary demand, this more comprehensive model offers the opportunity to identify changes in product features that will result in the greatest increase in demand. It also offers the opportunity to differentially target consumer segments depending upon whether consumers are most likely to enter the market, increase their consumption level, or switch brands. In the proposed hierarchical Bayes model, an integrative framework that jointly models the discrete choice and continuous quantity components of consumer decision is employed instead of treating the two as independent. The model includes parameters that capture individual specific reservation value, attribute preference, and expenditure sensitivity. The model development is based upon the microeconomic theory of utility maximization. Heterogeneity in model parameters across the sample is captured by using a random effects specification guided by the underlying microeconomic model. This requires that some of the effects are strictly positive. This is accommodated through the use of a gamma distribution of heterogeneity for some of the parameters. A normal distribution of heterogeneity is used for the remaining parameters. Gibbs sampling is used to estimate the model. The key methodological contribution of this paper is that we show how to specify a hierarchical Bayes continuous random effects model that integrates consumer choice and quantity decisions such that individual-level parameters can be estimated. Individual level estimates are desirable because insights into primary demand involve nonlinear functions of model parameters. For example, consumers not in the market are those whose utilities for the choice alternatives fall below some reservation value. The proposed methodology yields individual specific estimates of reservation values and expenditure sensitivity, which allow assessment of the origins of demand other than the switch ing behavior of consumers. The methodology can also be used to help identify changes in product features most likely to bring new customers into a market. Our work differs from previous research in this area as we lay the framework needed to obtain individual-level parameter estimates in a continuous random effects model that integrates choice and quantity. The methodology is demonstrated with survey data collected about consumer preferences and consumption for a food item. For the data available, a large response heterogeneity was observed across all model parameters. In spite of limited data available at the individual level, a majority of the individual level estimates were found to be significant. Predictive tests demonstrated the superiority of the proposed model over existing latent class and aggregate models. Particularly, significant gains in predictive accuracy were observed for the \"no-buy\" behavior of the respondents. These gains demonstrate that by structurally linking the choice and quantity models results in a more accurate characterization of the market than existing finite mixture approaches that model choice and quantity independently. We show that our joint model makes more efficient use of the available data and results in better parameter estimates than those that assume independence. Finally, the individual level demand analysis is illustrated through a simple example involving a $1.00 price cut. We demonstrate practical usefulness of the model for targeting by developing the demographic, attitudinal, and behavioral profiles of consumer groups most likely to increase consumption, enter the market, or switch brands because of a price cut decision."
            },
            "slug": "A-Hierarchical-Bayes-Model-of-Primary-and-Secondary-Arora-Allenby",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Bayes Model of Primary and Secondary Demand"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135405658"
                        ],
                        "name": "Jaehwan Kim",
                        "slug": "Jaehwan-Kim",
                        "structuredName": {
                            "firstName": "Jaehwan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaehwan Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976827"
                        ],
                        "name": "Greg M. Allenby",
                        "slug": "Greg-M.-Allenby",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Allenby",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg M. Allenby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16252192"
                        ],
                        "name": "Peter E. Rossi",
                        "slug": "Peter-E.-Rossi",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rossi",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter E. Rossi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 154403667,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a929c09ed1dc8fc5ed76fe9e8f7aa6a965c5b669",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Consumers are often observed to purchase more than one variety of a product on a given shopping trip. The simultaneous demand for varieties is observed not only for packaged goods such as yogurt or soft drinks, but in many other product categories such as movies, music compact disks, and apparel. Multinomial MN choice models cannot be applied to data exhibiting the simultaneous choice of more than one variety. The random utility interpretation of either the MN logit or probit model uses a linear utility specification that cannot accommodate interior solutions with more than one variety alternative chosen. \n \nTo analyze data with multiple varieties chosen requires a nonstandard utility specification. Standard demand models in the economics literature exhibit only interior solutions. We propose a demand model based on a translated additive utility structure. The model nests the linear utility structure, while allowing for the possibility of a mixture of corner and interior solutions where more than one but not all varieties are selected. We use a random utility specification in which the unobservable portion of marginal utility follows a log-normal distribution. The distribution of quantity demanded the basis of the likelihood function is derived from these log-normal random utility errors. The likelihood function for this class of models with mixtures of corner and interior solutions is a mixed distribution with both a continuous density portion and probability mass points for the corners. The probability mass points must be calculated by integrals of the log-normal errors over rectangular regions. We evaluate these high-dimensional integrals using the GHK approximation. We employ a Bayesian hierarchical model, allowing household-specific utility parameters. \n \nOur utility specification related to the approach of Wales and Woodland 1983 who employ a translated quadratic utility function. Wales and Woodland were only able to study, at the most, three varieties because there was no practical way to evaluate the utility function at that time. In addition, the quadratic utility specification is not a globally valid utility function, making welfare computations and policy experiments questionable. Hendel 1999 and Dube 1999 present an alternative approach in the utility function which is constructed by summing up over unobservable consumption occasions. While only one variety is consumed on each occasion, the marginal utilities of varieties change over the consumption occasions, giving rise to a simultaneous purchase of multiple varieties. \n \nOur Bayesian inference approach allows us to obtain individual household estimates of utility parameters. Household utility estimates are used to compute the value of each variety. We compute a compensating value for the removal of each flavor; that is, we compute the monetary equivalent of the household's loss in utility from removal of a flavor. These calculations show that households highly value popular flavors and would incur substantial utility losses from removal of these flavors from the yogurt assortment. \n \nNext we consider the implications of our model for retailer assortment and pricing policies. Given limited shelf space, only a subset of the possible varieties can be displayed for purchase at any one time. If consumers value variety, then a retailer with lower variety must compensate the consumers in some way, such as a lower price level. We see this trade-off between price and variety across different retailing formats. Discount or warehouse format retailers often have both lower variety and lower prices. To measure this trade-off, we explore the utility loss from reduction in variety and find the reductions in price that will compensate for this utility loss. These price reduction calculations must be based on a valid utility structure. \n \nHeterogeneity in tastes is critical in these utility computations and policy experiments. We find that a relatively small fraction of households with extreme preferences dominate the compensating value computations. That is, some households are observed to purchase mostly or exclusively one variety. These households must be heavily compensated for the removal of this variety from the assortment. In some retailing contexts, customization of the assortment is possible at the customer level. We show that such customization virtually eliminates any utility loss from reduction in variety."
            },
            "slug": "Modeling-Consumer-Demand-for-Variety-Kim-Allenby",
            "title": {
                "fragments": [],
                "text": "Modeling Consumer Demand for Variety"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976827"
                        ],
                        "name": "Greg M. Allenby",
                        "slug": "Greg-M.-Allenby",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Allenby",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg M. Allenby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16252192"
                        ],
                        "name": "Peter E. Rossi",
                        "slug": "Peter-E.-Rossi",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rossi",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter E. Rossi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 2] a hierarchical Bayes model is estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 244
                            }
                        ],
                        "text": "a pool of similar objects; in finance forecasting models for predicting the value of many possibly related indicators simultaneously is often required; in marketing modeling the preferences of many individuals simultaneously is common practice [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 52
                            }
                        ],
                        "text": "We compared our method with Hierarchical Bayes (HB) [1, 2] which is considered to be a state of the art method for preference modeling of a population of individual consumers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 50
                            }
                        ],
                        "text": "This is the standard problem of conjoint analysis [1, 2] for preference modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This is for example the standard setup in marketing applications of preference modeling [1, 2] where the same choice panel questions (the same \u201cx\u2019s\u201d) are given to many individual consumers, each individual provides his/her own preferences (the \u201cy\u2019s\u201d), and we assume that there is some commonality among the preferences of the individuals (the \u201cft\u2019s\u201d)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "A Hierarchical Bayes Model of Primary and Secondary Demand."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 187
                            }
                        ],
                        "text": "In other words we assume that the tasks are related in a way that the true models are all close to some model w0 (playing the role of the mean of the Gaussian used for Hierarchical Bayes [1, 2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 128
                            }
                        ],
                        "text": "the response accuracy of the individual consumers) which is modeled according to the assumptions of Hierarchial Bayes (HB), see [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 43
                            }
                        ],
                        "text": "For example, hierarchical Bayesian methods [1, 2, 4, 15] assume that all functions wt come from a particular probability distribution such as a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "We follow the intuition of Hierarchical Bayes [1, 2, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122172955,
            "fieldsOfStudy": [
                "Business",
                "Economics"
            ],
            "id": "9526001d3b34ec027f743d043936db3b84abfaa8",
            "isKey": true,
            "numCitedBy": 750,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Marketing-models-of-consumer-heterogeneity-Allenby-Rossi",
            "title": {
                "fragments": [],
                "text": "Marketing models of consumer heterogeneity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "satisfies the properties in Proposition 1 of [20], where x = (x1, ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "The generalization to nonlinear models will then be done through the use of Reproducing Kernel Hilbert Spaces (RKHS), see for example [20, 25, 26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "We note that the above ideas appear in greater generality in [20] where the notion of operator\u2013 valued kernels is derived."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "The methods we develop below may be extended to handle such scenarios, for example through the appropriate choice of a matrix\u2013valued kernel [20] discussed in section 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20], the solution to this problem, F \u2217 = \u3008w\u2217,\u03a6\u3009 has the form in equation (16)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Learning Vector\u2013Valued Functions. Research Note RN/03/08"
            },
            "venue": {
                "fragments": [],
                "text": "Dept of Computer Science,"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056181"
                        ],
                        "name": "C. Braak",
                        "slug": "C.-Braak",
                        "structuredName": {
                            "firstName": "Cajo",
                            "lastName": "Braak",
                            "middleNames": [
                                "ter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Braak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125627657,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fbbe850ad2a29218572314aa576286a014d459fe",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Discussion-to-'Predicting-multivariate-responses-in-Braak",
            "title": {
                "fragments": [],
                "text": "Discussion to 'Predicting multivariate responses in multiple linear regression' by L. Breiman & J.H. Friedman"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144947807"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Brown",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31722372"
                        ],
                        "name": "J. Zidek",
                        "slug": "J.-Zidek",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Zidek",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zidek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 25
                            }
                        ],
                        "text": "[10] P.J. Brown and J.V. Zidek."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Brown and Zidek [10] consider the case of regression and propose an extension of the standard ridge regression to multivariate ridge regression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122671539,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2cfbd05d82ed222495f7a8f4c335732ee0c8d780",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Multivariate-Ridge-Regression-Brown-Zidek",
            "title": {
                "fragments": [],
                "text": "Adaptive Multivariate Ridge Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] to optimize the Lagrangian function (5)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": false,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Polyhedral Adaptive Conjoint Estimation. Working paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "- Assume ft = g (0) + g((1)) t + g (2) t + ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kst(x, z) :=  1 \u03bc  K1(x, z) + \u03b4stK2(x, z), s, t = 1"
            },
            "venue": {
                "fragments": [],
                "text": ". . , T. 17 Other directions - Kernels can be defined so that tasks are clustered (Bakker and Heskes 2003): use w01, w02, . . . w0K for K clusters. - Consider many tasks that share similar features: learn common features among tasks by defining the kernel matrix"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 296
                            }
                        ],
                        "text": "\u2026question was subsequently transformed into 6 data points (twice the number of comparisons of the \u201cwinner\u201d product among the four and the remaining 3 \u201closer\u201d products) of 16 dimensions each that were used for the classification learning problem corresponding to this preference modeling problem [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "In particular we simply replicated the experimental setup of [3, 12, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "The actual utility function wt was therefore a vector\nwt = (wt1, ..., wt4, wt5, ..., wt8, wt9, ..., wt12, wt12, ..., wt16)\nwhere (wt1, ..., wt4), (wt5, ..., wt8), (wt9, ..., wt12), and (wt12, ..., wt16) are four 4\u2013dimensional vectors sampled from the aforementioned Gaussian."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Robust Conjoint Estimation. INSEAD Working Paper"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized Robust Conjoint Estimation. INSEAD Working Paper"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive Multivariate Ridge Regression Multi \u2013 Task Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 12,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Regularized-multi--task-learning-Evgeniou-Pontil/e219a61354d972a28954e655a7c53373508a08b6?sort=total-citations"
}