{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 173
                            }
                        ],
                        "text": "\u2026straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams 1989a, Williams 1989b, Kuhn, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 192
                            }
                        ],
                        "text": "This type of method includes the straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams 1989a, Williams 1989b, Kuhn, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10477025"
                        ],
                        "name": "K. Birmiwal",
                        "slug": "K.-Birmiwal",
                        "structuredName": {
                            "firstName": "Kailash",
                            "lastName": "Birmiwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Birmiwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49328772"
                        ],
                        "name": "P. Sarwal",
                        "slug": "P.-Sarwal",
                        "structuredName": {
                            "firstName": "Parul",
                            "lastName": "Sarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114124179"
                        ],
                        "name": "S. Sinha",
                        "slug": "S.-Sinha",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sinha",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sinha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28889305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67191ded56991a25d39db57b7f6a6149e42a2693",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. A new supervised learning algorithm which does not require any gradient computation is presented. In the new gradient-free (G-F) algorithm, the error between the actual output and the desired output is not measured by the least-squared norm as in the backpropagation algorithm, but by the up-norm. In the G-F algorithm, the weights are updated in each iteration only after incorporating all the input patterns. The authors use the example of the XOR problem to evaluate the performance of the algorithm. A Monte-Carlo simulation is performed and the results obtained are encouraging.<<ETX>>"
            },
            "slug": "A-new-gradient-free-learning-algorithm-Birmiwal-Sarwal",
            "title": {
                "fragments": [],
                "text": "A new gradient-free learning algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new supervised learning algorithm which does not require any gradient computation is presented, and the error between the actual output and the desired output is not measured by the least-squared norm as in the backpropagation algorithm, but by the up-norm."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965640"
                        ],
                        "name": "C. I. Thorbergsson",
                        "slug": "C.-I.-Thorbergsson",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Thorbergsson",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. I. Thorbergsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34258216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e7336daf17a81cf846d79b65c166658600ae527",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a cost function for learning in feed-forward neural networks which is an explicit function of the internal representation in addition to the weights. The learning problem can then be formulated as two simple perceptrons and a search for internal representations. Back-propagation is recovered as a limit. The frequency of successful solutions is better for this algorithm than for back-propagation when weights and hidden units are updated on the same timescale i.e. once every learning step."
            },
            "slug": "A-Cost-Function-for-Internal-Representations-Krogh-Thorbergsson",
            "title": {
                "fragments": [],
                "text": "A Cost Function for Internal Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A cost function for learning in feed-forward neural networks is introduced which is an explicit function of the internal representation in addition to the weights which can then be formulated as two simple perceptrons and a search for internal representations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3034179"
                        ],
                        "name": "T. Grossman",
                        "slug": "T.-Grossman",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Grossman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Grossman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426224"
                        ],
                        "name": "E. Domany",
                        "slug": "E.-Domany",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Domany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Domany"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16511611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6deb1ddc764259fbdc7733ef80473081bff31d5",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements. Whereas existing algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local and biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity."
            },
            "slug": "Learning-by-Choice-of-Internal-Representations-Grossman-Meir",
            "title": {
                "fragments": [],
                "text": "Learning by Choice of Internal Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A learning algorithm for multilayer neural networks composed of binary linear threshold elements that treats the internal representations as the fundamental entities to be determined by the local and biologically plausible Perceptron Learning Rule."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60666828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424710825d726e10b016204ed2bc979e2a342d10",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi..."
            },
            "slug": "Experimental-Analysis-of-the-Real-time-Recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Experimental Analysis of the Real-time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A series of simulation experiments are used to investigate the power and properties of the real-time recurrent learning algorithm, a gradient-following learning algorithm for completely recurrent networks running in continually sampled time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "Although applicable only to fixed-point training, the algorithms of Almeida (Almeida, 1989) and Pineda (Pineda, 1988) have much in common with these dynamical training algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18612099"
                        ],
                        "name": "B. Ladendorf",
                        "slug": "B.-Ladendorf",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Ladendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ladendorf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12608180,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7649115331fb03ec168cfea8b308de1aa2705429",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connected-recognition-with-a-recurrent-network-Kuhn-Watrous",
            "title": {
                "fragments": [],
                "text": "Connected recognition with a recurrent network"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086268"
                        ],
                        "name": "F. Failside",
                        "slug": "F.-Failside",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Failside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Failside"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10802530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b247fe6efc9e1011555428647f390dd98bdd3446",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns, and three possible architectures are explored. As an example, dynamic nets are applied to the problem of speech coding, in which a time sequence of speech data are coded by one net and decoded by another. The use of dynamic nets gives a better signal to noise ratio than that achieved using static nets."
            },
            "slug": "Static-and-Dynamic-Error-Propagation-Networks-with-Robinson-Failside",
            "title": {
                "fragments": [],
                "text": "Static and Dynamic Error Propagation Networks with Application to Speech Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a generalisation of error propagation nets to deal with time varying, or dynamic patterns, and three possible architectures are explored."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3034179"
                        ],
                        "name": "T. Grossman",
                        "slug": "T.-Grossman",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Grossman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Grossman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "The moving targets method for feedforward nets is analogous to the method of Grossman, Meir, and Domany (Grossman, 1990a, 1990b) for networks with discrete node values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17208114,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bbab4128271300fdb000dd3e22a704eb6327955",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new learning algorithm, lear ning by choice of int ern al repr esentations (CHIR), was recently int roduc ed. Th e basic version of thi s algorit hs was developed for a two-layer, single-out put, feed\u00ad forward network of binary neurons . This paper presents a gener alized version of t he CHIR algorithm th at is capable of training mult iple\u00ad output net works. A way to ada pt the algorit hm to mult ilayered feed\u00ad forward networks is also presented. 'v Ve test t he new version on two typical learnin g tas ks: t he combined parity- symm et ry problem an d t he rand om problem (random associat ions). The dependence of the algori th m performance on the network size and on t he learning pa\u00ad ramet ers is st udied."
            },
            "slug": "The-CHIR-Algorithm:-A-Generalization-for-and-Grossman",
            "title": {
                "fragments": [],
                "text": "The CHIR Algorithm: A Generalization for Multiple-Output and Multilayered Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A gener alized version of the CHIR algorithm capable of training mult iple\u00ad output net works and the dependence of the algori th m performance on the network size and on t he learning pa\u00ad ramet ers is st udied."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 124
                            }
                        ],
                        "text": "This type of method includes the straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Originally the conjugate gradient algorithm (Press, 1988) was used, with a linesearch algorithm from Fletcher (Fletcher, 1980)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608024"
                        ],
                        "name": "W. Vetterling",
                        "slug": "W.-Vetterling",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vetterling",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vetterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21949124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5686b4bcc45223a770216a161384b2542be1018",
            "isKey": false,
            "numCitedBy": 4823,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: informatique ; numerical recipes Note: contient un CDRom Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Numerical-Recipes-in-FORTRAN-The-Art-of-Scientific-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in FORTRAN - The Art of Scientific Computing, 2nd Edition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents a list of recommended recipes for making CDRom decks and some examples of how these recipes can be modified to suit theommelier's needs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 88
                            }
                        ],
                        "text": "The derivation of the derivatives with respect to the moving targets is spelled out in (Rohwer, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 94
                            }
                        ],
                        "text": "The formal relationship between these and the method of Rohwer and Forrest is spelled out in (Rohwer 1989a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 101
                            }
                        ],
                        "text": "A careful comparison of moving targets with back-propagation in time and teacher forcing appears in (Rohwer, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 111
                            }
                        ],
                        "text": "A set of numerical experiments performed with the activation deficit form of the algorithm (4) is reported in (Rohwer, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "A difficulty with convergence to chaotic attractors reported in (Rohwer, 1989b) appears to have mysteriously disappeared with the adoption of this error measure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 27
                            }
                        ],
                        "text": "Further details appear in (Rohwer, 1989b)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 'Moving Targets' Training Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. DANIP,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143885710"
                        ],
                        "name": "B. Lev",
                        "slug": "B.-Lev",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lev"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "Werbos (Werbos, 1983) used the term \"moving targets\" to describe the qualitative idea that a network should set itself intermediate objectives, and vary these objectives as information is accumulated on their attainability and their usefulness for achieving overall objectives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 154147618,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "9790e17f3ea8153f51256054d0459044e6f61149",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Energy-models-and-studies-Lev",
            "title": {
                "fragments": [],
                "text": "Energy models and studies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124525506,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "493cd5e15d6b8726ce27ed0595f1993c27525670",
            "isKey": false,
            "numCitedBy": 17356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-recipes-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Although applicable only to fixed-point training, the algorithms of Almeida (Almeida, 1989) and Pineda (Pineda, 1988) have much in common with these dynamical training algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61438451,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9ffe5e8a763c191d5e4a6c8407fdf1422027d72b",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Back-propagation-in-non-feedforward-networks-Almeida",
            "title": {
                "fragments": [],
                "text": "Back propagation in non-feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123962567"
                        ],
                        "name": "M. C. Seiler",
                        "slug": "M.-C.-Seiler",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Seiler",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Seiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50678756"
                        ],
                        "name": "F. A. Seiler",
                        "slug": "F.-A.-Seiler",
                        "structuredName": {
                            "firstName": "Fritz",
                            "lastName": "Seiler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. A. Seiler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62717952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e980fbf251ecb28ba85eb092fc66ce284bb63be",
            "isKey": false,
            "numCitedBy": 13115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Recipes-in-C:-The-Art-of-Scientific-Seiler-Seiler",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in C: The Art of Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 56
                            }
                        ],
                        "text": "The formal relationship between these and the method of Rohwer and Forrest is spelled out in (Rohwer 1989a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "This type of method includes the straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams 1989a, Williams 1989b, Kuhn, 1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026the straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams 1989a, Williams 1989b, Kuhn,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training Time Dependence in Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE ICNN"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 237
                            }
                        ],
                        "text": "\u2026straightforward extension of the back-propagation method to back-propagation through time (Rumelhart, 1986), the methods of Rohwer and Forrest (Rohwer, 1987), Pearlmutter (Pearlmutter, 1989), and the forward propagation of derivatives (Robinson, 1988, Williams 1989a, Williams 1989b, Kuhn, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Static and Dynamic Error Propagation Networks with Applications to Speech Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training Recurrent Networks\", in Neural Networks from Models to Applications, L"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Minimal Disturbance Back Propagation Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "The Minimal Disturbance Back Propagation Algorithm"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/The-\"Moving-Targets\"-Training-Algorithm-Rohwer/af7802a50a8c294ebfd539ad72158475e5ecd9f2?sort=total-citations"
}